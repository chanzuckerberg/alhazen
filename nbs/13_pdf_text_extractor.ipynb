{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Text Extractor Utility\n",
    "\n",
    "> Extracts unstructured text from scientific papers published as PDF files ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.pdf_research_article_text_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gburns/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import dataclasses\n",
    "from langchain.document_loaders.pdf import BasePDFLoader\n",
    "from langchain.document_loaders.base import BaseBlobParser\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.blob_loaders import Blob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk.data\n",
    "import re\n",
    "from typing import Optional, List, Iterator, Mapping, Any, Dict\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class PyMuPDFBlock:\n",
    "    \"\"\"A block of text occuring in a PDF file.\"\"\"\n",
    "\n",
    "    page: int\n",
    "\n",
    "    x0:float\n",
    "    x1:float\n",
    "    width:float\n",
    "\n",
    "    y0:float\n",
    "    y1:float\n",
    "    height:float\n",
    "    \n",
    "    nlines:int\n",
    "    font_size_proxy:float\n",
    "\n",
    "    text:str\n",
    "    t:str\n",
    "\n",
    "    def __init__(self, p:int, x0:float, y0:float, x1:float, y1:float, text:str):\n",
    "        self.x0 = x0\n",
    "        self.y0 = y0\n",
    "        self.x1 = x1\n",
    "        self.y1 = y1\n",
    "        self.page = p\n",
    "        \n",
    "        self.text = text\n",
    "        self.width = x1 - x0\n",
    "        self.height = y1 - y0\n",
    "        self.nlines = text.count('\\n')\n",
    "        if self.nlines == 0:\n",
    "            self.nlines = 1 \n",
    "        self.font_size_proxy = (self.height / self.nlines)\n",
    "        self.t = re.sub('\\-\\n','',self.text)\n",
    "        self.t = re.sub('\\n',' ',self.text)\n",
    "        self.t = re.sub('\\s+',' ',self.text)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"PyMuPDFBlock(page={self.page}, x0={self.x0}, x1={self.x1}, y0={self.y0}, y1={self.y1}, text={self.text})\"\n",
    "    \n",
    "    def __str__(self): \n",
    "        return f\"PyMuPDFBlock(page={self.page}, x0={self.x0}, x1={self.x1}, y0={self.y0}, y1={self.y1}, text={self.text})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if self.page == other.page and self.x0 == other.x0 and self.x1 == other.x1 and self.y0 == other.y0 and self.y1 == other.y1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "class PyMuPDFBlockLoader(BasePDFLoader):\n",
    "    \"\"\"Load `PDF` files using `PyMuPDF`.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize with a file path.\"\"\"\n",
    "        try:\n",
    "            import fitz  # noqa:F401\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"`PyMuPDF` package not found, please install it with \"\n",
    "                \"`pip install pymupdf`\"\n",
    "            )\n",
    "\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def load(self, **kwargs: Optional[Any]) -> List[Document]:\n",
    "        \"\"\"Load file.\"\"\"\n",
    "\n",
    "        parser = PyMuPDFBlockParser(text_kwargs=kwargs)\n",
    "        blob = Blob.from_path(self.file_path)\n",
    "        blocks = parser.parse(blob)\n",
    "    \n",
    "        return blocks\n",
    "    \n",
    "class PyMuPDFBlockParser(BaseBlobParser):\n",
    "    \"\"\"Parse `PDF` using `PyMuPDF`.\"\"\"\n",
    "\n",
    "    def __init__(self, text_kwargs: Optional[Mapping[str, Any]] = None) -> None:\n",
    "        \"\"\"Initialize the parser.\n",
    "\n",
    "        Args:\n",
    "            text_kwargs: Keyword arguments to pass to ``fitz.Page.get_text()``.\n",
    "        \"\"\"\n",
    "        self.text_kwargs = text_kwargs or {}\n",
    "\n",
    "    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n",
    "        \"\"\"Lazily parse the blob.\"\"\"\n",
    "        import fitz\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    " \n",
    "        with blob.as_bytes_io() as file_path:\n",
    "\n",
    "            doc = fitz.open(file_path)  # open document\n",
    "\n",
    "            text_dict = {}\n",
    "            sdist = {}\n",
    "            fdist = {}        \n",
    "            all_spans = []\n",
    "            for page in doc:\n",
    "                d = page.get_textpage().extractDICT()\n",
    "                for block in d.get('blocks'):\n",
    "                    for line in block.get('lines'):\n",
    "                        for span in line.get('spans'):\n",
    "                            all_spans.append(span)\n",
    "                            text = span.get('text','')\n",
    "                            text_size = span.get('size',0)\n",
    "                            if sdist.get(text_size) is None:\n",
    "                                sdist[text_size] = len(text)\n",
    "                            else:\n",
    "                                sdist[text_size] += len(text)                    \n",
    "                            text_font = span.get('font',0)\n",
    "                            if fdist.get(text_font) is None:\n",
    "                                fdist[text_font] = len(text)\n",
    "                            else:\n",
    "                                fdist[text_font] += len(text)\n",
    "            \n",
    "            # VERY SIMPLE APPROACH TO START WITH\n",
    "            # Find the most common font size and use that to estimate the \n",
    "            # bounds of the page to exclude headers and footers\n",
    "            #\n",
    "            # Want to add a structured model to predict type of \n",
    "            # text blocks in the document  \n",
    "\n",
    "            most_common_font = max(sdist, key=sdist.get)\n",
    "\n",
    "            min_x0 = min([sp.get('bbox')[0]  for sp in all_spans if sp.get('size') == most_common_font])\n",
    "            min_y0 = min([sp.get('bbox')[1]  for sp in all_spans if sp.get('size') == most_common_font])\n",
    "            max_x1 = max([sp.get('bbox')[2]  for sp in all_spans if sp.get('size') == most_common_font])\n",
    "            max_y2 = max([sp.get('bbox')[3]  for sp in all_spans if sp.get('size') == most_common_font])\n",
    "                        \n",
    "            doc_text = ''\n",
    "            for page in doc:\n",
    "                for (x0, y0, x1, y1, text, bn, bt) in page.get_textpage().extractBLOCKS():\n",
    "                    if x0 < min_x0 or y0 < min_y0 or x1 > max_x1 or y1 > max_y2:\n",
    "                        continue\n",
    "                    doc_text += text\n",
    "                    doc_text += '\\n\\n'        \n",
    "            yield Document(\n",
    "                    page_content=doc_text,\n",
    "                    metadata=dict({\n",
    "                        \"file_path\": blob.source,\n",
    "                        \"total_pages\": len(doc),\n",
    "                    }))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
