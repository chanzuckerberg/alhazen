{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA on a single research article\n",
    "\n",
    "> Building a simple chatbot to allow the user to ask questions about a single paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.single_paper_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "import alhazen.utils.nxml_text_extractor as te \n",
    "import re\n",
    "import os\n",
    "\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index import Document, ServiceContext, set_global_service_context\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "import requests\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class SingleOpenAccessArticleChatBot:\n",
    "\n",
    "    def __init__(self, temp_dir='/tmp/alhazen',\n",
    "                embed_model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "                gguf_file_name='llama-2-70b-chat.Q5_K_M.gguf',\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=256,\n",
    "                context_window=3900,\n",
    "                sentence_window_size=3):\n",
    "\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        \n",
    "        if temp_dir[-1:] != '/':\n",
    "            temp_dir += '/'\n",
    "        self.temp_dir = temp_dir\n",
    "\n",
    "        if os.path.exists(temp_dir+gguf_file_name) is False:\n",
    "            os.makedirs(temp_dir, exist_ok=True)\n",
    "            # download relevant file to local disk\n",
    "                        \n",
    "        self.llm = LlamaCPP( \n",
    "            model_path=temp_dir+gguf_file_name,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            context_window=context_window,\n",
    "            generate_kwargs={},\n",
    "            model_kwargs={\"n_gpu_layers\": 1},\n",
    "            messages_to_prompt=messages_to_prompt,\n",
    "            completion_to_prompt=completion_to_prompt,\n",
    "            verbose=True)     \n",
    "\n",
    "        # create the sentence window node parser w/ default settings\n",
    "        self.node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "            window_size=sentence_window_size,\n",
    "            window_metadata_key=\"window\",\n",
    "            original_text_metadata_key=\"original_text\",\n",
    "            )\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "        # create a service context\n",
    "        self.service_context = ServiceContext.from_defaults(\n",
    "            llm=self.llm,\n",
    "            embed_model=self.embed_model,\n",
    "            node_parser=self.node_parser)\n",
    "\n",
    "    def get_crossref_data_from_doi(self, doi):\n",
    "\n",
    "        stem = 'https://api.crossref.org/v1/works/'\n",
    "        url = stem + doi\n",
    "        json_data = requests.get(url).text\n",
    "        return json_data\n",
    "\n",
    "    def get_ft_url_from_doi(self, doi):\n",
    "        \n",
    "        try:\n",
    "            engine = create_engine(\n",
    "                'databricks://token:'+os.environ['DB_TOKEN']+ \n",
    "                '@czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com'+\n",
    "                '?http_path=/sql/1.0/warehouses/1c4df94f2f1a6305')\n",
    "\n",
    "            get_ft_url_from_doi_sql = '''\n",
    "                SELECT DISTINCT p.pmc_id, p.doi, YEAR(p.publication_date) as year, p.title, p.abstract, p.full_text_format, p.full_text_url, a.last_name\n",
    "                FROM scipubstore.ingestion.papers as p \n",
    "                    JOIN scipubstore.ingestion.authors as a on (p.paper_id=a.paper_id) \n",
    "                WHERE p.doi = '{}' and a.author_index=1\n",
    "                ORDER BY p.full_text_url DESC\n",
    "            '''.format(doi)\n",
    "\n",
    "            with engine.connect() as con:\n",
    "                stmt = db.text(get_ft_url_from_doi_sql)\n",
    "                rs = con.execute(stmt)\n",
    "                df = pd.DataFrame(rs.fetchall(), columns=rs.keys())\n",
    "        except:\n",
    "            print('Unable to connect to Databricks, did you set the DB_TOKEN environment variable?')\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def run_gradio(self):\n",
    "\n",
    "        with gr.Blocks() as demo:\n",
    "            gr.Markdown('# Alhazen - Single Paper Q/A Agent v.0.1')\n",
    "\n",
    "            def load_jats_paper(doi):\n",
    "                df = self.get_ft_url_from_doi(doi)\n",
    "                print(df.columns)\n",
    "                if df.shape[0] == 0:\n",
    "                    return('No paper found with that DOI')\n",
    "                title = df['title'].values[0]\n",
    "                first_author = df['last_name'].values[0]\n",
    "                \n",
    "                year = df['year'].values[0]  \n",
    "                url = df['full_text_url'].values[0]\n",
    "                xml = requests.get(url).text\n",
    "                doc = te.NxmlDoc(doi, xml)\n",
    "                df1 = doc.build_simple_document_dataframe()\n",
    "                df_text = df1[df1['FIG_REF'] == '']\n",
    "                for i, row in df_text.iterrows():\n",
    "                    if row.TAG != 'p' and row.PLAIN_TEXT[-1:]!='.':\n",
    "                        row.PLAIN_TEXT += '.'\n",
    "                text_list = df_text['PLAIN_TEXT'].values.tolist()\n",
    "                text_length = sum([len(t) for t in text_list])\n",
    "                documents = [Document(text=t) for t in text_list]\n",
    "                self.index = VectorStoreIndex.from_documents(documents, service_context=self.service_context)\n",
    "                self.query_engine = self.index.as_query_engine()\n",
    "                return('Loaded paper: ' + first_author + ' (' + str(year) + ') \"' + title + '\\n' + str(text_length))\n",
    "\n",
    "            def chat(message, chat_history):\n",
    "                bot_message = self.query_engine.query(message)\n",
    "                chat_history.append((message, bot_message.response))\n",
    "                return \"\", chat_history\n",
    "\n",
    "            with gr.Tab(\"Paper\"):\n",
    "                doi_tb = gr.Textbox(label=\"Load DOI...\",value='10.1101/2022.01.23.477440')\n",
    "                index_status = gr.Textbox(label='Index Status: ')\n",
    "                text_button = gr.Button(\"Download File  Document\")\n",
    "                text_button.click(load_jats_paper, doi_tb, index_status)\n",
    "\n",
    "            with gr.Tab(\"Knowledge Bot\"):\n",
    "                chatbot = gr.Chatbot()\n",
    "                msg = gr.Textbox()\n",
    "                clear = gr.ClearButton([msg, chatbot])\n",
    "                msg.submit(chat, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "        demo.queue().launch(debug = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a chatbot. \n",
    "\n",
    "1. Download the GGUF file you want the chatbot to use for Llama2 and put it in a temp directory\n",
    "2. Set the environment variable `DB_TOKEN` to your databricks access token \n",
    "   1. You need access to the Databricks SQL warehouse named `sci_general_sql` (ID: 1c4df94f2f1a6305) on the general czi-prod databricks instance: `https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/` \n",
    "3. Execute the following code: \n",
    "\n",
    "```python\n",
    "from alhazen.gradio import SingleOpenAccessArticleChatBot\n",
    "\n",
    "chatbot = SingleOpenAccessArticleChatBot(temp_dir='/Users/gburns/Documents/Coding/ChatGPT_etc/LLMs/')\n",
    "chatbot.run_gradio()\n",
    "```\n",
    "4. Navigate to the URL that is printed out.\n",
    "\n",
    "This should load a page with instructions that you can follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def chatbot(temp_dir='/tmp/alhazen'):\n",
    "    soaa = SingleOpenAccessArticleChatBot(temp_dir=temp_dir)\n",
    "    soaa.run_gradio()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
