{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA on a single research article\n",
    "\n",
    "> Building a simple chatbot to allow the user to ask questions about a single paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.single_paper_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import alhazen.utils.nxml_text_extractor as te \n",
    "import re\n",
    "import os\n",
    "\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index import Document, ServiceContext, set_global_service_context\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "import requests\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "\n",
    "import gradio as gr\n",
    "from databricks import sql\n",
    "import os\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "GGUF_LOOKUP_URL = {\n",
    "    \"llama-2-70b-chat.Q5_K_M.gguf\": \"https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf\",\n",
    "    \"llama-2-13b-chat.Q5_K_M.gguf\": \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    \"llama-2-7b-chat.Q5_K_M.gguf\": \"https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\"\n",
    "}\n",
    "\n",
    "class SingleOpenAccessArticleChatBot:\n",
    "\n",
    "    def __init__(self, temp_dir='/tmp/alhazen',\n",
    "                embed_model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "                gguf_file_name='llama-2-13b-chat.Q5_K_M.gguf',\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=256,\n",
    "                context_window=3900,\n",
    "                sentence_window_size=3):\n",
    "\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        \n",
    "        if temp_dir[-1:] != '/':\n",
    "            temp_dir += '/'\n",
    "        self.temp_dir = temp_dir\n",
    "\n",
    "        # download relevant file to local disk if not already there\n",
    "        if os.path.exists(temp_dir+gguf_file_name) is False:\n",
    "            print('Downloading GGUF file: ' + gguf_file_name)\n",
    "            os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "            # download file from HuggingFace but show progress bar  \n",
    "            r = requests.get(GGUF_LOOKUP_URL[gguf_file_name], stream=True)\n",
    "            with open(temp_dir+gguf_file_name, 'wb') as f:\n",
    "                total_length = int(r.headers.get('content-length'))\n",
    "                dl = 0\n",
    "                for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                    dl += len(chunk)\n",
    "                    f.write(chunk)\n",
    "                    done = int(50 * dl / total_length)\n",
    "                    print('\\r[{}{}]'.format('=' * done, ' ' * (50-done)), end='')\n",
    "                print('\\n')\n",
    "            print('Download complete.')\n",
    "                        \n",
    "        self.llm = LlamaCPP( \n",
    "            model_path=temp_dir+gguf_file_name,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            context_window=context_window,\n",
    "            generate_kwargs={},\n",
    "            model_kwargs={\"n_gpu_layers\": 1},\n",
    "            messages_to_prompt=messages_to_prompt,\n",
    "            completion_to_prompt=completion_to_prompt,\n",
    "            verbose=True)     \n",
    "\n",
    "        # create the sentence window node parser w/ default settings\n",
    "        self.node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "            window_size=sentence_window_size,\n",
    "            window_metadata_key=\"window\",\n",
    "            original_text_metadata_key=\"original_text\",\n",
    "            )\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "        # create a service context\n",
    "        self.service_context = ServiceContext.from_defaults(\n",
    "            llm=self.llm,\n",
    "            embed_model=self.embed_model,\n",
    "            node_parser=self.node_parser)\n",
    "\n",
    "    def get_crossref_data_from_doi(self, doi):\n",
    "\n",
    "        stem = 'https://api.crossref.org/v1/works/'\n",
    "        url = stem + doi\n",
    "        json_data = requests.get(url).text\n",
    "        return json_data\n",
    "\n",
    "    def get_ft_url_from_doi(self, doi):\n",
    "        \n",
    "        try:\n",
    "            '''<<< NOTES ABOUT HOW TO CONNECT TO DATABRICKS USING SQLALCHEMY >>>\n",
    "            engine = create_engine(\n",
    "                'databricks://token:'+os.environ['DB_TOKEN']+ \n",
    "                '@czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com'+\n",
    "                '?http_path=/sql/1.0/warehouses/1c4df94f2f1a6305')\n",
    "            sql = 'SELECT * FROM BLAHBLAHBLAH\n",
    "            with engine.connect() as con:\n",
    "                stmt = db.text(get_ft_url_from_doi_sql)\n",
    "                rs = con.execute(stmt)\n",
    "                df = pd.DataFrame(rs.fetchall(), columns=rs.keys())'''\n",
    "\n",
    "            get_ft_url_from_doi_sql = '''\n",
    "                SELECT DISTINCT p.pmc_id, p.doi, YEAR(p.publication_date) as year, p.title, p.abstract, p.full_text_format, p.full_text_url, a.last_name\n",
    "                FROM scipubstore.ingestion.papers as p \n",
    "                    JOIN scipubstore.ingestion.authors as a on (p.paper_id=a.paper_id) \n",
    "                WHERE p.doi = '{}' and a.author_index=1\n",
    "                ORDER BY p.full_text_url DESC\n",
    "            '''.format(doi)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            with sql.connect(server_hostname = 'czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com',\n",
    "                                http_path = '/sql/1.0/warehouses/1c4df94f2f1a6305',\n",
    "                                access_token = os.getenv(\"DB_TOKEN\")) as connection:\n",
    "\n",
    "                with connection.cursor() as cursor:\n",
    "                    cursor.execute(get_ft_url_from_doi_sql)\n",
    "                    result = cursor.fetchall()\n",
    "                    df = pd.DataFrame([row.asDict() for row in result])\n",
    "\n",
    "        except:\n",
    "\n",
    "            msg = 'Error attempting to query Databricks for URL data, did you set the DB_TOKEN environment variable?'\n",
    "            raise Exception(msg)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def run_gradio(self):\n",
    "\n",
    "        with gr.Blocks() as demo:\n",
    "            gr.Markdown('# Alhazen - Single Paper Q/A Agent v.0.1')\n",
    "\n",
    "            def load_jats_paper(doi):\n",
    "                df = self.get_ft_url_from_doi(doi)\n",
    "                print(df.columns)\n",
    "                if df.shape[0] == 0:\n",
    "                    return('No paper found with that DOI')\n",
    "                title = df['title'].values[0]\n",
    "                first_author = df['last_name'].values[0]\n",
    "                \n",
    "                year = df['year'].values[0]  \n",
    "                url = df['full_text_url'].values[0]\n",
    "                xml = requests.get(url).text\n",
    "                doc = te.NxmlDoc(doi, xml)\n",
    "                df1 = doc.build_simple_document_dataframe()\n",
    "                df_text = df1[df1['FIG_REF'] == '']\n",
    "                for i, row in df_text.iterrows():\n",
    "                    if row.TAG != 'p' and row.PLAIN_TEXT[-1:]!='.':\n",
    "                        row.PLAIN_TEXT += '.'\n",
    "                text_list = df_text['PLAIN_TEXT'].values.tolist()\n",
    "                text_length = sum([len(t) for t in text_list])\n",
    "                documents = [Document(text=t) for t in text_list]\n",
    "                self.index = VectorStoreIndex.from_documents(documents, service_context=self.service_context)\n",
    "                self.query_engine = self.index.as_query_engine()\n",
    "                return('Loaded paper: ' + first_author + ' (' + str(year) + ') \"' + title + '\\n' + str(text_length))\n",
    "\n",
    "            def chat(message, chat_history):\n",
    "                bot_message = self.query_engine.query(message)\n",
    "                chat_history.append((message, bot_message.response))\n",
    "                return \"\", chat_history\n",
    "\n",
    "            with gr.Tab(\"Paper\"):\n",
    "                doi_tb = gr.Textbox(label=\"Load DOI...\",value='10.1101/2022.01.23.477440')\n",
    "                index_status = gr.Textbox(label='Index Status: ')\n",
    "                text_button = gr.Button(\"Download File  Document\")\n",
    "                text_button.click(load_jats_paper, doi_tb, index_status)\n",
    "\n",
    "            with gr.Tab(\"Knowledge Bot\"):\n",
    "                chatbot = gr.Chatbot()\n",
    "                msg = gr.Textbox()\n",
    "                clear = gr.ClearButton([msg, chatbot])\n",
    "                msg.submit(chat, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "        demo.queue().launch(debug = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def chatbot(temp_dir='/tmp/alhazen',\n",
    "                embed_model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "                gguf_file_name='llama-2-13b-chat.Q5_K_M.gguf',\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=256,\n",
    "                context_window=3900,\n",
    "                sentence_window_size=3):\n",
    "    \n",
    "    chatbot = SingleOpenAccessArticleChatBot(temp_dir=temp_dir,\n",
    "                embed_model_name=embed_model_name, \n",
    "                gguf_file_name=gguf_file_name,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                context_window=context_window,\n",
    "                sentence_window_size=sentence_window_size)\n",
    "    chatbot.run_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
