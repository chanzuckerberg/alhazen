{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Tools  \n",
    "\n",
    " > A library of classes that provide query access to a number of online academic search services including (Pubmed, PMC, and European PMC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.searchEngineUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "from enum import Enum\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError\n",
    "from time import time,sleep\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "PAGE_SIZE = 10000\n",
    "TIME_THRESHOLD = 0.3333334\n",
    "\n",
    "class NCBI_Database_Type(Enum):\n",
    "  \"\"\"\n",
    "  Simple enumeration of the different NCBI databases supported by this tool\n",
    "  \"\"\"\n",
    "  pubmed = 'pubmed'\n",
    "  PMC = 'PMC'\n",
    "\n",
    "class ESearchQuery:\n",
    "  \"\"\"\n",
    "  Class to provide query interface for ESearch (i.e., query terms in elaborate ways, return a list of ids)\n",
    "  Each instance of this class executes queries of a given type\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, api_key=None, oa=False, db='pubmed'):\n",
    "    \"\"\"\n",
    "    :param api_key: API Key for NCBI EUtil Services \n",
    "    :param oa: Is this query searching for open access papers? \n",
    "    :param db: The database being queried\n",
    "    \"\"\"\n",
    "    self.api_key = api_key\n",
    "    self.idPrefix = ''\n",
    "    self.oa = oa\n",
    "    self.db = db\n",
    "  \n",
    "  def build_query_tuples(self, df, check_threshold, name_col, terms_col, sep):\n",
    "    '''\n",
    "    Given a dataframe defining a set of 'OR' queries (i.e., entirely broken into | clauses), \n",
    "    check each indvidual term in Pubmed and return complete queries with problematic terms \n",
    "    stripped.\n",
    "    '''\n",
    "    query_tuples = []\n",
    "    phrase_counts = []\n",
    "    for ind in df.index:\n",
    "      search_l = []\n",
    "      terms_to_check = [df[name_col][ind]]\n",
    "      terms_to_check.extend(df[terms_col][ind].split('|'))\n",
    "      for s in terms_to_check: \n",
    "        go_no_go, phrase, count = _check_query_phrase(self, s.strip())\n",
    "        print(go_no_go, phrase, count)\n",
    "        if go_no_go:\n",
    "          search_l.append(phrase)\n",
    "          sleep(0.10)\n",
    "          if count>check_threshold:\n",
    "            phrase_counts.append((phrase, count))\n",
    "      query_tuples.append( (ind, df[name_col][ind], ' OR '.join(search_l)) )\n",
    "    return query_tuples, phrase_counts\n",
    "  \n",
    "  def _check_query_phrase(self, phrase):\n",
    "    \"\"\"\n",
    "    Checks whether a phrase would work on Pubmed or would be expanded (which can lead to unpredictable errors). \n",
    "    Use this as a check for synonyms.   \n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    m1 = re.match('^[a-zA-Z0-9]{1,5}$', phrase)\n",
    "    if m1 is not None:\n",
    "      return False, 'Abbreviation', 0\n",
    "\n",
    "    m2 = re.search('[(\\)]', phrase)\n",
    "    if m2 is not None:\n",
    "      return False, 'Brackets', 0\n",
    "\n",
    "    m3 = re.search('[\\,\\;]', phrase)\n",
    "    if m3 is not None:\n",
    "      phrase = '(\"' + '\" AND \"'.join(re.split('[\\,\\;]', phrase.strip()))+'\")'\n",
    "\n",
    "    if self.api_key is not None: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else:\n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db='+self.db + '&term='\n",
    "    url =  esearch_stem + quote('\"'+phrase+'\"')\n",
    "\n",
    "    esearch_response = urlopen(url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count = int(esearch_soup.find('Count').string)\n",
    "    #n_translations = len(esearch_soup.find('TranslationStack').findAll('TermSet'))\n",
    "    phrase_not_found = esearch_soup.find('PhraseNotFound')\n",
    "    quoted_phrase_not_found = esearch_soup.find('QuotedPhraseNotFound')\n",
    "    if phrase_not_found is not None or quoted_phrase_not_found is not None:\n",
    "      return False, '\"'+phrase+'\" not found', 0\n",
    "    if count == 0:\n",
    "      return False, phrase, count\n",
    "    return True, phrase, count        \n",
    "\n",
    "  def execute_count_query(self, query):\n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    if self.oa:\n",
    "      if self.db == NCBI_Database_Type.PMC:\n",
    "        query = '\"open access\"[filter] AND (' + query + ')'\n",
    "        idPrefix = 'PMC'\n",
    "      elif self.db == NCBI_Database_Type.pubmed:\n",
    "        query = '\"loattrfree full text\"[sb] AND (' + query + ')'\n",
    "    if self.api_key: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else:\n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db='+self.db + '&term='\n",
    "    query = quote_plus(query)\n",
    "    esearch_response = urlopen(esearch_stem + query)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count_tag = esearch_soup.find('Count')\n",
    "    if count_tag is None:\n",
    "      raise Exception('No Data returned from \"' + self.query + '\"')\n",
    "    return int(count_tag.string)\n",
    "\n",
    "  def execute_query_on_website(self, q, pm_order='relevance'):\n",
    "    \"\"\"\n",
    "    Executes a query on the Pubmed database and returns papers in order of relevance or date.\n",
    "    This is important to determine accuracy of complex queries are based on the composition of the first page of results.  \n",
    "    \"\"\"\n",
    "    query = 'https://pubmed.ncbi.nlm.nih.gov/?format=pmid&size=10&term='+re.sub('\\s+','+',q)\n",
    "    if pm_order == 'date':\n",
    "      query += '&sort=date'\n",
    "    response = urlopen(query)\n",
    "    data = response.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(data, \"lxml-xml\")\n",
    "    pmids = re.split('\\s+', soup.find('body').text.strip())\n",
    "    return pmids\n",
    "\n",
    "  def execute_query(self, query):\n",
    "    \"\"\"\n",
    "    Executes a query on the eutils service and returns data as a Pandas Dataframe\n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    if self.oa:\n",
    "      if self.db == NCBI_Database_Type.PMC:\n",
    "        query = '\"open access\"[filter] AND (' + query + ')'\n",
    "        idPrefix = 'PMC'\n",
    "      elif self.db == NCBI_Database_Type.pubmed:\n",
    "        query = '\"loattrfree full text\"[sb] AND (' + query + ')'\n",
    "\n",
    "    if self.api_key: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=' + self.db + '&term='\n",
    "\n",
    "    query = quote_plus(query)\n",
    "    print(esearch_stem + query)\n",
    "    esearch_response = urlopen(esearch_stem + query)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count_tag = esearch_soup.find('Count')\n",
    "    if count_tag is None:\n",
    "        raise Exception('No Data returned from \"' + query + '\"')\n",
    "    count = int(count_tag.string)\n",
    "\n",
    "    latest_time = time()\n",
    "\n",
    "    ids = []\n",
    "    for i in tqdm(range(0,count,PAGE_SIZE)):\n",
    "      full_query = esearch_stem + query + '&retstart=' + str(i)+ '&retmax=' + str(PAGE_SIZE)\n",
    "      esearch_response = urlopen(full_query)\n",
    "      esearch_data = esearch_response.read().decode('utf-8')\n",
    "      esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "      for pmid_tag in esearch_soup.find_all('Id') :\n",
    "        ids.append(self.idPrefix + pmid_tag.text)\n",
    "      delta_time = time() - latest_time\n",
    "      if delta_time < TIME_THRESHOLD :\n",
    "        sleep(TIME_THRESHOLD - delta_time)\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EFetchQuery:\n",
    "    \"\"\"\n",
    "    Class to provide query interface for EFetch (i.e., query based on a list of ids)\n",
    "    Each instance of this class executes queries of a given type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key=None, db='pubmed'):\n",
    "        \"\"\"\n",
    "        :param api_key: API Key for NCBI EUtil Services \n",
    "        :param oa: Is this query searching for open access papers? \n",
    "        :param db: The database being queried\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.db = db\n",
    "\n",
    "    def execute_efetch(self, id):\n",
    "        \"\"\"\n",
    "        Executes a query for a single specific identifier\n",
    "        \"\"\"\n",
    "        if self.api_key:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+self.api_key+'&db=pubmed&retmode=xml&id='\n",
    "        else:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id='\n",
    "        efetch_response = urlopen(efetch_stem + str(id))\n",
    "        return self._generate_rows_from_medline_records(efetch_response.read().decode('utf-8'))\n",
    "\n",
    "    def generate_data_frame_from_id_list(self, id_list):\n",
    "        \"\"\"\n",
    "        Executes a query for a list of ID values\n",
    "        \"\"\"\n",
    "        if self.api_key:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+self.api_key+'&db=pubmed&retmode=xml&id='\n",
    "        else:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id='\n",
    "        page_size = 100\n",
    "        i = 0\n",
    "        url = efetch_stem\n",
    "        efetch_df = pd.DataFrame()\n",
    "        for line in tqdm(id_list):\n",
    "            try:\n",
    "                l = re.split('\\s+', str(line))\n",
    "                pmid = l[0]\n",
    "                i += 1\n",
    "                if i >= page_size:\n",
    "                    efetch_response = urlopen(url)\n",
    "                    df = self._generate_rows_from_medline_records(efetch_response.read().decode('utf-8'))\n",
    "                    efetch_df = efetch_df.append(df)\n",
    "                    url = efetch_stem\n",
    "                    i = 0\n",
    "\n",
    "                if re.search('\\d$', url):\n",
    "                    url += ','\n",
    "                url += pmid.strip()\n",
    "            except URLError as e:\n",
    "                sleep(10)\n",
    "                print(\"URLError({0}): {1}\".format(e.errno, e.strerror))\n",
    "            except TypeError as e2:\n",
    "                pause = 1\n",
    "\n",
    "        if url != efetch_stem:\n",
    "            efetch_response = urlopen(url)\n",
    "            df = self._generate_rows_from_medline_records(efetch_response.read().decode('utf-8'))\n",
    "            efetch_df = efetch_df.append(df)\n",
    "        return efetch_df\n",
    "\n",
    "    def generate_mesh_data_frame_from_id_list(self, id_list):\n",
    "        \"\"\"\n",
    "        Executes a query for MeSH data from a list of ID values\n",
    "        \"\"\"\n",
    "        url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "        if self.api_key:\n",
    "          payload = 'api_key='+self.api_key+'&db=pubmed&retmode=xml&id='\n",
    "        else:\n",
    "          payload = 'db=pubmed&retmode=xml&id='\n",
    "          \n",
    "        headers = {'content-type': 'application/xml'}\n",
    "\n",
    "        page_size = 10000\n",
    "        i = 0\n",
    "        efetch_df = pd.DataFrame()\n",
    "        for line in tqdm(id_list):\n",
    "            try:\n",
    "                l = re.split('\\s+', line)\n",
    "                pmid = l[0]\n",
    "                i += 1\n",
    "                if i >= page_size:\n",
    "                    print('   running query')\n",
    "                    r = requests.post(url, data=payload)\n",
    "                    efetch_data = r.content.decode('utf-8')\n",
    "                    df = self._generate_mesh_rows_from_medline_records(efetch_data)\n",
    "                    efetch_df = efetch_df.append(df)\n",
    "                    payload = 'db=pubmed&retmode=xml&id='\n",
    "                    i = 0\n",
    "\n",
    "                if re.search('\\d$', payload):\n",
    "                    payload += ','\n",
    "                payload += pmid.strip()\n",
    "            except URLError as e:\n",
    "                sleep(10)\n",
    "                print(\"URLError({0}): {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "        if payload[-1] != '=':\n",
    "            r = requests.post(url, data=payload)\n",
    "            efetch_data = r.content.decode('utf-8')\n",
    "            df = self._generate_mesh_rows_from_medline_records(efetch_data)\n",
    "            efetch_df = efetch_df.append(df)\n",
    "\n",
    "        return efetch_df\n",
    "\n",
    "    def _generate_mesh_rows_from_medline_records(self, record):\n",
    "\n",
    "        soup2 = BeautifulSoup(record, \"lxml-xml\")\n",
    "\n",
    "        rows = []\n",
    "        cols = ['PMID','MESH']\n",
    "        for citation_tag in tqdm(soup2.find_all('MedlineCitation')):\n",
    "\n",
    "            pmid_tag = citation_tag.find('PMID')\n",
    "\n",
    "            mesh_labels = []\n",
    "\n",
    "            if pmid_tag is None:\n",
    "                continue\n",
    "\n",
    "            for meshTag in citation_tag.findAll('MeshHeading'):\n",
    "                desc = meshTag.find('DescriptorName')\n",
    "                qual_list = meshTag.findAll('QualifierName')\n",
    "                if len(qual_list)>0:\n",
    "                    mesh_labels.append('%s/%s'%(desc.text.replace('\\n', ' '),'/'.join(q.text.replace('\\n', ' ') for q in qual_list)))\n",
    "                else:\n",
    "                    mesh_labels.append(desc.text.replace('\\n', ' '))\n",
    "            mesh_data = \",\".join(mesh_labels)\n",
    "\n",
    "            rows.append((pmid_tag.text, mesh_data))\n",
    "\n",
    "        df = pd.DataFrame(data=rows, columns=cols)\n",
    "        return df\n",
    "\n",
    "    def _generate_rows_from_medline_records(self, record):\n",
    "\n",
    "        soup2 = BeautifulSoup(record, \"lxml-xml\")\n",
    "\n",
    "        rows = []\n",
    "        cols = ['PMID', 'YEAR', 'PUBLICATION_TYPE', 'TITLE', 'ABSTRACT','MESH','KEYWORDS']\n",
    "        for citation_tag in soup2.find_all('MedlineCitation'):\n",
    "\n",
    "            pmid_tag = citation_tag.find('PMID')\n",
    "            title_tag = citation_tag.find('ArticleTitle')\n",
    "            abstract_txt = ''\n",
    "            for x in citation_tag.findAll('AbstractText'):\n",
    "                if x.label is not None:\n",
    "                    abstract_txt += ' ' + x.label + ': '\n",
    "                abstract_txt += x.text\n",
    "\n",
    "            mesh_labels = []\n",
    "            for meshTag in citation_tag.findAll('MeshHeading'):\n",
    "                desc = meshTag.find('DescriptorName')\n",
    "                qual_list = meshTag.findAll('QualifierName')\n",
    "                if len(qual_list)>0:\n",
    "                    mesh_labels.append('%s/%s'%(desc.text.replace('\\n', ' '),'/'.join(q.text.replace('\\n', ' ') for q in qual_list)))\n",
    "                else:\n",
    "                    mesh_labels.append(desc.text.replace('\\n', ' '))\n",
    "            mesh_data = \",\".join(mesh_labels)\n",
    "\n",
    "            if pmid_tag is None or title_tag is None or abstract_txt == '':\n",
    "                continue\n",
    "\n",
    "            year_tag = citation_tag.find('PubDate').find('Year')\n",
    "\n",
    "            year = ''\n",
    "            if year_tag is not None:\n",
    "                year = year_tag.text\n",
    "\n",
    "            is_review = '|'.join([x.text for x in citation_tag.findAll('PublicationType')])\n",
    "\n",
    "            keyword_data = '|'.join([meshTag.text.replace('\\n', ' ') for meshTag in citation_tag.findAll('Keyword')])\n",
    "\n",
    "            rows.append((pmid_tag.text, year, is_review, title_tag.text, abstract_txt, mesh_data, keyword_data))\n",
    "\n",
    "        df = pd.DataFrame(data=rows, columns=cols)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuroPMCQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EuroPMCQuery():\n",
    "    \"\"\"\n",
    "    A class that executes search queries on the European PMC API \n",
    "    \"\"\"\n",
    "    def __init__(self, oa=False, db='pubmed'):\n",
    "        \"\"\"\n",
    "        Initialization of the class\n",
    "        :param oa:\n",
    "        \"\"\"\n",
    "        self.oa = oa\n",
    "\n",
    "    def run_empc_query(self, q, page_size=1000, timeout=60, \n",
    "                       extra_columns=[\"id\", \"source\", \"doi\", 'title', 'pubYear', 'abstractText', 'pubType']):\n",
    "        EMPC_API_URL = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize='+str(page_size)+'&synonym=TRUE'\n",
    "        if len(extra_columns)>0:\n",
    "            EMPC_API_URL += '&resultType=core'\n",
    "        url = EMPC_API_URL + '&query=' + q\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        data = json.loads(r.text)\n",
    "        numFound = data['hitCount']\n",
    "        print(url + ', ' + str(numFound) + ' European PMC PAPERS FOUND')\n",
    "        ids_from_q = []\n",
    "        cursorMark = '*'\n",
    "        for i in tqdm(range(0, numFound, page_size)):\n",
    "            url = EMPC_API_URL + '&cursorMark=' + cursorMark + '&query=' + q\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            data = json.loads(r.text)\n",
    "            #print(data)\n",
    "            if data.get('nextCursorMark'):\n",
    "                cursorMark = data['nextCursorMark']\n",
    "            for d in data['resultList']['result']:\n",
    "                if d.get('pubType','') == 'patent':\n",
    "                    continue\n",
    "                tup = [d.get('id',-1), d.get('doi','')]\n",
    "                for c in extra_columns:\n",
    "                    tup.append(d.get(c,'')) \n",
    "                ids_from_q.append(tup)\n",
    "        ids_from_q = sorted(list(ids_from_q), key = lambda x: x[0])\n",
    "        print(' Returning '+str(len(ids_from_q)))\n",
    "        return (numFound, ids_from_q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
