{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airtable Utilities\n",
    "\n",
    "> Simple library to provide lightweight input/output functions for Airtable. Airtable is an excellent vehicle for interacting with users.\n",
    "\n",
    "Note - this approach requires manual construction of Airtable notebooks to match the existing format of notebooks so some overhead is needed to check formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.airtableUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.metrics import agreement\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import masi_distance, binary_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class AirtableUtils:\n",
    "  \"\"\"This class permits simple input / output from airtable\n",
    "  \n",
    "  Attributes:\n",
    "    * api_key: an API key obtained from Airtable to provide authentication\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, api_key):\n",
    "    \"\"\" Initialize the interface with an API key. \n",
    "    \"\"\"\n",
    "    self.api_key = api_key\n",
    "    self.task = None\n",
    "    \n",
    "  def _get_airtable_url(self, file, table):\n",
    "    return 'https://api.airtable.com/v0/%s/%s?api_key=%s'%(file, table, self.api_key) \n",
    "\n",
    "  def read_airtable(self, file, table):\n",
    "    \"\"\" Read an airtable into a Pandas Dataframe. \n",
    "    \"\"\"\n",
    "    url = self._get_airtable_url(file, table)\n",
    "    x = requests.get(url)\n",
    "    js = json.loads(x.text)\n",
    "    if( js.get('records') is None ):\n",
    "      raise Exception(\"Airtable \"+url+\" not found.\" )\n",
    "\n",
    "    df = pd.DataFrame([r.get('fields') for r in js.get('records')])\n",
    "    df = df.fillna('')\n",
    "\n",
    "    if 'ID' not in df.columns:\n",
    "      df.reset_index(inplace=True)\n",
    "      df = df.rename(columns = {'index':'ID'})\n",
    "\n",
    "    df = df.sort_values(by=['ID'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "  def read_airtable(self, file, table):\n",
    "    data_rows = []\n",
    "    headers = {'Authorization': 'Bearer '+self.api_key, 'Content-Type': 'application/json'}\n",
    "  #  base_url = start_url + '&maxRecords=100&fields%5B%5D=ID&fields%5B%5D=Title&fields%5B%5D=Abstract' + \\\n",
    "  #    '&fields%5B%5D=Comments&fields%5B%5D=Disease%20Research%20Categories&fields%5B%5D=Irrelevant?' + \\\n",
    "  #    '&fields%5B%5D=TimeofLastCurationAction'\n",
    "    base_url = self._get_airtable_url(file, table)\n",
    "    offset = 'GO'\n",
    "    while offset!='STOP':\n",
    "      print('.', end = '')\n",
    "      #print(offset)\n",
    "      if offset != 'GO':\n",
    "        url = base_url + '&offset='+offset\n",
    "      else:\n",
    "        url = base_url\n",
    "      #print(url)\n",
    "      r = requests.get(url, headers=headers)      \n",
    "      rdata = json.loads(r.text)\n",
    "      #print(len(rdata.get('records',[])))\n",
    "      #print(rdata)\n",
    "      for r in rdata.get('records',[]):\n",
    "        data_rows.append(r['fields'])\n",
    "      if( rdata.get('offset') is not None ):\n",
    "        offset = rdata['offset']\n",
    "      else: \n",
    "        offset = 'STOP'\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    df = df.replace('\"', '')\n",
    "    #df = df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=True)\n",
    "    print('|')\n",
    "    return df\n",
    "    \n",
    "  \n",
    "  def build_curated_dataframe(self, files, tables):\n",
    "    curated_df = pd.DataFrame()\n",
    "    for f in files:\n",
    "      for t in tables:\n",
    "        try:\n",
    "          df = self.read_airtable(f, t)\n",
    "          df['at_f'] = f\n",
    "          df['at_t'] = t\n",
    "          print('%d rows added'%(len(df)))\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "        curated_df = curated_df.append(df)\n",
    "    curated_df = curated_df.reset_index(drop=True)\n",
    "    return curated_df\n",
    "  \n",
    "  def send_df_to_airtable(self, file, table, df):\n",
    "    \"\"\" Send a dataframe to an airtable table.\n",
    "    \n",
    "    _Note: the dataframe's columns must match the structure of the table exactly_ \n",
    "    \"\"\"\n",
    "    # note need to check size of payload - 10 JSON records only with 'fields' hash entry\n",
    "    headers = {'Authorization': 'Bearer '+self.api_key, 'Content-Type': 'application/json'}\n",
    "    records = []\n",
    "    for i, row in df.iterrows():\n",
    "      if i % 10 == 0 and i > 0:\n",
    "        records = []\n",
    "        r = requests.post(url, headers=headers, data=payload)      \n",
    "      records.append(fields_json = json.dumps(mnrow.to_dict()))\n",
    "    airtable_data = r.content.decode('utf-8')\n",
    "\n",
    "  def send_records_to_airtable(self, file, table, records):\n",
    "    url = self._get_airtable_url(file, table)\n",
    "\n",
    "    # note need to check size of payload - 20 JSON records only with 'fields' hash entry\n",
    "    headers = {'Authorization': 'Bearer '+self.api_key, 'Content-Type': 'application/json'}\n",
    "    rec_set = []\n",
    "    for i, row in tqdm(enumerate(records)):\n",
    "      if i % 10 == 0 and i > 0:\n",
    "        payload = json.dumps({'records':rec_set}) \n",
    "        r = requests.post(url, headers=headers, data=payload)  \n",
    "        print(r.text)\n",
    "        rec_set = []\n",
    "      rec_set.append({'fields':row})   \n",
    "    if len(records)>0:\n",
    "      payload = json.dumps({'records':rec_set}) \n",
    "      r = requests.post(url, headers=headers, data=payload)   \n",
    "      print(r.text)\n",
    "      \n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "  def build_nltk_annotation_task_from_curated_df(self, df, \n",
    "                                                 doc_id_column, \n",
    "                                                 category_column, \n",
    "                                                 curator_column,\n",
    "                                                 distance_function=masi_distance):\n",
    "    document_task_data = []\n",
    "    curators = {}\n",
    "    docs = {}\n",
    "    categories = {}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "      category_array = str(row[category_column]).split(',')\n",
    "      item = str(row[doc_id_column])\n",
    "      td_row = (row[curator_column], item, frozenset(category_array))\n",
    "      document_task_data.append(td_row)\n",
    "      if docs.get(item) is None:\n",
    "        docs[item] = 1\n",
    "      else:\n",
    "        docs[item] = docs[item] + 1\n",
    "\n",
    "      if curators.get(row[curator_column]) is None:\n",
    "        curators[row[curator_column]] = 1\n",
    "      else:\n",
    "        curators[row[curator_column]] = curators[row[curator_column]] + 1\n",
    "      for c in str(row[category_column]).split(','):\n",
    "        if categories.get(c) is None:\n",
    "          categories[c] = 1\n",
    "        else:\n",
    "          categories[c] = categories[c] + 1\n",
    "\n",
    "    doc_task = AnnotationTask(distance = distance_function)\n",
    "    doc_task.load_array(document_task_data)\n",
    "    self.docs = docs\n",
    "    self.curators\n",
    "    return docs, curators, categories, doc_task\n",
    "  \n",
    "  def _get_avg_doc_agr(item, curators, task):\n",
    "    temp_list = [] \n",
    "    sum = 0.0\n",
    "    cnt = 0.0\n",
    "    for i in range(len(curators)):\n",
    "      for j in range(i):\n",
    "        try:\n",
    "          sum += task.agr(curators[i], curators[j], str(item))\n",
    "          cnt += 1.0\n",
    "        except StopIteration:\n",
    "          # No need to do anything - we get this error if attempting to compute agreement \n",
    "          # between curators where one of them never entered a score. \n",
    "          print('', end = '')\n",
    "    if cnt > 0.0:\n",
    "      avg = sum/cnt\n",
    "    else: \n",
    "      avg = 0.0;\n",
    "    return avg\n",
    "\n",
    "  def _get_consensus(self, item, curators, task):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    best = 0.0 \n",
    "    for i in range(len(curators)):\n",
    "      for j in range(i):\n",
    "        try:\n",
    "          agr = task.agr(curators[i], curators[j], str(item))\n",
    "          if agr == 1.0:\n",
    "            l = [x for x in self.doc_task.data if x['coder']==curators[i] and x['item']==item]\n",
    "            return list(l[0]['labels'])[0]\n",
    "        except StopIteration:\n",
    "          # No need to do anything - we get this error if attempting to compute agreement \n",
    "          # between curators where one of them never entered a score. \n",
    "          print('', end = '')\n",
    "    return None\n",
    "\n",
    "  def get_consensus_per_doc(self, df, task):\n",
    "\n",
    "    cat_list = sorted(list({c:0 for cc in df.CATEGORIES for c in cc.split(',')}.keys()))\n",
    "    curators = df.CURATOR.unique()\n",
    "\n",
    "    item_curator_dict = {cc: {c: '' for c in curators} for cc in df.ID_PAPER}\n",
    "    for row in df.itertuples():\n",
    "      item_curator_dict[row.ID_PAPER][row.CURATOR] = row.CATEGORIES\n",
    "\n",
    "    sdf = df.drop(['DISEASE_NAME', 'URI', 'TIMESTAMP','CATEGORIES','IRRELEVANT', \\\n",
    "                   'COMMENTS','CURATOR', 'WORKSHEET', \\\n",
    "                   'TITLE','ABSTRACT'], axis=1).drop_duplicates()\n",
    "    sdf = sdf.reset_index(drop=True)\n",
    "\n",
    "    #cat_count_dict = {cc: {c: 0 for c in cat_list} for cc in df.ID_PAPER}\n",
    "    #for row in df.itertuples():\n",
    "    #  for t in row.CATEGORIES.split(','):\n",
    "    #    cat_count_dict[row.ID_PAPER][t] = cat_count_dict.get(row.ID_PAPER).get(t) + 1\n",
    "    #cat_counts = [[cat_count_dict[row.ID_PAPER][c] for c in cat_list ] for row in sdf.itertuples()]\n",
    "    #sdf['CATEGORY_COUNTS'] = cat_counts\n",
    "\n",
    "    sdf['AVG_AGREEMENT'] = [self._get_avg_doc_agr(str(row.ID_PAPER), curators, task) for row in sdf.itertuples()]\n",
    "    sdf['CONSENSUS'] = [self.get_consensus(str(row.ID_PAPER), curators, task) for row in sdf.itertuples()]\n",
    "\n",
    "    return sdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
