{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JATS Text Extractor Utility\n",
    "\n",
    "> Extracts structured text from JATS XML Files with offset annotations for sections, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.jats_text_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from collections import namedtuple\n",
    "from lxml import etree\n",
    "from lxml.etree import ElementTree\n",
    "from nxml2txt import rewritetex\n",
    "from nxml2txt import rewritemmla\n",
    "from nxml2txt import respace\n",
    "from nxml2txt import rewriteu2a\n",
    "from nxml2txt import standoff\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from urllib.request import urlopen\n",
    "from requests.utils import requote_uri\n",
    "\n",
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import csv\n",
    "import html\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from databricks import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_ft_url_from_doi(doi, file_path):\n",
    "    \n",
    "    if os.environ.get('DB_TOKEN') is None:  \n",
    "        msg = 'Error attempting to query Databricks for URL data, did you set the DB_TOKEN environment variable?'\n",
    "        raise Exception(msg)\n",
    "\n",
    "    get_ft_url_from_doi_sql = '''\n",
    "        SELECT DISTINCT p.pmc_id, p.doi, YEAR(p.publication_date) as year, p.title, p.abstract, p.full_text_format, p.full_text_url, a.last_name\n",
    "        FROM scipubstore.ingestion.papers as p \n",
    "            JOIN scipubstore.ingestion.authors as a on (p.paper_id=a.paper_id) \n",
    "        WHERE p.doi = '{}' and a.author_index=1\n",
    "        ORDER BY p.full_text_url DESC\n",
    "    '''.format(doi)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with sql.connect(server_hostname = 'czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com',\n",
    "                        http_path = '/sql/1.0/warehouses/1c4df94f2f1a6305',\n",
    "                        access_token = os.getenv(\"DB_TOKEN\")) as connection:\n",
    "\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(get_ft_url_from_doi_sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "    df = pd.DataFrame([row.asDict() for row in result])\n",
    "    if df.shape[0] == 0:\n",
    "        return('No paper found with that DOI')\n",
    "    \n",
    "    title = df['title'].values[0]\n",
    "    first_author = df['last_name'].values[0]\n",
    "    year = df['year'].values[0]  \n",
    "    url = df['full_text_url'].values[0]\n",
    "    xml = requests.get(url).text\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'namedtuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#| export\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m TexOptions \u001b[39m=\u001b[39m namedtuple(\u001b[39m'\u001b[39m\u001b[39mTexOptions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m U2aOptions \u001b[39m=\u001b[39m namedtuple(\u001b[39m'\u001b[39m\u001b[39mU2aOptions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhex keep_missing stdout directory overwrite\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m@dataclasses\u001b[39m\u001b[39m.\u001b[39mdataclass\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNxmlDoc\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'namedtuple' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "TexOptions = namedtuple('TexOptions', 'verbose')\n",
    "U2aOptions = namedtuple('U2aOptions', 'hex keep_missing stdout directory overwrite')\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class NxmlDoc:\n",
    "    \"\"\"A class that provides structure for full text papers specified under the JATS 'nxml' format.\"\"\"\n",
    "\n",
    "    # The XML content of the file\n",
    "    xml: str\n",
    "\n",
    "    # The identifier of the paper\n",
    "    ft_id: str\n",
    "\n",
    "    # Plain text of the paper's contents\n",
    "    text: str\n",
    "\n",
    "    # Standoff annotations superimposed over the  prompts\n",
    "    standoffs: str\n",
    "      \n",
    "    def __init__(self, ft_id, xml):\n",
    "        self.ft_id = ft_id\n",
    "\n",
    "        # HTML entities kill the XML parse\n",
    "        # but any '<' characters must be replaced with &lt; in XML (and '& with &amp;)\n",
    "        xml = xml.replace('<', '__less_than__')\n",
    "        xml = html.unescape(xml)\n",
    "        xml = xml.replace('&', '&amp;')\n",
    "        xml = xml.replace('<', '&lt;')\n",
    "        xml = xml.replace('__less_than__', '<')\n",
    "        xml = xml.encode('utf-8')\n",
    "        self.xml = xml\n",
    "        \n",
    "        tree = ElementTree( etree.fromstring(xml) )\n",
    "        tex_options = TexOptions(verbose=True)\n",
    "        rewritetex.process_tree(tree, options=tex_options)\n",
    "        \n",
    "        # process MathML annotations\n",
    "        rewritemmla.process_tree(tree)\n",
    "        \n",
    "        # normalize whitespace\n",
    "        #respace.process_tree(tree)\n",
    "        \n",
    "        # map unicodoffs = nxml2txt(nxmlfne to ASCII)\n",
    "        u2a_options = U2aOptions(keep_missing=True, hex=False, stdout=False, directory=None, overwrite=False)\n",
    "        rewriteu2a.process_tree(tree, options=u2a_options)\n",
    "        \n",
    "        # convert to text and standoffs\n",
    "        text, standoffs = standoff.convert_tree(tree)   \n",
    "        self.text = text\n",
    "        self.standoffs = standoffs\n",
    "\n",
    "        self.to_exclude = ['table-wrap-foot']\n",
    "        self.text_tag_types = ['front/article-title', 'front/abstract', \n",
    "                               'body/p', 'body/title', 'body/label',\n",
    "                               'back/p', 'back/title']\n",
    "        self.tag_types = {'text':['article-title', 'abstract', 'p', 'title', 'label', 'caption'],\n",
    "                'structure':['front', 'body', 'back', 'ref-list', 'sec', 'fig', 'supplementary-material'],\n",
    "                'xref': ['xref', 'ref', 'label', 'name', 'surname', 'year', 'pub-id', 'fpage']}\n",
    "\n",
    "    def get_figure_reference(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        for t in hits:\n",
    "          if t.element.tag == 'fig':      \n",
    "            return t.element.get('id','')\n",
    "        return ''\n",
    "\n",
    "    def get_top_level_sec_tag(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        for t in hits:\n",
    "          if t.element.tag == 'sec':      \n",
    "            if t.element.get('sec-type', None):\n",
    "              return t.element.get('sec-type')\n",
    "            elif t.element.find('title') is not None:\n",
    "              return t.element.find('title').text\n",
    "        return ''\n",
    "\n",
    "    def generate_tag_tree(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        tag_tree = '.'.join(['%s[%s...]'%(t.element.tag,self.text[t.start:t.start+8]) if t.element.tag=='sec' else t.element.tag for t in hits])\n",
    "        tag_tree = tag_tree+'.'+t.element.tag\n",
    "        return tag_tree\n",
    "    \n",
    "    def build_simple_document_dataframe(self):\n",
    "\n",
    "        text_tuples = []\n",
    "\n",
    "        try:\n",
    "\n",
    "            # two stage process - build a lookup list of all relevant tags \n",
    "            # - then use the tags start/end properties to identify text portions of the paper and render those.\n",
    "\n",
    "            this_doc_standoffs = {t:[] for tt in self.tag_types.keys() for t in self.tag_types[tt]}\n",
    "            \n",
    "            all_xrefs = []\n",
    "            for s in self.standoffs:\n",
    "                if this_doc_standoffs.get(s.element.tag) is not None:\n",
    "                    this_doc_standoffs.get(s.element.tag).append(s)\n",
    "                if s.element.tag == 'xref':\n",
    "                    all_xrefs.append(s)\n",
    "            #\n",
    "            # skip the whole file if there's no body tag.\n",
    "            #\n",
    "            if len(this_doc_standoffs.get('body')) == 0:\n",
    "                return None\n",
    "            \n",
    "            text_so_list = []\n",
    "            for ttt in self.text_tag_types:\n",
    "                part,tag = ttt.split('/')\n",
    "                part_so = this_doc_standoffs.get(part)[0]\n",
    "                for so in this_doc_standoffs.get(tag):\n",
    "                    if so.start < part_so.start or so.end > part_so.end:\n",
    "                        continue\n",
    "                    text_so_list.append(so)\n",
    "                    #print((row.PMID, local_id, so.element.tag, query_document_standoffs(so, text, standoffs), so.start, (so.end-so.start), text[so.start:so.end]))\n",
    "              \n",
    "            # Manipulate standoff annotations so that titles and labels fall naturally in the text \n",
    "            # and paragraph tags that hold other paragraphs (as is the case with pmid:26791617) don't trigger repeating text.\n",
    "            # Make sure the SOs only tile the document and do not overlap.\n",
    "            text_so_list = sorted(text_so_list, key=lambda x: x.start)\n",
    "            last_so = None\n",
    "            for so in text_so_list:\n",
    "                if last_so:\n",
    "                    if last_so.end > so.start:\n",
    "                        last_so.end = so.start-1\n",
    "                last_so = so\n",
    "              \n",
    "            sent_id = 0\n",
    "            for local_id, so in enumerate(text_so_list):\n",
    "                #tag_tree = generate_tag_tree(so, text, standoffs)\n",
    "                top_sec_title = self.get_top_level_sec_tag(so) \n",
    "                figure_reference = self.get_figure_reference(so) \n",
    "                so_text = self.text[so.start:so.end]\n",
    "                                \n",
    "                tuple = (self.ft_id, local_id, so.element.tag, top_sec_title, so.start, (so.end-so.start), figure_reference, so_text)\n",
    "                text_tuples.append(tuple)\n",
    "              \n",
    "        except etree.XMLSyntaxError as xmlErr:\n",
    "            print(\"XML Syntax Error: {0}\".format(xmlErr))\n",
    "        except UnicodeDecodeError as unicodeErr:\n",
    "            print(\"Unicode parsing Error: {0}\".format(unicodeErr))\n",
    "        #except TypeError as typeErr:\n",
    "        #  print(\"Type Error: {0}\".format(typeErr))  \n",
    "        #    print(\"ValueError: {0}\".format(valErr))\n",
    "        #    return None\n",
    "        \n",
    "        text_df = pd.DataFrame(text_tuples, columns=['PMID', 'PARAGRAPH_ID', 'TAG', 'TAG_TREE', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'])\n",
    "        return text_df\n",
    "\n",
    "\n",
    "    def build_enahanced_document_dataframe(self):\n",
    "\n",
    "        text_tuples = []\n",
    "\n",
    "        try:\n",
    "\n",
    "            # two stage process - build a lookup list of all relevant tags \n",
    "            # - then use the tags start/end properties to identify text portions of the paper and render those.\n",
    "\n",
    "            this_doc_standoffs = {t:[] for tt in self.tag_types.keys() for t in self.tag_types[tt]}\n",
    "            \n",
    "            all_xrefs = []\n",
    "            for s in self.standoffs:\n",
    "                if this_doc_standoffs.get(s.element.tag) is not None:\n",
    "                    this_doc_standoffs.get(s.element.tag).append(s)\n",
    "                if s.element.tag == 'xref':\n",
    "                    all_xrefs.append(s)\n",
    "            #\n",
    "            # skip the whole file if there's no body tag.\n",
    "            #\n",
    "            if len(this_doc_standoffs.get('body')) == 0:\n",
    "                return None\n",
    "            \n",
    "            ref_dict = self.extract_ref_dict_from_nxml()\n",
    "\n",
    "            text_so_list = []\n",
    "            for ttt in self.text_tag_types:\n",
    "                part,tag = ttt.split('/')\n",
    "                part_so = this_doc_standoffs.get(part)[0]\n",
    "                for so in this_doc_standoffs.get(tag):\n",
    "                    if so.start < part_so.start or so.end > part_so.end:\n",
    "                        continue\n",
    "                    text_so_list.append(so)\n",
    "                    #print((row.PMID, local_id, so.element.tag, query_document_standoffs(so, text, standoffs), so.start, (so.end-so.start), text[so.start:so.end]))\n",
    "              \n",
    "            # Manipulate standoff annotations so that titles and labels fall naturally in the text \n",
    "            # and paragraph tags that hold other paragraphs (as is the case with pmid:26791617) don't trigger repeating text.\n",
    "            # Make sure the SOs only tile the document and do not overlap.\n",
    "            text_so_list = sorted(text_so_list, key=lambda x: x.start)\n",
    "            last_so = None\n",
    "            for so in text_so_list:\n",
    "                if last_so:\n",
    "                    if last_so.end > so.start:\n",
    "                        last_so.end = so.start-1\n",
    "                last_so = so\n",
    "              \n",
    "            sent_id = 0\n",
    "            for local_id, so in enumerate(text_so_list):\n",
    "                #tag_tree = generate_tag_tree(so, text, standoffs)\n",
    "                top_sec_title = self.get_top_level_sec_tag(so) \n",
    "                figure_reference = self.get_figure_reference(so) \n",
    "                \n",
    "                # ANY EXCLUSION CRITERIA FOR TAGS PUT IT HERE\n",
    "                \n",
    "                # SEARCH FOR XREFS IN THIS TEXT BLOCK - AND SUB THEM INTO THE TEXT.\n",
    "                so_text = ''\n",
    "                prev_end = so.start\n",
    "                ref_xrefs = [x for x in all_xrefs if x.start>=so.start and x.end<=so.end and x.element.attrib['ref-type']=='bibr']\n",
    "                #print(ref_xrefs)\n",
    "                \n",
    "                if len(ref_xrefs) > 0:\n",
    "                    refbib_xrefs = [x for x in all_xrefs if x.start>=so.start and x.end<=so.end and \n",
    "                                (x.element.attrib['ref-type']=='bibr' or x.element.attrib['ref-type']=='fig')] \n",
    "                    for x in refbib_xrefs:\n",
    "                        if x.element.attrib['ref-type']=='bibr':\n",
    "                            ref_id = x.element.attrib['rid']\n",
    "                            ref = ref_dict.get(ref_id, None)\n",
    "                            if ref and ref.get('pmid'):\n",
    "                                ref_text= '<<REF:%s>>'%(ref.get('pmid'))\n",
    "                            elif ref:\n",
    "                                ref_text= '<<REF:%s-%s-%s-%s>>'%(ref.get('first_author','???'),ref.get('year','?'),ref.get('vol','?'),ref.get('page','?'))\n",
    "                            else:\n",
    "                                ref_text = '<<REF>>'\n",
    "                            so_text += self.text[prev_end:x.start] + ref_text\n",
    "                        else: \n",
    "                            fig_id = x.element.attrib['rid']\n",
    "                            fig_text = '%s <<FIG:%s>>'%(self.text[x.start:x.end],fig_id)\n",
    "                            so_text += self.text[prev_end:x.start] + fig_text\n",
    "                        #print(pmid, ref_id, ref_text)\n",
    "                        prev_end = x.end\n",
    "                    \n",
    "                    #if len(so_text)>0:\n",
    "                    #  print(so_text)\n",
    "                    so_text += self.text[prev_end:so.end]  \n",
    "                    so_text = html.unescape(so_text)\n",
    "                #__________________________________________________________________\n",
    "                else: # TRY TO USE REGEXES TO SUBSTITUTE REFERENCES IN PASSAGE TEXT\n",
    "                    fig_xrefs = [x for x in all_xrefs if x.start>=so.start and x.end<=so.end and x.element.attrib['ref-type']=='fig']\n",
    "                    so_text = ''\n",
    "                    prev_end = so.start\n",
    "                    for x in fig_xrefs:\n",
    "                        fig_id = x.element.attrib['rid']\n",
    "                        fig_text = '%s <<FIG:%s>>'%(self.text[x.start:x.end],fig_id)\n",
    "                        so_text += self.text[prev_end:x.start] + fig_text\n",
    "                        prev_end = x.end\n",
    "                    so_text += self.text[prev_end:so.end]  \n",
    "                    so_text = html.unescape(so_text)\n",
    "\n",
    "                    #print(so_text)\n",
    "                    for key in ref_dict:\n",
    "                        ref = ref_dict[key]\n",
    "                        if ref.get('pmid'):  \n",
    "                            ref_text= '<<REF:%s>>'%(ref.get('pmid'))\n",
    "                        else:\n",
    "                            ref_text= '<<REF:%s-%s-%s-%s>>'%(ref.get('first_author','???'),ref.get('year','?'),ref.get('vol','?'),ref.get('page','?'))\n",
    "                        if ref.get('year') and ref.get('second_author'):\n",
    "                            regex = '%s( and %s,|,|\\\\s+et al\\\\.\\\\,|\\\\s+et al){0,1}\\\\s+%s'%(ref.get('first_author',''),ref.get('second_author',''),ref.get('year',''))\n",
    "                        elif ref.get('year') and len(ref.get('first_author',''))>0:\n",
    "                            regex = '%s(,|\\\\s+et al\\\\.\\\\,|\\\\s+et al){0,1}\\\\s+%s'%(ref.get('first_author',''),ref.get('year',''))\n",
    "                        else:\n",
    "                            regex = '%s( and [A-Za-z]+|,|\\\\s+et al\\\\.\\\\,|\\\\s+et al){0,1}\\\\s+(19|20)[0-9][0-9]'%(ref.get('first_author',''))\n",
    "                        pattern = re.compile(regex)\n",
    "                        if pattern.search(so_text):\n",
    "                            so_text = pattern.sub(ref_text, so_text)\n",
    "                        #print( pattern.sub(ref_text,so_text))\n",
    "                \n",
    "                tuple = (self.ft_id, local_id, so.element.tag, top_sec_title, so.start, (so.end-so.start), figure_reference, so_text)\n",
    "                text_tuples.append(tuple)\n",
    "              \n",
    "        except etree.XMLSyntaxError as xmlErr:\n",
    "            print(\"XML Syntax Error: {0}\".format(xmlErr))\n",
    "        except UnicodeDecodeError as unicodeErr:\n",
    "            print(\"Unicode parsing Error: {0}\".format(unicodeErr))\n",
    "        #except TypeError as typeErr:\n",
    "        #  print(\"Type Error: {0}\".format(typeErr))  \n",
    "        #    print(\"ValueError: {0}\".format(valErr))\n",
    "        #    return None\n",
    "        \n",
    "        text_df = pd.DataFrame(text_tuples, columns=['PMID', 'PARAGRAPH_ID', 'TAG', 'TAG_TREE', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'])\n",
    "        return text_df\n",
    "    \n",
    "    def extract_ref_dict_from_nxml(self):\n",
    "\n",
    "        if self.xml is None:\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(self.xml, \"lxml\")\n",
    "\n",
    "        references = soup.find_all('ref')\n",
    "        all_ref_dict = {}\n",
    "        for r in references:\n",
    "            ref_dict = {}\n",
    "            ref_dict['ref'] = r.attrs.get('id')\n",
    "\n",
    "            ref_dict['author'] = ''\n",
    "            for t in r.descendants:\n",
    "                if type(t) is Tag and t.name == 'surname' and ref_dict.get('first_author', None) is None:\n",
    "                    ref_dict['first_author'] = re.sub(\"'\",\"''\",t.text)\n",
    "                if type(t) is Tag and t.name == 'surname' and ref_dict.get('first_author', None) is not None:\n",
    "                    ref_dict['second_author'] = re.sub(\"'\",\"''\",t.text)\n",
    "                if type(t) is Tag and t.name == 'name' and len(ref_dict.get('author')) > 0:\n",
    "                    ref_dict['author'] += ', '\n",
    "                if type(t) is Tag and t.name == 'surname':\n",
    "                    ref_dict['author'] += re.sub(\"'\",\"''\",t.text)\n",
    "                if type(t) is Tag and t.name == 'given-names':\n",
    "                    ref_dict['author'] += ' ' + re.sub(\"'\",\"''\",t.text)\n",
    "                elif type(t) is Tag and t.name == 'article-title':\n",
    "                    ref_dict['title'] = re.sub(\"'\",\"''\",t.text)\n",
    "                elif type(t) is Tag and t.name == 'source':\n",
    "                    ref_dict['journal'] = t.text\n",
    "                elif type(t) is Tag and t.name == 'year':\n",
    "                    m = re.match('(\\\\d\\\\d\\\\d\\\\d)', t.text)\n",
    "                    if m:\n",
    "                        ref_dict['year'] = m.group(1)\n",
    "                elif type(t) is Tag and t.name == 'volume':\n",
    "                    ref_dict['vol'] = t.text\n",
    "                elif type(t) is Tag and t.name == 'fpage':\n",
    "                    ref_dict['page'] = t.text\n",
    "                    \n",
    "            all_ref_dict[ref_dict.get('ref')] = ref_dict\n",
    "        \n",
    "        # Search pubmed for the PMIDs\n",
    "        # Need to add this back to the dict but cannot link data from pubmed ids to the original references\n",
    "        '''\n",
    "        clauses = []\n",
    "        for r in all_ref_dict: \n",
    "            ref_dict = all_ref_dict[r]\n",
    "            if ref_dict.get('first_author', None) is not None and \\\n",
    "                    ref_dict.get('year', None) is not None and \\\n",
    "                    ref_dict.get('vol', None) is not None and \\\n",
    "                    ref_dict.get('page', None) is not None:\n",
    "                c = \"(%s[au]+AND+%s[dp]+AND+%s[vi]+AND+%s[pg]')\"%(\n",
    "                        ref_dict.get('first_author'),\n",
    "                        ref_dict.get('year'), \n",
    "                        ref_dict.get('vol'), \n",
    "                        ref_dict.get('page')\n",
    "                    )     \n",
    "                clauses.append(c)\n",
    "        \n",
    "        if len(clauses)==0:\n",
    "            return all_ref_dict\n",
    "\n",
    "        stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+pubmed_api_key\n",
    "        pmids = []\n",
    "        for i in range(0, len(clauses), 50):\n",
    "            query = '+OR+'.join(clauses[i:i+50])\n",
    "            r = requests.get(stem+'&db=pubmed&term='+query)\n",
    "            soup2 = BeautifulSoup(r.text, \"lxml\")\n",
    "            for id in soup2.find_all('id'):\n",
    "                pmids.append(id.text)\n",
    "\n",
    "        # Search pubmed for the PMIDs\n",
    "        # query pubmed for the pmid\n",
    "        \n",
    "        #print('\\n'+lookup_sql+'\\n\\n')\n",
    "        lookup_df = execute_query(cs, lookup_sql, ['PMID', 'FIRST_AUTHOR', 'YEAR', 'VOLUME', 'PAGE'])\n",
    "        lookup = {('%s-%s-%s-%s'%(row.FIRST_AUTHOR, row.YEAR, row.VOLUME, row.PAGE)).lower():row.PMID for row in lookup_df.itertuples()}      \n",
    "        \n",
    "        #print('doc: ', end = '')\n",
    "        for r in all_ref_dict: \n",
    "            ref_dict = all_ref_dict[r]\n",
    "            if ref_dict.get('first_author', None) is not None and ref_dict.get('year', None) is not None and ref_dict.get('vol', None) is not None and ref_dict.get('page', None) is not None :\n",
    "            au = ref_dict['first_author']\n",
    "            dp = ref_dict['year']\n",
    "            vo = ref_dict['vol']\n",
    "            pg = ref_dict['page']\n",
    "            pmid = lookup.get(('%s-%s-%s-%s'%(au, dp, vo, pg)).lower(), None)\n",
    "            if pmid: \n",
    "                ref_dict['pmid'] = pmid\n",
    "                #print('.', end = '')\n",
    "        #print()\n",
    "        '''\n",
    "        \n",
    "        return all_ref_dict\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
