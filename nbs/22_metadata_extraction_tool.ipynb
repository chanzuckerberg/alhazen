{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods Metadata Extraction Tool   \n",
    "\n",
    "> Langchain tools that execute zero-shot extraction over a local database of full text papers previously imported into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.metadata_extraction_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "\n",
    "from alhazen.core import OllamaRunner, PromptTemplateRegistry, get_langchain_llm, get_cached_gguf, \\\n",
    "    get_langchain_embeddings, GGUF_LOOKUP_URL, MODEL_TYPE\n",
    "from alhazen.tools.basic import AlhazenToolMixin\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "\n",
    "from alhazen.utils.ceifns_db import *\n",
    "\n",
    "from alhazen.schema_sqla import ScientificKnowledgeCollection, ScientificKnowledgeExpression, \\\n",
    "    ScientificKnowledgeFragment, Note, ScientificKnowledgeCollection, \\\n",
    "    ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \\\n",
    "    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n",
    "    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n",
    "    InformationResource\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.schema import OutputParserException\n",
    "from langchain.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "\n",
    "from importlib_resources import files\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from sqlalchemy import create_engine, exists\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from time import time,sleep\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MetadataExtractionToolSchema(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "    name: str = Field(description=\"should be the name of the collection we will add papers to\")\n",
    "\n",
    "class MetadataExtractionTool(BaseTool, AlhazenToolMixin):\n",
    "    '''Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'''\n",
    "    name = 'metadata_extraction'\n",
    "    description = 'Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'\n",
    "    args_schema = MetadataExtractionToolSchema\n",
    "    \n",
    "    def _run(self, paper_id, section_name, extraction_type, item_type='JATSFullText'):\n",
    "        '''Runs the metadata extraction pipeline over a specified paper.'''\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()\n",
    "\n",
    "        ske = self.db.session.query(ScientificKnowledgeExpression) \\\n",
    "                .filter(ScientificKnowledgeExpression.id.like('%'+paper_id+'%')).first()    \n",
    "\n",
    "        # 1. Load the text of the paper from the local database\n",
    "        text = '\\n'.join([f.content for f in self.db.list_fragments_for_paper(paper_id, item_type) if section_name in f.name.lower()])\n",
    "\n",
    "        # 2. Build LangChain elements\n",
    "        pts = PromptTemplateRegistry()\n",
    "        pts.load_prompts_from_yaml('metadata_extraction.yaml')\n",
    "        prompt_elements_yaml = files(prompt_elements).joinpath('metadata_extraction.yaml').read_text()\n",
    "        prompt_elements_dict = yaml.safe_load(prompt_elements_yaml).get(extraction_type)\n",
    "        method_goal = prompt_elements_dict['method goal']\n",
    "        methodology = prompt_elements_dict['methodology']\n",
    "        metadata_specs = prompt_elements_dict.get('metadata specs',[])\n",
    "        metadata_extraction_prompt_template = pts.get_prompt_template('metadata extraction').generate_llama2_prompt_template()\n",
    "        run_name = 'metadata_extraction_' + re.sub(' ','_',extraction_type) + ':' + paper_id\n",
    "        extract_lcel = metadata_extraction_prompt_template | self.llm | JsonEnclosedByTextOutputParser()\n",
    "\n",
    "        # 3. Compile the extraction questions\n",
    "        question_text_list = [(\"%d. %s Record this value in the '%s' field of the output.\"\n",
    "                                \"Record any supporting sentences from the section text in the\"\n",
    "                                \" '%s_original_text' field of the output.\")\n",
    "                                %(i+1, spec.get('spec'), spec.get('name'), spec.get('name')) \n",
    "                                for i, spec in enumerate(metadata_specs)]\n",
    "        questions_output_specification = '\\n'.join(question_text_list)\n",
    "        questions_output_specification += '\\nGenerate only JSON formatted output with %d fields:\\n'%(len(metadata_specs)*2)\n",
    "        questions_output_specification += \", \".join(['%s, %s_original_text'%(spec.get('name'), spec.get('name')) for spec in metadata_specs])\n",
    "\n",
    "        # 4. Assemble chain input\n",
    "        s1 = {'section_text': text,\n",
    "                'methodology': methodology,\n",
    "                'method_goal': method_goal,\n",
    "                'questions_output_specification': questions_output_specification}\n",
    "        \n",
    "        # 5. Run the chain using the Ollama runner with a JsonEnclosedByTextOutputParser failsafe loop \n",
    "        output = None\n",
    "        attempts = 0\n",
    "        while output is None and attempts < 5:\n",
    "            try: \n",
    "                #with suppress_stdout_stderr():\n",
    "                output = extract_lcel.invoke(s1, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "                if output is None:\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "\n",
    "                for spec in metadata_specs:\n",
    "                    if spec.get('name') in output is False or spec.get('name')+'_original_text' in output is False:\n",
    "                        continue\n",
    "                    vname = spec.get('name')\n",
    "                    question = spec.get('spec')\n",
    "                    answer = output.get(vname) \n",
    "                    original_text = output.get(vname+'_original_text')\n",
    "                    note_content = json.dumps({\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'original_text': original_text}, indent=4)\n",
    "                    # add a note to the fragment\n",
    "                    n = Note(\n",
    "                        id=uuid.uuid4().hex[0:10],\n",
    "                        type='NoteAboutExpression', \n",
    "                        name=run_name+':'+vname,\n",
    "                        content=note_content, \n",
    "                        creation_date=datetime.now(), \n",
    "                        format='json')\n",
    "                    n.is_about.append(ske)\n",
    "                    self.db.session.add(n)\n",
    "                    self.db.session.flush()\n",
    "                else:                     \n",
    "                    attempts += 1\n",
    "            except OutputParserException as e:\n",
    "                attempts += 1\n",
    "                print(e) \n",
    "                print('Retrying...')\n",
    "                    \n",
    "        # commit the changes to the database\n",
    "        self.db.session.commit()\n",
    "        \n",
    "        return \"Final Answer: completed metadata extraction of an experiment of type '%s' from %s.\"%(methodology, paper_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to execute the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from alhazen.core import OllamaRunner\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb\n",
    "from alhazen.tools.basic import EMPCSearchTool \n",
    "from alhazen.tools.metadata_extraction_tool import MetadataExtractionTool \n",
    "from alhazen.toolkit import AlhazenToolkit\n",
    "import os\n",
    "\n",
    "# This is the directory where all Alhazen data will be stored\n",
    "os.environ['LOCAL_FILE_PATH'] = '/tmp/alhazen/'\n",
    "\n",
    "# This is the name of the database used for this application. \n",
    "# This will be (A) a postgres database, and (B) a subdirectory of the above folder where the system will store\n",
    "# local files (such as full text papers, ontology files, etc.)\n",
    "os.environ['DATABASE_NAME'] = 'em_tech'\n",
    "\n",
    "# This sets up the toolkit \n",
    "db = Ceifns_LiteratureDb(loc=os.environ['LOCAL_FILE_PATH'], name=os.environ['DATABASE_NAME'])\n",
    "ollr = OllamaRunner('mixtral')\n",
    "llm  = ollr.llm\n",
    "tk = AlhazenToolkit(db=db, ollr=ollr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = [t for t in tk.get_tools() if isinstance(t,MetadataExtractionTool)][0]\n",
    "t.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will call a predefined set of 20 questions to be asked \n",
    "# about de Teresa et al. 2022 (https://www.biorxiv.org/content/10.1101/2022.04.12.488077v1)\n",
    "#doi = '10.1101/2022.04.12.488077'\n",
    "#t.run(tool_input={'paper_id':doi, 'section_name':'method', 'extraction_type':'cryoet'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
