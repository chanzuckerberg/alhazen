{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol Workflow Extraction Tool   \n",
    "\n",
    "> Langchain tools that execute zero-shot extraction over a local database of full text papers previously imported into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.protocol_extraction_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.core import OllamaRunner, PromptTemplateRegistry, get_langchain_llm, get_cached_gguf, \\\n",
    "    get_langchain_embeddings, GGUF_LOOKUP_URL, MODEL_TYPE, load_alhazen_tool_environment, get_langchain_chatmodel\n",
    "from alhazen.tools.basic import AlhazenToolMixin\n",
    "from alhazen.utils.output_parsers import *\n",
    "from alhazen.utils.ceifns_db import *\n",
    "from alhazen.schema_sqla import *\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import jmespath\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain.schema import get_buffer_string, StrOutputParser, OutputParserException, format_document\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "import local_resources.linkml as linkml\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "from sqlalchemy import create_engine, exists\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "from time import time,sleep\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "skc = aliased(ScientificKnowledgeCollection)\n",
    "skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "ske = aliased(ScientificKnowledgeExpression)\n",
    "ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "ski = aliased(ScientificKnowledgeItem)\n",
    "ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "skf = aliased(ScientificKnowledgeFragment)\n",
    "n = aliased(Note)\n",
    "skc_hn = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "ske_hn = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "ski_hn = aliased(ScientificKnowledgeItemHasNotes)\n",
    "skf_hn = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ProtocolExtractionToolSchema(BaseModel):\n",
    "    paper_id: str = Field(description=\"the doi of the paper being analyzed, must start with the string 'doi:'\")\n",
    "    extraction_type: str = Field(description=\"This is the name of the type of extraction and must be one of the following strings: ['cryoet']\")\n",
    "\n",
    "class BaseProtocolExtractionTool(BaseTool, AlhazenToolMixin):\n",
    "    '''Runs a specified protocol extraction pipeline over a research paper that has been loaded in the local literature database.'''\n",
    "    name = 'protocol_workflow_extraction'\n",
    "    description = 'Runs a specified protocol extraction pipeline over a research paper that has been loaded in the local literature database.'\n",
    "    args_schema = ProtocolExtractionToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(self, paper_id, extraction_type):\n",
    "        '''Runs a specified protocol extraction pipeline over a research paper that has been loaded in the local literature database.'''\n",
    "        raise NotImplementedError('This method must be implemented by a subclass.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProcotolExtractionTool(BaseProtocolExtractionTool):\n",
    "    '''Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'''\n",
    "    name = 'protocol_extraction_over_methods_section'\n",
    "    description = 'Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'\n",
    "\n",
    "    def _run(self, paper_id, extraction_type):\n",
    "        '''Runs the metadata extraction pipeline over a specified paper.'''\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()\n",
    "\n",
    "        ske = self.db.session.query(ScientificKnowledgeExpression) \\\n",
    "                .filter(ScientificKnowledgeExpression.id.like('%'+paper_id+'%')).first()    \n",
    "\n",
    "        # Introspect the class name of the llm model for notes and logging\n",
    "        llm_class_name = self.llm.__class__.__name__\n",
    "        llm_model_desc = str(self.llm)\n",
    "\n",
    "        run_name = self.__class__.__name__ + '__' + re.sub(' ','_',extraction_type) + '__' + paper_id + '__' + llm_class_name + '__' + llm_model_desc\n",
    "\n",
    "        # 0. Use the first available full text item type\n",
    "        item_types = set()\n",
    "        item_type = None\n",
    "        for i in self.db.list_items_for_expression(paper_id):\n",
    "            item_types.add(i.type)\n",
    "        for i_type in item_types:\n",
    "            if i_type == 'CitationRecord':\n",
    "                continue\n",
    "            item_type = i_type\n",
    "            break\n",
    "        if item_type is None:\n",
    "            return {'report': \"Could not retrieve full text of the paper: %s.\"%(paper_id),\n",
    "                \"data\": {'list_of_answers': None, 'run_name': run_name} }\n",
    "\n",
    "        # 1. Build LangChain elements\n",
    "        pts = PromptTemplateRegistry()\n",
    "        \n",
    "        pts.load_prompts_from_yaml('protocol_extraction.yaml')\n",
    "        prompt_elements_yaml = files(prompt_elements).joinpath('metadata_extraction.yaml').read_text()\n",
    "        prompt_elements_dict = yaml.safe_load(prompt_elements_yaml).get(extraction_type)\n",
    "        all_protocol_steps = prompt_elements_dict['all protocol steps']\n",
    "        all_entities = prompt_elements_dict['all entities']\n",
    "        method_goal = prompt_elements_dict['method goal']\n",
    "        methodology = prompt_elements_dict['methodology']\n",
    "        metadata_specs = prompt_elements_dict.get('metadata specs',[])\n",
    "        metadata_extraction_prompt_template = pts.get_prompt_template('protocol diagram extraction').generate_chat_prompt_template()\n",
    "        extract_lcel = metadata_extraction_prompt_template | self.llm | MermaidExtractionOutputParser()\n",
    "        \n",
    "        # 2. Run through all available sections of the paper and identify only those that are predominantly methods sections.\n",
    "        start = datetime.now()\n",
    "        text = '\\n\\n'.join([f.content for f in self.db.list_fragments_for_paper(paper_id, item_type, fragment_types=['section'])])\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return {'report': \"Could not retrieve full text of the paper: %s.\"%(paper_id),\n",
    "                \"data\": {'list_of_answers': None, 'run_name': run_name} }\n",
    "\n",
    "        # 4. Assemble chain input\n",
    "        s1 = {'section_text': text,\n",
    "                'methodology': methodology,\n",
    "                'method_goal': method_goal}\n",
    "        \n",
    "        # 5. Run the chain with a MermaidExtractionOutputParser \n",
    "        output = None\n",
    "        full_answer = []\n",
    "        output = extract_lcel.invoke(s1, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "        total_execution_time = datetime.now() - start\n",
    "        time_per_variable = total_execution_time / len(metadata_specs)\n",
    "\n",
    "        if output is None:\n",
    "            return {'report': \"attempted and failed protocol extraction for an experiment of type '%s' from %s.\"%(methodology, paper_id),\n",
    "                \"data\": {'mermaid_code': None, 'run_name': run_name} }\n",
    "        \n",
    "        return {'report': \"completed protocol extraction for an experiment of type '%s' from %s.\"%(methodology, paper_id),\n",
    "                \"data\": {'mermaid_code': output, 'run_name': run_name} }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
