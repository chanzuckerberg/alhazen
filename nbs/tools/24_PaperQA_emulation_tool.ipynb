{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper QA Emulation Tool   \n",
    "\n",
    "> We here emulate the workflow of the PaperQA system by Andrew White (https://thewhitelab.org/). This is a Retrieval Augmented Generation (RAG) application using a Map-Reduce model where we query our embedded index for based on a question, we then write summaries for each returned document based on relevance to the underlying question. Finally, we synthesize each summary into an essay presented as an answer to the original question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.paperqa_emulation_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.core import PromptTemplateRegistry, load_alhazen_tool_environment\n",
    "from alhazen.tools.basic import AlhazenToolMixin\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "from alhazen.utils.ceifns_db import *\n",
    "from alhazen.schema_sqla import *\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain.schema import get_buffer_string, StrOutputParser, OutputParserException, format_document\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "import local_resources.linkml as linkml\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import re\n",
    "import regex\n",
    "from sqlalchemy import create_engine, exists\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "from time import time,sleep\n",
    "from typing import Optional, Type\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "skc = aliased(ScientificKnowledgeCollection)\n",
    "skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "ske = aliased(ScientificKnowledgeExpression)\n",
    "ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "ski = aliased(ScientificKnowledgeItem)\n",
    "ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "skf = aliased(ScientificKnowledgeFragment)\n",
    "n = aliased(Note)\n",
    "skc_hn = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "ske_hn = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "ski_hn = aliased(ScientificKnowledgeItemHasNotes)\n",
    "skf_hn = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PaperQAEmulationToolSchema(BaseModel):\n",
    "    question: str = Field(description=\"The question to be considered by this workflow.\")\n",
    "    n_sample_size: Optional[int] = Field(description=\"The number of documents queried from the index to be evaluated for relevance.\")\n",
    "    n_summary_size: Optional[int] = Field(description=\"The number of documents queried from the index to be used in generating an answer.\")\n",
    "    collection_id: Optional[int] = Field(None, description=\"The identifier of the collection to be used to answer the question.\")\n",
    "\n",
    "class PaperQAEmulationTool(BaseTool, AlhazenToolMixin):\n",
    "    '''Write a short essay to answer a scientific question based documents from a preset collection.'''\n",
    "    name = 'simple_qa_over_papers'\n",
    "    description = '''Runs a Map-Reduce model where we write a short essay to answer a scientific question based on a set of supporting documents.'''\n",
    "    args_schema = PaperQAEmulationToolSchema\n",
    "    return_direct:bool = True\n",
    "    \n",
    "    def _run(self, question, n_sample_size=15, n_summary_size=5, collection_id=-1):\n",
    "        '''Runs the metadata extraction pipeline over a specified paper.'''\n",
    "        \n",
    "        DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template='\"DOI\": \"{e_id}\",\"CITATION\": \"{citation}\", \"CONTENT\":\"{page_content}\"')\n",
    "        def _combine_documents(docs):\n",
    "            m = [{'CONTENT':d.page_content, 'DOI':d.metadata.get('e_id'), \"CITATION\": d.metadata.get('citation')} for d in docs]\n",
    "            return json.dumps(m)\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 1. Set up environment\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~\n",
    "        loc, db_name = load_alhazen_tool_environment()\n",
    "\n",
    "        os.environ['PGVECTOR_CONNECTION_STRING'] = \"postgresql+psycopg2:///\"+db_name\n",
    "            \n",
    "        vectorstore = PGVector.from_existing_index(\n",
    "                embedding = self.db.embed_model, \n",
    "                collection_name = 'ScienceKnowledgeItem') \n",
    "        \n",
    "        # set default values for optional parameters\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={'k':n_sample_size})\n",
    "        if collection_id != -1:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={'k':n_sample_size, 'filter': {'c_ids': collection_id}})\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 2. Run Map Function - Iterate over 'pages' of Documents returned from vectorstore to generate \n",
    "        #                       summaries\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                                \n",
    "        context_build_chain = (\n",
    "            RunnableParallel({\n",
    "                \"k\": itemgetter(\"k\"),\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"context\": itemgetter(\"question\") | retriever | _combine_documents \n",
    "            }\n",
    "        ))\n",
    "\n",
    "        pts = PromptTemplateRegistry()\n",
    "        pts.load_prompts_from_yaml('paperqa_emulation.yaml')\n",
    "        pt = pts.get_prompt_template('summarize paper set').generate_chat_prompt_template()\n",
    "        \n",
    "        summary_chain = (\n",
    "            RunnableParallel({\n",
    "                \"k\": itemgetter(\"k\"),\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"summary_length\": itemgetter(\"summary_length\"),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            })\n",
    "            | {\n",
    "                \"summary\": pt | self.llm | JsonEnclosedByTextOutputParser(),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        input = {'question': question, 'summary_length': 1000, 'k':5}    \n",
    "        out = context_build_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "        context = json.loads(out.get('context'))\n",
    "        citation_lookup = {d.get('DOI'):d.get('CITATION') for d in context}\n",
    "\n",
    "        page_size = 3\n",
    "        page_count = int(n_sample_size / page_size)\n",
    "        summaries = []\n",
    "        for pg in range(page_count):\n",
    "            paged_context = []\n",
    "            paged_doi_lookup = {}\n",
    "            paged_citation_lookup = {}\n",
    "            for i,j in enumerate(range(pg*page_size, (pg+1)*page_size)):\n",
    "                c = {k:context[j][k] for k in context[j] if k!='DOI'}\n",
    "                c['ID'] = i + 1\n",
    "                paged_context.append(c)\n",
    "                paged_citation_lookup[i+1] = context[j]['CITATION']\n",
    "                paged_doi_lookup[i+1] = context[j]['DOI']\n",
    "            paged_input = input.copy()\n",
    "            paged_input['context'] = json.dumps(paged_context)\n",
    "            out2 = summary_chain.invoke(paged_input, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "            summs = out2.get('summary', [])\n",
    "            if summs:\n",
    "                for m in out2.get('summary', []):\n",
    "                    id = m.get('ID')\n",
    "                    if paged_citation_lookup.get(id):\n",
    "                        n = {}\n",
    "                        n['SUMMARY'] = m['SUMMARY']\n",
    "                        n['RELEVANCE SCORE'] = m['RELEVANCE SCORE']\n",
    "                        n['CITATION'] = paged_citation_lookup[id]\n",
    "                        n['DOI'] = paged_doi_lookup[id]\n",
    "                        summaries.append(n)\n",
    "                        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 3. Order sumaries in terms of 'RELEVANCE'\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        ordered_summaries = []\n",
    "\n",
    "        # Check to make sure that RELEVANCE SCORES generated by the LLM are integers\n",
    "        checked_summaries = []\n",
    "        for s in summaries:\n",
    "            rel_score = s.get('RELEVANCE SCORE', 'Not Applicable')\n",
    "            try: \n",
    "                check = int(rel_score)\n",
    "                checked_summaries.append( s )\n",
    "            except ValueError:\n",
    "                skip_this = 0\n",
    "\n",
    "        doi_lookup = {}\n",
    "        for i, s in enumerate(sorted(checked_summaries, key=lambda x: int(x['RELEVANCE SCORE']), reverse=True)):\n",
    "            if int(s['RELEVANCE SCORE']) < 7:\n",
    "                continue\n",
    "            s['ID'] = i+1\n",
    "            doi_lookup[i+1] = s.get('DOI')\n",
    "            ordered_summaries.append({'ID':s.get('ID'),\n",
    "                                        'CITATION': s.get('CITATION'), \n",
    "                                        'SUMMARY': s.get('SUMMARY'), \n",
    "                                        'RELEVANCE': s.get('RELEVANCE SCORE')})\n",
    "            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 4. Run Reduce Function - Write the synthesis over summaries provided\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        pt2 = pts.get_prompt_template('write synthesis').generate_chat_prompt_template()\n",
    "\n",
    "        qa_synthesis_chain = (\n",
    "            RunnableParallel({\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            })\n",
    "            | {\n",
    "                \"answer\": pt2 | self.llm,\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        input = {'question': question, 'context': ordered_summaries[:n_summary_size]}    \n",
    "        out3 = qa_synthesis_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "\n",
    "        essay = out3.get('answer').content\n",
    "        essay += \"\\n\\n\\nREFERENCES\\n\" \n",
    "        essay += '\\n'.join(['[%d] %s (%s)'%(s['ID'],s['CITATION'],doi_lookup[s['ID']]) for s in ordered_summaries[:n_summary_size]])\n",
    "        \n",
    "        dois_to_record = [doi_lookup[s['ID']] for s in ordered_summaries[:n_summary_size]]\n",
    "\n",
    "        if collection_id != -1:\n",
    "            response = {'response': \"I answered this question: `%s` based on content from the collection with id: %s.\"%(question, collection_id),\n",
    "                    \"data\": essay }\n",
    "        else:\n",
    "            response = {'response': \"I answered this question: `%s` based on all content in our database. \"%(question),\n",
    "                    \"data\": essay }\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 5. Add Question + Notes to represent the Questions / Answer Generation\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        prov1 = json.dumps([{'action_type': 'user_input', \n",
    "                    'action': {'question': question}}])\n",
    "        q = UserQuestion(id=uuid.uuid4().hex[0:10],\n",
    "                    content=question,\n",
    "                    provenance=prov1,\n",
    "                    format='json',\n",
    "                    type='NoteAboutCollection')\n",
    "        self.db.session.add(q)\n",
    "        prov = json.dumps([{'action_type': self.name, \n",
    "                            'action': {'id': id, \n",
    "                                       'n_sample_size': n_sample_size, \n",
    "                                       'n_summary_size': n_summary_size, \n",
    "                                       'collection_id': collection_id,\n",
    "                                       'question': question}}])            \n",
    "        n = Note(id=uuid.uuid4().hex[0:10],\n",
    "                name='skc:%s.counts'%(id),\n",
    "                content=json.dumps(response),\n",
    "                provenance=prov,\n",
    "                format='json',\n",
    "                type='NoteAboutFragment')\n",
    "        self.db.session.add(n)\n",
    "        \n",
    "        if collection_id != -1:\n",
    "            c = self.db.session.query(SKC).filter(SKC.id == collection_id).all()[0]\n",
    "            n.is_about.append(c)\n",
    "            c.has_notes.append(n)\n",
    "\n",
    "        q.has_notes.append(n)\n",
    "        for doi in dois_to_record:\n",
    "            t, a = self.db.list_fragments_for_paper(doi, 'CitationRecord')\n",
    "            t.has_notes.append(n)\n",
    "            a.has_notes.append(n)\n",
    "        \n",
    "        self.db.session.commit()\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
