{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title / Abstract Extraction Tool   \n",
    "\n",
    "> Langchain tools that execute zero-shot extraction tasks over a local database of title/abstracts from papers previously imported into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.tiab_extraction_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.aliases import *\n",
    "from alhazen.core import PromptTemplateRegistry\n",
    "from alhazen.tools.basic import AlhazenToolMixin\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "from alhazen.utils.ceifns_db import *\n",
    "from alhazen.schema_sqla import *\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import jmespath\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain.schema import get_buffer_string, StrOutputParser, OutputParserException, format_document\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "import local_resources.linkml as linkml\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "from sqlalchemy import create_engine, exists, func, or_, desc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "from time import time,sleep\n",
    "import tiktoken\n",
    "from typing import Optional, Type\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TitleAbstractExtractionToolSchema(BaseModel):\n",
    "    collection_id: str = Field(description=\"the id of the collection being analyzed.\")\n",
    "    domain: str = Field(description=\"This is the name of the domain of study. It is typically very broad.\")\n",
    "    extraction_type: str = Field(description=\"This is the name of the type of extraction to be performed.  It is used to select the appropriate pipeline for the extraction.  It is also used to select the appropriate prompt for the extraction.\")\n",
    "    repeat_run: Optional[bool] = Field(description=\"Whether or not to repeat the classification task if it has been performed before. Defaults to False.\")\n",
    "    \n",
    "class BaseTitleAbstractExtractionTool(BaseTool, AlhazenToolMixin):\n",
    "    '''Runs a specified document classification pipeline over papers in a collection.'''\n",
    "    name = 'tiab_classification'\n",
    "    description = 'Runs a specified document classification pipeline over papers in a collection.'\n",
    "    args_schema = TitleAbstractExtractionToolSchema\n",
    "    return_direct:bool = True\n",
    "    prompt_name:str = 'binary methods paper'\n",
    "    examples = {}\n",
    "\n",
    "    def _run(self, collection_id, domain, extraction_type, repeat_run=False):\n",
    "        '''Runs a specified information extraction pipeline over papers in a collection.'''\n",
    "        raise NotImplementedError('This method must be implemented by a subclass.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TitleAbstractExtraction_OneDocAtATime_Tool(BaseTitleAbstractExtractionTool):\n",
    "    '''Runs a specified document classification pipeline over papers in a collection.'''\n",
    "    name = 'tiab_one_doc_classification'\n",
    "    description = 'Runs a specified document classification pipeline over papers in a collection by running a simple classifier over the text of each title + abstract.'\n",
    "\n",
    "    def _run(self, collection_id, extraction_type, repeat_run=False):\n",
    "        '''Runs a document classification pipeline over papers in a collection one paper at a time.'''\n",
    "    \n",
    "        output_list = []\n",
    "\n",
    "        # Introspect the class name of the llm model for notes and logging\n",
    "        llm_class_name = self.llm.__class__.__name__\n",
    "\n",
    "        run_name = self.__class__.__name__+'__'+re.sub(' ','_',extraction_type)+'__'+collection_id+'__'+llm_class_name\n",
    "\n",
    "        exp_q = self.db.session.query(SKE) \\\n",
    "                .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "                .filter(SKC_HM.ScientificKnowledgeCollection_id == str(collection_id)) \\\n",
    "                .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "                .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "                .filter(SKI.type=='CitationRecord') \\\n",
    "                .order_by(desc(SKE.publication_date))\n",
    "        for e in tqdm(exp_q.all()):\n",
    "        \n",
    "            # skip expressions if they have this type of note already.\n",
    "            if repeat_run is False: \n",
    "                repeat_flag = False\n",
    "                for n in self.db.read_notes_about_x(e):\n",
    "                    if n.type == 'TiAbExtractionNote__'+extraction_type:\n",
    "                        repeat_flag = True\n",
    "                        break\n",
    "                if repeat_flag:\n",
    "                    continue\n",
    "\n",
    "            # 1. Build LangChain elements\n",
    "            pts = PromptTemplateRegistry()\n",
    "            pts.load_prompts_from_yaml('tiab_prompts.yaml')\n",
    "\n",
    "            prompt_elements_yaml = files(prompt_elements).joinpath('metadata_extraction.yaml').read_text()\n",
    "            prompt_elements_dict = yaml.safe_load(prompt_elements_yaml).get(extraction_type)\n",
    "            extraction_specs = prompt_elements_dict.get('tiab metadata specs',[])\n",
    "\n",
    "            tiab_extraction_prompt_template = pts.get_prompt_template('multiquestion tiab extraction').generate_chat_prompt_template()\n",
    "            extract_lcel = tiab_extraction_prompt_template | self.llm | JsonOutputParser()        \n",
    "            \n",
    "            # 2. Run through all available sections of the paper and identify only those that are predominantly methods sections.\n",
    "            start = datetime.now()\n",
    "            title, abstract = [f.content for f in self.db.list_fragments_for_paper(e.id, 'CitationRecord')]\n",
    "\n",
    "            if len(title + abstract) == 0:\n",
    "                continue\n",
    "\n",
    "            # 3. Compile the extraction questions\n",
    "            question_text_list = [(\"%d. %s Record this value in the '%s' field of the output.\")\n",
    "                                %(i+1, spec.get('spec'), spec.get('name')) \n",
    "                                for i, spec in enumerate(extraction_specs)]\n",
    "            questions_output_specification = '\\n'.join(question_text_list)\n",
    "            questions_output_specification += '\\nGenerate only JSON formatted output with %d fields:\\n'%(len(extraction_specs))\n",
    "            questions_output_specification += \", \".join([spec.get('name') for spec in extraction_specs])\n",
    "\n",
    "            # 4. Assemble chain input\n",
    "            s1 = {'title': title,\n",
    "                'abstract': abstract,\n",
    "                'questions_output_specification': questions_output_specification}\n",
    "        \n",
    "            # 4. Run the chain with a JsonEnclosedByTextOutputParser \n",
    "            try: \n",
    "                output = extract_lcel.invoke(s1)#, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "            except Exception as ex:\n",
    "                # Note that the LLM may raise an exception if it is unable to classify the document.\n",
    "                print(ex)\n",
    "                output = \"ERROR: Unable to classify document.\"\n",
    "                continue\n",
    "            \n",
    "            total_execution_time = datetime.now() - start\n",
    "\n",
    "            output_list.append({'extraction':output, 'paper_id':e.id})\n",
    "            \n",
    "            # 5. add a note to the fragment\n",
    "            if output is not None:\n",
    "\n",
    "                # Sometime the LLM generates a list. If so, we just take the first item.\n",
    "                if isinstance(output, list):\n",
    "                    output = output[0]\n",
    "\n",
    "            n = Note(\n",
    "                id=uuid.uuid4().hex[0:10],\n",
    "                type='TiAbExtractionNote__'+extraction_type, \n",
    "                name=run_name+e.id,\n",
    "                provenance='time_taken: %s'%(str(total_execution_time)),\n",
    "                content=json.dumps(output, indent=4), \n",
    "                creation_date=datetime.now(), \n",
    "                format='json')\n",
    "            self.db.session.add(n)\n",
    "            n.is_about.append(e)\n",
    "            self.db.session.commit()\n",
    "                            \n",
    "        return {'response': \"completed document extraction of type '%s' for collection %s.\"%(classification_type, collection_id),\n",
    "                \"data\": output_list, \n",
    "                'run_name': run_name}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
