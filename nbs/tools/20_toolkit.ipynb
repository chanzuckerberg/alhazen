{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alhazen Toolkit   \n",
    "\n",
    "> A set of Langchain tools that populate and query a CEIFNS database by (1) Build collections of expressions; (2) locate and load items that represent expressions; (3) segregate the parts of items as 'fragments'; (4) analyze the fragments to generate notes that can then be summarized to provide summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Toolkit for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "\n",
    "from alhazen.core import OllamaRunner\n",
    "from alhazen.tools.basic import *\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.protocol_extraction_tool import * \n",
    "from alhazen.tools.paperqa_emulation_tool import * \n",
    "from alhazen.tools.tiab_classifier_tool import * \n",
    "from alhazen.tools.tiab_extraction_tool import * \n",
    "from alhazen.tools.tiab_mapping_tool import * \n",
    "from alhazen.utils.ceifns_db import *\n",
    "\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.agents.agent import RunnableAgent\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.pydantic_v1 import BaseModel, Extra, Field, root_validator\n",
    "from langchain.schema.prompt_template import format_document\n",
    "\n",
    "from importlib_resources import files\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from typing import List, Any, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary Agent Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# NOTE - Use LangChain's SQL_DATABASE TOOLKIT AS A MODEL \n",
    "# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/agent_toolkits/sql/toolkit.py#L18\n",
    "#\n",
    "# Use environment variables to denote the database name + base file location\n",
    "# os.environ['ALHAZEN_DB_NAME'] = 'em_tech'\n",
    "# os.environ['LOCAL_FILE_PATH'] = '/Users/gburns/alhazen/'\n",
    "\n",
    "class AlhazenToolkit(BaseModel):\n",
    "    '''Toolkit for building and querying an Alhazen CEIFNS (pron. 'SAI-FiNS') database \n",
    "    (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).'''\n",
    "\n",
    "    # The local literature database (Collections, Expressions, Items, and Fragments)\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True)\n",
    "    llm : BaseChatModel = Field(exclude=True)\n",
    "    agent : Optional[RunnableAgent] = Field(exclude=True) \n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_tools(self) -> List[BaseTool]:\n",
    "        \"\"\"Get the tools in the toolkit.\"\"\"\n",
    "\n",
    "        add_collection_tool_description = (\n",
    "            \"This tool builds a new collection of scientific papers based on querying external databases.\"\n",
    "        )\n",
    "        add_collection_tool = AddCollectionFromEPMCTool(db=self.db, description=add_collection_tool_description)\n",
    "\n",
    "        add_authors_to_collection_tool_description = (\n",
    "            \"This tool adds author- and institute-level information to an existing collection of scientific papers.\"\n",
    "        )\n",
    "        add_authors_to_collection_tool = AddAuthorsToCollectionTool(db=self.db, description=add_authors_to_collection_tool_description)\n",
    "        \n",
    "        describe_collection_tool_description = (\n",
    "            \"This tool describes the contents of a collection in the database. \"\n",
    "        )\n",
    "        describe_collection_tool = DescribeCollectionCompositionTool(db=self.db, description=describe_collection_tool_description)\n",
    "\n",
    "        delete_collection_tool_description = (\n",
    "            \"This tool deletes a collection from the database.\"\n",
    "        )\n",
    "        delete_collection_tool = DeleteCollectionTool(db=self.db, description=delete_collection_tool_description)\n",
    "\n",
    "        check_expression_tool_description = (\n",
    "            \"This tool checks if the database contains a paper. \"\n",
    "        )\n",
    "        check_expression_tool = CheckExpressionTool(db=self.db, description=check_expression_tool_description)\n",
    "\n",
    "        retrieve_full_text_for_a_collection_tool_description = (\n",
    "            \"This tool retrieves full text versions of all papers in a collection.\"\n",
    "        )\n",
    "        retrieve_full_text_for_a_collection_tool = RetrieveFullTextToolForACollection(db=self.db, description=retrieve_full_text_for_a_collection_tool_description)\n",
    "\n",
    "        retrieve_full_text_tool_description = (\n",
    "            \"This tool retrieves the full text version for a single paper. This is specified by one parameter: paper_id, which MUST start with 'doi:'. \"\n",
    "        )\n",
    "        retrieve_full_text_tool = RetrieveFullTextTool(db=self.db, description=retrieve_full_text_tool_description)\n",
    "\n",
    "        metadata_extraction_tool_description = (\n",
    "            \"This tool extracts all metadata for a specified type of experiment from a single full text paper.\"\n",
    "        )\n",
    "        metadata_extraction_tool = MetadataExtraction_EverythingEverywhere_Tool(\n",
    "            db=self.db, llm=self.llm, description=metadata_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        methods_metadata_extraction_tool_description = (\n",
    "            \"This tool extracts all metadata for a specified type of experiment from the methods section of a single full text paper.\"\n",
    "        )\n",
    "        methods_metadata_extraction_tool = MetadataExtraction_MethodsSectionOnly_Tool(\n",
    "            db=self.db, llm=self.llm, description=methods_metadata_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        protocol_extraction_tool_description = (\n",
    "            \"This tool draws a flowchart of the protocol described in a single full text paper.\"\n",
    "        )\n",
    "        protocol_extraction_tool = ProcotolEntitiesExtractionTool(\n",
    "            db=self.db, llm=self.llm, description=protocol_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        simple_extraction_tool_description = (\n",
    "            \"This tool extracts information from a single full text paper based on a specific question.\"\n",
    "        )\n",
    "        simple_extraction_tool = SimpleExtractionWithRAGTool(\n",
    "            db=self.db, llm=self.llm, description=simple_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        paperqa_emulation_tool_description = (\n",
    "            \"This tool writes a short essay to answer a scientific question.\"\n",
    "        )\n",
    "        paperqa_emulation_tool = PaperQAEmulationTool(\n",
    "            db=self.db, llm=self.llm, description=paperqa_emulation_tool_description\n",
    "        )\n",
    "\n",
    "        tiab_classifier_tool_description = (\n",
    "            \"This tool classifies a paper as a trial, intervention, analysis, or background paper.\"\n",
    "        )\n",
    "        tiab_classifier_tool = TitleAbstractClassifier_OneDocAtATime_Tool(\n",
    "            db=self.db, llm=self.llm, description=tiab_classifier_tool_description\n",
    "        )\n",
    "\n",
    "        tiab_mapper_tool_description = (\n",
    "            \"This analyzes the title and abstract to provide descriptions of background information, the goals and methods, and the results and conclusions for the study.\"\n",
    "        )\n",
    "        tiab_mapper_tool = TitleAbstractDiscourseMappingTool(\n",
    "            db=self.db, llm=self.llm, description=tiab_mapper_tool_description\n",
    "        )\n",
    "\n",
    "        tiab_extract_tool_description = (\n",
    "            \"This analyzes the title and abstract to provide descriptions of background information, the goals and methods, and the results and conclusions for the study.\"\n",
    "        )\n",
    "        tiab_extract_tool = TitleAbstractExtraction_OneDocAtATime_Tool(\n",
    "            db=self.db, llm=self.llm, description=tiab_extract_tool_description\n",
    "        )\n",
    "\n",
    "        tool_list = [\n",
    "            add_collection_tool,\n",
    "            add_authors_to_collection_tool,\n",
    "            describe_collection_tool,\n",
    "            delete_collection_tool,\n",
    "            retrieve_full_text_tool,\n",
    "            retrieve_full_text_for_a_collection_tool,\n",
    "            metadata_extraction_tool,\n",
    "            methods_metadata_extraction_tool,\n",
    "            simple_extraction_tool,\n",
    "            paperqa_emulation_tool,\n",
    "            protocol_extraction_tool,\n",
    "            check_expression_tool,\n",
    "            tiab_classifier_tool,\n",
    "            tiab_mapper_tool,\n",
    "            tiab_extract_tool\n",
    "        ]\n",
    "\n",
    "        #trial_run_tool_description = (\n",
    "        #    \"This tool executes my planning chain for a given user instruction and reports back on what I plan do.\"\n",
    "        #)\n",
    "        #trial_run_tool = IntrospectionTool(description=trial_run_tool_description)\n",
    "        #tool_list.append(trial_run_tool)\n",
    "\n",
    "        return tool_list\n",
    "    \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Extraction Toolkit for Experimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MetadataExtractionToolkit(BaseModel):\n",
    "    '''Toolkit for running and testing Alhazen Metadata Extraction Tools'''\n",
    "\n",
    "    # The local literature database (Collections, Expressions, Items, and Fragments)\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True)\n",
    "    llm : BaseChatModel = Field(exclude=True)\n",
    "    agent : Optional[RunnableAgent] = Field(exclude=True) \n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_tools(self) -> List[BaseTool]:\n",
    "        \"\"\"Get the tools in the toolkit.\"\"\"\n",
    "\n",
    "        metadata_extraction_everything_everywhere_tool_description = (\n",
    "            \"This tool asks all metadata questions in a single shot over the whole document.\"\n",
    "        )\n",
    "        metadata_extraction_everything_everywhere_tool = MetadataExtraction_EverythingEverywhere_Tool(\n",
    "            db=self.db, llm=self.llm, description=metadata_extraction_everything_everywhere_tool_description\n",
    "        )\n",
    "\n",
    "        metadata_extraction_MethodsSectionOnly_tool_description = (\n",
    "            \"This tool asks all metadata questions in a single shot over the methods section only.\"\n",
    "        )\n",
    "        metadata_extraction_MethodsSectionOnly_tool = MetadataExtraction_MethodsSectionOnly_Tool(\n",
    "            db=self.db, llm=self.llm, description=metadata_extraction_MethodsSectionOnly_tool_description\n",
    "        )\n",
    "\n",
    "        metadata_extraction_rag_on_sections_tool_description = (\n",
    "            \"This tool asks each question separately over whole sections selected by RAG.\"\n",
    "        )\n",
    "        metadata_extraction_rag_on_sections_tool = MetadataExtraction_RAGOnSections_Tool(\n",
    "            db=self.db, llm=self.llm, description=metadata_extraction_rag_on_sections_tool_description\n",
    "        )\n",
    "\n",
    "        metadata_extraction_simple_extraction_tool_description = (\n",
    "            \"This tool asks a single question over sections selected by RAG.\"\n",
    "        )\n",
    "        metadata_extraction_simple_extraction_tool = SimpleExtractionWithRAGTool(\n",
    "            db=self.db, llm=self.llm, description=metadata_extraction_simple_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        tool_list = [\n",
    "            metadata_extraction_everything_everywhere_tool,\n",
    "            metadata_extraction_MethodsSectionOnly_tool,\n",
    "            metadata_extraction_rag_on_sections_tool,\n",
    "            metadata_extraction_simple_extraction_tool\n",
    "        ]\n",
    "\n",
    "        return tool_list\n",
    "    \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
