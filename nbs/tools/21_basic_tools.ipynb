{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tools for Alhazen\n",
    "\n",
    "> Simple tools to demonstrate utility and 'agentic' functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Tools:\n",
    "\n",
    "### Building, Describing, and Deleting Collections:\n",
    "1. **AddCollectionFromEPMCTool**: Execute a query against the European PMC, creating a collection, downloading available full-text papers and saving expressions, items, and fragments to the database to denote that publication.\n",
    "2. **DescribeCollectionCompositionTool**: Describe the composition of a collection in the database.\n",
    "3. **DeleteCollectionTool**: Delete a collection from the database.\n",
    "\n",
    "### Searching for papers in collections:\n",
    "3. **List__Paged_Expressions_CollectionCTool**: Delete a collection from the database.\n",
    "\n",
    "## Tools under development:\n",
    "1. Develop queries across external data sources\n",
    "2. Extract information from a collection using an LLM-based analysis to create Notes \n",
    "3. Filter a collection by an LLM-based analysis by tagging fragments with Notes\n",
    "5. Report on the state of the database in terms of numbers of collections, expressions, items, and fragments\n",
    "6. Prepare a report over a core research question by collecting a number of notes and synthesizing them into a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "\n",
    "from alhazen.core import PromptTemplateRegistry, load_alhazen_tool_environment\n",
    "from alhazen.utils.searchEngineUtils import download_full_text_paper_for_doi\n",
    "from alhazen.utils.web_robot import *\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb\n",
    "\n",
    "import json\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.agents.agent import RunnableAgent\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain.llms.base import BaseLanguageModel\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from pyalex import Works\n",
    "\n",
    "from sqlalchemy import create_engine, exists, func, or_, desc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Optional, Type\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "skc = aliased(ScientificKnowledgeCollection)\n",
    "skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "ske = aliased(ScientificKnowledgeExpression)\n",
    "ske_ha = aliased(ScientificKnowledgeExpressionHasAuthors)\n",
    "ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "ski = aliased(ScientificKnowledgeItem)\n",
    "ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "skf = aliased(ScientificKnowledgeFragment)\n",
    "aut_iao = aliased(AuthorIsAuthorOf)\n",
    "aut = aliased(Author)\n",
    "aut_aff = aliased(AuthorAffiliations)\n",
    "org = aliased(Organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# NOTE - Use LangChain's SQL_DATABASE TOOL AS A MODEL \n",
    "# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/tools/sql_database/tool.py\n",
    "\n",
    "class AlhazenToolMixin(BaseModel):\n",
    "    '''Base tool for interacting with an Alhazen CEIFNS (pron. 'SAI-FiNS') database \n",
    "    (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).'''\n",
    "\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True)\n",
    "    llm: Optional[BaseChatModel] = Field(exclude=True)\n",
    "    slm: Optional[BaseChatModel] = Field(exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "class IntrospectionToolMixin(BaseModel):\n",
    "    '''Base class for providing information about Alhazen's capabilities.'''\n",
    "\n",
    "    agent: Optional[RunnableAgent] = Field(exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class IntrospectionToolSchema(BaseModel):\n",
    "    input: str = Field(description=\"A hypothetical input to an agent.\")\n",
    "\n",
    "class IntrospectionTool(IntrospectionToolMixin, BaseTool): \n",
    "    name = \"introspect\"\n",
    "    description = \"This tool permits Alhazen to answer questions how its agent would plan to answer specific questions.\"\n",
    "    args_schema: Type[IntrospectionToolSchema] = IntrospectionToolSchema\n",
    "\n",
    "    def _set_agent(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        input: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 1. Check to see if the agent was instantiated\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        if self.agent is None:\n",
    "            raise Exception('Introspection tool not instantiated with agent')\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 2. Run agent's plan method \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        payload = {'intermediate_steps': [], 'input': input}\n",
    "        output = self.agent.invoke(payload)\n",
    "\n",
    "        return output\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"add_collection_from_epmc_query does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class AddCollectionFromEPMCToolSchema(BaseModel):\n",
    "    id: str = Field(description=\"A code that serves as the id of the collection we will add papers to.\")\n",
    "    name: str = Field(description=\"A human-readable name of the collection we will add papers to.\")\n",
    "    query: str = Field(description=\"A search query (using AND/OR/NOT statements) to be executed on the European PMC online literature repository for scientific publications.\")\n",
    "\n",
    "class AddCollectionFromEPMCTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"add_collection_from_epmc_query\"\n",
    "    description = \"This tool constructs a collection in the database from an external source based on a search query .\"\n",
    "    args_schema: Type[AddCollectionFromEPMCToolSchema] = AddCollectionFromEPMCToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        id: str, \n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        n_papers_added = 0\n",
    "\n",
    "        try: \n",
    "            \n",
    "            c = self.db.add_corpus_from_epmc_query(id, name, query, commit_this=False)\n",
    "\n",
    "            if self.db.embed_model is not None:\n",
    "                skes = self.db.list_unindexed_skes(id)\n",
    "                self.db.embed_expression_list(skes)\n",
    "            \n",
    "            # Need to add provenance for the data we have added to the model. \n",
    "            c.provenance = json.dumps([{'action_type': self.name, 'action': {'id': id, 'name': name, 'query': query} }])\n",
    "            self.db.session.flush()\n",
    "\n",
    "            #    if full_text:\n",
    "            #        path = loc+db_name+'/ft/'\n",
    "            #        ft_exist = download_full_text_paper_for_doi(doi, path)\n",
    "            #        if ft_exist:\n",
    "            #            self.db.add_full_text_for_expression(e)\n",
    "            \n",
    "            expression_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \n",
    "            expression_count = expression_q.first()[0]\n",
    "\n",
    "            jats_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'JATSFullText')\n",
    "            jats_ft_count = jats_ft_q.first()[0]\n",
    "            \n",
    "            pdf_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'PDFFullText')\n",
    "            pdf_ft_count = pdf_ft_q.first()[0]\n",
    "\n",
    "            n = Note(id=uuid.uuid4().hex[0:10],\n",
    "                    name='skc:%s.counts'%(id),\n",
    "                    content='{\"ske_count\": %s, \"jats_ft_count\": %s, \"pdf_ft_count\": %s}' \\\n",
    "                        %(expression_count, jats_ft_count, pdf_ft_count),\n",
    "                    format='json',\n",
    "                    type='NoteAboutCollection')\n",
    "            c.has_notes.append(n)        \n",
    "            self.db.session.add(n)\n",
    "\n",
    "            self.db.session.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'response': 'We added a collection to the database called `{}` containing {} papers from this query: `{}`.'.format(name, n_papers_added, query)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"add_collection_from_epmc_query does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ListCollectionsToolSchema(BaseModel):\n",
    "    name_filter: str = Field(description=\"A search string to filter the names of collections by.\")\n",
    "\n",
    "class ListCollectionsTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"list_collections\"\n",
    "    description = \"This tool lists available collections in the database.\"\n",
    "    args_schema: Type[ListCollectionsToolSchema] = ListCollectionsToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        search_string: str = '', \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        try: \n",
    "            if search_string != '':\n",
    "                collections = self.db.session.query(skc) \\\n",
    "                    .filter(skc.name.like('%'+search_string+'%')).all()\n",
    "            else:\n",
    "                collections = self.db.session.query(skc).all()\n",
    "\n",
    "        except Exception as e:\n",
    "            return {'response': 'Error, cannot list available collections.', 'data': []}\n",
    "\n",
    "        return {'response': 'Tool listed available {} collections.'.format(len(collections)),\n",
    "                'data': collections}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        search_string: str = '', \n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"delete_collection does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class AddCollectionFromOpenAlexToolSchema(BaseModel):\n",
    "    id: str = Field(description=\"A code that serves as the id of the collection we will add papers to.\")\n",
    "    name: str = Field(description=\"A human-readable name of the collection we will add papers to.\")\n",
    "    dois: List[str] = Field(description=\"A list of dois to be added separately to the named collection.\")\n",
    "\n",
    "class AddCollectionFromOpenAlexTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"add_collection_from_openalex_dois\"\n",
    "    description = \"This tool constructs a collection in the database from an external source based on a list of dois.\"\n",
    "    args_schema: Type[AddCollectionFromOpenAlexToolSchema] = AddCollectionFromOpenAlexToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        id: str, \n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        n_papers_added = 0\n",
    "\n",
    "        try: \n",
    "            \n",
    "            c = self.db.add_corpus_from_epmc_query(id, name, query, commit_this=False)\n",
    "            \n",
    "            # Need to add provenance for the data we have added to the model. \n",
    "            c.provenance = json.dumps([{'action_type': self.name, 'action': {'id': id, 'name': name, 'query': query} }])\n",
    "            self.db.session.flush()\n",
    "\n",
    "            #    if full_text:\n",
    "            #        path = loc+db_name+'/ft/'\n",
    "            #        ft_exist = download_full_text_paper_for_doi(doi, path)\n",
    "            #        if ft_exist:\n",
    "            #            self.db.add_full_text_for_expression(e)\n",
    "            \n",
    "            expression_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \n",
    "            expression_count = expression_q.first()[0]\n",
    "\n",
    "            jats_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'JATSFullText')\n",
    "            jats_ft_count = jats_ft_q.first()[0]\n",
    "            \n",
    "            pdf_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'PDFFullText')\n",
    "            pdf_ft_count = pdf_ft_q.first()[0]\n",
    "\n",
    "            n = Note(id=uuid.uuid4().hex[0:10],\n",
    "                    name='skc:%s.counts'%(id),\n",
    "                    content='{\"ske_count\": %s, \"jats_ft_count\": %s, \"pdf_ft_count\": %s}' \\\n",
    "                        %(expression_count, jats_ft_count, pdf_ft_count),\n",
    "                    format='json',\n",
    "                    type='NoteAboutCollection')\n",
    "            c.has_notes.append(n)        \n",
    "            self.db.session.add(n)\n",
    "\n",
    "            self.db.session.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'response': 'We added a collection to the database called `{}` containing {} papers from this query: `{}`.'.format(name, n_papers_added, query)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"add_collection_from_epmc_query does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class AddAuthorsToCollectionToolSchema(BaseModel):\n",
    "    id: str = Field(description=\"A code that serves as the id of the collection we will add papers to.\")\n",
    "\n",
    "class AddAuthorsToCollectionTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"add_authors_to_collection_query\"\n",
    "    description = \"This tool adds authors to a collection.\"\n",
    "    args_schema: Type[AddAuthorsToCollectionToolSchema] = AddAuthorsToCollectionToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        id: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        n_papers_added = 0\n",
    "\n",
    "        try: \n",
    "            \n",
    "            print('Adding authors')\n",
    "            for e in tqdm(self.db.list_expressions(id)):\n",
    "        \n",
    "                # Adding authors from open alex to build this. \n",
    "                # NOTE THAT THERE IS A 100,000 PER DAY TO LIMIT \n",
    "                # TO QUERY OPEN-ALEX WITHOUT A PAID LICENSE.\n",
    "                # UGLY HACK TO ADD AUTHORS. \n",
    "                try:\n",
    "                    doi = e.id.replace('doi:', 'https://doi.org/')\n",
    "                    w = Works()[doi]\n",
    "                    for aa in w['authorships']:\n",
    "                        #~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        # Add the author information \n",
    "                        #~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        raw_a_name = aa.get('raw_author_name')\n",
    "                        a = aa.get('author', {})\n",
    "                        a_id = a.get('id', '')\n",
    "                        a_name = a.get('display_name', '')\n",
    "                        orcid = a.get('orcid')\n",
    "                        q3 = self.db.session.query(aut).filter(aut.id == a_id)\n",
    "                        author_ice = None\n",
    "                        for temp_a in q3.all():\n",
    "                            author_ice = temp_a\n",
    "                            break\n",
    "                        if author_ice is None:\n",
    "                            author_ice = Author(id=a_id, name=a_name, type='Author')\n",
    "                            self.db.session.add(author_ice)\n",
    "                            if orcid:\n",
    "                                author_ice.xref.append(orcid)\n",
    "                        q4 = self.db.session.query(ske_ha) \\\n",
    "                            .filter(ske_ha.ScientificKnowledgeExpression_id == e.id) \\\n",
    "                            .filter(ske_ha.has_authors_id == a_id)\n",
    "                        if len(q4.all()) == 0:\n",
    "                            e.has_authors.append(author_ice)\n",
    "                            author_ice.is_author_of.append(e)\n",
    "                        \n",
    "                        #~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        # Add paper affiliations \n",
    "                        #~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        raw_inst_names =  aa.get('raw_affiliation_names')\n",
    "                        for inst in aa.get('institutions', []):\n",
    "                            inst_id = inst.get('id', '')\n",
    "                            inst_country = inst.get('country_code', '')\n",
    "                            inst_name = inst.get('display_name', '')\n",
    "                            ror = inst.get('ror')\n",
    "                        \n",
    "                            q5 = self.db.session.query(org).filter(org.id == inst_id)\n",
    "                            org_ice = None\n",
    "                            for oo in q5.all():\n",
    "                                org_ice = oo\n",
    "                                break\n",
    "                            if org_ice is None:\n",
    "                                q5a = self.db.session.query(Country) \\\n",
    "                                    .filter(Country.code2 == inst_country)\n",
    "                                org_ice = Organization(id=inst_id, \n",
    "                                                       name=inst_name,\n",
    "                                                       type='Organization',\n",
    "                                                       country=[c for c in q5a.all()])\n",
    "                                self.db.session.add(org_ice)\n",
    "                                if ror:\n",
    "                                    org_ice.xref.append(ror)\n",
    "                            q6 = self.db.session.query(aut_aff) \\\n",
    "                                .filter(aut_aff.Author_id == a_id) \\\n",
    "                                .filter(aut_aff.affiliations_id == inst_id)\n",
    "                            if len(q6.all()) == 0:\n",
    "                                author_ice.affiliations.append(org_ice)\n",
    "                    self.db.session.flush()\n",
    "                \n",
    "                except Exception as exc: \n",
    "                    print(exc)\n",
    "                    print('errors with adding author information via OpenAlex')\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'response': 'We added authors and institutions to the collection with the id=`{}`.'.format(id)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"add_collection_from_epmc_query does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CheckExpressionToolSchema(BaseModel):\n",
    "    query: str = Field(description=\"a search query for the paper in the database\")\n",
    "    \n",
    "class CheckExpressionTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"describe_expression_in_local_database\"\n",
    "    description = \"This tool searches for a paper in the database and reports if we have full text version of it.\"\n",
    "    args_schema: Type[CheckExpressionToolSchema] = CheckExpressionToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        try:      \n",
    "            ske = aliased(ScientificKnowledgeExpression)\n",
    "            ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "            ski = aliased(ScientificKnowledgeItem)\n",
    "\n",
    "            expression_q = self.db.session.query(ske.id, ske.content, ske.type, func.aggregate_strings(ski.type, ',')) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(or_(ske.id.like('%'+query+'%'), ske.content.like('%'+query+'%'))) \\\n",
    "                    .group_by(ske.id, ske.content, ske.type)\n",
    "            df = pd.DataFrame(expression_q.all(), columns=['e_id', 'citation', 'e_type', 'i_type'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        if len(df) == 0:\n",
    "            return {'response': 'Could not find a match to query `{}`'.format(query)}\n",
    "        elif len(df) > 1:\n",
    "            return {'response': 'Found more than one match to query `{}`'.format(query)}\n",
    "            \n",
    "        p_id = None\n",
    "        ft_available = False\n",
    "        for i, row in df.iterrows():\n",
    "            if 'FullText' in row.i_type:\n",
    "                p_id = row.e_id\n",
    "                ft_available = True\n",
    "\n",
    "        return {'response': 'Found one match to query `{}` with paper_id={}, full_text_available={}'.format(query, p_id, ft_available)}\n",
    "        \n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"describe_expression_in_local_database does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DescribeCollectionCompositionToolSchema(BaseModel):\n",
    "    id: str = Field(description=\"should be the id of the collection we wish to describe\")\n",
    "    \n",
    "class DescribeCollectionCompositionTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"describe_collection_in_local_database\"\n",
    "    description = \"This tool describes the contents of a Collection, in terms of counts of papers\"\n",
    "    args_schema: Type[DescribeCollectionCompositionToolSchema] = DescribeCollectionCompositionToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        id: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        try:      \n",
    "            collection_q = self.db.session.query(skc.name) \\\n",
    "                    .filter(skc.id == str(id)) \n",
    "            df = pd.DataFrame(collection_q.all(), columns=['collection_name'])\n",
    "            if len(df) == 0:\n",
    "                return {'response': 'The collection with the id `{}` does not exist.'.format(id)}\n",
    "\n",
    "            name = collection_q.first()[0]\n",
    "\n",
    "            expression_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \n",
    "            expression_count = expression_q.first()[0]\n",
    "\n",
    "            jats_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'JATSFullText')\n",
    "            jats_ft_count = jats_ft_q.first()[0]\n",
    "\n",
    "            pdf_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'PDFFullText')\n",
    "            pdf_ft_count = pdf_ft_q.first()[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'response': 'The collection with the id `{}` is called `{}`. It contains {} papers; including {} PDF and {} JATS full text papers.'.format(id, name, expression_count, pdf_ft_count, jats_ft_count)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"add_collection_from_epmc_query does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DeleteCollectionToolSchema(BaseModel):\n",
    "    collection_id: str = Field(description=\"should be the collection_id of the collection we will add papers to\")\n",
    "\n",
    "class DeleteCollectionTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"delete_collection\"\n",
    "    description = \"This deletes a collection from the database based on an collection_id value.\"\n",
    "    args_schema: Type[DeleteCollectionToolSchema] = DeleteCollectionToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        collection_id: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        try:             \n",
    "            self.db.delete_collection(collection_id, commit_this=True)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'response': 'Successfully deleted a collection with collection_id:`{}`.'.format(collection_id)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"delete_collection does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RetrieveFullTextToolForACollectionSchema(BaseModel):\n",
    "    collection_id: str = Field(description=\"the identifier of the collection containing papers to be retrieved\")\n",
    "\n",
    "class RetrieveFullTextToolForACollection(AlhazenToolMixin, BaseTool): \n",
    "    name = \"retrieve_full_text_for_papers_in_collection\"\n",
    "    description = \"This retrieves all full text papers in a given collection.\"\n",
    "    args_schema: Type[RetrieveFullTextToolForACollectionSchema] = RetrieveFullTextToolForACollectionSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        collection_id: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        loc = load_alhazen_tool_environment()\n",
    "        db_name = self.db.name\n",
    "\n",
    "        skc = aliased(ScientificKnowledgeCollection)\n",
    "        for c in self.db.session.query(skc) \\\n",
    "                .filter(skc.id == str(collection_id)).all():\n",
    "            break \n",
    "        \n",
    "        ske = aliased(ScientificKnowledgeExpression)\n",
    "        exp_q = self.db.session.query(ske) \\\n",
    "                .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                .filter(skc_hm.ScientificKnowledgeCollection_id == str(collection_id)) \\\n",
    "                .order_by(desc(ske.publication_date))\n",
    "        for e in tqdm(exp_q.all()):\n",
    "            doi = e.id.replace('doi:', '')\n",
    "            path = loc+db_name+'/ft/'\n",
    "\n",
    "            # Check if we have already downloaded the full text for the paper\n",
    "            check = self.db.check_full_text(e)\n",
    "            if check.get('nxml_present') is False and \\\n",
    "                    check.get('pdf_present') is False and \\\n",
    "                    check.get('html_present') is False and \\\n",
    "                    check.get('prior_attempts') is False:\n",
    "                try:\n",
    "                    ft_exist = download_full_text_paper_for_doi(doi, path, headless=True)  \n",
    "                    if ft_exist: \n",
    "                        self.db.write_note_about_x(e, 'successfully downloaded full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)\n",
    "                    else:\n",
    "                        self.db.write_note_about_x(e, 'failed to download full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)                    \n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    content = repr(ex)\n",
    "                    self.db.write_note_about_x(e, 'errored whilst downloading full text for %s'%(e.id), content, 'log', 'NoteAboutExpression', commit_this=True)                    \n",
    "     \n",
    "            check2 = self.db.check_full_text(e)\n",
    "            if check2.get('nxml_present') or check2.get('pdf_present') or check2.get('html_present') and \\\n",
    "                (check2.get('pdf_item_present') is False and check2.get('nxml_item_present') is False):\n",
    "                try:\n",
    "                    self.db.add_full_text_for_expression(e)\n",
    "                    self.db.session.commit()\n",
    "\n",
    "                    check3 = self.db.check_full_text(e)\n",
    "                    if check3.get('pdf_item_present') or check3.get('nxml_item_present'):\n",
    "                        self.db.write_note_about_x(e, 'successfully added full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)\n",
    "                    else:\n",
    "                        self.db.write_note_about_x(e, 'failed to add full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    self.db.session.rollback()\n",
    "                    content = repr(ex)\n",
    "                    self.db.write_note_about_x(e, 'errored whilst adding full text for %s'%(e.id), content, 'log', 'NoteAboutExpression', commit_this=True)\n",
    "            \n",
    "        return {'response': 'I retrieved full text papers for the collection named `{}` (id=`{}).'.format(c.name, c.id)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"retrieve_full_text_for_papers_in_collection does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RetrieveFullTextToolSchema(BaseModel):\n",
    "    paper_id: str = Field(description=\"the digitial objecty identifier (doi) of the paper being retrieved from external sources\")\n",
    "\n",
    "class RetrieveFullTextTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"retrieve_full_text_for_paper_id\"\n",
    "    description = \"This retrieves a full text paper based on its doi, copies it to local disk, and adds it the database.\"\n",
    "    args_schema: Type[RetrieveFullTextToolSchema] = RetrieveFullTextToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        paper_id: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        loc = load_alhazen_tool_environment()\n",
    "        db_name = self.db.name\n",
    "\n",
    "        try: \n",
    "            ske = aliased(ScientificKnowledgeExpression)\n",
    "            e = self.db.session.query(ske) \\\n",
    "                    .filter(ske.id == str(paper_id)).first()\n",
    "            doi = e.id.replace('doi:', '')\n",
    "            path = loc+db_name+'/ft/'\n",
    "\n",
    "                        # Check if we have already downloaded the full text for the paper\n",
    "            check = self.db.check_full_text(e)\n",
    "            if check.get('nxml_present') is False and \\\n",
    "                    check.get('pdf_present') is False and \\\n",
    "                    check.get('html_present') is False and \\\n",
    "                    check.get('prior_attempts') is False:\n",
    "                try:\n",
    "                    ft_exist = download_full_text_paper_for_doi(doi, path, headless=True)  \n",
    "                    if ft_exist: \n",
    "                        self.db.write_note_about_x(e, 'successfully downloaded full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)\n",
    "                    else:\n",
    "                        self.db.write_note_about_x(e, 'failed to download full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)                    \n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    self.db.session.rollback()\n",
    "                    content = repr(ex)\n",
    "                    self.db.write_note_about_x(e, 'errored whilst downloading full text for %s'%(e.id), content, 'log', 'NoteAboutExpression', commit_this=True)                    \n",
    "     \n",
    "            check2 = self.db.check_full_text(e)\n",
    "            if check2.get('nxml_present') or check2.get('pdf_present') or check2.get('html_present') and \\\n",
    "                (check2.get('pdf_item_present') is False and check2.get('nxml_item_present') is False):\n",
    "                try:\n",
    "                    self.db.add_full_text_for_expression(e)\n",
    "                    self.db.session.commit()\n",
    "\n",
    "                    check3 = self.db.check_full_text(e)\n",
    "                    if check3.get('pdf_item_present') or check3.get('nxml_item_present'):\n",
    "                        self.db.write_note_about_x(e, 'successfully added full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)\n",
    "                    else:\n",
    "                        self.db.write_note_about_x(e, 'failed to add full text for %s'%(e.id), '', 'log', 'NoteAboutExpression', commit_this=True)\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    self.db.session.rollback()\n",
    "                    content = repr(ex)\n",
    "                    self.db.write_note_about_x(e, 'errored whilst adding full text for %s'%(e.id), content, 'log', 'NoteAboutExpression', commit_this=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'response': 'The full text paper with doi:`{}` is available in the database.'.format(paper_id)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"delete_collection does not support async\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
