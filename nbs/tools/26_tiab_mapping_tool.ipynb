{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classifier Tool   \n",
    "\n",
    "> Langchain tools that execute zero-shot classification tasks over a local database of title/abstracts from papers previously imported into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.tiab_mapping_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.aliases import *\n",
    "from alhazen.core import PromptTemplateRegistry\n",
    "from alhazen.tools.basic import AlhazenToolMixin\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "from alhazen.utils.ceifns_db import *\n",
    "from alhazen.schema_sqla import *\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import jmespath\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain.schema import get_buffer_string, StrOutputParser, OutputParserException, format_document\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "import local_resources.linkml as linkml\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "from sqlalchemy import create_engine, exists, func, or_, desc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "from time import time,sleep\n",
    "import tiktoken\n",
    "from typing import Optional, Type\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TitleAbstractMappingToolSchema(BaseModel):\n",
    "    collection_id: str = Field(description=\"the id of the collection being analyzed.\")\n",
    "    repeat_run: Optional[bool] = Field(description=\"Whether or not to repeat the classification task if it has been performed before. Defaults to False.\")\n",
    "    run_label: Optional[str] = Field(description=\"This is a label that will be used to identify the run of the tool in the database. Defaults to 'dev'\")\n",
    "\n",
    "class BaseTitleAbstractMappingTool(BaseTool, AlhazenToolMixin):\n",
    "    '''Runs a specified document mapping pipeline over papers in a collection.'''\n",
    "    name = 'tiab_mapping'\n",
    "    description = 'Runs a specified document mapping pipeline over papers in a collection.'\n",
    "    args_schema = TitleAbstractMappingToolSchema\n",
    "    return_direct:bool = True\n",
    "    \n",
    "    def _run(self, collection_id, repeat_run=False):\n",
    "        '''Runs a specified document mapping pipeline over papers in a collection.'''\n",
    "        raise NotImplementedError('This method must be implemented by a subclass.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TitleAbstractDiscourseMappingTool(BaseTitleAbstractMappingTool):\n",
    "    '''Runs through the text of each title + abstract and split them based on discourse.'''\n",
    "    name = 'tiab_one_doc_classification'\n",
    "    description = '''Runs through the text of each title + abstract and split them based on discourse.'''\n",
    "    \n",
    "    def _run(self, collection_id, run_label='dev', repeat_run=False):\n",
    "        '''Runs through the text of each title + abstract and split them based on discourse.'''\n",
    "\n",
    "        # 1. Build LangChain elements\n",
    "        pts = PromptTemplateRegistry()\n",
    "        pts.load_prompts_from_yaml('tiab_prompts.yaml')\n",
    "        tiab_mapper_prompt_template = pts.get_prompt_template('split_by_discourse').generate_chat_prompt_template()\n",
    "        mapper_lcel = tiab_mapper_prompt_template | self.llm | JsonOutputParser()        \n",
    "\n",
    "        output_list = []\n",
    "        llm_class_name = self.llm.__class__.__name__\n",
    "        llm_model_desc = str(self.llm)\n",
    "        run_metadata = {\n",
    "            'tool': self.__class__.__name__,\n",
    "            'mapping_type': 'discourse_tags',\n",
    "            'llm_class': llm_class_name,\n",
    "            'llm_desc': llm_model_desc}\n",
    "\n",
    "        exp_q = self.db.session.query(SKE) \\\n",
    "                .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "                .filter(SKC_HM.ScientificKnowledgeCollection_id == str(collection_id)) \\\n",
    "                .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "                .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "                .filter(SKI.type=='CitationRecord') \\\n",
    "                .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n",
    "                .order_by(desc(SKE.publication_date))\n",
    "\n",
    "        for e in tqdm(exp_q.all()):\n",
    "        \n",
    "            # skip expressions if they have this type of note already.\n",
    "            if repeat_run is False: \n",
    "                repeat_flag = False\n",
    "                for n in self.db.read_notes_about_x(e):\n",
    "                    if n.type == 'TiAbMappingNote__Discourse':\n",
    "                        repeat_flag = True\n",
    "                        break\n",
    "                if repeat_flag:\n",
    "                    continue\n",
    "\n",
    "            # 2. Run through all available sections of the paper and identify only those that are predominantly methods sections.\n",
    "            start = datetime.now()\n",
    "            title, abstract = [f.content for f in self.db.list_fragments_for_paper(e.id, 'CitationRecord')]\n",
    "\n",
    "            if len(title + abstract) == 0:\n",
    "                continue\n",
    "\n",
    "            # 3. Assemble chain input\n",
    "            s1 = {'title': title, 'abstract': abstract}\n",
    "        \n",
    "            # 4. Run the chain with a JsonEnclosedByTextOutputParser \n",
    "            try: \n",
    "                output = mapper_lcel.invoke(s1)#, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "            except Exception as e:\n",
    "                # Note that the LLM may raise an exception if it is unable to classify the document.\n",
    "                print(e)\n",
    "                output = \"ERROR: Unable to classify document.\"\n",
    "\n",
    "            total_execution_time = datetime.now() - start\n",
    "\n",
    "            output['paper_id'] = e.id\n",
    "            output_list.append(output)\n",
    "\n",
    "            # 5. add a note to the fragment\n",
    "            run_metadata_1 = run_metadata.copy()\n",
    "            run_metadata_1['time_taken'] = str(total_execution_time)\n",
    "            if output is not None:\n",
    "                n = Note(\n",
    "                    id=uuid.uuid4().hex[0:10],\n",
    "                    type='TiAbMappingNote__Discourse', \n",
    "                    name=e.id+'__discourse_text_'+run_label,\n",
    "                    provenance=json.dumps(run_metadata_1, indent=4),\n",
    "                    content=json.dumps(output, indent=4), \n",
    "                    creation_date=datetime.now(), \n",
    "                    format='json')\n",
    "                self.db.session.add(n)\n",
    "                n.is_about.append(e)\n",
    "                self.db.session.commit()\n",
    "\n",
    "        output_list = []\n",
    "                            \n",
    "        return {'response': \"completed discourse mapping for collection %s.\"%(collection_id),\n",
    "                \"data\": output_list, \n",
    "                'run': run_metadata}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
