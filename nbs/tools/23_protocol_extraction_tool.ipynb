{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol Workflow Extraction Tool   \n",
    "\n",
    "> Langchain tools that execute zero-shot extraction over a local database of full text papers previously imported into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.protocol_extraction_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.core import PromptTemplateRegistry\n",
    "from alhazen.tools.basic import AlhazenToolMixin\n",
    "from alhazen.utils.output_parsers import *\n",
    "from alhazen.utils.ceifns_db import *\n",
    "from alhazen.schema_sqla import *\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import jmespath\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain.schema import get_buffer_string, StrOutputParser, OutputParserException, format_document\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "import local_resources.linkml as linkml\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "from sqlalchemy import create_engine, exists\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "from time import time,sleep\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "skc = aliased(ScientificKnowledgeCollection)\n",
    "skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "ske = aliased(ScientificKnowledgeExpression)\n",
    "ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "ski = aliased(ScientificKnowledgeItem)\n",
    "ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "skf = aliased(ScientificKnowledgeFragment)\n",
    "n = aliased(Note)\n",
    "skc_hn = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "ske_hn = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "ski_hn = aliased(ScientificKnowledgeItemHasNotes)\n",
    "skf_hn = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ProtocolExtractionToolSchema(BaseModel):\n",
    "    paper_id: str = Field(description=\"the doi of the paper being analyzed, must start with the string 'doi:'\")\n",
    "    \n",
    "class BaseProtocolExtractionTool(BaseTool, AlhazenToolMixin):\n",
    "    '''Runs a specified protocol extraction pipeline over a research paper that has been loaded in the local literature database.'''\n",
    "    name = 'protocol_workflow_extraction'\n",
    "    description = 'Runs a specified protocol extraction pipeline over a research paper that has been loaded in the local literature database.'\n",
    "    args_schema = ProtocolExtractionToolSchema\n",
    "    return_direct:bool = True\n",
    "\n",
    "    def _run(self, paper_id, extraction_type):\n",
    "        '''Runs a specified protocol extraction pipeline over a research paper that has been loaded in the local literature database.'''\n",
    "        raise NotImplementedError('This method must be implemented by a subclass.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProcotolDiagramExtractionTool(BaseProtocolExtractionTool):\n",
    "    '''Extracts a mermaid diagram of the protocol from a paper.'''\n",
    "    name = 'protocol_diagram_extraction'\n",
    "    description = 'Extracts a mermaid diagram of the protocol from a paper.'\n",
    "\n",
    "    def _run(self, paper_id):\n",
    "        '''Extracts a mermaid diagram of the protocol from a paper.'''\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()\n",
    "\n",
    "        # Introspect the class name of the llm model for notes and logging\n",
    "        llm_class_name = self.llm.__class__.__name__\n",
    "\n",
    "        run_metadata = {\n",
    "            'tool': self.__class__.__name__,\n",
    "            'doi': paper_id,\n",
    "            'llm_class': llm_class_name}\n",
    "\n",
    "        # 0. Use the first available full text item type\n",
    "        item_types = set()\n",
    "        item_type = None\n",
    "        for i in self.db.list_items_for_expression(paper_id):\n",
    "            item_types.add(i.type)\n",
    "        for i_type in item_types:\n",
    "            if i_type == 'CitationRecord':\n",
    "                continue\n",
    "            item_type = i_type\n",
    "            break\n",
    "        if item_type is None:\n",
    "            return {'response': \"Could not retrieve full text of the paper: %s.\"%(paper_id),\n",
    "                \"data\": {'mermaid_code': None, 'run': run_metadata} }\n",
    "\n",
    "        # 1. Build LangChain elements\n",
    "        pts = PromptTemplateRegistry()\n",
    "        \n",
    "        pts.load_prompts_from_yaml('protocol_extraction.yaml')\n",
    "        pt = pts.get_prompt_template('protocol diagram extraction').generate_chat_prompt_template()\n",
    "        extract_lcel = pt | self.llm | MermaidExtractionOutputParser()\n",
    "        \n",
    "        # 2. Run through all available sections of the paper and identify only those that are predominantly methods sections.\n",
    "        start = datetime.now()\n",
    "        fragments = [f.content for f in self.db.list_fragments_for_paper(paper_id, item_type, fragment_types=['section'])]\n",
    "        on_off = False\n",
    "        text = ''\n",
    "        for t in fragments:\n",
    "            l1 = t.split('\\n')[0].lower()\n",
    "            if 'method' in l1:\n",
    "                on_off = True\n",
    "            elif 'results' in l1 or 'discussion' in l1 or 'conclusion' in l1 or 'acknowledgements' in l1 \\\n",
    "                    or 'references' in l1 or 'supplementary' in l1 or 'appendix' in l1 or 'introduction' in l1 or 'abstract' in l1 or 'cited' in l1:\n",
    "                on_off = False\n",
    "            if on_off:\n",
    "                if len(text) > 0:\n",
    "                    text += '\\n\\n'\n",
    "                text += t\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return {'response': \"No text generated for the paper: %s.\"%(paper_id),\n",
    "                \"data\": {'mermaid_code': None, 'run': run_metadata} }\n",
    "\n",
    "        # 4. Assemble chain input\n",
    "        s1 = {'section_text': text}\n",
    "        \n",
    "        # 5. Run the chain with a MermaidExtractionOutputParser \n",
    "        output = None\n",
    "        output = extract_lcel.invoke(s1)#, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "        total_execution_time = datetime.now() - start\n",
    "\n",
    "        if output is None:\n",
    "            return {'response': \"attempted and failed protocol extraction for an experiment from %s.\"%(paper_id),\n",
    "                \"data\": None, \n",
    "                'run': run_metadata}\n",
    "        \n",
    "        return {'response': \"completed protocol extraction for an experiment from %s.\"%(paper_id),\n",
    "                \"data\": output, \n",
    "                'run': run_metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProcotolEntitiesExtractionTool(BaseProtocolExtractionTool):\n",
    "    '''Extracts all entities used in a protocol.'''\n",
    "    name = 'protocol_entities_extraction'\n",
    "    description = 'Extracts all entities used in a protocol.'\n",
    "\n",
    "    def _run(self, paper_id):\n",
    "        '''Extracts all entities used in a protocol.'''\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()\n",
    "\n",
    "        # Introspect the class name of the llm model for notes and logging\n",
    "        llm_class_name = self.llm.__class__.__name__\n",
    "\n",
    "        run_metadata = {\n",
    "            'tool': self.__class__.__name__,\n",
    "            'doi': paper_id,\n",
    "            'llm_class': llm_class_name}\n",
    "\n",
    "        # 0. Use the first available full text item type\n",
    "        item_types = set()\n",
    "        item_type = None\n",
    "        for i in self.db.list_items_for_expression(paper_id):\n",
    "            item_types.add(i.type)\n",
    "        for i_type in item_types:\n",
    "            if i_type == 'CitationRecord':\n",
    "                continue\n",
    "            item_type = i_type\n",
    "            break\n",
    "        if item_type is None:\n",
    "            return {'response': \"Could not retrieve full text of the paper: %s.\"%(paper_id),\n",
    "                \"data\": None, \n",
    "                'run': run_metadata}\n",
    "\n",
    "        # 1. Build LangChain elements\n",
    "        pts = PromptTemplateRegistry()\n",
    "        \n",
    "        pts.load_prompts_from_yaml('protocol_extraction.yaml')\n",
    "        pt = pts.get_prompt_template('entity extraction').generate_chat_prompt_template()\n",
    "        parser = JsonOutputParser()\n",
    "\n",
    "        extract_lcel = pt | self.llm | parser\n",
    "        \n",
    "        # 2. Use heuristics to find the start of the methods section and run through until you find the next top-level section\n",
    "        # Very ugly hack, need to update based on better reading of section headings\n",
    "        start = datetime.now()\n",
    "        fragments = [f.content for f in self.db.list_fragments_for_paper(paper_id, item_type, fragment_types=['section'])]\n",
    "        on_off = False\n",
    "        text = ''\n",
    "        for t in fragments:\n",
    "            l1 = t.split('\\n')[0].lower()\n",
    "            if 'method' in l1:\n",
    "                on_off = True\n",
    "            elif 'results' in l1 or 'discussion' in l1 or 'conclusion' in l1 or 'acknowledgements' in l1 \\\n",
    "                    or 'references' in l1 or 'supplementary' in l1 or 'appendix' in l1 or 'introduction' in l1 or 'abstract' in l1 or 'cited' in l1:\n",
    "                on_off = False\n",
    "            if on_off:\n",
    "                if len(text) > 0:\n",
    "                    text += '\\n\\n'\n",
    "                text += t\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return {'response': \"No text extracted for the paper: %s.\"%(paper_id),\n",
    "                \"data\": None, \n",
    "                'run': run_metadata} \n",
    "\n",
    "        # 4. Assemble chain input\n",
    "        s1 = {'section_text': text}\n",
    "        \n",
    "        # 5. Run the chain with a MermaidExtractionOutputParser \n",
    "        output = None\n",
    "        output = extract_lcel.invoke(s1)#, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "        total_execution_time = datetime.now() - start\n",
    "\n",
    "        if output is None:\n",
    "            return {'response': \"attempted and failed to list entitles for proctocol from %s.\"%(paper_id),\n",
    "                \"data\": None,\n",
    "                'run': run_metadata}\n",
    "        \n",
    "        return {'response': \"completed list of entitles for proctocol from %s.\"%(paper_id),\n",
    "                \"data\": output, \n",
    "                'run': run_metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProcotolProcessesExtractionTool(BaseProtocolExtractionTool):\n",
    "    '''Extracts all processes used in a protocol.'''\n",
    "    name = 'protocol_processes_extraction'\n",
    "    description = 'Extracts all processes used in a protocol.'\n",
    "\n",
    "    def _run(self, paper_id):\n",
    "        '''Extracts all processes used in a protocol.'''\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()\n",
    "\n",
    "        # Introspect the class name of the llm model for notes and logging\n",
    "        llm_class_name = self.llm.__class__.__name__\n",
    "\n",
    "        run_metadata = {\n",
    "            'tool': self.__class__.__name__,\n",
    "            'doi': paper_id,\n",
    "            'llm_class': llm_class_name}\n",
    "\n",
    "        # 0. Use the first available full text item type\n",
    "        item_types = set()\n",
    "        item_type = None\n",
    "        for i in self.db.list_items_for_expression(paper_id):\n",
    "            item_types.add(i.type)\n",
    "        for i_type in item_types:\n",
    "            if i_type == 'CitationRecord':\n",
    "                continue\n",
    "            item_type = i_type\n",
    "            break\n",
    "        if item_type is None:\n",
    "            return {'response': \"Could not retrieve full text of the paper: %s.\"%(paper_id),\n",
    "                \"data\": {'entities': None, 'run': run_metadata} }\n",
    "\n",
    "        # 1. Build LangChain elements\n",
    "        pts = PromptTemplateRegistry()\n",
    "        \n",
    "        pts.load_prompts_from_yaml('protocol_extraction.yaml')\n",
    "        pt = pts.get_prompt_template('process extraction').generate_chat_prompt_template()\n",
    "        parser = JsonOutputParser()\n",
    "\n",
    "        extract_lcel = pt | self.llm | parser\n",
    "        \n",
    "        # 2. Use heuristics to find the start of the methods section and run through until you find the next top-level section\n",
    "        # Very ugly hack, need to update based on better reading of section headings\n",
    "        start = datetime.now()\n",
    "        fragments = [f.content for f in self.db.list_fragments_for_paper(paper_id, item_type, fragment_types=['section'])]\n",
    "        on_off = False\n",
    "        text = ''\n",
    "        for t in fragments:\n",
    "            l1 = t.split('\\n')[0].lower()\n",
    "            if 'method' in l1:\n",
    "                on_off = True\n",
    "            elif 'results' in l1 or 'discussion' in l1 or 'conclusion' in l1 or 'acknowledgements' in l1 \\\n",
    "                    or 'references' in l1 or 'supplementary' in l1 or 'appendix' in l1 or 'introduction' in l1 or 'abstract' in l1 or 'cited' in l1:\n",
    "                on_off = False\n",
    "            if on_off:\n",
    "                if len(text) > 0:\n",
    "                    text += '\\n\\n'\n",
    "                text += t\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return {'response': \"No text extracted for the paper: %s.\"%(paper_id),\n",
    "                \"data\": {'list_of_answers': None, 'run': run_metadata} }\n",
    "\n",
    "        # 4. Assemble chain input\n",
    "        s1 = {'section_text': text}\n",
    "        \n",
    "        # 5. Run the chain with a MermaidExtractionOutputParser \n",
    "        output = None\n",
    "        output = extract_lcel.invoke(s1)#, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "        total_execution_time = datetime.now() - start\n",
    "\n",
    "        if output is None:\n",
    "            return {'response': \"attempted and failed to list entitles for proctocol from %s.\"%(paper_id),\n",
    "                \"data\": None, \n",
    "                'run': run_metadata} \n",
    "        \n",
    "        return {'response': \"completed list of entitles for proctocol from %s.\"%(paper_id),\n",
    "                \"data\": output, \n",
    "                'run': run_metadata}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
