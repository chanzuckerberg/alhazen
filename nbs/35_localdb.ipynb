{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building databases of published works  \n",
    "\n",
    "> Pragmatic tools for constructing databases of scientific works based on queries defined with Boolean Logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.local_literature_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabulate queries in a spreadsheet and generate a database based on the data from those queries. \n",
    "\n",
    "**Example**:  Define a dataframe with an `id` column and a `query` column (expressing a search query in Boolean Logic):\n",
    "\n",
    "| ID | DISEASE NAME | QUERY  | \n",
    "|----|--------------|--------|\n",
    "| 1 | Adult Polyglucosan Body Disease | adult polyglucosan body disease \\| adult polyglucosan body neuropathy\n",
    "| 2 | AGAT deficiency |  \"GATM deficiency\" \\| \"AGAT deficiency\" \\| \"arginine:glycine amidinotransferase deficiency\" \\| \"L-arginine:glycine amidinotransferase deficiency\"\n",
    "| 3 | Guanidinoacetate methyltransferase deficiency | \"guanidinoacetate methyltransferase deficiency\" \\| \"GAMT deficiency\"\n",
    "| 4 | CLOVES Syndrome | \"CLOVES syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation epidermal) & (nevi-spinal) & syndrome \\| (congenital lipomatous overgrowth) & (vascular malformations) & (Epidermal nevi) & ((skeletal\\|spinal) & abnormalities) \\| CLOVE syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation) & (epidermal nevi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "from alhazen.schema_sqla import ScientificKnowledgeCollection, \\\n",
    "    ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \\\n",
    "    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n",
    "    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n",
    "    InformationResource, Note, NoteIsAbout\n",
    "import alhazen.schema_python as linkml_py\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import local_resources.linkml as linkml\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from sqlalchemy import create_engine, exists\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import sqlite3  \n",
    "import sys\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import yaml\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't export this but leave it in the notebook code for future reference\n",
    "from databricks import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't export this but leave it in the notebook code for future reference\n",
    "def get_nxml_from_scipubstore_doi(doi, base_file_path):\n",
    "    \n",
    "    if os.environ.get('DB_TOKEN') is None:  \n",
    "        msg = 'Error attempting to query Databricks for URL data, did you set the DB_TOKEN environment variable?'\n",
    "        raise Exception(msg)\n",
    "\n",
    "    get_ft_url_from_doi_sql = '''\n",
    "        SELECT DISTINCT p.pmc_id, p.doi, YEAR(p.publication_date) as year, p.title, p.abstract, p.full_text_format, p.full_text_url, a.last_name\n",
    "        FROM scipubstore.ingestion.papers as p \n",
    "            JOIN scipubstore.ingestion.authors as a on (p.paper_id=a.paper_id) \n",
    "        WHERE p.doi = '{}' and a.author_index=1\n",
    "        ORDER BY p.full_text_url DESC\n",
    "    '''.format(doi)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with sql.connect(server_hostname = 'czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com',\n",
    "                        http_path = '/sql/1.0/warehouses/1c4df94f2f1a6305',\n",
    "                        access_token = 'databricks-access-token-here') as connection:\n",
    "\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(get_ft_url_from_doi_sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "    df = pd.DataFrame([row.asDict() for row in result])\n",
    "    if df.shape[0] == 0:\n",
    "        return('No paper found with that DOI')\n",
    "    \n",
    "    title = df['title'].values[0]\n",
    "    first_author = df['last_name'].values[0]\n",
    "    year = df['year'].values[0]  \n",
    "    url = df['full_text_url'].values[0]\n",
    "    xml = requests.get(url).text\n",
    "\n",
    "    file_path = Path(base_file_path + '/' + doi + '.nxml')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_information_content_entity_iri(ice, id_prefix):\n",
    "    \"\"\"Reads an identifier for a given prefix\"\"\"\n",
    "    idmap = {k[:k.find(':')]:k[k.find(':')+1:] for k in ice.xref} \n",
    "    return idmap.get(id_prefix)\n",
    "\n",
    "def get_nxml_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "    \n",
    "    efetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+api_key+'&db=pmc&id='+pmc_id+'&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(efetch_url)\n",
    "    efetch_response = urlopen(efetch_url)\n",
    "    efetch_data = efetch_response.read().decode('utf-8')\n",
    "    xml = BeautifulSoup(efetch_data, \"lxml-xml\")\n",
    "    body_tag = xml.findAll('body')\n",
    "    if body_tag is None:\n",
    "        return    \n",
    "    \n",
    "    file_path = Path(base_file_path + '/' + doi + '.nxml')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(xml))\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def get_pdf_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "\n",
    "    # OA Dataset \n",
    "    oapi_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id='+pmc_id+'&format=pdf'\n",
    "    sleep(0.1)\n",
    "    print(oapi_url)\n",
    "    oapi_response = urlopen(oapi_url)\n",
    "    oapi_data = oapi_response.read().decode('utf-8')    \n",
    "    oapi_soup = BeautifulSoup(oapi_data, \"lxml-xml\")\n",
    "    \n",
    "    pdf_link_tag = oapi_soup.find('link')\n",
    "    if(pdf_link_tag is None):\n",
    "        print('No PDF found for that DOI')\n",
    "        return\n",
    "    \n",
    "    pdf_url = pdf_link_tag['href']\n",
    "    if pdf_url.startswith('ftp:'):\n",
    "        pdf_url = pdf_url.replace('ftp:','https:') \n",
    "    file_path = Path(base_file_path + '/' + doi + '.pdf')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    download_file(pdf_url, file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class QuerySpec:\n",
    "    name: str\n",
    "    id_col: str\n",
    "    query_col: str\n",
    "    name_col: str\n",
    "    col_map: dict[str, str] = field(default_factory=dict)\n",
    "    sections: list[str] = field(default_factory=list)\n",
    "\n",
    "    def process_cdf(self, cdf: pd.DataFrame):\n",
    "        cdf = cdf.rename(columns={self.id_col:'ID', self.query_col:'QUERY'})\n",
    "        cdf = cdf.rename(columns=self.col_map)\n",
    "        cdf = cdf.fillna('').rename(\n",
    "            columns={c:re.sub('[\\s\\(\\)]','_', c.upper()) for c in cdf.columns}\n",
    "            )\n",
    "        cdf.QUERY = [re.sub('^http[s]*://', '', r.QUERY) if r.QUERY[:4]=='http' else r.QUERY \n",
    "                     for i,r in cdf.iterrows()] \n",
    "        cdf.QUERY = [re.sub('/$', '', r.QUERY.strip())  \n",
    "                        for i,r in cdf.iterrows()]\n",
    "        return cdf\n",
    "\n",
    "class LocalLiteratureDb:\n",
    "  \n",
    "  \"\"\"This class runs a set of queries on external literature databases to build a local database of linked corpora and papers.\n",
    "\n",
    "  Functionality includes:\n",
    "\n",
    "    * Define a spreadsheet with a column of queries expressed in boolean logic\n",
    "    * Optional: Define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n",
    "    * Iterate over sources (currently only European PMC, but possibly others) to execute all combinations of queries and subqueries\n",
    "    * Builds a local SQLite database with tables for collections, expressions, items, and fragments. \n",
    "    * Provides an API for querying the database and returning results as sqlAlchemy objects.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, loc, name):\n",
    "    self.name = name\n",
    "    \n",
    "    if loc[-1] != '/':\n",
    "      loc = loc + '/' \n",
    "    self.loc = loc\n",
    "    if os.path.exists(loc) is False:\n",
    "      os.mkdir(loc)\n",
    "\n",
    "    db_path = Path(loc+name+'/sciknow.db')\n",
    "    db_dir = db_path.parent\n",
    "    if db_dir.exists() is False:\n",
    "      os.makedirs(db_dir)\n",
    "\n",
    "    if db_path.exists() is False:\n",
    "      schema_sql = files(linkml).joinpath('schema.sql').read_text()\n",
    "      connection = sqlite3.connect(db_path)\n",
    "      cursor = connection.cursor()\n",
    "      cursor.executescript(schema_sql) \n",
    "    \n",
    "    self.engine = create_engine('sqlite:///'+loc+name+'/sciknow.db')\n",
    "    self.session = None # instantiate session when needed\n",
    "\n",
    "    log_path = '%s%s_log.txt'%(loc, name)\n",
    "    if os.path.exists(log_path) is False:\n",
    "      Path(log_path).touch()\n",
    "\n",
    "  def add_sections_as_fragments_from_nxmldoc(self, item, d):\n",
    "    \"\"\"Builds a set of fragments from an NxmlDoc object\"\"\"\n",
    "\n",
    "    # ['PMID', 'PARAGRAPH_ID', 'TAG', 'TAG_TREE', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'\n",
    "    fragments = []\n",
    "    try: \n",
    "      df = d.build_simple_document_dataframe()\n",
    "    except Exception as ex:\n",
    "      return\n",
    "    \n",
    "    # Can search for a substring in top section headers\n",
    "    # df3 = df[df.TOP_SECTION.str.contains('method', case=False)]\n",
    "\n",
    "    df3 = df[df.TOP_SECTION!='']\n",
    "    df4 = df3.groupby('SECTION_TREE').agg({'OFFSET': 'min', 'LENGTH': 'sum', 'PLAIN_TEXT':lambda x: '\\n'.join(x)}).sort_values('OFFSET')\n",
    "    df4 = df4.reset_index(drop=False)\n",
    "    \n",
    "    if df4 is None:\n",
    "      return\n",
    "    \n",
    "    for i, tup in df4.iterrows():      \n",
    "      fragment = ScientificKnowledgeFragment(id=item.id[:10]+'.'+str(i), \n",
    "                                             type='section',\n",
    "                                             offset=tup.OFFSET,\n",
    "                                             length=tup.LENGTH,\n",
    "                                             name=tup.SECTION_TREE,\n",
    "                                             content=tup.PLAIN_TEXT)\n",
    "      self.session.add(fragment)\n",
    "      self.session.flush()\n",
    "      item.has_part.append(fragment)\n",
    "      fragment.part_of = item.id\n",
    "      fragments.append(fragment)\n",
    "      self.session.flush()     \n",
    "\n",
    "  def add_fragments_from_nxmldoc(self, item, d):\n",
    "    \"\"\"Builds a set of fragments from an NxmlDoc object\"\"\"\n",
    "\n",
    "    # ['PMID', 'PARAGRAPH_ID', 'TAG', 'TAG_TREE', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'\n",
    "    fragments = []\n",
    "    df = d.build_enahanced_document_dataframe()\n",
    "    if df is None:\n",
    "      return\n",
    "    for i, tup in d.build_enahanced_document_dataframe().iterrows():      \n",
    "      fragment = ScientificKnowledgeFragment(id=item.id[:10]+'.'+str(tup.PARAGRAPH_ID), \n",
    "                                             type=tup.TAG,\n",
    "                                             offset=tup.OFFSET,\n",
    "                                             length=tup.LENGTH,\n",
    "                                             content=tup.PLAIN_TEXT)\n",
    "      self.session.add(fragment)\n",
    "      self.session.flush()\n",
    "      item.has_part.append(fragment)\n",
    "      fragment.part_of = item.id\n",
    "      fragments.append(fragment)\n",
    "      self.session.flush()     \n",
    "\n",
    "  def add_full_text_for_expression(self, e,  \n",
    "                                   get_nxml=True, \n",
    "                                   get_pdf=False, \n",
    "                                   get_html=False):\n",
    "    doi = read_information_content_entity_iri(e, 'doi')\n",
    "    if doi is None:\n",
    "      return  \n",
    "\n",
    "    # Copy file to local directory\n",
    "    base_file_path = '%s%s/nxml/'%(self.loc, self.name)\n",
    "    if get_nxml:\n",
    "      nxml_file_path = base_file_path+doi+'.nxml'\n",
    "      if(os.path.exists(nxml_file_path) is False):\n",
    "        try:\n",
    "          get_nxml_from_pubmed_doi(doi, base_file_path)\n",
    "        except Exception as ex:\n",
    "          return\n",
    "      if os.path.exists(nxml_file_path):\n",
    "        with open(nxml_file_path, 'r') as f:\n",
    "          xml = f.read()\n",
    "          try:\n",
    "            d = NxmlDoc(doi, xml)\n",
    "          except Exception as ex:\n",
    "            return\n",
    "          doc_text = d.text \n",
    "          if 'body' in [so.element.tag for so in d.standoffs]:\n",
    "            ski_type = 'FullTextPaper'\n",
    "          else:\n",
    "            ski_type = 'CitationRecord'     \n",
    "          ski = ScientificKnowledgeItem(id=str(uuid.uuid4().hex), \n",
    "                                              content=doc_text,\n",
    "                                              xref=['file:'+nxml_file_path],\n",
    "                                              type=ski_type,)\n",
    "          e.has_representation.append(ski)\n",
    "          self.session.add(ski)\n",
    "          self.session.flush()\n",
    "          self.add_sections_as_fragments_from_nxmldoc(ski, d)\n",
    "\n",
    "    if get_pdf:\n",
    "      #get_pdf_from_pubmed_doi(doi, base_file_path)\n",
    "      placeholder = 1\n",
    "\n",
    "    if get_html:\n",
    "      placeholder = 1   \n",
    "\n",
    "  def add_full_text_for_collection(self, collection_id, \n",
    "                                   get_nxml=True, get_pdf=True, get_html=True):\n",
    "    for e in self.list_expressions(collection_id=collection_id):\n",
    "      doi = read_information_content_entity_iri(e, 'doi')\n",
    "      if doi is None:\n",
    "        continue  \n",
    "      self.add_full_text_for_expression(e, get_nxml, get_pdf, get_html)      \n",
    "\n",
    "  def add_corpus_from_epmc(self, qt, qt2, sections=['paper_title', 'ABSTRACT'], sections2=['paper_title', 'ABSTRACT']):\n",
    "    \"\"\"Adds corpora based on coupled QueryTranslator objects.\"\"\"\n",
    "    \n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "\n",
    "    if self.session.query(exists().where(InformationResource.name=='EPMC')).scalar():\n",
    "      info_resource = self.session.query(InformationResource) \\\n",
    "          .filter(InformationResource.name=='EPMC').first()\n",
    "    else:\n",
    "      info_resource = InformationResource(id='skem:EPMC', \n",
    "                                          iri=['skem:EPMC'], \n",
    "                                          name='European PubMed Central', \n",
    "                                          type='skem:InformationResource',\n",
    "                                          xref=['https://europepmc.org/'])\n",
    "\n",
    "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
    "    if qt2:\n",
    "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections2)\n",
    "    else: \n",
    "      (subset_ids, epmc_subset_queries) = ([0],[''])\n",
    "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
    "      for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "        query = q\n",
    "        corpus_id = str(i)\n",
    "        corpus_name = qt.df.loc[qt.df['ID']==i][qt.name_col].values[0]\n",
    "        if query is None or query=='nan' or len(query)==0: \n",
    "          continue\n",
    "        if len(sq) > 0:\n",
    "          query = '(%s) AND (%s)'%(q, sq)\n",
    "          corpus_id = str(i)+'.'+str(j)\n",
    "          corpus_name2 = qt2.df.loc[qt2.df['ID']==i][qt2.name_col].values[0]\n",
    "          corpus_name = corpus_name + '/'+ corpus_name2\n",
    "        \n",
    "        # does this collection already exist?  \n",
    "        if self.session.query(exists().where(ScientificKnowledgeCollection.id==corpus_id)).scalar():\n",
    "          cp = self.session.query(ScientificKnowledgeCollection) \\\n",
    "              .join(ScientificKnowledgeCollection.has_members, isouter=True) \\\n",
    "              .filter(ScientificKnowledgeCollection.id==corpus_id).first()\n",
    "          if cp is not None:\n",
    "            for p in cp.has_members:\n",
    "              cp.has_members.remove(p)\n",
    "            self.session.flush()\n",
    "            self.session.delete(cp)\n",
    "            self.session.commit()\n",
    "\n",
    "        corpus = ScientificKnowledgeCollection(id=corpus_id,\n",
    "                                                           type='skem:ScientificKnowledgeCollection',\n",
    "                                                           provenance=[query], \n",
    "                                                           name=corpus_name,\n",
    "                                                           has_members=[])\n",
    "        self.session.add(corpus)\n",
    "        self.session.commit()\n",
    "\n",
    "        epmcq = EuroPMCQuery()\n",
    "        numFound, pubs = epmcq.run_empc_query(query)\n",
    "        for p in tqdm(pubs):\n",
    "          p_id = str(p.id)\n",
    "          p_check = self.session.query(ScientificKnowledgeExpression) \\\n",
    "              .filter(ScientificKnowledgeExpression.id==p_id).first()\n",
    "          if p_check is not None:\n",
    "            p = p_check\n",
    "          corpus.has_members.append(p)\n",
    "          for item in p.has_representation:\n",
    "            for f in item.has_part:          \n",
    "              f.part_of = item.id\n",
    "              self.session.add(f)\n",
    "              self.session.flush()\n",
    "            item.represented_by = p.id\n",
    "            self.session.add(item)\n",
    "            self.session.flush()\n",
    "          self.session.add(p)\n",
    "          self.session.flush()\n",
    "        self.session.commit()\n",
    "\n",
    "  def check_query_terms(self, qt, qt2=None, pubmed_api_key=''):\n",
    "    pmq = ESearchQuery(api_key=pubmed_api_key)\n",
    "    terms = set()\n",
    "    for t in qt.terms2id.keys():\n",
    "        terms.add(t)\n",
    "    if qt2 is not None:\n",
    "        for t2 in qt2.terms2id.keys():\n",
    "            terms.add(t2)\n",
    "    check_table = {} \n",
    "    for t in tqdm(terms):\n",
    "        (is_ok, t2, c) = pmq._check_query_phrase(t)\n",
    "        check_table[t] = (is_ok, c)\n",
    "    return check_table\n",
    "  \n",
    "  def list_collections(self, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if search_term:\n",
    "      q = (self.session.query(ScientificKnowledgeCollection) \n",
    "          .filter(ScientificKnowledgeCollection.name == search_term))\n",
    "    else:\n",
    "      q = self.session.query(ScientificKnowledgeCollection)\n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_expressions(self, collection_id=None, search_term=None): \n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if collection_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeCollectionHasMembers,\n",
    "                              ScientificKnowledgeExpression,\n",
    "                              ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem) \n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id==collection_id)\n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id)\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "         q = (self.session.query(ScientificKnowledgeCollection, \n",
    "                              ScientificKnowledgeCollectionHasMembers,\n",
    "                              ScientificKnowledgeExpression ) \n",
    "            .filter(ScientificKnowledgeCollection.id == ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id)\n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id)\n",
    "            .filter(ScientificKnowledgeCollection.id == collection_id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeExpression,\n",
    "                              ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem) \n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id)\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(ScientificKnowledgeExpression)\n",
    "      \n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_notes_for_fragments_in_paper(self, run_name, paper_id, item_type='FullTextPaper'):\n",
    "    '''returns notes of a specific type associated with fragments from a given paper .'''\n",
    "    q1 = self.session.query(ScientificKnowledgeItem) \\\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id) \\\n",
    "            .filter(ScientificKnowledgeItem.type == item_type) \\\n",
    "            .filter(ScientificKnowledgeExpression.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "      return []\n",
    "    for f in i.has_part:\n",
    "      for n in f.has_notes:\n",
    "        if n.name == run_name:\n",
    "          yield(n)\n",
    "         \n",
    "  def list_fragments_for_paper(self, paper_id, item_type='FullTextPaper'):\n",
    "    '''Loads fragments from a given paper sections of a specified paper from the local database.'''\n",
    "    q1 = self.session.query(ScientificKnowledgeItem) \\\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id) \\\n",
    "            .filter(ScientificKnowledgeItem.type == item_type) \\\n",
    "            .filter(ScientificKnowledgeExpression.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "        raise Exception('No items of format: %s found for expression: %s'%(item_type, str(paper_id)))\n",
    "    fragments = []\n",
    "    for f in i.has_part:\n",
    "      fragments.append(f)\n",
    "    return sorted(fragments, key=lambda f: f.offset)\n",
    "  \n",
    "  def list_fragments(self, expression_id=None, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if expression_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem,\n",
    "                              ScientificKnowledgeItemHasPart,\n",
    "                              ScientificKnowledgeFragment) \n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.id == ScientificKnowledgeItemHasPart.ScientificKnowledgeItem_id)\n",
    "            .filter(ScientificKnowledgeItemHasPart.has_part_id == ScientificKnowledgeFragment.id)\n",
    "            .filter(ScientificKnowledgeFragment.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = (self.session.query(ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem,\n",
    "                              ScientificKnowledgeItemHasPart,\n",
    "                              ScientificKnowledgeFragment) \n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.id == ScientificKnowledgeItemHasPart.ScientificKnowledgeItem_id)\n",
    "            .filter(ScientificKnowledgeItemHasPart.has_part_id == ScientificKnowledgeFragment.id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem,\n",
    "                              ScientificKnowledgeItemHasPart,\n",
    "                              ScientificKnowledgeFragment) \n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.id == ScientificKnowledgeItemHasPart.ScientificKnowledgeItem_id)\n",
    "            .filter(ScientificKnowledgeItemHasPart.has_part_id == ScientificKnowledgeFragment.id)\n",
    "            .filter(ScientificKnowledgeFragment.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(ScientificKnowledgeFragment)\n",
    "    for c in q.all():\n",
    "      yield(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
