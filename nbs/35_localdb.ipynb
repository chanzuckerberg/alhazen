{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building databases of published works  \n",
    "\n",
    "> Pragmatic tools for constructing databases of scientific works based on queries defined with Boolean Logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.local_literature_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabulate queries in a spreadsheet and generate a database based on the data from those queries. \n",
    "\n",
    "**Example**:  Define a dataframe with an `id` column and a `query` column (expressing a search query in Boolean Logic):\n",
    "\n",
    "| ID | DISEASE NAME | QUERY  | \n",
    "|----|--------------|--------|\n",
    "| 1 | Adult Polyglucosan Body Disease | adult polyglucosan body disease \\| adult polyglucosan body neuropathy\n",
    "| 2 | AGAT deficiency |  \"GATM deficiency\" \\| \"AGAT deficiency\" \\| \"arginine:glycine amidinotransferase deficiency\" \\| \"L-arginine:glycine amidinotransferase deficiency\"\n",
    "| 3 | Guanidinoacetate methyltransferase deficiency | \"guanidinoacetate methyltransferase deficiency\" \\| \"GAMT deficiency\"\n",
    "| 4 | CLOVES Syndrome | \"CLOVES syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation epidermal) & (nevi-spinal) & syndrome \\| (congenital lipomatous overgrowth) & (vascular malformations) & (Epidermal nevi) & ((skeletal\\|spinal) & abnormalities) \\| CLOVE syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation) & (epidermal nevi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "import alhazen.schema_sqla as linkml_sqla\n",
    "import alhazen.schema_python as linkml_py\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import local_resources.linkml as linkml\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from sqlalchemy import create_engine, exists\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import sqlite3  \n",
    "import sys\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't export this but leave it in the notebook code for future reference\n",
    "from databricks import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't export this but leave it in the notebook code for future reference\n",
    "def get_nxml_from_scipubstore_doi(doi, base_file_path):\n",
    "    \n",
    "    if os.environ.get('DB_TOKEN') is None:  \n",
    "        msg = 'Error attempting to query Databricks for URL data, did you set the DB_TOKEN environment variable?'\n",
    "        raise Exception(msg)\n",
    "\n",
    "    get_ft_url_from_doi_sql = '''\n",
    "        SELECT DISTINCT p.pmc_id, p.doi, YEAR(p.publication_date) as year, p.title, p.abstract, p.full_text_format, p.full_text_url, a.last_name\n",
    "        FROM scipubstore.ingestion.papers as p \n",
    "            JOIN scipubstore.ingestion.authors as a on (p.paper_id=a.paper_id) \n",
    "        WHERE p.doi = '{}' and a.author_index=1\n",
    "        ORDER BY p.full_text_url DESC\n",
    "    '''.format(doi)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with sql.connect(server_hostname = 'czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com',\n",
    "                        http_path = '/sql/1.0/warehouses/1c4df94f2f1a6305',\n",
    "                        access_token = 'databricks-access-token-here') as connection:\n",
    "\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(get_ft_url_from_doi_sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "    df = pd.DataFrame([row.asDict() for row in result])\n",
    "    if df.shape[0] == 0:\n",
    "        return('No paper found with that DOI')\n",
    "    \n",
    "    title = df['title'].values[0]\n",
    "    first_author = df['last_name'].values[0]\n",
    "    year = df['year'].values[0]  \n",
    "    url = df['full_text_url'].values[0]\n",
    "    xml = requests.get(url).text\n",
    "\n",
    "    file_path = Path(base_file_path + '/' + doi + '.nxml')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_nxml_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "    \n",
    "    efetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+api_key+'&db=pmc&id='+pmc_id+'&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(efetch_url)\n",
    "    efetch_response = urlopen(efetch_url)\n",
    "    efetch_data = efetch_response.read().decode('utf-8')\n",
    "    xml = BeautifulSoup(efetch_data, \"lxml-xml\")\n",
    "    body_tag = xml.findAll('body')\n",
    "    if body_tag is None:\n",
    "        return    \n",
    "    \n",
    "    file_path = Path(base_file_path + '/' + doi + '.nxml')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(xml))\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def get_pdf_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "\n",
    "    # OA Dataset \n",
    "    oapi_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id='+pmc_id+'&format=pdf'\n",
    "    sleep(0.1)\n",
    "    print(oapi_url)\n",
    "    oapi_response = urlopen(oapi_url)\n",
    "    oapi_data = oapi_response.read().decode('utf-8')    \n",
    "    oapi_soup = BeautifulSoup(oapi_data, \"lxml-xml\")\n",
    "    \n",
    "    pdf_link_tag = oapi_soup.find('link')\n",
    "    if(pdf_link_tag is None):\n",
    "        print('No PDF found for that DOI')\n",
    "        return\n",
    "    \n",
    "    pdf_url = pdf_link_tag['href']\n",
    "    if pdf_url.startswith('ftp:'):\n",
    "        pdf_url = pdf_url.replace('ftp:','https:') \n",
    "    file_path = Path(base_file_path + '/' + doi + '.pdf')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    download_file(pdf_url, file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class QuerySpec:\n",
    "    name: str\n",
    "    id_col: str\n",
    "    query_col: str\n",
    "    name_col: str\n",
    "    col_map: dict[str, str] = field(default_factory=dict)\n",
    "    sections: list[str] = field(default_factory=list)\n",
    "\n",
    "    def process_cdf(self, cdf: pd.DataFrame):\n",
    "        cdf = cdf.rename(columns={self.id_col:'ID', self.query_col:'QUERY'})\n",
    "        cdf = cdf.rename(columns=self.col_map)\n",
    "        cdf = cdf.fillna('').rename(\n",
    "            columns={c:re.sub('[\\s\\(\\)]','_', c.upper()) for c in cdf.columns}\n",
    "            )\n",
    "        cdf.QUERY = [re.sub('^http[s]*://', '', r.QUERY) if r.QUERY[:4]=='http' else r.QUERY \n",
    "                     for i,r in cdf.iterrows()] \n",
    "        cdf.QUERY = [re.sub('/$', '', r.QUERY.strip())  \n",
    "                        for i,r in cdf.iterrows()]\n",
    "        return cdf\n",
    "\n",
    "class LocalLiteratureDb:\n",
    "\n",
    "  \"\"\"This class runs a set of queries on external literature databases to build a local database of linked corpora and papers.\n",
    "\n",
    "  Functionality includes:\n",
    "\n",
    "    * Define a spreadsheet with a column of queries expressed in boolean logic\n",
    "    * Optional: Define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n",
    "    * Iterate over different sources (Pubmed + European Pubmed) to execute all combinations of queries and subqueries\n",
    "    \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, loc, name):\n",
    "    self.name = name\n",
    "    \n",
    "    if loc[-1] != '/':\n",
    "      loc = loc + '/' \n",
    "    self.loc = loc\n",
    "    if os.path.exists(loc) is False:\n",
    "      os.mkdir(loc)\n",
    "\n",
    "    db_path = Path(loc+name+'/sciknow.db')\n",
    "    db_dir = db_path.parent\n",
    "    if db_dir.exists() is False:\n",
    "      os.makedirs(db_dir)\n",
    "\n",
    "    if db_path.exists() is False:\n",
    "      schema_sql = files(linkml).joinpath('schema.sql').read_text()\n",
    "      connection = sqlite3.connect(db_path)\n",
    "      cursor = connection.cursor()\n",
    "      cursor.executescript(schema_sql) \n",
    "    \n",
    "    self.engine = create_engine('sqlite:///'+loc+name+'/sciknow.db')\n",
    "    self.session = None # instantiate session when needed\n",
    "\n",
    "    log_path = '%s%s_log.txt'%(loc, name)\n",
    "    if os.path.exists(log_path) is False:\n",
    "      Path(log_path).touch()\n",
    "\n",
    "  def add_corpus_from_epmc(self, qt, qt2, sections=['paper_title', 'ABSTRACT'], sections2=['paper_title', 'ABSTRACT']):\n",
    "    \n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "\n",
    "    if self.session.query(exists().where(linkml_sqla.InformationResource.id=='EPMC')).scalar():\n",
    "      info_resource = self.session.query(linkml_sqla.InformationResource) \\\n",
    "          .filter(linkml_sqla.InformationResource.id=='EPMC').first()\n",
    "    else:\n",
    "      info_resource = linkml_sqla.InformationResource(id='EPMC', \n",
    "                                                      name='European Pubmed Central', \n",
    "                                                      xref=['https://europepmc.org/'])\n",
    "    \n",
    "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
    "    if qt2:\n",
    "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections2)\n",
    "    else: \n",
    "      (subset_ids, epmc_subset_queries) = ([0],[''])\n",
    "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
    "      for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "        query = q\n",
    "        corpus_id = str(i)\n",
    "        corpus_name = qt.df.iloc[i][qt.name_col]\n",
    "        if query is None or query=='nan' or len(query)==0: \n",
    "          continue\n",
    "        if len(sq) > 0:\n",
    "          query = '(%s) AND (%s)'%(q, sq)\n",
    "          corpus_id = str(i)+'.'+str(j)\n",
    "          corpus_name2 = qt2.df.iloc[j][qt2.name_col]\n",
    "          corpus_name = corpus_name + '/'+ corpus_name2\n",
    "        \n",
    "        # does this collection already exist?  \n",
    "        if self.session.query(exists().where(linkml_sqla.ScientificPublicationCollection.id==corpus_id)).scalar():\n",
    "          cp = self.session.query(linkml_sqla.ScientificPublicationCollection) \\\n",
    "              .join(linkml_sqla.ScientificPublicationCollection.has_part, isouter=True) \\\n",
    "              .filter(linkml_sqla.ScientificPublicationCollection.id==corpus_id).first()\n",
    "          if cp is not None:\n",
    "            for p in cp.has_part:\n",
    "              cp.has_part.remove(p)\n",
    "            self.session.flush()\n",
    "            self.session.delete(cp)\n",
    "            self.session.commit()\n",
    "        #self.session.query(linkml_sqla.ScientificPublicationCollection).delete()\n",
    "        #self.session.commit()\n",
    "\n",
    "        corpus = linkml_sqla.ScientificPublicationCollection(id=corpus_id, logical_query=query, name=corpus_name)\n",
    "        corpus.information_sources.append(info_resource)\n",
    "        self.session.add(corpus)\n",
    "        self.session.commit()\n",
    "\n",
    "        epmcq = EuroPMCQuery()\n",
    "        numFound, epmc_publications = epmcq.run_empc_query(query)\n",
    "        for p in tqdm(epmc_publications):\n",
    "          p_check = self.session.query(linkml_sqla.ScientificPublication).filter(linkml_sqla.ScientificPublication.id==p.id).first()\n",
    "          if p_check is not None:\n",
    "            p = p_check\n",
    "          self.session.add(p)\n",
    "          corpus.has_part.append(p)      \n",
    "        try:      \n",
    "          self.session.commit()\n",
    "        except TypeError as e:\n",
    "          print('Error somewhere but commit should be OK')\n",
    "          #print(e)\n",
    "          #self.session.rollback()\n",
    "          #continue\n",
    " \n",
    "        self.session.commit()\n",
    "\n",
    "  def check_query_terms(self, qt, qt2=None, pubmed_api_key=''):\n",
    "    pmq = ESearchQuery(api_key=pubmed_api_key)\n",
    "    terms = set()\n",
    "    for t in qt.terms2id.keys():\n",
    "        terms.add(t)\n",
    "    if qt2 is not None:\n",
    "        for t2 in qt2.terms2id.keys():\n",
    "            terms.add(t2)\n",
    "    check_table = {} \n",
    "    for t in tqdm(terms):\n",
    "        (is_ok, t2, c) = pmq._check_query_phrase(t)\n",
    "        check_table[t] = (is_ok, c)\n",
    "    return check_table\n",
    "  \n",
    "  def add_corpora(self, cdf, id_col, query_col, name_col, notes_col=None, col_map={}, sections=['TITLE','ABSTRACT']):  \n",
    "    cdf = cdf.rename(columns={id_col:'ID', query_col:'QUERY'})\n",
    "    cdf = cdf.rename(columns=col_map)\n",
    "    cdf = cdf.fillna('').rename(\n",
    "        columns={c:re.sub('[\\s\\(\\)]','_', c.upper()) for c in cdf.columns}\n",
    "        )\n",
    "    cdf.QUERY = [re.sub('^http[s]*://', '', r.QUERY) if r.QUERY[:4]=='http' else r.QUERY \n",
    "                  for i,r in cdf.iterrows()] \n",
    "    cdf.QUERY = [re.sub('/$', '', r.QUERY.strip())  \n",
    "                    for i,r in cdf.iterrows()] \n",
    "    qt = QueryTranslator(cdf, id_col, query_col, name_col, notes_col)\n",
    "    self.add_corpus_from_epmc(qt, None, sections=sections)\n",
    "\n",
    "  def list_corpora(self, corpus_id=None): \n",
    "    cs = self.session.query(linkml_sqla.ScientificPublicationCollection).all()\n",
    "    for c in cs:\n",
    "      yield(c)\n",
    "  \n",
    "  def list_corpus_publications(self, corpus_id): \n",
    "    cp = self.session.query(linkml_sqla.ScientificPublicationCollection) \\\n",
    "            .join(linkml_sqla.ScientificPublicationCollection.has_part) \\\n",
    "            .filter(linkml_sqla.ScientificPublicationCollection.id==corpus_id).first()\n",
    "    for p in cp.has_part:\n",
    "      yield(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
