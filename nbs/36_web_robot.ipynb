{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Robots  \n",
    "\n",
    "> Web robots that automates the process of obtaining full text papers (and other interactions with the web)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.web_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pickle as pkl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from splinter import Browser\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def retrieve_pdf_from_doidotorg(doi, base_dir):\n",
    "    browser = Browser()#, headless=True)\n",
    "    doi = doi.replace('https://doi.org/', '').replace('doi', '')\n",
    "    stem = doi.split('/')[0]\n",
    "    if os.path.exists(base_dir+'/'+stem) is False:\n",
    "        os.makedirs(base_dir+'/'+stem)\n",
    "    hrefs = set()\n",
    "    try:\n",
    "        # visit bioRxiv's paper page\n",
    "        browser.visit('https://doi.org/'+doi)\n",
    "        sleep(randint(5,10)*0.1)\n",
    "        for link in browser.find_by_css('a'):\n",
    "            if link['href'] is not None and 'pdf' in link['href']:\n",
    "                hrefs.add(link['href'])\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    link = min(hrefs, key=len) # prints \"i\"\n",
    "    response = requests.get(link)\n",
    "    with open(base_dir+'/'+doi+'.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "#retrieve_pdf_from_doidotorg('10.1083/jcb.202204093', '/Users/gburns/alhazen/em_tech/temp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def retrieve_full_text_links_from_biorxiv(doi, base_dir):\n",
    "    browser = Browser()#, headless=True)\n",
    "    doi = doi.replace('https://doi.org/', '').replace('doi', '')\n",
    "    if doi.startswith('10.1101/') is False:\n",
    "        print('Not a BioRxiv DOI')\n",
    "        return None\n",
    "    if os.path.exists(base_dir+'/10.1101') is False:\n",
    "        os.makedirs(base_dir+'/10.1101')\n",
    "    hrefs = []\n",
    "    try:\n",
    "        # visit bioRxiv's paper page\n",
    "        browser.visit('https://www.biorxiv.org/content/'+doi)\n",
    "        sleep(randint(5,10)*0.1)\n",
    "        for link in browser.find_by_css('a[class=\"dropdown-link\"]'):\n",
    "            if link['href']:\n",
    "                hrefs.append(link['href'])\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    files = []\n",
    "    for link in hrefs:\n",
    "        if link.endswith('.pdf'):\n",
    "            response = requests.get(link)\n",
    "            with open(base_dir+'/'+doi+'.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            files.append(base_dir+'/'+doi+'.pdf')\n",
    "        elif link.endswith('.xml'):\n",
    "            response = requests.get(link)\n",
    "            with open(base_dir+'/'+doi+'.nxml', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            files.append(base_dir+'/'+doi+'.nxml')\n",
    "    return files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def execute_search_on_biorxiv(search_term):\n",
    "    browser = Browser()#, headless=True)\n",
    "    all_dois = []\n",
    "\n",
    "    try:\n",
    "        # visit bioRxiv's search page\n",
    "        browser.visit('https://www.biorxiv.org/search')\n",
    "\n",
    "        # fill in the search form\n",
    "        sleep(randint(5,10)*0.1)\n",
    "        print('fill search term')\n",
    "        browser.find_by_id('edit-txtsimple').fill(search_term)\n",
    "\n",
    "        sleep(randint(5,10)*0.1)\n",
    "        print('scroll to bottom')\n",
    "        #browser.scroll_to('bottom')\n",
    "        \n",
    "        print('click search button')        \n",
    "        browser.find_by_css('a[class=\"search-choice-close\"]').click()    \n",
    "        #browser.find_by_css('input[class=\"form-submit\"]').click()    \n",
    "        browser.find_by_id('edit-actions').find_by_value('Search').click()\n",
    "        \n",
    "        sleep(randint(5,10)*0.1)\n",
    "        print('load next page')\n",
    "        # Extract the number of results\n",
    "        formatted_string = browser.find_by_id('page-title').text\n",
    "        \n",
    "        # Use regular expressions to get the number from a string formatted 'XXX Results'\n",
    "        m = re.search(r'\\d+ Results', formatted_string)\n",
    "        if m:\n",
    "            num_results = int(re.search(r'\\d+', formatted_string).group())\n",
    "            \n",
    "            loop_count = 0\n",
    "            while True:\n",
    "                \n",
    "                # Extract each result from the list of the web page\n",
    "                doi_links = browser.find_by_css('span[class=\"highwire-cite-metadata-doi highwire-cite-metadata\"]')\n",
    "                all_dois.extend([re.sub('doi: ', '', t.text) for t in doi_links])\n",
    "                #\n",
    "                # Is next button absent?\n",
    "                next_button_not_present = browser.is_element_not_present_by_css('a[class=\"link-icon link-icon-after\"]')\n",
    "\n",
    "                if next_button_not_present:\n",
    "                    break\n",
    "            \n",
    "                # Find the next button on the page\n",
    "                next_button = browser.find_by_css('a[class=\"link-icon link-icon-after\"]')\n",
    "\n",
    "                # Click the next button\n",
    "                next_button.click()\n",
    "                sleep(randint(5,10)*0.1)\n",
    "                #print('load page number'+str(loop_count))\n",
    "\n",
    "                loop_count += 1\n",
    "                if loop_count > 100:\n",
    "                    break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    browser.quit()\n",
    "    return all_dois\n",
    "\n",
    "#all_dois = run_biorxiv_assistant('cellxgene')\n",
    "#print(all_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nxml_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Given a DOI, see if we can find it in NCBI's OA dataset \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "    \n",
    "    efetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+api_key+'&db=pmc&id='+pmc_id+'&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(efetch_url)\n",
    "    efetch_response = urlopen(efetch_url)\n",
    "    efetch_data = efetch_response.read().decode('utf-8')\n",
    "    xml = BeautifulSoup(efetch_data, \"lxml-xml\")\n",
    "    body_tag = xml.findAll('body')\n",
    "    if body_tag is None:\n",
    "        return    \n",
    "    \n",
    "    file_path = Path(base_file_path + '/' + doi + '.nxml')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(xml))\n",
    "    return [file_path]\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def get_pdf_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return []\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "\n",
    "    # OA Dataset \n",
    "    oapi_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id='+pmc_id+'&format=pdf'\n",
    "    sleep(0.1)\n",
    "    print(oapi_url)\n",
    "    oapi_response = urlopen(oapi_url)\n",
    "    oapi_data = oapi_response.read().decode('utf-8')    \n",
    "    oapi_soup = BeautifulSoup(oapi_data, \"lxml-xml\")\n",
    "    \n",
    "    pdf_link_tag = oapi_soup.find('link')\n",
    "    if(pdf_link_tag is None):\n",
    "        print('No PDF found for that DOI')\n",
    "        return []\n",
    "    \n",
    "    pdf_url = pdf_link_tag['href']\n",
    "    if pdf_url.startswith('ftp:'):\n",
    "        pdf_url = pdf_url.replace('ftp:','https:') \n",
    "    file_path = Path(base_file_path + '/' + doi + '.pdf')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    download_file(pdf_url, file_path)  \n",
    "    return [file_path] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def get_html_from_pmc_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Given a DOI, navigate to the PMC HTML page and reconstruct NXML from that \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return []\n",
    "    pmc_id = id_tag.string\n",
    "\n",
    "    # navigate to the PMC HTML page (https://www.ncbi.nlm.nih.gov/pmc/articles/<PMC_ID>/)\n",
    "    try:\n",
    "        browser = Browser()#, headless=True)\n",
    "        browser.visit('https://www.ncbi.nlm.nih.gov/pmc/articles/'+pmc_id+'/')\n",
    "        screenshot_path = browser.html_snapshot(base_file_path)\n",
    "        temp_html = base_file_path + 'ft/' + doi + '.html'\n",
    "        p = Path(temp_html)\n",
    "        d = p.parent\n",
    "        if d.exists is False:\n",
    "            os.makedirs(d.absolute()) \n",
    "        os.rename(screenshot_path, temp_html)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    finally:\n",
    "        browser.quit()\n",
    "\n",
    "    return [screenshot_path]\n",
    "\n",
    "def clean_and_convert_tags(soup, tag):\n",
    "\n",
    "    new_tag = soup.new_tag(tag.name)\n",
    "    \n",
    "    if( re.match('^h\\d', tag.name) ):\n",
    "        new_tag.name = 'title'\n",
    "            \n",
    "    # Add a placeholder tag for this figure but don't fill it in currently\n",
    "    elif( tag.name == 'div' and 'class' in tag.attrs and 'fig' in tag.attrs['class'] ):\n",
    "        new_tag = soup.new_tag('fig')\n",
    "        new_tag.attrs['id'] = tag.attrs['id']\n",
    "        new_tag.attrs['position'] = 'float'\n",
    "        return new_tag\n",
    "\n",
    "    # Treat all other div tags as new 'sec' tags in new version \n",
    "    elif( tag.name == 'div' ):\n",
    "        new_tag.name = 'sec'\n",
    "    \n",
    "    # top-level link tag to external reference, strip away tags from within the tag\n",
    "    elif( tag.name == 'a' and 'class' in tag.attrs and 'bibr' in tag.attrs['class'] ):\n",
    "        new_tag = soup.new_tag('xref')\n",
    "        new_tag.attrs['ref-type'] = 'bibr' \n",
    "        new_tag.append(NavigableString(tag.string))\n",
    "        return new_tag\n",
    "\n",
    "    # top-level link tag to figure / table (<xref rid=\"fig4\" ref-type=\"fig\">)\n",
    "    elif( tag.name == 'a' and 'class' in tag.attrs and 'fig' in tag.attrs['class'] ):\n",
    "        new_tag = soup.new_tag('xref')\n",
    "        new_tag.attrs['ref-type'] = 'fig' \n",
    "        new_tag.append(NavigableString(tag.string))\n",
    "        return new_tag\n",
    "    \n",
    "    for c in tag.contents:\n",
    "        if( type(c) is Tag ):\n",
    "            new_c = clean_and_convert_tags(soup, c)\n",
    "            if( new_c is not None ):\n",
    "                new_tag.append(new_c)\n",
    "        elif( type(c) is NavigableString ):\n",
    "            new_tag.append(NavigableString(c))\n",
    "                \n",
    "    return new_tag\n",
    "\n",
    "def extract_reconstructed_nxml(html):\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    title = soup.find('h1', attrs={'class': 'content-title'})  \n",
    "    pmcid = soup.find('li', attrs={'class': 'accid'})  \n",
    "    sections = soup.find_all('div', attrs={'class': 'tsec sec'})\n",
    "    \n",
    "    if( pmcid is None ):\n",
    "        return None\n",
    "\n",
    "    making_soup = BeautifulSoup(\"<article></article>\", \"lxml\")\n",
    "    article = making_soup.article\n",
    "    \n",
    "    front = making_soup.new_tag(\"front\")\n",
    "    article.append(front)\n",
    "    \n",
    "    meta = making_soup.new_tag('article-meta')\n",
    "    front.append(meta)\n",
    "    \n",
    "    article_id = making_soup.new_tag('article-id')\n",
    "    meta.append(article_id)\n",
    "    article_id.append( NavigableString( pmcid.text ) )\n",
    "    \n",
    "    title_group = making_soup.new_tag('title-group')\n",
    "    front.append(title_group)\n",
    "    \n",
    "    article_title = making_soup.new_tag('article-title')\n",
    "    article_title.append(NavigableString(title.text))\n",
    "    title_group.append(article_title)\n",
    "       \n",
    "    for sec in sections:\n",
    "        for h2 in sec.children:\n",
    "            if( type(h2) is Tag and h2.name=='h2' and h2.text=='Abstract'):\n",
    "                abstract = making_soup.new_tag('abstract')\n",
    "                front.append(abstract)\n",
    "                for node in h2.next_siblings:\n",
    "                    if( type(node) is Tag):\n",
    "                        abstract.append(clean_and_convert_tags(making_soup, node))\n",
    "    \n",
    "    body = making_soup.new_tag('body')\n",
    "    article.append(body)\n",
    "    for sec in sections:\n",
    "        for h2 in sec.children:\n",
    "            sec = making_soup.new_tag('sec')\n",
    "            if( type(h2) is Tag and h2.name=='h2' \n",
    "                    and h2.text!='Abstract'\n",
    "                    and h2.text!='References'\n",
    "                    and h2.text!='Footnotes'\n",
    "                    and h2.text!='Acknowledgments'):\n",
    "                body.append(sec)\n",
    "                sec.append(clean_and_convert_tags(making_soup, h2))\n",
    "                for p in h2.next_siblings:\n",
    "                    if( type(p) is Tag ):\n",
    "                        new_tag = clean_and_convert_tags(making_soup, p)\n",
    "                        if( new_tag is not None ):\n",
    "                            sec.append(new_tag)\n",
    "                   \n",
    "    return making_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key=d086451c882fabace54d7b049b6fb8481908&db=pmc&term=10.1093/database/baab040[doi]&retmode=xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/gburns/alhazen/temp/z2rthap2.html']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.environ['NCBI_API_KEY'] = 'd086451c882fabace54d7b049b6fb8481908'\n",
    "#get_html_from_pmc_doi('10.1093/database/baab040', '/Users/gburns/alhazen/temp/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
