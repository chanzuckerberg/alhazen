{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Robots  \n",
    "\n",
    "> Web robots that automates the process of obtaining full text papers (and other interactions with the web)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions spawn a web-browser to search external websites and retrieve papers and files for collation into an underlying document store. Developers using `Alhazen` must abide by data licensing requirements and third party websites terms and conditions. Users of this code should ensure that they do not infringe upon third party privacy or intellectual property rights through the use of this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.web_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pickle as pkl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from splinter import Browser\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "import fitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def retrieve_pdf_from_doidotorg(doi, base_dir, headless=False):\n",
    "\n",
    "    if headless:\n",
    "        browser = Browser(headless=True)\n",
    "    else:\n",
    "        browser = Browser()\n",
    "    doi = doi.replace('https://doi.org/', '').replace('doi', '')\n",
    "    stem_list = doi.split('/')\n",
    "    stem = '/'.join(stem_list[0:len(stem_list)-1])\n",
    "    if os.path.exists(base_dir+'/'+stem) is False:\n",
    "        os.makedirs(base_dir+'/'+stem)\n",
    "    hrefs = set()\n",
    "    try:\n",
    "        # visit doi.org page\n",
    "        browser.visit('https://doi.org/'+doi)\n",
    "        sleep(randint(5,10)*0.1)\n",
    "\n",
    "        for link in browser.find_by_css('a'):\n",
    "            if link['href'] is not None and 'pdf' in link['href']:\n",
    "                hrefs.add(link['href'])\n",
    "\n",
    "        for link in sorted(list(hrefs), key=len):\n",
    "            response = requests.get(link)\n",
    "            with open(base_dir+'/'+doi+'.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            try: \n",
    "                with fitz.open(base_dir+'/'+doi+'.pdf') as doc:  # open document\n",
    "                    n_pages = len([page for page in doc])\n",
    "            except Exception as e2:\n",
    "                os.remove(base_dir+'/'+doi+'.pdf')\n",
    "                continue\n",
    "            # If we can open the pdf, we are done\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        browser.quit()\n",
    "\n",
    "\n",
    "#retrieve_pdf_from_doidotorg('10.1083/jcb.202204093', '/Users/gburns/alhazen/em_tech/temp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def retrieve_full_text_links_from_biorxiv(doi, base_dir):\n",
    "    browser = Browser()#headless=True)\n",
    "    doi = doi.replace('https://doi.org/', '').replace('doi', '')\n",
    "    if doi.startswith('10.1101/') is False:\n",
    "        print('Not a BioRxiv DOI')\n",
    "        return None\n",
    "    if os.path.exists(base_dir+'/10.1101') is False:\n",
    "        os.makedirs(base_dir+'/10.1101')\n",
    "    hrefs = []\n",
    "    try:\n",
    "        # visit bioRxiv's paper page\n",
    "        browser.visit('https://www.biorxiv.org/content/'+doi)\n",
    "        sleep(randint(5,10)*0.1)\n",
    "        for link in browser.find_by_css('a[class=\"dropdown-link\"]'):\n",
    "            if link['href']:\n",
    "                hrefs.append(link['href'])\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    files = []\n",
    "    for link in hrefs:\n",
    "        if link.endswith('.pdf'):\n",
    "            response = requests.get(link)\n",
    "            with open(base_dir+'/'+doi+'.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            files.append(base_dir+'/'+doi+'.pdf')\n",
    "        elif link.endswith('.xml'):\n",
    "            response = requests.get(link)\n",
    "            with open(base_dir+'/'+doi+'.nxml', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            files.append(base_dir+'/'+doi+'.nxml')\n",
    "    return files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def execute_search_on_biorxiv(search_term):\n",
    "    browser = Browser()#headless=True)\n",
    "    all_dois = []\n",
    "\n",
    "    try:\n",
    "        # visit bioRxiv's search page\n",
    "        browser.visit('https://www.biorxiv.org/search')\n",
    "\n",
    "        # fill in the search form\n",
    "        sleep(randint(5,10)*0.1)\n",
    "        print('fill search term')\n",
    "        browser.find_by_id('edit-txtsimple').fill(search_term)\n",
    "\n",
    "        sleep(randint(5,10)*0.1)\n",
    "        print('scroll to bottom')\n",
    "        #browser.scroll_to('bottom')\n",
    "        \n",
    "        print('click search button')        \n",
    "        browser.find_by_css('a[class=\"search-choice-close\"]').click()    \n",
    "        #browser.find_by_css('input[class=\"form-submit\"]').click()    \n",
    "        browser.find_by_id('edit-actions').find_by_value('Search').click()\n",
    "        \n",
    "        sleep(randint(5,10)*0.1)\n",
    "        print('load next page')\n",
    "        # Extract the number of results\n",
    "        formatted_string = browser.find_by_id('page-title').text\n",
    "        \n",
    "        # Use regular expressions to get the number from a string formatted 'XXX Results'\n",
    "        m = re.search(r'\\d+ Results', formatted_string)\n",
    "        if m:\n",
    "            num_results = int(re.search(r'\\d+', formatted_string).group())\n",
    "            \n",
    "            loop_count = 0\n",
    "            while True:\n",
    "                \n",
    "                # Extract each result from the list of the web page\n",
    "                doi_links = browser.find_by_css('span[class=\"highwire-cite-metadata-doi highwire-cite-metadata\"]')\n",
    "                all_dois.extend([re.sub('doi: ', '', t.text) for t in doi_links])\n",
    "                #\n",
    "                # Is next button absent?\n",
    "                next_button_not_present = browser.is_element_not_present_by_css('a[class=\"link-icon link-icon-after\"]')\n",
    "\n",
    "                if next_button_not_present:\n",
    "                    break\n",
    "            \n",
    "                # Find the next button on the page\n",
    "                next_button = browser.find_by_css('a[class=\"link-icon link-icon-after\"]')\n",
    "\n",
    "                # Click the next button\n",
    "                next_button.click()\n",
    "                sleep(randint(5,10)*0.1)\n",
    "                #print('load page number'+str(loop_count))\n",
    "\n",
    "                loop_count += 1\n",
    "                if loop_count > 100:\n",
    "                    break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    browser.quit()\n",
    "    return all_dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def get_html_from_pmc_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Given a DOI, navigate to the PMC HTML page and reconstruct NXML from that \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('No paper found with that DOI')\n",
    "      return []\n",
    "    pmc_id = id_tag.string\n",
    "\n",
    "    # navigate to the PMC HTML page (https://www.ncbi.nlm.nih.gov/pmc/articles/<PMC_ID>/)\n",
    "    try:\n",
    "        browser = Browser()#headless=True)\n",
    "        browser.visit('https://www.ncbi.nlm.nih.gov/pmc/articles/'+pmc_id+'/')\n",
    "        screenshot_path = browser.html_snapshot(base_file_path)\n",
    "        temp_html = base_file_path + '/' + doi + '.html'\n",
    "        p = Path(temp_html)\n",
    "        d = p.parent\n",
    "        if d.exists is False:\n",
    "            os.makedirs(d.absolute()) \n",
    "        os.rename(screenshot_path, temp_html)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    finally:\n",
    "        browser.quit()\n",
    "\n",
    "    return [screenshot_path]\n",
    "\n",
    "def clean_and_convert_tags(soup, tag):\n",
    "\n",
    "    new_tag = soup.new_tag(tag.name)\n",
    "    \n",
    "    if( re.match('^h\\d', tag.name) ):\n",
    "        new_tag.name = 'title'\n",
    "            \n",
    "    # Add a placeholder tag for this figure but don't fill it in currently\n",
    "    elif( tag.name == 'div' and 'class' in tag.attrs and 'fig' in tag.attrs['class'] ):\n",
    "        new_tag = soup.new_tag('fig')\n",
    "        new_tag.attrs['id'] = tag.attrs['id']\n",
    "        new_tag.attrs['position'] = 'float'\n",
    "        return new_tag\n",
    "\n",
    "    # Treat all other div tags as new 'sec' tags in new version \n",
    "    elif( tag.name == 'div' ):\n",
    "        new_tag.name = 'sec'\n",
    "    \n",
    "    # top-level link tag to external reference, strip away tags from within the tag\n",
    "    elif( tag.name == 'a' and 'class' in tag.attrs and 'bibr' in tag.attrs['class'] ):\n",
    "        new_tag = soup.new_tag('xref')\n",
    "        new_tag.attrs['ref-type'] = 'bibr' \n",
    "        new_tag.append(NavigableString(tag.string))\n",
    "        return new_tag\n",
    "\n",
    "    # top-level link tag to figure / table (<xref rid=\"fig4\" ref-type=\"fig\">)\n",
    "    elif( tag.name == 'a' and 'class' in tag.attrs and 'fig' in tag.attrs['class'] ):\n",
    "        new_tag = soup.new_tag('xref')\n",
    "        new_tag.attrs['ref-type'] = 'fig' \n",
    "        new_tag.append(NavigableString(tag.string))\n",
    "        return new_tag\n",
    "    \n",
    "    for c in tag.contents:\n",
    "        if( type(c) is Tag ):\n",
    "            new_c = clean_and_convert_tags(soup, c)\n",
    "            if( new_c is not None ):\n",
    "                new_tag.append(new_c)\n",
    "        elif( type(c) is NavigableString ):\n",
    "            new_tag.append(NavigableString(c))\n",
    "                \n",
    "    return new_tag\n",
    "\n",
    "def extract_reconstructed_nxml(html):\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    title = soup.find('h1', attrs={'class': 'content-title'})  \n",
    "    pmcid = soup.find('li', attrs={'class': 'accid'})  \n",
    "    sections = soup.find_all('div', attrs={'class': 'tsec sec'})\n",
    "    \n",
    "    if( pmcid is None ):\n",
    "        return None\n",
    "\n",
    "    making_soup = BeautifulSoup(\"<article></article>\", \"lxml\")\n",
    "    article = making_soup.article\n",
    "    \n",
    "    front = making_soup.new_tag(\"front\")\n",
    "    article.append(front)\n",
    "    \n",
    "    meta = making_soup.new_tag('article-meta')\n",
    "    front.append(meta)\n",
    "    \n",
    "    article_id = making_soup.new_tag('article-id')\n",
    "    meta.append(article_id)\n",
    "    article_id.append( NavigableString( pmcid.text ) )\n",
    "    \n",
    "    title_group = making_soup.new_tag('title-group')\n",
    "    front.append(title_group)\n",
    "    \n",
    "    article_title = making_soup.new_tag('article-title')\n",
    "    article_title.append(NavigableString(title.text))\n",
    "    title_group.append(article_title)\n",
    "       \n",
    "    for sec in sections:\n",
    "        for h2 in sec.children:\n",
    "            if( type(h2) is Tag and h2.name=='h2' and h2.text=='Abstract'):\n",
    "                abstract = making_soup.new_tag('abstract')\n",
    "                front.append(abstract)\n",
    "                for node in h2.next_siblings:\n",
    "                    if( type(node) is Tag):\n",
    "                        abstract.append(clean_and_convert_tags(making_soup, node))\n",
    "    \n",
    "    body = making_soup.new_tag('body')\n",
    "    article.append(body)\n",
    "    for sec in sections:\n",
    "        for h2 in sec.children:\n",
    "            sec = making_soup.new_tag('sec')\n",
    "            if( type(h2) is Tag and h2.name=='h2' \n",
    "                    and h2.text!='Abstract'\n",
    "                    and h2.text!='References'\n",
    "                    and h2.text!='Footnotes'\n",
    "                    and h2.text!='Acknowledgments'):\n",
    "                body.append(sec)\n",
    "                sec.append(clean_and_convert_tags(making_soup, h2))\n",
    "                for p in h2.next_siblings:\n",
    "                    if( type(p) is Tag ):\n",
    "                        new_tag = clean_and_convert_tags(making_soup, p)\n",
    "                        if( new_tag is not None ):\n",
    "                            sec.append(new_tag)\n",
    "                   \n",
    "    return making_soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
