{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tools for Alhazen\n",
    "\n",
    "> Simple tools to demonstrate utility and 'agentic' functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools:\n",
    "1. Develop queries across external data sources\n",
    "2. Execute a query against EPMC, creating a collection and saving expressions, items, and fragments to the database\n",
    "3. Add all available full text to a collection\n",
    "4. Filter a collection by an LLM-based analysis by tagging fragments with Notes\n",
    "5. Extract information from a collection using an LLM-based analysis to create Notes \n",
    "6. Report on the state of the database in terms of numbers of collections, expressions, items, and fragments\n",
    "7. Prepare a report over a core research question by collecting a number of notes and synthesizing them into a report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "![Workflow](https://lucid.app/publicSegments/view/fea24e0a-61c7-4807-8ea2-e4859753c31b/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.basic_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "\n",
    "from typing import Optional, Type\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "from alhazen.schema_sqla import ScientificKnowledgeCollection, \\\n",
    "    ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \\\n",
    "    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n",
    "    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n",
    "    InformationResource, Note, NoteIsAbout\n",
    "from alhazen.utils.local_literature_db import QuerySpec, LocalLiteratureDb\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine, exists, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EMPCSearchToolSchema(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "    name: str = Field(description=\"should be the name of the collection we will add papers to\")\n",
    "\n",
    "class EMPCSearchTool(BaseTool):\n",
    "    name = \"epmc_search\"\n",
    "    description = \"useful for when you need to search for biomedical scientific papers\"\n",
    "    args_schema: Type[EMPCSearchToolSchema] = EMPCSearchToolSchema\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        date_query: Optional[str] = None,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        if os.environ.get('LLMS_TEMP_DIR') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LLMS_TEMP_DIR']\n",
    "        if os.environ.get('DB_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['DB_NAME']\n",
    "\n",
    "        try: \n",
    "            db = LocalLiteratureDb(loc, db_name)\n",
    "            if db.session is None:\n",
    "                session_class = sessionmaker(bind=db.engine)\n",
    "                db.session = session_class()\n",
    "\n",
    "            r = db.session.query(func.max(ScientificKnowledgeCollection.id)).first()\n",
    "            max_id = r[0]\n",
    "            if max_id is None:\n",
    "                c_id = '0'\n",
    "            else:\n",
    "                c_id = str(int(float(max_id)) + 1)\n",
    "\n",
    "            cdf1 = pd.DataFrame([{'ID': c_id, 'NAME': name, 'QUERY': query}])        \n",
    "            qs1 = QuerySpec(db_name, 'ID', 'QUERY', 'NAME', {}, ['TITLE','ABSTRACT'])\n",
    "            qt1 = QueryTranslator(cdf1.sort_values('ID'), 'ID', 'QUERY', 'NAME')\n",
    "\n",
    "            if(date_query is not None):\n",
    "                cdf2 = pd.DataFrame([{'ID': None, 'NAME': None, 'QUERY': date_query}])        \n",
    "                qs2 = QuerySpec(db_name, 'ID', 'QUERY', 'NAME', {}, ['FIRST_PDATE'])\n",
    "                qt2 = QueryTranslator(cdf2.sort_values('ID'), 'ID', 'QUERY', 'NAME')\n",
    "                db.add_corpus_from_epmc(qt1, qt2, sections=qs1.sections, sections2=qs2.sections, page_size=100)\n",
    "            else: \n",
    "                db.add_corpus_from_epmc(qt1, None, sections=qs1.sections, page_size=100)\n",
    "\n",
    "            r2 = db.session.query(func.count(ScientificKnowledgeExpression.id)) \\\n",
    "                    .filter(ScientificKnowledgeCollection.id == ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id) \\\n",
    "                    .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id) \\\n",
    "                    .filter(ScientificKnowledgeCollection.id == str(c_id)).first()\n",
    "            n_papers_added = r2[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            db.session.close()\n",
    "\n",
    "        return {'action': 'Final Answer', \n",
    "                'action_input': 'Successfully constructed a collection called `{}` containing {} papers by querying EMPC'.format(name, n_papers_added)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"epmc_search does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "bad character range 2-1 at position 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mDB_NAME\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtemp\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m t \u001b[39m=\u001b[39m EMPCSearchTool()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m t\u001b[39m.\u001b[39m_run(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mStuff\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m        query\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCryoelectron Tomography | Cryo Electron Tomography | Cryo-Electron Tomography | Cryo-ET | CryoE\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m        date_query\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[2022-12-01 TO 2023-12-01]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     n_papers_added \u001b[39m=\u001b[39m r2[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     db\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     cdf2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([{\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mNAME\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mQUERY\u001b[39m\u001b[39m'\u001b[39m: date_query}])        \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     qs2 \u001b[39m=\u001b[39m QuerySpec(db_name, \u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mQUERY\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNAME\u001b[39m\u001b[39m'\u001b[39m, {}, [\u001b[39m'\u001b[39m\u001b[39mFIRST_PDATE\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     qt2 \u001b[39m=\u001b[39m QueryTranslator(cdf2\u001b[39m.\u001b[39msort_values(\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mQUERY\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNAME\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     db\u001b[39m.\u001b[39madd_corpus_from_epmc(qt1, qt2, sections\u001b[39m=\u001b[39mqs1\u001b[39m.\u001b[39msections, sections2\u001b[39m=\u001b[39mqs2\u001b[39m.\u001b[39msections, page_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/10_basic_tools.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m: \n",
      "File \u001b[0;32m~/Documents/Coding/ChatGPT_etc/alzhazen/alhazen/utils/queryTranslator.py:89\u001b[0m, in \u001b[0;36mQueryTranslator.__init__\u001b[0;34m(self, df, id_col, query_col, name_col, notes_col)\u001b[0m\n\u001b[1;32m     87\u001b[0m     redq \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(t, \u001b[39mid\u001b[39m, redq)\n\u001b[1;32m     88\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     redq \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mt\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mid\u001b[39m, redq)\n\u001b[1;32m     90\u001b[0m \u001b[39m#print(redq)                      \u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mredq_list\u001b[39m.\u001b[39mappend((row_id, redq))\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/re/__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    179\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/re/__init__.py:294\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mas it is an undocumented flag \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mwithout an obvious purpose. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt use it.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    293\u001b[0m               \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m--> 294\u001b[0m p \u001b[39m=\u001b[39m _compiler\u001b[39m.\u001b[39mcompile(pattern, flags)\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[1;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    297\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/re/_compiler.py:743\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[1;32m    742\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 743\u001b[0m     p \u001b[39m=\u001b[39m _parser\u001b[39m.\u001b[39mparse(p, flags)\n\u001b[1;32m    744\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/re/_parser.py:980\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    977\u001b[0m state\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m flags\n\u001b[1;32m    978\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[0;32m--> 980\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39m)\n\u001b[1;32m    981\u001b[0m p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m fix_flags(\u001b[39mstr\u001b[39m, p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m source\u001b[39m.\u001b[39mnext \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/re/_parser.py:455\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    453\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    454\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    456\u001b[0m                        \u001b[39mnot\u001b[39;00m nested \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m items))\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    458\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/re/_parser.py:612\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[39mif\u001b[39;00m hi \u001b[39m<\u001b[39m lo:\n\u001b[1;32m    611\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbad character range \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (this, that)\n\u001b[0;32m--> 612\u001b[0m         \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(msg, \u001b[39mlen\u001b[39m(this) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(that))\n\u001b[1;32m    613\u001b[0m     setappend((RANGE, (lo, hi)))\n\u001b[1;32m    614\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31merror\u001b[0m: bad character range 2-1 at position 6"
     ]
    }
   ],
   "source": [
    "os.environ['LLMS_TEMP_DIR'] = '/Users/gburns/alhazen/'\n",
    "os.environ['DB_NAME'] = 'temp'\n",
    "\n",
    "t = EMPCSearchTool()\n",
    "t._run(name='Stuff', \n",
    "       query='Cryoelectron Tomography | Cryo Electron Tomography | Cryo-Electron Tomography | Cryo-ET | CryoE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853\n"
     ]
    }
   ],
   "source": [
    "db_name = os.environ['DB_NAME']\n",
    "loc = os.environ['LLMS_TEMP_DIR']\n",
    "\n",
    "db = LocalLiteratureDb(loc, db_name)\n",
    "if db.session is None:\n",
    "    session_class = sessionmaker(bind=db.engine)\n",
    "    db.session = session_class()\n",
    "\n",
    "r2 = db.session.query(func.count(ScientificKnowledgeExpression.id)) \\\n",
    "                    .filter(ScientificKnowledgeCollection.id == ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id) \\\n",
    "                    .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id) \\\n",
    "                    .filter(ScientificKnowledgeCollection.id == str(0.0))\n",
    "print(str(r2.first()[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.orm.query.Query at 0x2b3f63110>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
