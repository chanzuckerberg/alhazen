{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tools for Alhazen\n",
    "\n",
    "> Simple tools to demonstrate utility and 'agentic' functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Tools:\n",
    "\n",
    "### Building, Describing, and Deleting Collections:\n",
    "1. **AddCollectionFromEPMCTool**: Execute a query against the European PMC, creating a collection, downloading available full-text papers and saving expressions, items, and fragments to the database to denote that publication.\n",
    "2. **DescribeCollectionCompositionTool**: Describe the composition of a collection in the database.\n",
    "3. **DeleteCollectionTool**: Delete a collection from the database.\n",
    "\n",
    "### Searching for papers in collections:\n",
    "3. **List__Paged_Expressions_CollectionCTool**: Delete a collection from the database.\n",
    "\n",
    "## Tools under development:\n",
    "1. Develop queries across external data sources\n",
    "2. Extract information from a collection using an LLM-based analysis to create Notes \n",
    "3. Filter a collection by an LLM-based analysis by tagging fragments with Notes\n",
    "5. Report on the state of the database in terms of numbers of collections, expressions, items, and fragments\n",
    "6. Prepare a report over a core research question by collecting a number of notes and synthesizing them into a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "\n",
    "from alhazen.core import OllamaRunner, PromptTemplateRegistry, get_langchain_llm, get_cached_gguf, \\\n",
    "    get_langchain_embeddings, GGUF_LOOKUP_URL, MODEL_TYPE\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.langchain_utils import suppress_stdout_stderr\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import download_full_text_paper_for_doi\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "from alhazen.utils.web_robot import *\n",
    "from alhazen.schema_sqla import ScientificKnowledgeCollection, \\\n",
    "    ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \\\n",
    "    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n",
    "    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n",
    "    InformationResource, Note, NoteIsAbout\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine, exists, func\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from typing import Optional, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# NOTE - Use LangChain's SQL_DATABASE TOOL AS A MODEL \n",
    "# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/tools/sql_database/tool.py\n",
    "\n",
    "class AlhazenToolMixin(BaseModel):\n",
    "    '''Base tool for interacting with an Alhazen CEIFNS (pron. 'SAI-FiNS') database \n",
    "    (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).'''\n",
    "\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True)\n",
    "    ollr : Optional[OllamaRunner] = Field(exclude=True)\n",
    "    llm : Optional[Ollama] = Field(exclude=True)\n",
    "    llm_model : str = Field(default='mixtral')\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class AddCollectionFromEPMCToolSchema(BaseModel):\n",
    "    id: str = Field(description=\"A code that serves as the id of the collection we will add papers to.\")\n",
    "    name: str = Field(description=\"A human-readable name of the collection we will add papers to.\")\n",
    "    query: str = Field(description=\"A search query (using AND/OR/NOT statements) to be executed on the European PMC online literature repository for scientific publications.\")\n",
    "    full_text: bool = Field(description=\"A boolean parameter which is set to 'True' for the tool to search available online sources for each \" + \\\n",
    "                            \"individual paper and set to 'False' to only search for the citation record. This significantly adds to execution time.\")\n",
    "\n",
    "class AddCollectionFromEPMCTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"add_collection_from_epmc_query\"\n",
    "    description = \"This tool searches the European PMC literature database for references to biomedical scientific papers.\"\n",
    "    args_schema: Type[AddCollectionFromEPMCToolSchema] = AddCollectionFromEPMCToolSchema\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        id: str, \n",
    "        name: str,\n",
    "        query: str,\n",
    "        full_text: bool,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "        if loc[-1:] != '/':\n",
    "            loc += '/'\n",
    "        if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "        n_papers_added = 0\n",
    "\n",
    "        try: \n",
    "            \n",
    "            self.db.add_corpus_from_epmc_query(id, name, query, commit_this=False)\n",
    "\n",
    "            skc = aliased(ScientificKnowledgeCollection)\n",
    "            skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "            ske = aliased(ScientificKnowledgeExpression)\n",
    "            ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "            ski = aliased(ScientificKnowledgeItem)\n",
    "            ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "            skf = aliased(ScientificKnowledgeFragment)\n",
    "\n",
    "            q1 = self.db.session.query(skc) \\\n",
    "                    .filter(skc.id == str(id)) \n",
    "            c = q1.first()\n",
    "\n",
    "            q2 = self.db.session.query(ske) \\\n",
    "                    .filter(skc.id == skc_hm.ScientificKnowledgeCollection_id) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc.id == str(id)) \n",
    "            for e in q2.all():\n",
    "                n_papers_added += 1\n",
    "                doi = e.id.replace('doi:', '')\n",
    "                if full_text:\n",
    "                    path = loc+db_name+'/ft/'\n",
    "                    ft_exist = download_full_text_paper_for_doi(doi, path)\n",
    "                    if ft_exist:\n",
    "                        self.db.add_full_text_for_expression(e)\n",
    "            \n",
    "            expression_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \n",
    "            expression_count = expression_q.first()[0]\n",
    "\n",
    "            jats_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'JATSFullText')\n",
    "            jats_ft_count = jats_ft_q.first()[0]\n",
    "            \n",
    "            pdf_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'PDFFullText')\n",
    "            pdf_ft_count = pdf_ft_q.first()[0]\n",
    "\n",
    "            n = Note(id='skc:%s.counts'%(id),\n",
    "                    content='{\"ske_count\": %s, \"jats_ft_count\": %s, \"pdf_ft_count\": %s}' \\\n",
    "                        %(expression_count, jats_ft_count, pdf_ft_count),\n",
    "                    format='json',\n",
    "                    type='NoteAboutCollection')\n",
    "            c.has_notes.append(n)        \n",
    "            self.db.session.add(n)\n",
    "\n",
    "            self.db.session.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'action': 'Final Answer', \n",
    "                'action_input': 'Successfully constructed a collection called `{}` containing {} papers by querying the European PMC Database.'.format(name, n_papers_added)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"add_collection_from_epmc_query does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DescribeCollectionCompositionToolSchema(BaseModel):\n",
    "    id: str = Field(description=\"should be the id of the collection we wish to describe\")\n",
    "    \n",
    "class DescribeCollectionCompositionTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"add_collection_from_epmc_query\"\n",
    "    description = \"This tool describes the contents of a Collection, in terms of counts of papers\"\n",
    "    args_schema: Type[DescribeCollectionCompositionToolSchema] = DescribeCollectionCompositionToolSchema\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        id: str, \n",
    "        name: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "        if loc[-1:] != '/':\n",
    "            loc += '/'\n",
    "        if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "        try:      \n",
    "            skc = aliased(ScientificKnowledgeCollection)\n",
    "            skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "            ske = aliased(ScientificKnowledgeExpression)\n",
    "            ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "            ski = aliased(ScientificKnowledgeItem)\n",
    "            ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "            skf = aliased(ScientificKnowledgeFragment)\n",
    "            \n",
    "            expression_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \n",
    "            expression_count = expression_q.first()[0]\n",
    "\n",
    "            jats_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'JATSFullText')\n",
    "            jats_ft_count = jats_ft_q.first()[0]\n",
    "\n",
    "            pdf_ft_q = self.db.session.query(func.count(ske.id)) \\\n",
    "                    .filter(skc_hm.has_members_id == ske.id) \\\n",
    "                    .filter(skc_hm.ScientificKnowledgeCollection_id == str(id)) \\\n",
    "                    .filter(ske.id == ske_hr.ScientificKnowledgeExpression_id) \\\n",
    "                    .filter(ske_hr.has_representation_id == ski.id) \\\n",
    "                    .filter(ski.type == 'PDFFullText')\n",
    "            pdf_ft_count = pdf_ft_q.first()[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'action': 'Final Answer', \n",
    "                'action_input': 'The collection called `{}` contains {} papers; including {} PDF and {} JATS full text papers.'.format(name, expression_count, pdf_ft_count, jats_ft_count)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"add_collection_from_epmc_query does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DeleteCollectionToolSchema(BaseModel):\n",
    "    id: str = Field(description=\"should be the id of the collection we will add papers to\")\n",
    "\n",
    "class DeleteCollectionTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"delete_collection\"\n",
    "    description = \"This deletes a collection from the database based on an id value.\"\n",
    "    args_schema: Type[DeleteCollectionToolSchema] = DeleteCollectionToolSchema\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        id: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "        if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "        try: \n",
    "            \n",
    "            self.db.delete_collection(id, commit_this=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'action': 'Final Answer', \n",
    "                'action_input': 'Successfully deleted a collection with id:`{}`.'.format(id)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"delete_collection does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RetrieveFullTextToolSchema(BaseModel):\n",
    "    paper_id: str = Field(description=\"the digitial objecty identifier (doi) of the paper being retrieved from external sources\")\n",
    "\n",
    "class RetrieveFullTextTool(AlhazenToolMixin, BaseTool): \n",
    "    name = \"retrieve_full_text_for_paper_id\"\n",
    "    description = \"This retrieves a full text paper based on its doi, copies it to local disk, and adds it the database.\"\n",
    "    args_schema: Type[RetrieveFullTextToolSchema] = RetrieveFullTextToolSchema\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        paper_id: str, \n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "        if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "        try: \n",
    "\n",
    "            ske = aliased(ScientificKnowledgeExpression)\n",
    "            e = self.db.session.query(ske) \\\n",
    "                    .filter(ske.id == str(paper_id)).first()\n",
    "            doi = e.id.replace('doi:', '')\n",
    "            path = loc+db_name+'/ft/'\n",
    "            ft_exist = download_full_text_paper_for_doi(doi, path)\n",
    "            if ft_exist:\n",
    "                self.db.add_full_text_for_expression(e)\n",
    "                self.db.session.commit()\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'action': 'Final Answer', \n",
    "                'action_input': 'Successfully added full text for a paper with doi:`{}`.'.format(paper_id)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"delete_collection does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['LOCAL_FILE_PATH'] = '/Users/gburns/alhazen/'\n",
    "#os.environ['ALHAZEN_DB_NAME'] = 'temp'\n",
    "#\n",
    "#t = EMPCSearchTool()\n",
    "#t._run(name='Stuff', \n",
    "#       query='Cryoelectron Tomography | Cryo Electron Tomography | Cryo-Electron Tomography | Cryo-ET | CryoE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853\n"
     ]
    }
   ],
   "source": [
    "#db_name = os.environ['DB_NAME']\n",
    "#loc = os.environ['LLMS_TEMP_DIR']\n",
    "#\n",
    "#db = Ceifns_LiteratureDb(loc, db_name)\n",
    "#if db.session is None:\n",
    "#    session_class = sessionmaker(bind=db.engine)\n",
    "#    db.session = session_class()\n",
    "#\n",
    "#r2 = db.session.query(func.count(ScientificKnowledgeExpression.id)) \\\n",
    "#                    .filter(ScientificKnowledgeCollection.id == ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id) \\\n",
    "#                    .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id) \\\n",
    "#                    .filter(ScientificKnowledgeCollection.id == str(0.0))\n",
    "#print(str(r2.first()[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
