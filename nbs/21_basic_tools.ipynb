{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tools for Alhazen\n",
    "\n",
    "> Simple tools to demonstrate utility and 'agentic' functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Tools:\n",
    "1. **EMPCSearchTool**: Execute a query against EPMC, creating a collection and saving expressions, items, and fragments to the database\n",
    "2. **FullTextRetrievalTool**: Add all available full text to a collection\n",
    "\n",
    "## Tools under development:\n",
    "1. Develop queries across external data sources\n",
    "2. Extract information from a collection using an LLM-based analysis to create Notes \n",
    "3. Filter a collection by an LLM-based analysis by tagging fragments with Notes\n",
    "5. Report on the state of the database in terms of numbers of collections, expressions, items, and fragments\n",
    "6. Prepare a report over a core research question by collecting a number of notes and synthesizing them into a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_nxml_from_pubmed_doi' from 'alhazen.utils.web_robot' (/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/alhazen/utils/web_robot.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malhazen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearchEngineUtils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ESearchQuery, EuroPMCQuery\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malhazen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqueryTranslator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryTranslator, QueryType\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malhazen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweb_robot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_nxml_from_pubmed_doi, get_pdf_from_pubmed_doi\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malhazen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_sqla\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ScientificKnowledgeCollection, \\\n\u001b[1;32m     16\u001b[0m     ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \\\n\u001b[1;32m     17\u001b[0m     ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n\u001b[1;32m     18\u001b[0m     ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n\u001b[1;32m     19\u001b[0m     InformationResource, Note, NoteIsAbout\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malhazen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mceifns_db\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuerySpec, Ceifns_LiteratureDb\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_nxml_from_pubmed_doi' from 'alhazen.utils.web_robot' (/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/alhazen/utils/web_robot.py)"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "\n",
    "from alhazen.core import OllamaRunner, PromptTemplateRegistry, get_langchain_llm, get_cached_gguf, \\\n",
    "    get_langchain_embeddings, GGUF_LOOKUP_URL, MODEL_TYPE\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.langchain_utils import suppress_stdout_stderr\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "from alhazen.utils.web_robot import *\n",
    "from alhazen.schema_sqla import ScientificKnowledgeCollection, \\\n",
    "    ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \\\n",
    "    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n",
    "    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n",
    "    InformationResource, Note, NoteIsAbout\n",
    "from alhazen.utils.ceifns_db import QuerySpec, Ceifns_LiteratureDb\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine, exists, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from typing import Optional, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# NOTE - Use LangChain's SQL_DATABASE TOOL AS A MODEL \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/tools/sql_database/tool.py\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAlhazenToolMixin\u001b[39;00m(BaseModel):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Base tool for interacting with an Alhazen CEIFNS (pron. 'SAI-FiNS') database \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).'''\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     db: Ceifns_LiteratureDb \u001b[38;5;241m=\u001b[39m Field(exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# NOTE - Use LangChain's SQL_DATABASE TOOL AS A MODEL \n",
    "# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/tools/sql_database/tool.py\n",
    "\n",
    "class AlhazenToolMixin(BaseModel):\n",
    "    '''Base tool for interacting with an Alhazen CEIFNS (pron. 'SAI-FiNS') database \n",
    "    (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).'''\n",
    "\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True)\n",
    "    ollr : OllamaRunner = Field(exclude=True)\n",
    "    llm : Ollama = Field(exclude=True)\n",
    "    llm_model : str = Field(default='mixtral')\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EMPCSearchToolSchema(BaseModel):\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True, description=\"The CEIFNS Database we will be populating\")\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "    name: str = Field(description=\"should be the name of the collection we will add papers to\")\n",
    "\n",
    "class EMPCSearchTool(BaseTool): \n",
    "    name = \"epmc_search\"\n",
    "    description = \"This tool searches the European PMC literature database for references to biomedical scientific papers.\"\n",
    "    args_schema: Type[EMPCSearchToolSchema] = EMPCSearchToolSchema\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        date_query: Optional[str] = None,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "        if os.environ.get('DATABASE_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['DATABASE_NAME']\n",
    "\n",
    "        try: \n",
    "            \n",
    "            r = self.db.session.query(func.max(ScientificKnowledgeCollection.id)).first()\n",
    "            max_id = r[0]\n",
    "            if max_id is None:\n",
    "                c_id = '0'\n",
    "            else:\n",
    "                c_id = str(int(float(max_id)) + 1)\n",
    "\n",
    "            cdf1 = pd.DataFrame([{'ID': c_id, 'NAME': name, 'QUERY': query}])        \n",
    "            qs1 = QuerySpec(db_name, 'ID', 'QUERY', 'NAME', {}, ['TITLE','ABSTRACT'])\n",
    "            qt1 = QueryTranslator(cdf1.sort_values('ID'), 'ID', 'QUERY', 'NAME')\n",
    "\n",
    "            if(date_query is not None):\n",
    "                cdf2 = pd.DataFrame([{'ID': None, 'NAME': None, 'QUERY': date_query}])        \n",
    "                qs2 = QuerySpec(db_name, 'ID', 'QUERY', 'NAME', {}, ['FIRST_PDATE'])\n",
    "                qt2 = QueryTranslator(cdf2.sort_values('ID'), 'ID', 'QUERY', 'NAME')\n",
    "                self.db.add_corpus_from_epmc(qt1, qt2, sections=qs1.sections, sections2=qs2.sections, page_size=100)\n",
    "            else: \n",
    "                self.db.add_corpus_from_epmc(qt1, None, sections=qs1.sections, page_size=100)\n",
    "\n",
    "            r2 = self.db.session.query(func.count(ScientificKnowledgeExpression.id)) \\\n",
    "                    .filter(ScientificKnowledgeCollection.id == ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id) \\\n",
    "                    .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id) \\\n",
    "                    .filter(ScientificKnowledgeCollection.id == str(c_id)).first()\n",
    "            n_papers_added = r2[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:  \n",
    "            self.db.session.close()\n",
    "\n",
    "        return {'action': 'Final Answer', \n",
    "                'action_input': 'Successfully constructed a collection called `{}` containing {} papers by querying the European PMC Database.'.format(name, n_papers_added)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"epmc_search does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class FullTextRetrievalToolSchema(BaseModel):\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True, description=\"The CEIFNS Database we will be populating.\")\n",
    "    collection_name: str = Field(description=\"The name of the collection we will add full-text papers to.\")\n",
    "\n",
    "class FullTextRetrievalTool(BaseTool): \n",
    "    name = \"Full Text Article Retrieval\"\n",
    "    description = \"Useful for when you need to find full-text articles for a collection of papers.\"\n",
    "    args_schema: Type[EMPCSearchToolSchema] = EMPCSearchToolSchema\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        name: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "        path = os.path.join(loc, 'ft')\n",
    "        if os.environ.get('DATABASE_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['DATABASE_NAME']\n",
    "\n",
    "        # Iterate over the papers in the collection and search for their DOIs\n",
    "        count = 0\n",
    "        query = self.db.session.query(ScientificKnowledgeExpression) \\\n",
    "                .filter(ScientificKnowledgeCollection.name == name) \\\n",
    "                .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id) \n",
    "        for e in query.all():\n",
    "\n",
    "            # pause for a random amount of time between 0 and 2 seconds\n",
    "            #time.sleep(random.random() * 2)\n",
    "            \n",
    "            doi = e.doi\n",
    "            nxml_flag = False\n",
    "            pdf_flag = False \n",
    "            html_flag = False\n",
    "            \n",
    "            if doi is not None:\n",
    "\n",
    "                # Try NCBI open access NXML first\n",
    "                ft_path = get_nxml_from_pubmed_doi(doi, path)\n",
    "                if ft_path is not None:\n",
    "                    count += 1\n",
    "                    nxml_flag = True\n",
    "\n",
    "                # If the DOI starts with, '10.1101/', it's a bioRxiv paper, so go get it there. \n",
    "                if doi.startswith('10.1101/') and nxml_flag is False:\n",
    "                    ft_path = retrieve_full_text_links_from_biorxiv(doi, path)\n",
    "                    if ft_path is not None:\n",
    "                        count += 1\n",
    "                        nxml_flag = True\n",
    "                    \n",
    "                # Try to find an NCBI HTML page\n",
    "                if nxml_flag is False: \n",
    "                    ft_path = get_html_from_pmc_doi(doi, path)\n",
    "                    if ft_path is not None:\n",
    "                        count += 1\n",
    "                        html_flag = True\n",
    "\n",
    "                # Try NCBI open access PDF\n",
    "                if nxml_flag is False and html_flag is False: \n",
    "                    ft_path = get_pdf_from_pubmed_doi(doi, path)\n",
    "                    if ft_path is not None:\n",
    "                        count += 1\n",
    "                        pdf_flag = True\n",
    "\n",
    "                # Try to get the pdf from the doi\n",
    "                if nxml_flag is False and html_flag is False and pdf_flag is False: \n",
    "                    ft_path = retrieve_pdf_from_doidotorg(doi, path)\n",
    "                    if ft_path is not None:\n",
    "                        count += 1\n",
    "                        pdf_flag = True\n",
    "                \n",
    "                if nxml_flag or html_flag or pdf_flag:\n",
    "                    self.db.add_full_text_for_expression(e, nxml_flag, pdf_flag, html_flag)      \n",
    "\n",
    "        return {'action': 'Final Answer', \n",
    "                'action_input': 'Successfully added full text papers for a collection called `{}` containing {} papers.'.format(name, count)}\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        name: str,\n",
    "        query: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"full_text_retrieval does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['LOCAL_FILE_PATH'] = '/Users/gburns/alhazen/'\n",
    "#os.environ['DATABASE_NAME'] = 'temp'\n",
    "#\n",
    "#t = EMPCSearchTool()\n",
    "#t._run(name='Stuff', \n",
    "#       query='Cryoelectron Tomography | Cryo Electron Tomography | Cryo-Electron Tomography | Cryo-ET | CryoE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853\n"
     ]
    }
   ],
   "source": [
    "#db_name = os.environ['DB_NAME']\n",
    "#loc = os.environ['LLMS_TEMP_DIR']\n",
    "#\n",
    "#db = Ceifns_LiteratureDb(loc, db_name)\n",
    "#if db.session is None:\n",
    "#    session_class = sessionmaker(bind=db.engine)\n",
    "#    db.session = session_class()\n",
    "#\n",
    "#r2 = db.session.query(func.count(ScientificKnowledgeExpression.id)) \\\n",
    "#                    .filter(ScientificKnowledgeCollection.id == ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id) \\\n",
    "#                    .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id) \\\n",
    "#                    .filter(ScientificKnowledgeCollection.id == str(0.0))\n",
    "#print(str(r2.first()[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
