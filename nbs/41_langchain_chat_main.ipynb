{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Side Code Langserve \n",
    "\n",
    "> A server-side delivery of langchain agents and chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "from langchain.prompts import (ChatPromptTemplate, MessagesPlaceholder,\n",
    "                               PromptTemplate)\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from langchain.schema.runnable import (Runnable, RunnableBranch,\n",
    "                                       RunnableLambda, RunnableMap)\n",
    "from langchain.vectorstores import Chroma\n",
    "from langserve import add_routes\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langserve import add_routes\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools.render import render_text_description, render_text_description_and_args\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain import hub\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser\n",
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
    "\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "from alhazen.utils.output_parsers import ReActJsonSingleInputOutputParser_llama2\n",
    "\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "from uuid import UUID\n",
    "\n",
    "# Hack to fix Incorrect formatting for Llama Chat models\n",
    "from alhazen.utils.langchain_utils import ChatPromptValue_to_string\n",
    "from langchain.prompts.chat import ChatPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REPHRASE_TEMPLATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOllama(model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mllama2:70b\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m retriever \u001b[39m=\u001b[39m get_retriever()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m answer_chain \u001b[39m=\u001b[39m create_chain(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     llm,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     retriever,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m add_routes(app, answer_chain, path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/chat\u001b[39m\u001b[39m\"\u001b[39m, input_type\u001b[39m=\u001b[39mChatRequest)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_chain\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     retriever: BaseRetriever,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Runnable:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     retriever_chain \u001b[39m=\u001b[39m create_retriever_chain(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         llm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         retriever,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     )\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFindDocs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     _context \u001b[39m=\u001b[39m RunnableMap(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m: retriever_chain \u001b[39m|\u001b[39m format_docs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     )\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRetrieveDocs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     prompt \u001b[39m=\u001b[39m ChatPromptTemplate\u001b[39m.\u001b[39mfrom_messages(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, RESPONSE_TEMPLATE),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     )\n",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_retriever_chain\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     llm: BaseLanguageModel, retriever: BaseRetriever\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Runnable:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     CONDENSE_QUESTION_PROMPT \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(REPHRASE_TEMPLATE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     condense_question_chain \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         CONDENSE_QUESTION_PROMPT \u001b[39m|\u001b[39m llm \u001b[39m|\u001b[39m StrOutputParser()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     )\u001b[39m.\u001b[39mwith_config(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCondenseQuestion\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     conversation_chain \u001b[39m=\u001b[39m condense_question_chain \u001b[39m|\u001b[39m retriever\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REPHRASE_TEMPLATE' is not defined"
     ]
    }
   ],
   "source": [
    "# Code from the Langchain Chat repo\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    "    expose_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    question: str\n",
    "    chat_history: Optional[List[Dict[str, str]]]\n",
    "\n",
    "def get_embeddings_model() -> Embeddings:\n",
    "    return SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    #return OpenAIEmbeddings(chunk_size=200)\n",
    "\n",
    "def get_retriever() -> BaseRetriever:\n",
    "    chromadb_client = Chroma(\"langchain_store\")\n",
    "    return chromadb_client.as_retriever()\n",
    "\n",
    "def create_retriever_chain(\n",
    "    llm: BaseLanguageModel, retriever: BaseRetriever\n",
    ") -> Runnable:\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)\n",
    "    condense_question_chain = (\n",
    "        CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()\n",
    "    ).with_config(\n",
    "        run_name=\"CondenseQuestion\",\n",
    "    )\n",
    "    conversation_chain = condense_question_chain | retriever\n",
    "    return RunnableBranch(\n",
    "        (\n",
    "            RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "                run_name=\"HasChatHistoryCheck\"\n",
    "            ),\n",
    "            conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\n",
    "        ),\n",
    "        (\n",
    "            RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "                run_name=\"Itemgetter:question\"\n",
    "            )\n",
    "            | retriever\n",
    "        ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\n",
    "    ).with_config(run_name=\"RouteDependingOnChatHistory\")\n",
    "\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(doc_string)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "def serialize_history(request: ChatRequest):\n",
    "    chat_history = request[\"chat_history\"] or []\n",
    "    converted_chat_history = []\n",
    "    for message in chat_history:\n",
    "        if message.get(\"human\") is not None:\n",
    "            converted_chat_history.append(HumanMessage(content=message[\"human\"]))\n",
    "        if message.get(\"ai\") is not None:\n",
    "            converted_chat_history.append(AIMessage(content=message[\"ai\"]))\n",
    "    return converted_chat_history\n",
    "\n",
    "def create_chain(\n",
    "    llm: BaseLanguageModel,\n",
    "    retriever: BaseRetriever,\n",
    ") -> Runnable:\n",
    "    retriever_chain = create_retriever_chain(\n",
    "        llm,\n",
    "        retriever,\n",
    "    ).with_config(run_name=\"FindDocs\")\n",
    "    _context = RunnableMap(\n",
    "        {\n",
    "            \"context\": retriever_chain | format_docs,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        }\n",
    "    ).with_config(run_name=\"RetrieveDocs\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", RESPONSE_TEMPLATE),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response_synthesizer = (prompt | llm | StrOutputParser()).with_config(\n",
    "        run_name=\"GenerateResponse\",\n",
    "    )\n",
    "    return (\n",
    "        {\n",
    "            \"question\": RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "                run_name=\"Itemgetter:question\"\n",
    "            ),\n",
    "            \"chat_history\": RunnableLambda(serialize_history).with_config(\n",
    "                run_name=\"SerializeHistory\"\n",
    "            ),\n",
    "        }\n",
    "        | _context\n",
    "        | response_synthesizer\n",
    "    )\n",
    "\n",
    "llm = ChatOllama(model='llama2:70b')\n",
    "retriever = get_retriever()\n",
    "answer_chain = create_chain(\n",
    "    llm,\n",
    "    retriever,\n",
    ")\n",
    "\n",
    "add_routes(app, answer_chain, path=\"/chat\", input_type=ChatRequest)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Provides access to an agent interface via langserve.\n",
    "# Very useful for NextJS applications + demos\n",
    "\n",
    "import nest_asyncio\n",
    "from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain.tools.playwright.utils import (\n",
    "    create_async_playwright_browser,  # A synchronous browser is available, though it isn't compatible with jupyter.\n",
    ")\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#async_browser = create_async_playwright_browser()\n",
    "#browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "#tools = browser_toolkit.get_tools()\n",
    "tools = tools = load_tools([\"ddg-search\", \"pubmed\", \"arxiv\"], )\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react-multi-input-json\")\n",
    "prompt = prompt.partial(\n",
    "    tools=render_text_description_and_args(tools),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "\n",
    "chat_model = ChatOllama(model='llama2:70b')\n",
    "chat_model_with_stop = chat_model.bind(stop=[\"\\nObservation\"])\n",
    "ChatPromptValue.to_string = ChatPromptValue_to_string\n",
    "\n",
    "#llm = Ollama(model='llama2:70b')\n",
    "#llm_with_stop = llm.bind(stop=[\"\\nObservation\"])\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model_with_stop\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# We need to add these input/output schemas because the current AgentExecutor\n",
    "# is lacking in schemas.\n",
    "class ChatRequest(BaseModel):\n",
    "    question: str\n",
    "    chat_history: Optional[List[Dict[str, str]]]\n",
    "\n",
    "class Input(BaseModel):\n",
    "    input: str\n",
    "\n",
    "class Output(BaseModel):\n",
    "    output: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Alhazen Server\",\n",
    "    version=\"0.0.1\",\n",
    "    description=\"An api server using Langchain's Runnable interfaces for Alhazen\",\n",
    ")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    "    expose_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Adds routes to the app for using the chain under:\n",
    "# /invoke\n",
    "# /batch\n",
    "# /stream\n",
    "add_routes(app, agent_executor, input_type=Input, output_type=Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': {'output': \"Tom Cruise's height is 5 feet 7 inches (1.70 meters)\"},\n",
       " 'callback_events': [],\n",
       " 'metadata': {'run_id': '562b570c-ade1-4be4-b6eb-5d44bea63c60'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.post(\n",
    "    \"http://localhost:8080/invoke/\",\n",
    "    json={'input': {'input': 'What is Tom Cruise\\'s height?'}}\n",
    ")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m agent_executor\u001b[39m.\u001b[39minvoke(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mSearch for papers about cancer.\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py:87\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     83\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     85\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m     86\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(\n\u001b[1;32m     88\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m     89\u001b[0m         callbacks\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     90\u001b[0m         tags\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     91\u001b[0m         metadata\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     92\u001b[0m         run_name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     93\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     94\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py:1245\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1245\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_take_next_step(\n\u001b[1;32m   1246\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1247\u001b[0m         color_mapping,\n\u001b[1;32m   1248\u001b[0m         inputs,\n\u001b[1;32m   1249\u001b[0m         intermediate_steps,\n\u001b[1;32m   1250\u001b[0m         run_manager\u001b[39m=\u001b[39mrun_manager,\n\u001b[1;32m   1251\u001b[0m     )\n\u001b[1;32m   1252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1253\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1254\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1255\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py:1032\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1031\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mplan(\n\u001b[1;32m   1033\u001b[0m         intermediate_steps,\n\u001b[1;32m   1034\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1035\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[1;32m   1036\u001b[0m     )\n\u001b[1;32m   1037\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1038\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py:385\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mintermediate_steps\u001b[39m\u001b[39m\"\u001b[39m: intermediate_steps}}\n\u001b[0;32m--> 385\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunnable\u001b[39m.\u001b[39minvoke(inputs, config\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks})\n\u001b[1;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1426\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1425\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1426\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39minvoke(\n\u001b[1;32m   1427\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[1;32m   1428\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m             patch_config(\n\u001b[1;32m   1430\u001b[0m                 config, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mseq:step:\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1431\u001b[0m             ),\n\u001b[1;32m   1432\u001b[0m         )\n\u001b[1;32m   1433\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/base.py:2786\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m   2781\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2782\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   2783\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2784\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2785\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[0;32m-> 2786\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbound\u001b[39m.\u001b[39minvoke(\n\u001b[1;32m   2787\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m   2788\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   2789\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs},\n\u001b[1;32m   2790\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chat_models/base.py:142\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    139\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    141\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    143\u001b[0m             [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_input(\u001b[39minput\u001b[39m)],\n\u001b[1;32m    144\u001b[0m             stop\u001b[39m=\u001b[39mstop,\n\u001b[1;32m    145\u001b[0m             callbacks\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    146\u001b[0m             tags\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    147\u001b[0m             metadata\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    148\u001b[0m             run_name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    149\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    150\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    151\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chat_models/base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    457\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    458\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(prompt_messages, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    340\u001b[0m                 m,\n\u001b[1;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[1;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[i] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    343\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[1;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chat_models/ollama.py:98\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m        ])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_messages_as_text(messages)\n\u001b[0;32m---> 98\u001b[0m final_chunk \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_stream_with_aggregation(\n\u001b[1;32m     99\u001b[0m     prompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    101\u001b[0m chat_generation \u001b[39m=\u001b[39m ChatGeneration(\n\u001b[1;32m    102\u001b[0m     message\u001b[39m=\u001b[39mAIMessage(content\u001b[39m=\u001b[39mfinal_chunk\u001b[39m.\u001b[39mtext),\n\u001b[1;32m    103\u001b[0m     generation_info\u001b[39m=\u001b[39mfinal_chunk\u001b[39m.\u001b[39mgeneration_info,\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/llms/ollama.py:178\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    171\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    176\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GenerationChunk:\n\u001b[1;32m    177\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mfor\u001b[39;00m stream_resp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_stream(prompt, stop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    179\u001b[0m         \u001b[39mif\u001b[39;00m stream_resp:\n\u001b[1;32m    180\u001b[0m             chunk \u001b[39m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/llms/ollama.py:154\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\n\u001b[1;32m    149\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[1;32m    151\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    152\u001b[0m     }\n\u001b[0;32m--> 154\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    155\u001b[0m     url\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url\u001b[39m}\u001b[39;00m\u001b[39m/api/generate/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m     headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    157\u001b[0m     json\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams},\n\u001b[1;32m    158\u001b[0m     stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    160\u001b[0m response\u001b[39m.\u001b[39mencoding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url, data\u001b[39m=\u001b[39mdata, json\u001b[39m=\u001b[39mjson, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"input\": \"Search for papers about cancer.\"}\n",
    ")\n",
    "print(response[\"output\"])\n",
    "\n",
    "#input = {\n",
    "#    \"input\": \"Search for papers about cancer.\",\n",
    "#    'agent_scratchpad': \"\"\n",
    "#}       \n",
    "#\n",
    "#agent_executor.with_types(input_type=Input, output_type=Output).invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39muvicorn\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     uvicorn\u001b[39m.\u001b[39mrun(app, host\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocalhost\u001b[39m\u001b[39m\"\u001b[39m, port\u001b[39m=\u001b[39m\u001b[39m8080\u001b[39m)\n",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39muvicorn\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nb_scratchpad/41_langchain_chat_main.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     uvicorn\u001b[39m.\u001b[39mrun(app, host\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocalhost\u001b[39m\u001b[39m\"\u001b[39m, port\u001b[39m=\u001b[39m\u001b[39m8080\u001b[39m)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/alhazen/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
