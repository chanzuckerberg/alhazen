{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from .core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "One thousand years ago, Ḥasan Ibn al-Haytham (965-1039 AD) studied optics through experimentation and observation. He advocated that a hypothesis must be supported by experiments based on confirmable procedures or mathematical reasoning—an early pioneer in the scientific method _five centuries_ before Renaissance scientists ([Website](https://www.ibnalhaytham.com/), [Wikipedia](https://en.wikipedia.org/wiki/Ibn_al-Haytham), [Tbakhi & Amir 2007](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6074172/)). \n",
    "\n",
    "We use the latinized form of his name ('Alhazen') as the name of this project to honor his contribution (which goes largely unrecognized from within non-Islamic communities). \n",
    "\n",
    "Famously, he was quoted as saying:\n",
    "\n",
    ">The duty of the man who investigates the writings of scientists, if learning the truth is his goal, is to make himself an enemy of all that he reads, and, applying his mind to the core and margins of its content, attack it from every side. He should also suspect himself as he performs his critical examination of it, so that he may avoid falling into either prejudice or leniency.\n",
    "\n",
    "Here, we seek to develop an AI capable of applying scientific knowledge engineering to support CZI's mission. We seek to honor Ibn al-Haytham's critical view of published knowledge by creating a AI-powered system for scientific discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "This library uses either the MLC.AI project (<https://mlc.ai/>) or Ollama (<https://ollama.ai/>) and\n",
    "primarily functions by executing a local LLM on the user’s machine - and then call that system via a REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Llama2 locally with Ollama (RECOMMENDED)\n",
    "\n",
    "Instructions: <https://ollama.ai/download>\n",
    "\n",
    "1.  Very simple download and install. Just follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Llama2 locally with MLC.AI\n",
    "\n",
    "Instructions: <https://mlc.ai/mlc-llm/docs/>\n",
    "\n",
    "1.  Download and install pre-built pip wheels for mlc-ai and mlc-llm\n",
    "    from <https://mlc.ai/package/>\n",
    "\n",
    "    For Metal installation on Mac, run this command:\n",
    "\n",
    "    `pip install --pre --force-reinstall mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels/`\n",
    "\n",
    "2.  Verify installation\n",
    "\n",
    "    `python -m mlc_chat.rest --help`\n",
    "\n",
    "    This should print help information for the REST API.\n",
    "\n",
    "3.  Download the quantized Llama2 model from github and place it in the\n",
    "    dist/prebuilt directory.\n",
    "\n",
    "        mkdir -p dist/prebuilt\n",
    "        git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\n",
    "        cd dist/prebuilt\n",
    "        git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-70b-chat-hf-q4f16_1\n",
    "\n",
    "4.  Run the REST API\n",
    "\n",
    "    `python -m mlc_chat.rest --device-name metal  --model Llama-2-70b-chat-hf  --quantization q4f16_1`\n",
    "\n",
    "    This should print output from the server that should end with:\n",
    "\n",
    "    `INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)`\n",
    "\n",
    "## How to use\n",
    "\n",
    "At this stage, development and use of Alhazen will be performed via\n",
    "local Jupyter notebooks and the (https://nbdev.fast.ai/)[nbdev] framework. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
