{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from .core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alhazen\n",
    "\n",
    "> An intelligent agent to help read and understand scientific research based on extant knowledge (i.e., what is already known and reported in the scientific literature, online databases, wikipedia, or any other sources that we can find)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an early proof-of-concept prototype developed within CZI's Research Science Team (RST). It is intended to be used as a downloadable library that can be run on a high-end local machine (M2 Apple Macbook with 32+GB of memory - no support for Windows or Linux yet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Install from source\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/chanzuckerberg/alzhazen\n",
    "conda create -n alhazen python=3.11\n",
    "conda activate alhazen\n",
    "cd alhazen\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "## Other dependencies\n",
    "\n",
    "### Databricks\n",
    "\n",
    "You will need to run remote queries on CZI's Databricks general prod instance: <https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/>\n",
    "\n",
    "You will need to have a Databricks token in your environment variables. You can generate one by following the instructions here: <https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-token>\n",
    "\n",
    "Set this token as an environment variable called `DB_TOKEN` in your shell:\n",
    "\n",
    "```bash\n",
    "export DB_TOKEN=<your token>\n",
    "```\n",
    "\n",
    "### GGUF Files from HuggingFace (TheBloke)\n",
    "\n",
    "The tool uses quantized model files from HuggingFace and will place them for you into a temporary location on disk (`/tmp/alhazen/` is the default):\n",
    "\n",
    "* [Llama-2-70B](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF) (recommended [file](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q5_K_M.gguf), requires 51.25 GB) \n",
    "* [Llama-2-13B](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF) (recommended [file](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/blob/main/llama-2-13b-chat.Q5_K_M.gguf), requires 11.73 GB) \n",
    "\n",
    "## How to use\n",
    "\n",
    "We use the fire library to create a modular command line interface (CLI) for Alhazen.\n",
    "\n",
    "For example to run the chatbot for the single paper QA task, execute the following command:\n",
    "\n",
    "```bash\n",
    "python -m fire alhazen.apps <tool_name> <tool_args>\n",
    "```\n",
    "\n",
    "for example, run the following command to chat with the single paper QA chatbot:\n",
    "\n",
    "```bash\n",
    "python -m fire alhazen.apps single_paper_chatbot '/path/to/pdf/or/nxml/files/' 'mistral-7b-instruct'`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Status and Capabilities\n",
    "\n",
    "This project is still very early, but we are attempting to provide access to \n",
    "the full range of capabilities of the project as we develop them. We will provide some access to each capability \n",
    "through [Gradio](https://gradio.app/) as we develop them, and will eventually synthesise them into a single\n",
    "agent-driven interface.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where does the Name 'Alhazen' come from?\n",
    "\n",
    "One thousand years ago, Ḥasan Ibn al-Haytham (965-1039 AD) studied optics through experimentation and observation. He advocated that a hypothesis must be supported by experiments based on confirmable procedures or mathematical reasoning — an early pioneer in the scientific method _five centuries_ before Renaissance scientists started following the same paradigm ([Website](https://www.ibnalhaytham.com/), [Wikipedia](https://en.wikipedia.org/wiki/Ibn_al-Haytham), [Tbakhi & Amir 2007](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6074172/)). \n",
    "\n",
    "We use the latinized form of his name ('Alhazen') to honor his contribution (which goes largely unrecognized from within non-Islamic communities). \n",
    "\n",
    "Famously, he was quoted as saying:\n",
    "\n",
    ">The duty of the man who investigates the writings of scientists, if learning the truth is his goal, is to make himself an enemy of all that he reads, and, applying his mind to the core and margins of its content, attack it from every side. He should also suspect himself as he performs his critical examination of it, so that he may avoid falling into either prejudice or leniency.\n",
    "\n",
    "Here, we seek to develop an AI capable of applying scientific knowledge engineering to support CZI's mission. We seek to honor Ibn al-Haytham's critical view of published knowledge by creating a AI-powered system for scientific discovery.\n",
    "\n",
    "Note - when describing our agent, we will use non-gendered pronouns (they/them/it) to refer to the agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
