{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Alhazen Agent / Chatbot.\n",
    "\n",
    "> Building a simple Gradio application to allow the user to chat to Alhazen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp apps.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.core import OllamaRunner, PromptTemplateRegistry\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \n",
    "from alhazen.toolkit import AlhazenToolkit\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser\n",
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.tools.render import render_text_description, render_text_description_and_args\n",
    "from langchain.agents.output_parsers.json import JSONAgentOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "from uuid import UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class AlhazenAgentChatBot:\n",
    "    '''Provide access to an Alhazen agent via a simple chat interface with Gradio.'''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "\n",
    "        self.db = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "        self.ollr = OllamaRunner('mixtral')\n",
    "        self.llm  = self.ollr.llm\n",
    "\n",
    "        tk = AlhazenToolkit(db=self.db, ollr=self.ollr)\n",
    "        self.tools = tk.get_tools()\n",
    "\n",
    "        pts = PromptTemplateRegistry()\n",
    "        pts.load_prompts_from_yaml('alhazen_base.yaml')\n",
    "\n",
    "        self.prompt = pts.get_prompt_template('structured chat agent').generate_chat_prompt_template()\n",
    "        \n",
    "        agent = create_structured_chat_agent(self.llm, self.tools, self.prompt)\n",
    "\n",
    "        self.agent_executor = AgentExecutor(agent=agent, \n",
    "                                            tools=self.tools,\n",
    "                                            verbose=True, \n",
    "                                            handle_parsing_errors=True)\n",
    "\n",
    "    # We need to add these input/output schemas because the current AgentExecutor\n",
    "    # is lacking in schemas.\n",
    "    class ChatRequest(BaseModel):\n",
    "        question: str\n",
    "        chat_history: Optional[List[Dict[str, str]]]\n",
    "\n",
    "    class Input(BaseModel):\n",
    "        input: str\n",
    "\n",
    "    class Output(BaseModel):\n",
    "        output: str\n",
    "\n",
    "    def run_gradio(self):\n",
    "\n",
    "        def add_text(history, text):\n",
    "            #print('add_text: history: %s, text: %s'%(history, text))\n",
    "            history = history + [(text, None)]\n",
    "            return history, gr.Textbox(value=\"\", interactive=False)\n",
    "            \n",
    "        def clear_chat(history):\n",
    "            return []\n",
    "\n",
    "        def bot(history):\n",
    "            #print('bot: history: %s'%(history))\n",
    "            # prompt to send to the agent is the last message from the user\n",
    "            prompt = history[-1][0]\n",
    "            response = self.agent_executor.invoke({\"input\": prompt}, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "            print('RESPONSE: %s'%(str(response)))\n",
    "            history[-1][1] = str(response.get('output'))\n",
    "            print('WHOLE HISTORY: %s'%(history))\n",
    "            return history\n",
    "\n",
    "        with gr.Blocks() as demo:\n",
    "            with gr.Tab(\"Chat\"):\n",
    "                chatbot = gr.Chatbot(\n",
    "                    [],\n",
    "                    elem_id=\"chatbot\",\n",
    "                    bubble_full_width=False,\n",
    "                    #avatar_images=(None, files(alhazen_resources).joinpath('alhazen.png'))\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    txt = gr.Textbox(\n",
    "                        scale=4,\n",
    "                        show_label=False,\n",
    "                        placeholder=\"Enter text and press enter.\",\n",
    "                        container=False,\n",
    "                    )\n",
    "                    clear_btn = gr.Button(\"Clear\")\n",
    "            with gr.Tab(\"Dashboard\"):\n",
    "                with gr.Row():\n",
    "                    doc_text = gr.HTML(label=\"File Contents\")\n",
    "            \n",
    "            txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(bot, chatbot, chatbot)\n",
    "            txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)                \n",
    "            clear_btn.click(clear_chat, [], [chatbot], queue=False)\n",
    "                \n",
    "        demo.queue()\n",
    "        demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'em_tech' \n",
    "loc = '/Users/gully.burns/alhazen'\n",
    "\n",
    "db = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "ollr = OllamaRunner('mixtral')\n",
    "llm  = ollr.llm\n",
    "\n",
    "cb = AlhazenAgentChatBot()\n",
    "agent_response = cb.agent_executor({\"input\": 'Hello'})\n",
    "print(agent_response.get('output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cb.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/utils.py:827: UserWarning: Expected 1 arguments for function <function AlhazenAgentChatBot.run_gradio.<locals>.clear_chat at 0x41f80f060>, received 0.\n",
      "  warnings.warn(\n",
      "/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/utils.py:831: UserWarning: Expected at least 1 arguments for function <function AlhazenAgentChatBot.run_gradio.<locals>.clear_chat at 0x41f80f060>, received 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: The user is saying hello, so I should respond with a friendly greeting.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": {\n",
      "    \"greeting\": \"Hello! Welcome to the scientific paper analysis system. How can I assist you today?\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: {'input': 'Hello', 'output': {'greeting': 'Hello! Welcome to the scientific paper analysis system. How can I assist you today?'}}\n",
      "WHOLE HISTORY: [['Hello', \"{'greeting': 'Hello! Welcome to the scientific paper analysis system. How can I assist you today?'}\"]]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: The user wants to know what this system can do. I will list the capabilities of the system by providing a JSON blob with the \"action\" set to \"Final Answer\" and the \"action\\_input\" set to a description of the system's abilities.\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"This system can perform various actions related to scientific research. It can add collections from EPMC queries, describe collections, delete collections, retrieve full text for a paper ID, extract metadata for a paper, and perform Q&A over papers in a collection.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: {'input': 'What can you do?', 'output': 'This system can perform various actions related to scientific research. It can add collections from EPMC queries, describe collections, delete collections, retrieve full text for a paper ID, extract metadata for a paper, and perform Q&A over papers in a collection.'}\n",
      "WHOLE HISTORY: [['Hello', \"{'greeting': 'Hello! Welcome to the scientific paper analysis system. How can I assist you today?'}\"], ['What can you do?', 'This system can perform various actions related to scientific research. It can add collections from EPMC queries, describe collections, delete collections, retrieve full text for a paper ID, extract metadata for a paper, and perform Q&A over papers in a collection.']]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: The user is asking for my name, so I don't need to use any tools. I can simply provide a response with the \"Final Answer\" action.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": {\n",
      "    \"response\": \"I am a helpful AI assistant.\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RESPONSE: {'input': \"What's your name\", 'output': {'response': 'I am a helpful AI assistant.'}}\n",
      "WHOLE HISTORY: [['Hello', \"{'greeting': 'Hello! Welcome to the scientific paper analysis system. How can I assist you today?'}\"], ['What can you do?', 'This system can perform various actions related to scientific research. It can add collections from EPMC queries, describe collections, delete collections, retrieve full text for a paper ID, extract metadata for a paper, and perform Q&A over papers in a collection.'], [\"What's your name\", \"{'response': 'I am a helpful AI assistant.'}\"]]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: The user wants to know how to add a collection from EPMC. I will guide them by using the `add_collection_from_epmc_query` tool.\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"add_collection_from_epmc_query\",\n",
      "  \"action_input\": {\n",
      "    \"id\": \"example_id\",\n",
      "    \"name\": \"Example Collection\",\n",
      "    \"query\": \"cancer AND treatment\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/utils.py\", line 673, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gr/nnpvfcf57tsfqf7kbw_xz3c80000gq/T/ipykernel_20825/2229851808.py\", line 68, in bot\n",
      "    response = self.agent_executor.invoke({\"input\": prompt})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 87, in invoke\n",
      "    return self(\n",
      "           ^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 310, in __call__\n",
      "    raise e\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 304, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1245, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1095, in _take_next_step\n",
      "    observation = tool.run(\n",
      "                  ^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/tools/base.py\", line 365, in run\n",
      "    raise e\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/tools/base.py\", line 337, in run\n",
      "    self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\n",
      "TypeError: DescribeCollectionCompositionTool._run() missing 1 required positional argument: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/utils.py\", line 673, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gr/nnpvfcf57tsfqf7kbw_xz3c80000gq/T/ipykernel_20825/2229851808.py\", line 68, in bot\n",
      "    response = self.agent_executor.invoke({\"input\": prompt})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 87, in invoke\n",
      "    return self(\n",
      "           ^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 310, in __call__\n",
      "    raise e\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 304, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1245, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1095, in _take_next_step\n",
      "    observation = tool.run(\n",
      "                  ^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/tools/base.py\", line 365, in run\n",
      "    raise e\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/tools/base.py\", line 337, in run\n",
      "    self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\n",
      "TypeError: DescribeCollectionCompositionTool._run() missing 1 required positional argument: 'name'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/output_parsers/json.py\", line 43, in parse\n",
      "    response = parse_json_markdown(text)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/output_parsers/json.py\", line 145, in parse_json_markdown\n",
      "    parsed = parser(json_str)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 4 column 25 (char 68)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1032, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 385, in plan\n",
      "    output = self.runnable.invoke(inputs, config={\"callbacks\": callbacks})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/base.py\", line 1426, in invoke\n",
      "    input = step.invoke(\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/output_parser.py\", line 183, in invoke\n",
      "    return self._call_with_config(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/base.py\", line 847, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/output_parser.py\", line 184, in <lambda>\n",
      "    lambda inner_input: self.parse_result([Generation(text=inner_input)]),\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/output_parser.py\", line 226, in parse_result\n",
      "    return self.parse(result[0].text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/output_parsers/json.py\", line 55, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: {text}\") from e\n",
      "langchain.schema.output_parser.OutputParserException: Could not parse LLM output:  Thought: To answer this question, I need to explain how to use the `add_collection_from_epmc_query` tool.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"inform\",\n",
      "  \"action_input\": {\n",
      "    \"message\": \"The 'add\\_collection\\_from\\_epmc\\_query' tool allows you to execute a search for scientific papers in the EPMC database based on a query and then build a collection out of the papers returned. Here is an example of how to use it:\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/utils.py\", line 673, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gr/nnpvfcf57tsfqf7kbw_xz3c80000gq/T/ipykernel_20825/2229851808.py\", line 68, in bot\n",
      "    response = self.agent_executor.invoke({\"input\": prompt})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 87, in invoke\n",
      "    return self(\n",
      "           ^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 310, in __call__\n",
      "    raise e\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 304, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1245, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1043, in _take_next_step\n",
      "    raise ValueError(\n",
      "ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output:  Thought: To answer this question, I need to explain how to use the `add_collection_from_epmc_query` tool.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"inform\",\n",
      "  \"action_input\": {\n",
      "    \"message\": \"The 'add\\_collection\\_from\\_epmc\\_query' tool allows you to execute a search for scientific papers in the EPMC database based on a query and then build a collection out of the papers returned. Here is an example of how to use it:\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/output_parsers/json.py\", line 43, in parse\n",
      "    response = parse_json_markdown(text)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/output_parsers/json.py\", line 145, in parse_json_markdown\n",
      "    parsed = parser(json_str)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 4 column 25 (char 68)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1032, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 385, in plan\n",
      "    output = self.runnable.invoke(inputs, config={\"callbacks\": callbacks})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/base.py\", line 1426, in invoke\n",
      "    input = step.invoke(\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/output_parser.py\", line 183, in invoke\n",
      "    return self._call_with_config(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/base.py\", line 847, in _call_with_config\n",
      "    output = call_func_with_variable_args(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/runnable/config.py\", line 308, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/output_parser.py\", line 184, in <lambda>\n",
      "    lambda inner_input: self.parse_result([Generation(text=inner_input)]),\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/schema/output_parser.py\", line 226, in parse_result\n",
      "    return self.parse(result[0].text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/output_parsers/json.py\", line 55, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: {text}\") from e\n",
      "langchain.schema.output_parser.OutputParserException: Could not parse LLM output:  Thought: To answer this question, I need to explain how to use the `add_collection_from_epmc_query` tool.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"inform\",\n",
      "  \"action_input\": {\n",
      "    \"message\": \"The 'add\\_collection\\_from\\_epmc\\_query' tool allows you to execute a search for scientific papers in the EPMC database based on a query and then build a collection out of the papers returned. Here is an example of how to use it:\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/utils.py\", line 673, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gr/nnpvfcf57tsfqf7kbw_xz3c80000gq/T/ipykernel_20825/2229851808.py\", line 68, in bot\n",
      "    response = self.agent_executor.invoke({\"input\": prompt})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 87, in invoke\n",
      "    return self(\n",
      "           ^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 310, in __call__\n",
      "    raise e\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/chains/base.py\", line 304, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1245, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain/agents/agent.py\", line 1043, in _take_next_step\n",
      "    raise ValueError(\n",
      "ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output:  Thought: To answer this question, I need to explain how to use the `add_collection_from_epmc_query` tool.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"inform\",\n",
      "  \"action_input\": {\n",
      "    \"message\": \"The 'add\\_collection\\_from\\_epmc\\_query' tool allows you to execute a search for scientific papers in the EPMC database based on a query and then build a collection out of the papers returned. Here is an example of how to use it:\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    }
   ],
   "source": [
    "os.environ['ALHAZEN_DB_NAME'] = 'em_tech' \n",
    "os.environ['LOCAL_FILE_PATH'] = '/Users/gully.burns/alhazen'\n",
    "cb = AlhazenAgentChatBot()\n",
    "cb.run_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "alhazen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
