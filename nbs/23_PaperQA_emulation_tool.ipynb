{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper QA Emulation Tool   \n",
    "\n",
    "> We here emulate the workflow of the PaperQA system by Andrew White (https://thewhitelab.org/). This is a Retrieval Augmented Generation (RAG) application using a Map-Reduce model where we query our embedded index for based on a question, we then write summaries for each returned document based on relevance to the underlying question. Finally, we synthesize each summary into an essay presented as an answer to the original question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.paperqa_emulation_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.core import OllamaRunner, PromptTemplateRegistry, get_langchain_llm, get_cached_gguf, \\\n",
    "    get_langchain_embeddings, GGUF_LOOKUP_URL, MODEL_TYPE\n",
    "from alhazen.tools.basic import AlhazenToolMixin\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "from alhazen.utils.ceifns_db import *\n",
    "from alhazen.schema_sqla import ScientificKnowledgeCollection, \\\n",
    "    ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \\\n",
    "    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n",
    "    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n",
    "    InformationResource, Note, \\\n",
    "    ScientificKnowledgeCollectionHasNotes, ScientificKnowledgeExpressionHasNotes, \\\n",
    "    ScientificKnowledgeItemHasNotes, ScientificKnowledgeFragmentHasNotes \n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain.schema import get_buffer_string, StrOutputParser, OutputParserException, format_document\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "import local_resources.linkml as linkml\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import re\n",
    "import regex\n",
    "from sqlalchemy import create_engine, exists\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "from time import time,sleep\n",
    "from typing import Optional, Type\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "skc = aliased(ScientificKnowledgeCollection)\n",
    "skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "ske = aliased(ScientificKnowledgeExpression)\n",
    "ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "ski = aliased(ScientificKnowledgeItem)\n",
    "ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "skf = aliased(ScientificKnowledgeFragment)\n",
    "n = aliased(Note)\n",
    "skc_hn = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "ske_hn = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "ski_hn = aliased(ScientificKnowledgeItemHasNotes)\n",
    "skf_hn = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PaperQAEmulationToolSchema(BaseModel):\n",
    "    question: str = Field(description=\"The question to be considered by this workflow.\")\n",
    "    n_sample_size: Optional[int] = Field(description=\"The number of documents queried from the index to be evaluated for relevance.\")\n",
    "    n_summary_size: Optional[int] = Field(description=\"The number of documents queried from the index to be used in generating an answer.\")\n",
    "    collection_id: Optional[int] = Field(None, description=\"The identifier of the collection to be used to answer the question.\")\n",
    "\n",
    "class PaperQAEmulationTool(BaseTool, AlhazenToolMixin):\n",
    "    '''Runs a Map-Reduce model where we write a short essay to answer a scientific question based on a set of supporting documents.'''\n",
    "    name = 'simple_qa_over_papers'\n",
    "    description = '''Runs a Map-Reduce model where we write a short essay to answer a scientific question based on a set of supporting documents.'''\n",
    "    args_schema = PaperQAEmulationToolSchema\n",
    "    \n",
    "    def _run(self, question, n_sample_size=15, n_summary_size=5, collection_id=-1):\n",
    "        '''Runs the metadata extraction pipeline over a specified paper.'''\n",
    "        \n",
    "        DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template='\"DOI\": \"{e_id}\",\"CITATION\": \"{citation}\", \"CONTENT\":\"{page_content}\"')\n",
    "        def _combine_documents(docs):\n",
    "            m = [{'CONTENT':d.page_content, 'DOI':d.metadata.get('e_id'), \"CITATION\": d.metadata.get('citation')} for d in docs]\n",
    "            return json.dumps(m)\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 1. Set up environment\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~\n",
    "        if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "            raise Exception('Where are you storing your local literature database?')\n",
    "        loc = os.environ['LOCAL_FILE_PATH']\n",
    "        if loc[-1:] != '/':\n",
    "            loc += '/'\n",
    "        if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "            raise Exception('Which database do you want to use for this application?')\n",
    "        db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "        os.environ['PGVECTOR_CONNECTION_STRING'] = \"postgresql+psycopg2:///\"+db_name\n",
    "            \n",
    "        vectorstore = PGVector.from_existing_index(\n",
    "                embedding = self.db.embed_model, \n",
    "                collection_name = 'ScienceKnowledgeItem') \n",
    "        \n",
    "        # set default values for optional parameters\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={'k':n_sample_size})\n",
    "        if collection_id != -1:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={'k':n_sample_size, 'filter': {'c_ids': 2}})\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 2. Run Map Function - Iterate over 'pages' of Documents returned from vectorstore to generate \n",
    "        #                       summaries\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                                \n",
    "        pts = PromptTemplateRegistry()\n",
    "        pts.load_prompts_from_yaml('paperqa_emulation.yaml')\n",
    "        pt = pts.get_prompt_template('summarize paper set').generate_chat_prompt_template()\n",
    "        \n",
    "        context_build_chain = (\n",
    "            RunnableParallel({\n",
    "                \"k\": itemgetter(\"k\"),\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"context\": itemgetter(\"question\") | retriever | _combine_documents \n",
    "            }\n",
    "        ))\n",
    "\n",
    "        summary_chain = (\n",
    "            RunnableParallel({\n",
    "                \"k\": itemgetter(\"k\"),\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"summary_length\": itemgetter(\"summary_length\"),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            })\n",
    "            | {\n",
    "                \"summary\": pt | ChatOllama(model='mixtral') | JsonEnclosedByTextOutputParser(),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        input = {'question': question, 'summary_length': 1000, 'k':5}    \n",
    "        out = context_build_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "        context = json.loads(out.get('context'))\n",
    "        print(len(context))\n",
    "\n",
    "        page_count = int(n_sample_size / n_summary_size)\n",
    "        summaries = {}\n",
    "        for pg in range(page_count):\n",
    "            paged_context = [context[i] for i in range(pg*n_summary_size, (pg+1)*n_summary_size)]\n",
    "            paged_input = input.copy()\n",
    "            paged_input['context'] = json.dumps(paged_context)\n",
    "            out2 = summary_chain.invoke(paged_input, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "            summs = out2.get('summary', [])\n",
    "            if summs:\n",
    "                for m in out2.get('summary', []):\n",
    "                    summaries[m.get('DOI')] = {}\n",
    "                    for k in m:\n",
    "                        summaries[m.get('DOI')][k] = m[k]\n",
    "                for m2 in json.loads(out2.get('context','[]')):\n",
    "                    if summaries.get(m2.get('DOI')):\n",
    "                        summaries[m2.get('DOI')]['CITATION'] = m2.get('CITATION')\n",
    "                        summaries[m2.get('DOI')]['CONTENT'] = m2.get('CONTENT')\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 3. Order sumaries in terms of 'RELEVANCE'\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        ordered_summaries = []\n",
    "        for i, s in enumerate(sorted(summaries.values(), key=lambda x: int(x['RELEVANCE SCORE']), reverse=True)):\n",
    "            if int(s['RELEVANCE SCORE']) < 7:\n",
    "                continue\n",
    "            s['ID'] = i+1\n",
    "            ordered_summaries.append({'ID':s.get('ID'),\n",
    "                                        'CITATION': s.get('CITATION'), \n",
    "                                        'SUMMARY': s.get('SUMMARY'), \n",
    "                                        'RELEVANCE': s.get('RELEVANCE SCORE')})\n",
    "            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 4. Run Reduce Function - Write the synthesis over summaries provided\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        pt2 = pts.get_prompt_template('write synthesis').generate_chat_prompt_template()\n",
    "\n",
    "        qa_synthesis_chain = (\n",
    "            RunnableParallel({\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            })\n",
    "            | {\n",
    "                \"answer\": pt2 | ChatOllama(model='mixtral'),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        input = {'question': question, 'context': ordered_summaries[:5]}    \n",
    "        out3 = qa_synthesis_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "\n",
    "        answer = {'answer':out3.get('answer').content,\n",
    "                'context':ordered_summaries}\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
