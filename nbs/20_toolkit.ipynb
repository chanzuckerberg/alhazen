{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alhazen Toolkit   \n",
    "\n",
    "> A set of Langchain tools that populate and query a CEIFNS database by (1) Build collections of expressions; (2) locate and load items that represent expressions; (3) segregate the parts of items as 'fragments'; (4) analyze the fragments to generate notes that can then be summarized to provide summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "\n",
    "from alhazen.core import OllamaRunner\n",
    "from alhazen.tools.basic import *\n",
    "from alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool \n",
    "from alhazen.utils.ceifns_db import *\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.pydantic_v1 import BaseModel, Extra, Field, root_validator\n",
    "from langchain.schema.prompt_template import format_document\n",
    "\n",
    "from importlib_resources import files\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# NOTE - Use LangChain's SQL_DATABASE TOOLKIT AS A MODEL \n",
    "# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/agent_toolkits/sql/toolkit.py#L18\n",
    "#\n",
    "# Use environment variables to denote the database name + base file location\n",
    "# os.environ['ALHAZEN_DB_NAME'] = 'em_tech'\n",
    "# os.environ['LOCAL_FILE_PATH'] = '/Users/gburns/alhazen/'\n",
    "\n",
    "class AlhazenToolkit(BaseModel):\n",
    "    '''Toolkit for building and querying an Alhazen CEIFNS (pron. 'SAI-FiNS') database \n",
    "    (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).'''\n",
    "\n",
    "    # The local literature database (Collections, Expressions, Items, and Fragments)\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True)\n",
    "    ollr : OllamaRunner = Field(exclude=True)\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_tools(self) -> List[BaseTool]:\n",
    "        \"\"\"Get the tools in the toolkit.\"\"\"\n",
    "        add_collection_tool_description = (\n",
    "            \"This tool executes a search for scientific papers in the EPMC database based on a query\"\n",
    "            \" and then builds a collection out of the papers returned. \"\n",
    "            \"Input to this tool has three parameters: \\n\"\n",
    "            \"- 'id' which is string that denotes the identifier of the collection in the database.\\n\"\n",
    "            \"- 'query' which is string that defines a query using Boolean logic for search terms.\\n\"\n",
    "            \"- 'name' which is a string that defines a descriptive name for the collection.\\n\"\n",
    "            \"The tool will execute an query over the remote database, create the collection and add papers to the collection to our local database.\"\n",
    "            \"If successful, it will return 'Final Answer'. If not, it will return an error report.\"\n",
    "        )\n",
    "        add_collection_tool = AddCollectionFromEPMCTool(db=self.db, description=add_collection_tool_description)\n",
    "        \n",
    "        describe_collection_tool_description = (\n",
    "            \"This tool describes the contents of a collection in the database. \"\n",
    "            \"Input to this tool has one parameters: \\n\"\n",
    "            \"- 'id' which is string that denotes the identifier of the collection in the database.\\n\"\n",
    "            \"If successful, it will return 'Final Answer'. If not, it will return an error report.\"\n",
    "        )\n",
    "        describe_collection_tool = DescribeCollectionCompositionTool(db=self.db, description=describe_collection_tool_description)\n",
    "\n",
    "        delete_collection_tool_description = (\n",
    "            \"This tool deletes a collection from the database. \"\n",
    "            \"Input to this tool has one parameters: \\n\"\n",
    "            \"- 'id' which is string that denotes the identifier of the collection in the database.\\n\"\n",
    "            \"The tool will delete the collection from our local database.\"\n",
    "            \"If successful, it will return 'Final Answer'. If not, it will return an error report.\"\n",
    "        )\n",
    "        delete_collection_tool = DeleteCollectionTool(db=self.db, description=delete_collection_tool_description)\n",
    "\n",
    "        retrieve_full_text_tool_description = (\n",
    "            \"This tool invokes a web search for a copy of a full text paper from the web given a doi identifier. \"\n",
    "            \"Input to this tool has one parameters: \\n\"\n",
    "            \"- 'paper_id' which is a string that denotes the doi identifier of the paper in question.\\n\"\n",
    "            \"The tool will search online for the paper, return it and add it's text to the database.\"\n",
    "            \"If successful, it will return 'Final Answer'. If not, it will return an error report.\"\n",
    "        )\n",
    "        retrieve_full_text_tool = RetrieveFullTextTool(db=self.db, description=retrieve_full_text_tool_description)\n",
    "\n",
    "        metadata_extraction_tool_description = (\n",
    "            \"Input to this tool is a doi identifier, a search term for section titles in \"\n",
    "            \"the paper, and the name of a type of experiment (drawn from a predefined list). \"\n",
    "            \"The tool will execute an LLM over the paper to extract metadata from available text \"\n",
    "            \" and then insert the metadata into the database. The output is a \"\n",
    "            \"string that returns a completion message (either positive or an error report).\"\n",
    "        )\n",
    "        metadata_extraction_tool = MetadataExtractionTool(\n",
    "            db=self.db, ollr=self.ollr, llm=self.ollr.llm, description=metadata_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        paperqa_emulation_tool_description = (\n",
    "            \"Input to this tool is a question, the ID value for the collection that the question \"\n",
    "            \"will be asked over (collection_id), the number of documents to be sampled (n_sample_size), \"\n",
    "            \"the number of documents to be synthesized in the final analysis.\"\n",
    "            \"The tool will execute an Map-Reduce RAG pipeline to query the vector store, and then \"\n",
    "            \"summarize the information returned to write a response to the question.\"\n",
    "        )\n",
    "        paperqa_emulation_tool = PaperQAEmulationTool(\n",
    "            db=self.db, ollr=self.ollr, llm=self.ollr.llm, description=paperqa_emulation_tool_description\n",
    "        )\n",
    "                \n",
    "        return [\n",
    "            add_collection_tool,\n",
    "            describe_collection_tool,\n",
    "            delete_collection_tool,\n",
    "            retrieve_full_text_tool,\n",
    "            metadata_extraction_tool,\n",
    "            paperqa_emulation_tool\n",
    "        ]\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
