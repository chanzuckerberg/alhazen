{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alhazen Toolkit   \n",
    "\n",
    "> A set of Langchain tools that populate and query a CEIFNS database by (1) Build collections of expressions; (2) locate and load items that represent expressions; (3) segregate the parts of items as 'fragments'; (4) analyze the fragments to generate notes that can then be summarized to provide summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "\n",
    "from alhazen.core import OllamaRunner\n",
    "from alhazen.tools.basic import *\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool \n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool \n",
    "from alhazen.utils.ceifns_db import *\n",
    "\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.agents.agent import RunnableAgent\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.pydantic_v1 import BaseModel, Extra, Field, root_validator\n",
    "from langchain.schema.prompt_template import format_document\n",
    "\n",
    "from importlib_resources import files\n",
    "import local_resources.prompt_elements as prompt_elements\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from typing import List, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# NOTE - Use LangChain's SQL_DATABASE TOOLKIT AS A MODEL \n",
    "# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/agent_toolkits/sql/toolkit.py#L18\n",
    "#\n",
    "# Use environment variables to denote the database name + base file location\n",
    "# os.environ['ALHAZEN_DB_NAME'] = 'em_tech'\n",
    "# os.environ['LOCAL_FILE_PATH'] = '/Users/gburns/alhazen/'\n",
    "\n",
    "class AlhazenToolkit(BaseModel):\n",
    "    '''Toolkit for building and querying an Alhazen CEIFNS (pron. 'SAI-FiNS') database \n",
    "    (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).'''\n",
    "\n",
    "    # The local literature database (Collections, Expressions, Items, and Fragments)\n",
    "    db: Ceifns_LiteratureDb = Field(exclude=True)\n",
    "    llm : BaseChatModel = Field(exclude=True)\n",
    "    agent : Optional[RunnableAgent] = Field(exclude=True) \n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_tools(self) -> List[BaseTool]:\n",
    "        \"\"\"Get the tools in the toolkit.\"\"\"\n",
    "\n",
    "        add_collection_tool_description = (\n",
    "            \"This tool executes a search for scientific papers in the EPMC database based on a query\"\n",
    "            \" and then builds a collection out of the papers returned. \"\n",
    "            \"Input to this tool has three parameters: \\n\"\n",
    "            \"- 'id' which is string that denotes the identifier of the collection in the database.\\n\"\n",
    "            \"- 'query' which is string that defines a query using Boolean logic for search terms.\\n\"\n",
    "            \"- 'name' which is a string that defines a descriptive name for the collection.\\n\"\n",
    "            \"The tool will execute an query over the remote database, create the collection and add papers to the collection to our local database.\"\n",
    "            \"If successful, it will return 'Final Answer'. If not, it will return an error report.\"\n",
    "        )\n",
    "        add_collection_tool = AddCollectionFromEPMCTool(db=self.db, description=add_collection_tool_description)\n",
    "        \n",
    "        describe_collection_tool_description = (\n",
    "            \"This tool describes the contents of a collection in the database. \"\n",
    "            \"Input to this tool has one parameters: \\n\"\n",
    "            \"- 'id' which is string that denotes the identifier of the collection in the database.\\n\"\n",
    "            \"If successful, it will return 'Final Answer'. If not, it will return an error report.\"\n",
    "        )\n",
    "        describe_collection_tool = DescribeCollectionCompositionTool(db=self.db, description=describe_collection_tool_description)\n",
    "\n",
    "        delete_collection_tool_description = (\n",
    "            \"This tool deletes a collection from the database.\"\n",
    "        )\n",
    "        delete_collection_tool = DeleteCollectionTool(db=self.db, description=delete_collection_tool_description)\n",
    "\n",
    "        check_expression_tool_description = (\n",
    "            \"This tool checks if the database contains a paper. \"\n",
    "            \"Input to this tool has one parameter: \\n\"\n",
    "            \"- 'query' which is a search string for the paper (based on either the id or title).\\n\"\n",
    "            \"The tool will query the database and report its findings.\"\n",
    "        )\n",
    "        check_expression_tool = CheckExpressionTool(db=self.db, description=check_expression_tool_description)\n",
    "\n",
    "        retrieve_full_text_tool_description = (\n",
    "            \"This tool invokes a web search for single full text paper from the web given a doi identifier. \"\n",
    "            \"Input to this tool has one parameter: \\n\"\n",
    "            \"- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'\\n\"\n",
    "            \"The tool will search online for the paper, return it and add it's text to the database.\"\n",
    "            \"If successful, it will return 'Final Answer'. If not, it will return an error report.\"\n",
    "        )\n",
    "        retrieve_full_text_tool = RetrieveFullTextTool(db=self.db, description=retrieve_full_text_tool_description)\n",
    "\n",
    "        metadata_extraction_tool_description = (\n",
    "            \"This tool extracts experimental metadata for an experiment from a single full text paper.\"\n",
    "            \"In order to use this tool, you need to know if there is a full text version of the paper in the database\"\n",
    "            \"If there is not, you need to retrieve a copy of the full text before using this\" \n",
    "            \"Input to this tool has two parameters: \\n\"\n",
    "            \"- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n\"\n",
    "            \"- 'extraction_type' which denotes the type of experiment being analysed. This should be set to the string 'cryoet'.\\n\"\n",
    "            \"The tool will execute an LLM over the paper to extract metadata from available text \"\n",
    "            \" and then insert the metadata into the database. The output is a \"\n",
    "            \"string that returns a completion message (either positive or an error report).\"\n",
    "        )\n",
    "        metadata_extraction_tool = MetadataExtractionTool(\n",
    "            db=self.db, llm=self.llm, description=metadata_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        simple_extraction_tool_description = (\n",
    "            \"This tool extracts information from a single full text paper.\"\n",
    "            \"In order to use this tool, you need to know if there is a full text version of the paper in the database\"\n",
    "            \"If there is not, you need to retrieve a copy of the full text before using this.\" \n",
    "            \"Input to this tool has three parameters: \\n\"\n",
    "            \"- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n\"\n",
    "            \"- 'variable_name' which denotes the name of the variable that the question is attempting to describe (e.g. 'species_name', 'sample_preparation_method').\\n\"\n",
    "            \"- 'question' which is the question that must be answered to provide a value for the specified variable (e.g., 'Rat', 'Plunge Vitrificiation').\\n\"\n",
    "            \"The tool will execute an LLM over the paper to extract answers to the specified question. \"\n",
    "            \"Output from the tool will be a JSON-formatted string with two fields: 'report' and 'data'.\" \n",
    "            \"Text in the report field specifies how the tool performed this task.\"\n",
    "        )\n",
    "        simple_extraction_tool = SimpleExtractionWithRAGTool(\n",
    "            db=self.db, llm=self.llm, description=simple_extraction_tool_description\n",
    "        )\n",
    "\n",
    "        paperqa_emulation_tool_description = (\n",
    "            \"This tool answers scientific questions by searching for papers from Alhazen's database, \"\n",
    "            \"reading a small subset of them and then synthesizing a response in the form of an essay. \"\n",
    "            \"Only use this tool to respond to scientific questions that you already have built a database for.\"\n",
    "            \"Input to this tool is a question, the ID value for the collection that the question \"\n",
    "            \"will be asked over (collection_id), the number of documents to be sampled (n_sample_size), \"\n",
    "            \"the number of documents to be synthesized in the final analysis.\"\n",
    "            \"The tool will execute an Map-Reduce RAG pipeline to query the vector store, and then \"\n",
    "            \"summarize the information returned to write a response to the question.\"\n",
    "        )\n",
    "        paperqa_emulation_tool = PaperQAEmulationTool(\n",
    "            db=self.db, llm=self.llm, description=paperqa_emulation_tool_description\n",
    "        )\n",
    "        tool_list = [\n",
    "            add_collection_tool,\n",
    "            describe_collection_tool,\n",
    "            delete_collection_tool,\n",
    "            retrieve_full_text_tool,\n",
    "            metadata_extraction_tool,\n",
    "            simple_extraction_tool,\n",
    "            paperqa_emulation_tool,\n",
    "            check_expression_tool\n",
    "        ]\n",
    "\n",
    "        introspection_tool_description = (\n",
    "            \"This tool answers questions about the tools I have access to and what they do.\"\n",
    "            \"Always call this tool when a user asks 'How might you ...?'\"\n",
    "            \"Call the tool with the description of the task being asked about.\"\n",
    "        )\n",
    "        introspection_tool = IntrospectionTool(description=introspection_tool_description)\n",
    "        #tool_list.append(introspection_tool)\n",
    "\n",
    "        return tool_list\n",
    "    \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if self.db.session is None:\n",
    "            session_class = sessionmaker(bind=self.db.engine)\n",
    "            self.db.session = session_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
