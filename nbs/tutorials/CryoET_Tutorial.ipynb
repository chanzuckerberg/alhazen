{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CryoET Tutorial \n",
    "\n",
    "> Developing a simple tutorial to provide a walkthrough for users attempting to use Alhazen for the first time. This is based on analysis of the Cryo Electron Tomography literature and tools we have developed to analyze that data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CryoET\n",
    "\n",
    "Cryo-electron Tomography (CryoET) involves rapidly freezing biological samples in their natural state to preserve their three-dimensional structure without the need for staining or crystallization. This methodology allows researchers to visualize proteins and other biomolecules at near-atomic resolution.\n",
    "\n",
    "This digital library is based on capturing all papers that mention the technique in their titles, abstracts, or methods sections and then analyzing the various methods used and their applications. Our focus is on supporting the work of the Chan Zuckerberg Imaging Institute, [CZII](https://www.czimaginginstitute.org/) on developing [the CryoET data portal](https://cryoetdataportal.czscience.com/), an open source repository for CryoET-based data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports\n",
    "\n",
    "Setting python imports, environment variables, and other crucial set up parameters here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from alhazen.aliases import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.agent import AlhazenAgent\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.protocol_extraction_tool import *\n",
    "from alhazen.tools.tiab_classifier_tool import *\n",
    "from alhazen.tools.tiab_extraction_tool import *\n",
    "from alhazen.tools.tiab_mapping_tool import *\n",
    "from alhazen.toolkit import *\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database\n",
    "\n",
    "from alhazen.utils.searchEngineUtils import *\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "You must set the following environmental variables for this code:\n",
    "\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   \n",
    "\n",
    "Note that this notebook will build and use a database specified as `cryoet_tutorial`, specified below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n",
    "\n",
    "loc = os.environ['LOCAL_FILE_PATH']\n",
    "db_name = 'cryoet_tutorial'\n",
    "\n",
    "# Variable to prevent accidental deletion of the database or any records\n",
    "OK_TO_DELETE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup utils, agents, and tools \n",
    "\n",
    "This cell sets up a database engine (`ldb`) and lists the available large-language models you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ollama_llama3', 'ollama_mixtral', 'databricks_dbrx', 'databricks_mixtral', 'databricks_llama3', 'groq_mixtral', 'groq_llama3', 'gpt4_1106', 'gpt35'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_groq/chat_models.py:147: UserWarning: WARNING! stop is not default parameter.\n",
      "                    stop was transferred to model_kwargs.\n",
      "                    Please confirm that stop is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "llms_lookup = lookup_chat_models()\n",
    "print(llms_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "\n",
    "llm_gpt4_1106 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "llm_gpt35 = ChatOpenAI(model='gpt-3.5-turbo')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT TOOLS\n",
      "\tAddCollectionFromEPMCTool\n",
      "\tAddAuthorsToCollectionTool\n",
      "\tDescribeCollectionCompositionTool\n",
      "\tDeleteCollectionTool\n",
      "\tRetrieveFullTextTool\n",
      "\tRetrieveFullTextToolForACollection\n",
      "\tMetadataExtraction_EverythingEverywhere_Tool\n",
      "\tMetadataExtraction_MethodsSectionOnly_Tool\n",
      "\tSimpleExtractionWithRAGTool\n",
      "\tPaperQAEmulationTool\n",
      "\tProcotolEntitiesExtractionTool\n",
      "\tCheckExpressionTool\n",
      "\tTitleAbstractClassifier_OneDocAtATime_Tool\n",
      "\tTitleAbstractDiscourseMappingTool\n",
      "\tTitleAbstractExtraction_OneDocAtATime_Tool\n"
     ]
    }
   ],
   "source": [
    "llm = llms_lookup.get('databricks_llama3')\n",
    "\n",
    "cb = AlhazenAgent(llm, llm, db_name=db_name)\n",
    "print('AGENT TOOLS')\n",
    "for t in cb.tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts to Build / Delete the database\n",
    "\n",
    "If you need to restore a deleted database from backup, use the following shell commands:\n",
    "\n",
    "```\n",
    "$ createdb em_tech\n",
    "$ psql -d em_tech -f /local/file/path/em_tech/backup<date_time>.sql\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will delete your existing database (but will also store a copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OK_TO_DELETE:\n",
    "    drop_ceifns_database(db_name, backupFirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will backup your current database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OK_TO_DELETE:\n",
    "    current_date_time = datetime.now()\n",
    "    formatted_date_time = f'{current_date_time:%Y-%m-%d-%H-%M-%S}'\n",
    "    backup_path = loc+'/'+db_name+'/backup'+formatted_date_time+'.sql'\n",
    "    backup_ceifns_database(db_name, backup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will create a new, fresh, empty copy of your database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ceifns_database(db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CEIFNS database from queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a collection of all CryoET papers based on a query\n",
    "\n",
    "This runs a query on European PMC for terms + synonyms related to Cryo Electron Tomography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryoet_query = '''\n",
    "(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n",
    "    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n",
    "    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet)\n",
    "'''\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "addEMPCCollection_tool.run(tool_input={'id': '1', \n",
    "                                       'name': 'CryoET Papers', \n",
    "                                       'query': cryoet_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6228\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "q = ldb.session.query(SKE) \n",
    "output = []        \n",
    "for ske in q.all():\n",
    "    l.append(ske)\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Machine Learning also from a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_query = '''\n",
    "(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n",
    "    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n",
    "    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet ) AND \n",
    "(\"Machine Learning\" OR \"Artificial Intelligence\" OR \"Deep Learning\" OR \"Neural Networks\")\n",
    "'''\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "addEMPCCollection_tool.run(tool_input={'id': '2', \n",
    "                                       'name': 'Machine Learning in CryoET', \n",
    "                                       'query': ml_query, \n",
    "                                       'full_text': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates a new collection of randomly sampled papers to showcase full-text download capability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_new_collection_from_sample('3', 'CryoET Papers Tests', '1', 20, ['ScientificPrimaryResearchArticle', 'ScientificPrimaryResearchPreprint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey + Run Classifications over Papers\n",
    "\n",
    "This invoke the following classification process on the paper (defined in the prompt definition in `./local_resources/prompts/tiab_prompts`):\n",
    "    \n",
    "* A - Structural descriptions of Viral Pathogens (such as HIV, Influenza, SARS-CoV-2, etc.)\n",
    "* B - Studies of mutated protein structures associated with disease (such as Alzheimer's, Parkinson's, etc.) \n",
    "* C - Structural studies of bacterial pathogens (such as E. coli, Salmonella, etc.)\n",
    "* D - Structural studies of plant cells\n",
    "* E - Structural studies of material science of non-biological samples\n",
    "* F - Structural studies of transporters or transport mechanisms within cells, studies involving the cytoskeleton or active transport processes. \n",
    "* G - Structural studies of synapses or other mechansism of releasing vesicles over the plasma membrane\n",
    "* H - Structural studies of any other organelle or structured component of a cell. \n",
    "* I - Studies of dynamic biological processes at a cellular level (such as cell division, cell migration, etc.)\n",
    "* J - Studies of dynamics of molecular interactions within a cell.    \n",
    "* K - Development of new CryoET imaging methods (including grid preparation techniques, such as lift-out). \n",
    "* L - Development of new data analysis methods (including machine learning, segmentation, point-picking, object recognition, or reconstruction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\n",
    "#t.run({'collection_id': '3', 'classification_type':'cryoet_study_types', 'repeat_run':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "if OK_TO_DELETE:        \n",
    "    l = []\n",
    "    q = ldb.session.query(N, SKE) \\\n",
    "            .filter(N.id == NIA.Note_id) \\\n",
    "            .filter(NIA.is_about_id == SKE.id) \\\n",
    "            .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n",
    "\n",
    "    output = []        \n",
    "    print(len(q.all()))\n",
    "    for n, ske in q.all():\n",
    "        ldb.delete_note(n.id)    \n",
    "    print(len(q.all()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs a query over the notes extracted and saved to the database to show the zero-shot document classifications based on the titles + abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "q = ldb.session.query(N, SKE) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n",
    "        .order_by(SKE.id)\n",
    "\n",
    "output = []        \n",
    "for n, ske in q.all():\n",
    "        tup = json.loads(n.content)\n",
    "        tup['prov'] = n.name\n",
    "        tup['doi'] = 'http://doi.org/'+re.sub('doi:', '', ske.id)\n",
    "        tup['year'] = ske.publication_date.year\n",
    "        tup['month'] = ske.publication_date.month\n",
    "        tup['ref'] = ske.content\n",
    "        output.append(tup)\n",
    "df = pd.DataFrame(output).sort_values(['year', 'month'], ascending=[False, False])\n",
    "df.to_csv(loc+'/'+db_name+'/cryoet_study_types.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MetaData Extraction Chain over listed papers\n",
    "\n",
    "Here, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get full text copies of all the papers about CryoET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Get full text copies of all papers in the collection with id=\"3\".'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_paragraphs_extraction_api\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['HURIDOCS_NETWORK'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pdf_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/files/em_tech/ft/10.1002/1873-3468.12757.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m HuridocsPDFLoader(pdf_file_path, host\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHURIDOCS_NETWORK\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload(curi_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[43mdoi\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#url = 'http://172.19.0.5:5051/info'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://pdf_paragraphs_extraction_api:5051/info\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doi' is not defined"
     ]
    }
   ],
   "source": [
    "from alhazen.utils.pdf_research_article_text_extractor import HuridocsPDFLoader\n",
    "import requests\n",
    "pdf_file_path = '/home/jovyan/files/em_tech/ft/10.1002/1873-3468.12757.pdf'\n",
    "loader = HuridocsPDFLoader(pdf_file_path, host=os.environ['HURIDOCS_NETWORK'])\n",
    "docs = loader.load(curi_id='doi:'+doi)\n",
    "\n",
    "#url = 'http://172.19.0.5:5051/info'\n",
    "url = 'http://pdf_paragraphs_extraction_api:5051/info'\n",
    "r = requests.get(url)\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify which papers are in the sampled collection through their dois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKE.id) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKC.id=='2')  \n",
    "dois = [e.id for e in q.all()]\n",
    "dois\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over those dois and extract 15 metadata variables based on the questions shown in `./local_resources/prompt_elements/metadata_extraction.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in cb.tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "\n",
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "df = pd.DataFrame()\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df = pd.concat([df, pd.DataFrame(l)]) \n",
    "     \n",
    "# Iterate over papers to run the metadata extraction tool\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "\n",
    "    # Skip if the doi is already in the database\n",
    "    if len(df)>0 and d in df.doi.unique():\n",
    "        continue\n",
    "\n",
    "    # Run the metadata extraction tool on the doi\n",
    "    t2.run(tool_input={'paper_id': d, 'extraction_type': 'cryoet', 'run_label': 'test'})\n",
    "\n",
    "    # Add the results to the dataframe    \n",
    "    l2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df = pd.concat([df, pd.DataFrame(l2)]) \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_zip_archive_of_full_text_files('2', loc+'/'+db_name+'/full_text_files.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'MetadataExtractionNote') \n",
    "l = []\n",
    "for row in q3.all():\n",
    "    paper = row[0]\n",
    "    name = row[1]\n",
    "#    provenance = json.loads(row[2])\n",
    "    result = json.loads(row[3])\n",
    "    kv = {k:result[k] for k in result}\n",
    "    kv['DOI'] = paper\n",
    "    kv['run'] = name\n",
    "    l.append(kv)\n",
    "# create a dataframe from the list of dictionaries with DOI as the index column\n",
    "if len(l)>0:\n",
    "    df = pd.DataFrame(l).set_index(['DOI', 'run'])\n",
    "else: \n",
    "    df = pd.DataFrame()\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "if OK_TO_DELETE:\n",
    "    for row in q3.all():\n",
    "        d_id = row[0]\n",
    "        e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n",
    "        notes_to_delete = []\n",
    "        for n in ldb.read_notes_about_x(e):\n",
    "            notes_to_delete.append(n.id)\n",
    "        for n in notes_to_delete:\n",
    "            ldb.delete_note(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
