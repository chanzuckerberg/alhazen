{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Translation Tools  \n",
    "\n",
    " > A library permits translation of complex boolean AND/OR queries between online APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.langchain_utils\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage, AIMessage\n",
    "from langchain.schema import BaseMessage, BaseOutputParser, OutputParserException\n",
    "from langchain.schema import LLMResult\n",
    "from langchain.utils.input import print_text\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    List,\n",
    "    Union,\n",
    "    Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseOutputParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/30_general_langchain_utils.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/30_general_langchain_utils.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#| export\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/30_general_langchain_utils.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mJsonEnclosedByTextOutputParser\u001b[39;00m(BaseOutputParser[Any]):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/30_general_langchain_utils.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse the output of an LLM call to a JSON object.\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/30_general_langchain_utils.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseOutputParser' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "class JsonEnclosedByTextOutputParser(BaseOutputParser[Any]):\n",
    "    \"\"\"Parse the output of an LLM call to a JSON object.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> Any:\n",
    "        text = text.strip()\n",
    "        m = re.search('.*([\\[\\{](.|\\n)*[\\}\\]]).*', text, flags=re.M)\n",
    "        if m:\n",
    "            text1 = m.group(1)\n",
    "\n",
    "            # need to make sure all entries in the JSON are quoted\n",
    "            # so that the JSON parser can parse it\n",
    "            # e.g. {\"a\": E} -> {\"a\": \"E\"}\n",
    "            # this is a hack, but it works\n",
    "            text2 = re.sub(r':\\s*([a-zA-Z0-9_]+)\\s*([,\\]\\}])', r': \"\\1\"\\2', text1)\n",
    "            try:\n",
    "                return json.loads(text2)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise OutputParserException(f\"Invalid json output: {text2} derived from {text}\") from e\n",
    "        else: \n",
    "            raise OutputParserException(f\"Could not find json-formatted data in: {text}\")\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"json_enclosed_by_text_output_parser\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class suppress_stdout_stderr(object):\n",
    "    def __enter__(self):\n",
    "        self.outnull_file = open(os.devnull, 'w')\n",
    "        self.errnull_file = open(os.devnull, 'w')\n",
    "\n",
    "        self.old_stdout_fileno_undup    = sys.stdout.fileno()\n",
    "        self.old_stderr_fileno_undup    = sys.stderr.fileno()\n",
    "\n",
    "        self.old_stdout_fileno = os.dup ( sys.stdout.fileno() )\n",
    "        self.old_stderr_fileno = os.dup ( sys.stderr.fileno() )\n",
    "\n",
    "        self.old_stdout = sys.stdout\n",
    "        self.old_stderr = sys.stderr\n",
    "\n",
    "        os.dup2 ( self.outnull_file.fileno(), self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.errnull_file.fileno(), self.old_stderr_fileno_undup )\n",
    "\n",
    "        sys.stdout = self.outnull_file        \n",
    "        sys.stderr = self.errnull_file\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *_):        \n",
    "        sys.stdout = self.old_stdout\n",
    "        sys.stderr = self.old_stderr\n",
    "\n",
    "        os.dup2 ( self.old_stdout_fileno, self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.old_stderr_fileno, self.old_stderr_fileno_undup )\n",
    "\n",
    "        os.close ( self.old_stdout_fileno )\n",
    "        os.close ( self.old_stderr_fileno )\n",
    "\n",
    "        self.outnull_file.close()\n",
    "        self.errnull_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class OllamaRunner:\n",
    "    '''Class to run Ollama in a subprocess and to  \n",
    "     run LLMs or chains locally with a timeout to \n",
    "     prevent long-running processes from hanging \n",
    "     the server.'''\n",
    "    proc = None\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        self.llm = Ollama(model=model)\n",
    "\n",
    "    async def _start_server(self):\n",
    "        if self.proc is not None:\n",
    "            self._terminate_server()\n",
    "        self.proc = await asyncio.create_subprocess_shell(\n",
    "            'ollama serve',\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "    def _terminate_server(self):\n",
    "        \n",
    "        self.proc.terminate()\n",
    "        self.proc = None\n",
    "\n",
    "    def _callback(self, fut: asyncio.Future):\n",
    "        if fut.cancelled() or not fut.done():\n",
    "            print(\"Timed out! - Terminating server\")\n",
    "            fut.cancel()\n",
    "            \n",
    "    async def run_llm(self, prompt, timeout=300):\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(self.llm.agenerate([prompt]))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        if r is not None:\n",
    "            return '\\n'.join([t[0].text for t in r.generations])\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    async def run_chain(self, chain, input, timeout=300):\n",
    "        '''Incorporate the llm into a chain and run it.'''\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(chain.ainvoke(input))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        return r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
