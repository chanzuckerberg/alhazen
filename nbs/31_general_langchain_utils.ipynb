{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Utilities for Parts of the Langchain environment  \n",
    "\n",
    " > A library permits translation of complex boolean AND/OR queries between online APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.langchain_utils\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "\n",
    "import json\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage, AIMessage\n",
    "from langchain.schema import BaseMessage, BaseOutputParser, OutputParserException\n",
    "from langchain.schema import LLMResult\n",
    "from langchain.utils.input import print_text\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    List,\n",
    "    Union,\n",
    "    Optional\n",
    ")\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Union\n",
    "\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "from langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class suppress_stdout_stderr(object):\n",
    "    def __enter__(self):\n",
    "        self.outnull_file = open(os.devnull, 'w')\n",
    "        self.errnull_file = open(os.devnull, 'w')\n",
    "\n",
    "        self.old_stdout_fileno_undup    = sys.stdout.fileno()\n",
    "        self.old_stderr_fileno_undup    = sys.stderr.fileno()\n",
    "\n",
    "        self.old_stdout_fileno = os.dup ( sys.stdout.fileno() )\n",
    "        self.old_stderr_fileno = os.dup ( sys.stderr.fileno() )\n",
    "\n",
    "        self.old_stdout = sys.stdout\n",
    "        self.old_stderr = sys.stderr\n",
    "\n",
    "        os.dup2 ( self.outnull_file.fileno(), self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.errnull_file.fileno(), self.old_stderr_fileno_undup )\n",
    "\n",
    "        sys.stdout = self.outnull_file        \n",
    "        sys.stderr = self.errnull_file\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *_):        \n",
    "        sys.stdout = self.old_stdout\n",
    "        sys.stderr = self.old_stderr\n",
    "\n",
    "        os.dup2 ( self.old_stdout_fileno, self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.old_stderr_fileno, self.old_stderr_fileno_undup )\n",
    "\n",
    "        os.close ( self.old_stdout_fileno )\n",
    "        os.close ( self.old_stderr_fileno )\n",
    "\n",
    "        self.outnull_file.close()\n",
    "        self.errnull_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class OllamaRunner:\n",
    "    '''Class to run Ollama in a subprocess and to  \n",
    "     run LLMs or chains locally with a timeout to \n",
    "     prevent long-running processes from hanging \n",
    "     the server.'''\n",
    "    proc = None\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        self.llm = Ollama(model=model)\n",
    "\n",
    "    async def _start_server(self):\n",
    "        if self.proc is not None:\n",
    "            self._terminate_server()\n",
    "        self.proc = await asyncio.create_subprocess_shell(\n",
    "            'ollama serve',\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "    def _terminate_server(self):\n",
    "        self.proc.terminate()\n",
    "        self.proc = None\n",
    "\n",
    "    def _callback(self, fut: asyncio.Future):\n",
    "        if fut.cancelled() or not fut.done():\n",
    "            print(\"Timed out! - Terminating server\")\n",
    "            fut.cancel()\n",
    "            \n",
    "    async def run_llm(self, prompt, timeout=300):\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(self.llm.agenerate([prompt]))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        if r is not None:\n",
    "            return '\\n'.join([t[0].text for t in r.generations])\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    async def run_chain(self, chain, input, timeout=300):\n",
    "        '''Incorporate the llm into a chain and run it.'''\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(chain.ainvoke(input))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Hack from https://github.com/langchain-ai/langchain/issues/9917#issuecomment-1775620332\n",
    "# to get the chat prompt to submit correctly to the Llama2 model\n",
    "\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage, FunctionMessage, ChatMessage\n",
    "\n",
    "def get_buffer_string_for_llama(\n",
    "    messages: Sequence[BaseMessage], human_prefix: str = \"Human\", ai_prefix: str = \"AI\"\n",
    ") -> str:\n",
    "    \"\"\"Convert sequence of Messages to strings and concatenate them into one string.\n",
    "\n",
    "    Args:\n",
    "        messages: Messages to be converted to strings.\n",
    "        human_prefix: The prefix to prepend to contents of HumanMessages.\n",
    "        ai_prefix: THe prefix to prepend to contents of AIMessages.\n",
    "\n",
    "    Returns:\n",
    "        A single string concatenation of all input messages, \n",
    "        formatted for Llama Chat Models with <<SYS>>, <</SYS>>, [INST], and [/INST] tags.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "            messages = [\n",
    "                HumanMessage(content=\"Hi, how are you?\"),\n",
    "                AIMessage(content=\"Good, how are you?\"),\n",
    "            ]\n",
    "            get_buffer_string(messages)\n",
    "            \"<s><<SYS>>You are a helpful bot<</SYS>>\n",
    "            [INST] Hi, how are you?[/INST]\n",
    "            Good, how are you?\"</s>\n",
    "            <s>[INST]I'm fine, can you tell me who wrote 'Arabian Nights?'[/INST]\n",
    "            Antoine Galland has been for three centuries, credited as being the author of the Arabian Nights.</s>\n",
    "            ...\n",
    "    \"\"\"\n",
    "    string_messages = []\n",
    "    first_block = True \n",
    "    for m in messages:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            if first_block:\n",
    "                b = '[INST]' # assume the first message in the chain includes a system prompt and the remainders do not. \n",
    "                first_block = False \n",
    "            else:\n",
    "                b = '<s>[INST]'\n",
    "            e = '[/INST]'\n",
    "        elif isinstance(m, AIMessage):\n",
    "            b = ''\n",
    "            e = '</s>' # assume the AI message is the last message of each block\n",
    "        elif isinstance(m, SystemMessage):\n",
    "            b = \"<s><<SYS>>\" # assume the systems message is the first message\n",
    "            e = \"<</SYS>>\"\n",
    "        elif isinstance(m, FunctionMessage):\n",
    "            b = ''\n",
    "            e = ''\n",
    "        elif isinstance(m, ChatMessage):\n",
    "            b = ''\n",
    "            e = ''\n",
    "        else:\n",
    "            raise ValueError(f\"Got unsupported message type: {m}\")\n",
    "        message = f\"{b}{m.content}{e}\"\n",
    "        if isinstance(m, AIMessage) and \"function_call\" in m.additional_kwargs:\n",
    "            message += f\"{m.additional_kwargs['function_call']}\"\n",
    "        string_messages.append(message)\n",
    "\n",
    "    return \"\\n\".join(string_messages)\n",
    "\n",
    "def ChatPromptValue_to_string(self) -> str:\n",
    "    \"\"\"Return prompt as string.\"\"\"\n",
    "    return get_buffer_string_for_llama(self.messages)\n",
    "\n",
    "# Put this code in your notebook to get the chat prompt to submit correctly to the Llama2 model\n",
    "# from langchain.prompts.chat import ChatPromptValue\n",
    "#ChatPromptValue.to_string = ChatPromptValue_to_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
