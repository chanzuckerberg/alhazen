{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core functionality that is used over the whole library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export  \n",
    "\n",
    "import asyncio\n",
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from importlib_resources import files\n",
    "\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "from langchain_community.llms.openai import OpenAI\n",
    "from langchain_community.llms.ollama import Ollama \n",
    "#from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "import local_resources.prompts as prompts\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "from time import sleep\n",
    "import torch\n",
    "from typing import List, Tuple, Any, Dict\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4250576984.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    return ChatPromptTemplate.from_messages([\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class PromptTemplateSpec:\n",
    "    \"\"\"A class that provides structure for task instructions (to be converted to LangChain PromptTemplates).\"\"\"\n",
    "\n",
    "    # The name of this instruction template\n",
    "    name: str\n",
    "\n",
    "    # The description of this instruction template\n",
    "    description: str\n",
    "\n",
    "    # System prompts\n",
    "    system: str\n",
    "\n",
    "    # Instruction prompts\n",
    "    instruction: str\n",
    "    \n",
    "    # The input variables that this instruction template requires\n",
    "    input_variables: List[str] = dataclasses.field(default_factory=list)\n",
    "\n",
    "    # The output variables that this instruction template generates\n",
    "    output_variables: List[str] = dataclasses.field(default_factory=list)\n",
    "\n",
    "    def generate_prompt_template(self) -> PromptTemplate:\n",
    "        return PromptTemplate(\n",
    "            input_variables=self.input_variables, \n",
    "            template=self.system + '\\n' +self.instruction\n",
    "        )\n",
    "\n",
    "    def generate_llama2_prompt_template(self) -> PromptTemplate:\n",
    "        return PromptTemplate(\n",
    "            input_variables=self.input_variables, \n",
    "            template='''<s>[INST]\n",
    "                <<SYS>>'''+self.system+'''<</SYS>>\n",
    "                '''+self.instruction+'''\n",
    "                [/INST]</s>'''\n",
    "        )\n",
    "\n",
    "    def generate_chat_prompt_template(self) -> ChatPromptTemplate:\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.system),\n",
    "            (\"human\", self.instruction)])\n",
    "        return template\n",
    "\n",
    "    def generate_simple_instruction(self, input_map) -> str:\n",
    "        pt = self.generate_prompt_template(self)\n",
    "        return pt.format(input_map)\n",
    "\n",
    "class PromptTemplateRegistry:\n",
    "    \"\"\"A class that stores and tracks PromptTemplates for use within a given function.\"\"\"\n",
    "\n",
    "    # The name of this instruction template\n",
    "    registry: Dict[str, PromptTemplateSpec] = {}\n",
    "\n",
    "    def register_new_instruction_template(self, dict:Dict[str, str], override: bool = True):\n",
    "        # check if all required fields are present\n",
    "        assert 'name' in dict, \"name is required\"\n",
    "        assert 'description' in dict, \"description is required\"\n",
    "        assert 'system' in dict, \"system is required\"\n",
    "        assert 'instruction' in dict, \"instruction is required\"\n",
    "        assert 'input_variables' in dict, \"input_variables is required\"\n",
    "\n",
    "        name = dict['name']\n",
    "        description = dict['description']\n",
    "        system = dict['system']\n",
    "        instruction = dict['instruction']\n",
    "        input_variables = dict['input_variables']\n",
    "        output_variables = dict.get('output_variables')\n",
    "\n",
    "        self.register_instruction_template( PromptTemplateSpec(name, description, system, instruction, input_variables), override=override)\n",
    "\n",
    "    def deregister_instruction_template(self, name: str)->PromptTemplateSpec:\n",
    "        instruction_template = self.registry.pop(name)\n",
    "        return instruction_template\n",
    "\n",
    "    def register_instruction_template(self, template: PromptTemplateSpec, override: bool = False):\n",
    "        \"\"\"Register a new conversation template.\"\"\"\n",
    "        if not override:\n",
    "            assert template.name not in self.registry, f\"{template.name} has been registered.\"\n",
    "        self.registry[template.name] = template\n",
    "\n",
    "    def get_prompt_template(self, name: str) -> PromptTemplateSpec:\n",
    "        \"\"\"Get a conversation template.\"\"\"\n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"{name} has not been registered.\")\n",
    "        return self.registry[name]\n",
    "\n",
    "    def load_prompts_from_yaml(self, file_name: str):\n",
    "        prompts_yaml = files(prompts).joinpath(file_name).read_text()\n",
    "        prompts_dict = yaml.safe_load(prompts_yaml)\n",
    "        for pname in prompts_dict:\n",
    "            dct = prompts_dict[pname]\n",
    "            dct['name'] = pname\n",
    "            self.register_new_instruction_template(dct)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        out = \"Registered instruction templates:\\n\"\n",
    "        for name in self.registry:\n",
    "            out += '- %s'%(name) + '\\n'\n",
    "        return out\n",
    "\n",
    "# A global registry for all instruction templates\n",
    "#global instructions\n",
    "#instructions = PromptTemplateRegistry()\n",
    "#instructions.load_prompts_from_yaml('tiab_prompts.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_alhazen_tool_environment():\n",
    "    \"\"\"Set broad variables for Alhazen.\n",
    "    Currently only set default local file path.\"\"\"\n",
    "    if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "        raise Exception('Where are you storing your local literature database?')\n",
    "    loc = os.environ['LOCAL_FILE_PATH']\n",
    "    if loc[-1:] != '/':\n",
    "        loc += '/'\n",
    "\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlookup_chat_models\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mDict\u001b[49m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Utility function to provide access to all available chat models.\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     llm_ollama_mixtral \u001b[38;5;241m=\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixtral:instruct\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "def lookup_chat_models() -> Dict[str, Any]:\n",
    "    \"\"\"Utility function to provide access to all available chat models.\"\"\"\n",
    "\n",
    "    llm_ollama_mixtral = ChatOllama(model='mixtral:instruct') \n",
    "    llm_ollama_llama3 = ChatOllama(model='llama3:70b', stop=[\"<|eot_id|>\"])\n",
    "\n",
    "    chat_models = {\n",
    "        \"ollama_llama3\": llm_ollama_llama3, \n",
    "        \"ollama_mixtral\": llm_ollama_mixtral,\n",
    "    }\n",
    "    if os.environ.get('VERTEXAI_PROJECT_NAME') is not None and len(os.environ.get('VERTEXAI_PROJECT_NAME')) > 0:\n",
    "        llm_gemini10 = ChatVertexAI(model_name=\"gemini-1.0-pro\", convert_system_message_to_human=True)\n",
    "        chat_models['gemini1.0'] = llm_gemini10,\n",
    "\n",
    "    if os.environ.get('DATABRICKS_API_KEY') is not None and len(os.environ.get('DATABRICKS_API_KEY')) > 0:\n",
    "        llm_databricks_dbrx = ChatOpenAI(base_url='https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/serving-endpoints', \n",
    "                        api_key=os.environ['DATABRICKS_API_KEY'], \n",
    "                        model='databricks-dbrx-instruct')\n",
    "        chat_models['databricks_dbrx'] = llm_databricks_dbrx\n",
    "        llm_databricks_mixtral = ChatOpenAI(base_url='https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/serving-endpoints', \n",
    "                        api_key=os.environ['DATABRICKS_API_KEY'], \n",
    "                        model='databricks-mixtral-8x7b-instruct')\n",
    "        chat_models['databricks_mixtral'] = llm_databricks_mixtral\n",
    "        llm_databricks_llama3 = ChatOpenAI(base_url='https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/serving-endpoints', \n",
    "                        api_key=os.environ['DATABRICKS_API_KEY'], \n",
    "                        model='databricks-meta-llama-3-70b-instruct')\n",
    "        chat_models['databricks_llama3'] = llm_databricks_llama3\n",
    "\n",
    "    else:\n",
    "        print('llm_databricks_* chat models are not available. Please set DB_API_KEY environment variable.')\n",
    "\n",
    "    if os.environ.get('GROQ_API_KEY') is not None and len(os.environ.get('GROQ_API_KEY')) > 0:\n",
    "        llm_groq_mixtral = ChatGroq(model_name=\"mixtral-8x7b-32768\") \n",
    "        llm_groq_llama3 = ChatGroq(model_name=\"llama3-70b-8192\", stop=[\"<|eot_id|>\"]) \n",
    "        chat_models['groq_mixtral'] = llm_groq_mixtral\n",
    "        chat_models['groq_llama3'] = llm_groq_llama3\n",
    "    else:\n",
    "        print('groq_* chat models are not available. Please set GROQ_API_KEY environment variable.')\n",
    "\n",
    "    if os.environ.get('OPENAI_API_KEY') is not None and len(os.environ.get('OPENAI_API_KEY')) > 0:\n",
    "        llm_gpt4_1106 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "        llm_gpt35 = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "        chat_models['gpt4_1106'] = llm_gpt4_1106\n",
    "        chat_models['gpt35'] = llm_gpt35\n",
    "    else:\n",
    "        print('llm_openai_* chat models are not available. Please set OPENAI_API_KEY environment variable.')\n",
    "\n",
    "\n",
    "    return chat_models\n",
    "\n",
    "def lookup_embeddings() -> Dict[str, Any]:\n",
    "    \"\"\"Utility function to provide access to all available embedding models.\"\"\"\n",
    "\n",
    "    model_name = \"BAAI/bge-large-en\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        model_kwargs = {\"device\": \"mps\"}\n",
    "    elif torch.cuda.is_available():\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "    else:\n",
    "        model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    hf_bge = HuggingFaceBgeEmbeddings(\n",
    "      model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    embeddings = {        \n",
    "        \"hf_bge\": hf_bge\n",
    "    }\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class suppress_stdout_stderr(object):\n",
    "    def __enter__(self):\n",
    "        self.outnull_file = open(os.devnull, 'w')\n",
    "        self.errnull_file = open(os.devnull, 'w')\n",
    "\n",
    "        self.old_stdout_fileno_undup    = sys.stdout.fileno()\n",
    "        self.old_stderr_fileno_undup    = sys.stderr.fileno()\n",
    "\n",
    "        self.old_stdout_fileno = os.dup ( sys.stdout.fileno() )\n",
    "        self.old_stderr_fileno = os.dup ( sys.stderr.fileno() )\n",
    "\n",
    "        self.old_stdout = sys.stdout\n",
    "        self.old_stderr = sys.stderr\n",
    "\n",
    "        os.dup2 ( self.outnull_file.fileno(), self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.errnull_file.fileno(), self.old_stderr_fileno_undup )\n",
    "\n",
    "        sys.stdout = self.outnull_file        \n",
    "        sys.stderr = self.errnull_file\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *_):        \n",
    "        sys.stdout = self.old_stdout\n",
    "        sys.stderr = self.old_stderr\n",
    "\n",
    "        os.dup2 ( self.old_stdout_fileno, self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.old_stderr_fileno, self.old_stderr_fileno_undup )\n",
    "\n",
    "        os.close ( self.old_stdout_fileno )\n",
    "        os.close ( self.old_stderr_fileno )\n",
    "\n",
    "        self.outnull_file.close()\n",
    "        self.errnull_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class OllamaRunner:\n",
    "    '''Class to run Ollama in a subprocess and to  \n",
    "     run LLMs or chains locally with a timeout to \n",
    "     prevent long-running processes from hanging \n",
    "     the server.'''\n",
    "    proc = None\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        self.llm = Ollama(model=model)\n",
    "\n",
    "    async def _start_server(self):\n",
    "        if self.proc is not None:\n",
    "            self._terminate_server()\n",
    "        self.proc = await asyncio.create_subprocess_shell(\n",
    "            'ollama serve',\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "    def _terminate_server(self):\n",
    "        self.proc.terminate()\n",
    "        self.proc = None\n",
    "\n",
    "    def _callback(self, fut: asyncio.Future):\n",
    "        if fut.cancelled() or not fut.done():\n",
    "            print(\"Timed out! - Terminating server\")\n",
    "            fut.cancel()\n",
    "            \n",
    "    async def run_llm(self, prompt, timeout=300):\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(self.llm.agenerate([prompt]))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        if r is not None:\n",
    "            return '\\n'.join([t[0].text for t in r.generations])\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    async def run_chain(self, chain, input, timeout=300):\n",
    "        '''Incorporate the llm into a chain and run it.'''\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(chain.ainvoke(input))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        return r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
