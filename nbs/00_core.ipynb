{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core functionality that is used over the whole library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export  \n",
    "\n",
    "import asyncio\n",
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from importlib_resources import files\n",
    "\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "from langchain_community.llms.openai import OpenAI\n",
    "from langchain_community.llms.ollama import Ollama \n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "import local_resources.prompts as prompts\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from typing import List, Tuple, Any, Dict\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# LIBRARY-WIDE CONSTANTS AND ENUMS \n",
    "\n",
    "GGUF_LOOKUP_URL = {\n",
    "    \"llama-2-70b-chat\": \"https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q5_K_M.gguf\",\n",
    "    \"llama-2-13b-chat\": \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    \"llama-2-7b-chat\": \"https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    \"mistral-7b-instruct\": \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "}\n",
    "\n",
    "class MODEL_TYPE(Enum):\n",
    "    Ollama = auto()\n",
    "    LlamaCpp = auto()\n",
    "    OpenAI = auto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4250576984.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    return ChatPromptTemplate.from_messages([\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class PromptTemplateSpec:\n",
    "    \"\"\"A class that provides structure for task instructions (to be converted to LangChain PromptTemplates).\"\"\"\n",
    "\n",
    "    # The name of this instruction template\n",
    "    name: str\n",
    "\n",
    "    # The description of this instruction template\n",
    "    description: str\n",
    "\n",
    "    # System prompts\n",
    "    system: str\n",
    "\n",
    "    # Instruction prompts\n",
    "    instruction: str\n",
    "    \n",
    "    # The input variables that this instruction template requires\n",
    "    input_variables: List[str] = dataclasses.field(default_factory=list)\n",
    "\n",
    "    def generate_prompt_template(self) -> PromptTemplate:\n",
    "        return PromptTemplate(\n",
    "            input_variables=self.input_variables, \n",
    "            template=self.system + '\\n' +self.instruction\n",
    "        )\n",
    "\n",
    "    def generate_llama2_prompt_template(self) -> PromptTemplate:\n",
    "        return PromptTemplate(\n",
    "            input_variables=self.input_variables, \n",
    "            template='''<s>[INST]\n",
    "                <<SYS>>'''+self.system+'''<</SYS>>\n",
    "                '''+self.instruction+'''\n",
    "                [/INST]</s>'''\n",
    "        )\n",
    "\n",
    "    def generate_chat_prompt_template(self) -> ChatPromptTemplate:\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.system),\n",
    "            (\"human\", self.instruction)])\n",
    "        return template\n",
    "\n",
    "    def generate_simple_instruction(self, input_map) -> str:\n",
    "        pt = self.generate_prompt_template(self)\n",
    "        return pt.format(input_map)\n",
    "\n",
    "class PromptTemplateRegistry:\n",
    "    \"\"\"A class that stores and tracks PromptTemplates for use within a given function.\"\"\"\n",
    "\n",
    "    # The name of this instruction template\n",
    "    registry: Dict[str, PromptTemplateSpec] = {}\n",
    "\n",
    "    def register_new_instruction_template(self, dict:Dict[str, str], override: bool = True):\n",
    "        # check if all required fields are present\n",
    "        assert 'name' in dict, \"name is required\"\n",
    "        assert 'description' in dict, \"description is required\"\n",
    "        assert 'system' in dict, \"system is required\"\n",
    "        assert 'instruction' in dict, \"instruction is required\"\n",
    "        assert 'input_variables' in dict, \"input_variables is required\"\n",
    "\n",
    "        name = dict['name']\n",
    "        description = dict['description']\n",
    "        system = dict['system']\n",
    "        instruction = dict['instruction']\n",
    "        input_variables = dict['input_variables']\n",
    "\n",
    "        self.register_instruction_template( PromptTemplateSpec(name, description, system, instruction, input_variables), override=override)\n",
    "\n",
    "    def deregister_instruction_template(self, name: str)->PromptTemplateSpec:\n",
    "        instruction_template = self.registry.pop(name)\n",
    "        return instruction_template\n",
    "\n",
    "    def register_instruction_template(self, template: PromptTemplateSpec, override: bool = False):\n",
    "        \"\"\"Register a new conversation template.\"\"\"\n",
    "        if not override:\n",
    "            assert template.name not in self.registry, f\"{template.name} has been registered.\"\n",
    "        self.registry[template.name] = template\n",
    "\n",
    "    def get_prompt_template(self, name: str) -> PromptTemplateSpec:\n",
    "        \"\"\"Get a conversation template.\"\"\"\n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"{name} has not been registered.\")\n",
    "        return self.registry[name]\n",
    "\n",
    "    def load_prompts_from_yaml(self, file_name: str):\n",
    "        prompts_yaml = files(prompts).joinpath(file_name).read_text()\n",
    "        prompts_dict = yaml.safe_load(prompts_yaml)\n",
    "        for pname in prompts_dict:\n",
    "            dct = prompts_dict[pname]\n",
    "            dct['name'] = pname\n",
    "            self.register_new_instruction_template(dct)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        out = \"Registered instruction templates:\\n\"\n",
    "        for name in self.registry:\n",
    "            out += '- %s'%(name) + '\\n'\n",
    "        return out\n",
    "\n",
    "# A global registry for all instruction templates\n",
    "#global instructions\n",
    "#instructions = PromptTemplateRegistry()\n",
    "#instructions.load_prompts_from_yaml('tiab_prompts.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_alhazen_tool_environment():\n",
    "\n",
    "    if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "        raise Exception('Which database do you want to use for this application?')\n",
    "    db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "    if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "        raise Exception('Where are you storing your local literature database?')\n",
    "    loc = os.environ['LOCAL_FILE_PATH']\n",
    "    if loc[-1:] != '/':\n",
    "        loc += '/'\n",
    "    \n",
    "    return loc, db_name\n",
    "\n",
    "\n",
    "# download GGUF files from HuggingFace URL and save it to disk in defined directory, return local file path\n",
    "def get_cached_gguf(gguf_file: str) -> str:\n",
    "    import requests\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    from pathlib import Path\n",
    "\n",
    "    if gguf_file not in GGUF_LOOKUP_URL:\n",
    "        raise ValueError(f\"{gguf_file} has not been registered.\")\n",
    "    \n",
    "    url = GGUF_LOOKUP_URL[gguf_file]\n",
    "    local_dir = os.environ['LLMS_TEMP_DIR']\n",
    "\n",
    "    # create local directory if not exists\n",
    "    Path(local_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # download file\n",
    "    local_filename = url.split('/')[-1]\n",
    "    local_filepath = os.path.join(local_dir, local_filename)\n",
    "    if not os.path.exists(local_filepath):\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(local_filepath, 'wb') as f:\n",
    "            file_size = int(r.headers['content-length'])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Downloading\", total=file_size, unit_scale=True) as pbar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)\n",
    "    \n",
    "    return local_filepath\n",
    "\n",
    "def get_langchain_embeddings(model_type, **kwargs):\n",
    "\n",
    "    if model_type == MODEL_TYPE.LlamaCpp or model_type == MODEL_TYPE.Ollama:\n",
    "        model_name = \"BAAI/bge-large-en\"\n",
    "        model_kwargs = {\"device\": \"mps\"}\n",
    "        encode_kwargs = {\"normalize_embeddings\": True}\n",
    "        llme = HuggingFaceBgeEmbeddings(\n",
    "            model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        return llme\n",
    "    \n",
    "    elif model_type == MODEL_TYPE.OpenAI:    \n",
    "        if os.environ.get('OPENAI_API_KEY') is None:\n",
    "            raise ValueError(f\"OPENAI_API_KEY env. variable not set.\")\n",
    "\n",
    "        openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "        llme = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        \n",
    "        return llme\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model {model_type}\")\n",
    "\n",
    "def get_langchain_llm(model_type, llm_name, **kwargs):\n",
    "\n",
    "    if model_type == MODEL_TYPE.Ollama:\n",
    "\n",
    "        return Ollama(model=llm_name)        \n",
    "\n",
    "    elif model_type == MODEL_TYPE.LlamaCpp:\n",
    "\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        n_gpu_layers = kwargs.get('n_gpu_layers', 1)\n",
    "        temperature = kwargs.get('temperature', 0.1)\n",
    "        n_batch = kwargs.get('n_batch', 512)  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "        model_path = get_cached_gguf(llm_name)\n",
    "        n_ctx = kwargs.get('n_ctx', 4096)\n",
    "\n",
    "        llm = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_ctx=n_ctx,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            temperature=temperature,\n",
    "            n_batch=n_batch,\n",
    "            callback_manager=callback_manager,\n",
    "            f16_kv=True,\n",
    "            verbose=True, # Verbose is required to pass to the callback manager\n",
    "        )        \n",
    "\n",
    "        return llm\n",
    "\n",
    "    elif model_type == MODEL_TYPE.OpenAI:\n",
    "        \n",
    "        if os.environ.get('OPENAI_API_KEY') is None:\n",
    "            raise ValueError(f\"OPENAI_API_KEY env. variable not set.\")\n",
    "\n",
    "        openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "        llm = OpenAI(openai_api_key=openai_api_key, model_name=llm_name)\n",
    "        \n",
    "        return llm \n",
    "\n",
    "\n",
    "def get_langchain_chatmodel(model_type, llm_name, **kwargs):\n",
    "\n",
    "    if model_type == MODEL_TYPE.Ollama:\n",
    "\n",
    "        return ChatOllama(model=llm_name)        \n",
    "\n",
    "    elif model_type == MODEL_TYPE.LlamaCpp:\n",
    "\n",
    "        raise Exception(\"Can't run Chat Models with LLamaCPP\")\n",
    "\n",
    "    elif model_type == MODEL_TYPE.OpenAI:\n",
    "        \n",
    "        if os.environ.get('OPENAI_API_KEY') is None:\n",
    "            raise ValueError(f\"OPENAI_API_KEY env. variable not set.\")\n",
    "\n",
    "        openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "        llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=llm_name)\n",
    "        \n",
    "        return llm \n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(f\"Unknown model {model_type}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class suppress_stdout_stderr(object):\n",
    "    def __enter__(self):\n",
    "        self.outnull_file = open(os.devnull, 'w')\n",
    "        self.errnull_file = open(os.devnull, 'w')\n",
    "\n",
    "        self.old_stdout_fileno_undup    = sys.stdout.fileno()\n",
    "        self.old_stderr_fileno_undup    = sys.stderr.fileno()\n",
    "\n",
    "        self.old_stdout_fileno = os.dup ( sys.stdout.fileno() )\n",
    "        self.old_stderr_fileno = os.dup ( sys.stderr.fileno() )\n",
    "\n",
    "        self.old_stdout = sys.stdout\n",
    "        self.old_stderr = sys.stderr\n",
    "\n",
    "        os.dup2 ( self.outnull_file.fileno(), self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.errnull_file.fileno(), self.old_stderr_fileno_undup )\n",
    "\n",
    "        sys.stdout = self.outnull_file        \n",
    "        sys.stderr = self.errnull_file\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *_):        \n",
    "        sys.stdout = self.old_stdout\n",
    "        sys.stderr = self.old_stderr\n",
    "\n",
    "        os.dup2 ( self.old_stdout_fileno, self.old_stdout_fileno_undup )\n",
    "        os.dup2 ( self.old_stderr_fileno, self.old_stderr_fileno_undup )\n",
    "\n",
    "        os.close ( self.old_stdout_fileno )\n",
    "        os.close ( self.old_stderr_fileno )\n",
    "\n",
    "        self.outnull_file.close()\n",
    "        self.errnull_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class OllamaRunner:\n",
    "    '''Class to run Ollama in a subprocess and to  \n",
    "     run LLMs or chains locally with a timeout to \n",
    "     prevent long-running processes from hanging \n",
    "     the server.'''\n",
    "    proc = None\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        self.llm = Ollama(model=model)\n",
    "\n",
    "    async def _start_server(self):\n",
    "        if self.proc is not None:\n",
    "            self._terminate_server()\n",
    "        self.proc = await asyncio.create_subprocess_shell(\n",
    "            'ollama serve',\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "    def _terminate_server(self):\n",
    "        self.proc.terminate()\n",
    "        self.proc = None\n",
    "\n",
    "    def _callback(self, fut: asyncio.Future):\n",
    "        if fut.cancelled() or not fut.done():\n",
    "            print(\"Timed out! - Terminating server\")\n",
    "            fut.cancel()\n",
    "            \n",
    "    async def run_llm(self, prompt, timeout=300):\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(self.llm.agenerate([prompt]))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        if r is not None:\n",
    "            return '\\n'.join([t[0].text for t in r.generations])\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    async def run_chain(self, chain, input, timeout=300):\n",
    "        '''Incorporate the llm into a chain and run it.'''\n",
    "        # if server is not running, start it\n",
    "        if self.proc is None:\n",
    "            await self._start_server()\n",
    "        # create task\n",
    "        task = asyncio.create_task(chain.ainvoke(input))\n",
    "        task.add_done_callback(self._callback)\n",
    "        # try to await the task\n",
    "        try:\n",
    "            r = await asyncio.wait_for(task, timeout=timeout)\n",
    "        except asyncio.TimeoutError as ex:\n",
    "            print(ex)\n",
    "        return r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
