{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core functionality that is used over the whole library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export  \n",
    "\n",
    "# Support LangChain and lllama-index for OpenAI and LlamaCpp\n",
    "from langchain.llms import LlamaCpp as LlamaCppLangChain, OpenAI as OpenAILangChain\n",
    "from langchain.embeddings import LlamaCppEmbeddings, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from llama_index.llms import LlamaCPP as LlamaCppLlamaIndex, OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from importlib_resources import files\n",
    "import local_resources.prompts as prompts\n",
    "import yaml \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# LIBRARY-WIDE CONSTANTS AND ENUMS \n",
    "\n",
    "GGUF_LOOKUP_URL = {\n",
    "    \"llama-2-70b-chat\": \"https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q5_K_M.gguf\",\n",
    "    \"llama-2-13b-chat\": \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    \"llama-2-7b-chat\": \"https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    \"mistral-7b-instruct\": \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "}\n",
    "\n",
    "class MODEL_TYPE(Enum):\n",
    "    LlamaCpp = auto()\n",
    "    OpenAI = auto()\n",
    "\n",
    "class AGENT_TYPE(Enum):\n",
    "    LangChain = auto()\n",
    "    LlamaIndex = auto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TaskInstruction:\n",
    "    \"\"\"A class that provides structure for task instructions (to be converted to LangChain PromptTemplates).\"\"\"\n",
    "\n",
    "    # The name of this instruction template\n",
    "    name: str\n",
    "\n",
    "    # The description of this instruction template\n",
    "    description: str\n",
    "\n",
    "    # System prompts\n",
    "    system: str\n",
    "\n",
    "    # Instruction prompts\n",
    "    instruction: str\n",
    "    \n",
    "    # The input variables that this instruction template requires\n",
    "    input_variables: List[str] = dataclasses.field(default_factory=list)\n",
    "\n",
    "    def generate_prompt_template(self) -> PromptTemplate:\n",
    "        return PromptTemplate(\n",
    "            input_variables=self.input_variables, \n",
    "            template=self.system + '\\n' +self.instruction\n",
    "        )\n",
    "\n",
    "    def generate_llama2_prompt_template(self) -> PromptTemplate:\n",
    "        return PromptTemplate(\n",
    "            input_variables=self.input_variables, \n",
    "            template='''<s>[INST]\n",
    "                <<SYS>>'''+self.system+'''<</SYS>>\n",
    "                '''+self.instruction+'''\n",
    "                [/INST]</s>'''\n",
    "        )\n",
    "\n",
    "    def generate_simple_instruction(self, input_map) -> str:\n",
    "        pt = self.generate_prompt_template(self)\n",
    "        return pt.format(input_map)\n",
    "\n",
    "class TaskInstructionRegistry:\n",
    "    \"\"\"A class that stores and tracks PromptTemplates for use within a given function.\"\"\"\n",
    "\n",
    "    # The name of this instruction template\n",
    "    registry: Dict[str, TaskInstruction] = {}\n",
    "\n",
    "    def register_new_instruction_template(self, dict:Dict[str, str], override: bool = True):\n",
    "        # check if all required fields are present\n",
    "        assert 'name' in dict, \"name is required\"\n",
    "        assert 'description' in dict, \"description is required\"\n",
    "        assert 'system' in dict, \"system is required\"\n",
    "        assert 'instruction' in dict, \"instruction is required\"\n",
    "        assert 'input_variables' in dict, \"input_variables is required\"\n",
    "\n",
    "        name = dict['name']\n",
    "        description = dict['description']\n",
    "        system = dict['system']\n",
    "        instruction = dict['instruction']\n",
    "        input_variables = dict['input_variables']\n",
    "\n",
    "        self.register_instruction_template( TaskInstruction(name, description, system, instruction, input_variables), override=override)\n",
    "\n",
    "    def deregister_instruction_template(self, name: str)->TaskInstruction:\n",
    "        instruction_template = self.registry.pop(name)\n",
    "        return instruction_template\n",
    "\n",
    "    def register_instruction_template(self, template: TaskInstruction, override: bool = False):\n",
    "        \"\"\"Register a new conversation template.\"\"\"\n",
    "        if not override:\n",
    "            assert template.name not in self.registry, f\"{template.name} has been registered.\"\n",
    "        self.registry[template.name] = template\n",
    "\n",
    "    def get_instruction_template(self, name: str) -> TaskInstruction:\n",
    "        \"\"\"Get a conversation template.\"\"\"\n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"{name} has not been registered.\")\n",
    "        return self.registry[name]\n",
    "\n",
    "    def load_prompts_from_yaml(self, file_name: str):\n",
    "        prompts_yaml = files(prompts).joinpath(file_name).read_text()\n",
    "        prompts_dict = yaml.safe_load(prompts_yaml)\n",
    "        for pname in prompts_dict:\n",
    "            dct = prompts_dict[pname]\n",
    "            dct['name'] = pname\n",
    "            self.register_new_instruction_template(dct)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        out = \"Registered instruction templates:\\n\"\n",
    "        for name in self.registry:\n",
    "            out += '- %s'%(name) + '\\n'\n",
    "        return out\n",
    "\n",
    "# A global registry for all instruction templates\n",
    "#global instructions\n",
    "#instructions = TaskInstructionRegistry()\n",
    "#instructions.load_prompts_from_yaml('tiab_prompts.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# download GGUF files from HuggingFace URL and save it to disk in defined directory, return local file path\n",
    "def get_cached_gguf(gguf_file: str) -> str:\n",
    "    import requests\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    from pathlib import Path\n",
    "\n",
    "    if gguf_file not in GGUF_LOOKUP_URL:\n",
    "        raise ValueError(f\"{gguf_file} has not been registered.\")\n",
    "    \n",
    "    url = GGUF_LOOKUP_URL[gguf_file]\n",
    "    local_dir = os.environ['LLMS_TEMP_DIR']\n",
    "\n",
    "    # create local directory if not exists\n",
    "    Path(local_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # download file\n",
    "    local_filename = url.split('/')[-1]\n",
    "    local_filepath = os.path.join(local_dir, local_filename)\n",
    "    if not os.path.exists(local_filepath):\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(local_filepath, 'wb') as f:\n",
    "            file_size = int(r.headers['content-length'])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Downloading\", total=file_size, unit_scale=True) as pbar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)\n",
    "    \n",
    "    return local_filepath\n",
    "\n",
    "def get_langchain_embeddings(llm_name, **kwargs):\n",
    "\n",
    "    model_type = None\n",
    "    if llm_name == 'gpt-4' :\n",
    "        model_type = MODEL_TYPE.OpenAI\n",
    "    elif GGUF_LOOKUP_URL.get(llm_name) is not None:\n",
    "        model_type = MODEL_TYPE.LlamaCpp\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM {llm_name}\")\n",
    "\n",
    "    if model_type == MODEL_TYPE.LlamaCpp:\n",
    "\n",
    "        n_gpu_layers = kwargs.get('n_gpu_layers', 1)\n",
    "        n_batch = kwargs.get('n_batch', 512)  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "        model_path = get_cached_gguf(llm_name)\n",
    "        n_ctx = kwargs.get('n_ctx', 4096)\n",
    "\n",
    "        llme = LlamaCppEmbeddings(\n",
    "            model_path=model_path,\n",
    "            n_ctx=n_ctx,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_batch=n_batch,\n",
    "            f16_kv=True,\n",
    "            verbose=True, # Verbose is required to pass to the callback manager\n",
    "        )        \n",
    "\n",
    "        return llme\n",
    "    \n",
    "    elif model_type == MODEL_TYPE.OpenAI:\n",
    "        \n",
    "        if os.environ.get('OPENAI_API_KEY') is None:\n",
    "            raise ValueError(f\"OPENAI_API_KEY env. variable not set.\")\n",
    "\n",
    "        openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "        llme = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        \n",
    "        return llme\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(f\"Unknown model {model_type}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_langchain_llm(llm_name, **kwargs):\n",
    "\n",
    "    model_type = None\n",
    "    if llm_name == 'gpt-4' :\n",
    "        model_type = MODEL_TYPE.OpenAI\n",
    "    elif GGUF_LOOKUP_URL.get(llm_name) is not None:\n",
    "        model_type = MODEL_TYPE.LlamaCpp\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM {llm_name}\")\n",
    "\n",
    "    if model_type == MODEL_TYPE.LlamaCpp:\n",
    "\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        n_gpu_layers = kwargs.get('n_gpu_layers', 1)\n",
    "        temperature = kwargs.get('temperature', 0.1)\n",
    "        n_batch = kwargs.get('n_batch', 512)  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "        model_path = get_cached_gguf(llm_name)\n",
    "        n_ctx = kwargs.get('n_ctx', 4096)\n",
    "\n",
    "        llm = LlamaCppLangChain(\n",
    "            model_path=model_path,\n",
    "            n_ctx=n_ctx,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            temperature=temperature,\n",
    "            n_batch=n_batch,\n",
    "            callback_manager=callback_manager,\n",
    "            f16_kv=True,\n",
    "            verbose=True, # Verbose is required to pass to the callback manager\n",
    "        )        \n",
    "\n",
    "        return llm\n",
    "\n",
    "    elif model_type == MODEL_TYPE.OpenAI:\n",
    "        \n",
    "        if os.environ.get('OPENAI_API_KEY') is None:\n",
    "            raise ValueError(f\"OPENAI_API_KEY env. variable not set.\")\n",
    "\n",
    "        openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "        llm = OpenAILangChain(openai_api_key=openai_api_key)\n",
    "        \n",
    "        return llm \n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(f\"Unknown model {model_type}\")\n",
    "    \n",
    "def get_llamaindex_llm(llm_name, **kwargs):\n",
    "\n",
    "    model_type = None\n",
    "    if llm_name == 'gpt-4':\n",
    "        model_type = MODEL_TYPE.OpenAI\n",
    "    elif GGUF_LOOKUP_URL.get(llm_name) is not None:\n",
    "        model_type = MODEL_TYPE.LlamaCpp\n",
    "\n",
    "    temperature = kwargs.get('temperature', 0.1)\n",
    "    n_batch = kwargs.get('n_batch', 512)  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "    if model_type == MODEL_TYPE.LlamaCpp:\n",
    "\n",
    "        model_path = get_cached_gguf(llm_name)\n",
    "        n_ctx = kwargs.get('n_ctx', 4096)\n",
    "\n",
    "        llm = LlamaCppLlamaIndex( \n",
    "            model_path=model_path,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=n_batch,\n",
    "            context_window=n_ctx,\n",
    "            generate_kwargs={},\n",
    "            model_kwargs={\"n_gpu_layers\": 1},\n",
    "            messages_to_prompt=messages_to_prompt,\n",
    "            completion_to_prompt=completion_to_prompt,\n",
    "            verbose=True)\n",
    "\n",
    "        return llm\n",
    "\n",
    "    elif model_type == MODEL_TYPE.OpenAI:\n",
    "        \n",
    "        if os.environ.get('OPENAI_API_KEY') is None:\n",
    "            raise ValueError(f\"OPENAI_API_KEY env. variable not set.\")\n",
    "\n",
    "        openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "        llm = OpenAILlamaIndex(model=\"gpt-4\", temperature=temperature, max_tokens=n_batch)\n",
    "        \n",
    "        return llm \n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(f\"Unknown model {model_type}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
