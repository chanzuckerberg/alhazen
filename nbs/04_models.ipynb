{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> LangChain LLMS for use by the Alhazen Research Assistant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These include the following models:\n",
    "\n",
    "* Lllama.cpp\n",
    "* Ollama\n",
    "* HuggingFace Pipelines\n",
    "* OpenAI GPT-3.5\n",
    "* OpenAI GPT-4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from langchain.llms import LlamaCpp, OpenAI\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.utilities import SQLDatabase\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# download GGUF files from HuggingFace URL and save it to disk in defined directory, return local file path\n",
    "def get_cached_gguf(url, local_dir):\n",
    "    import requests\n",
    "    import os\n",
    "    import shutil\n",
    "    from tqdm import tqdm\n",
    "    from pathlib import Path\n",
    "\n",
    "    # create local directory if not exists\n",
    "    Path(local_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # download file\n",
    "    local_filename = url.split('/')[-1]\n",
    "    local_filepath = os.path.join(local_dir, local_filename)\n",
    "    if not os.path.exists(local_filepath):\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(local_filepath, 'wb') as f:\n",
    "            file_size = int(r.headers['content-length'])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Downloading\", total=file_size, unit_scale=True) as pbar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)\n",
    "    return local_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def instantiate_llms(langchain_model, **kwargs):\n",
    "\n",
    "    model = None\n",
    "    if langchain_model == \"LlamaCpp\":\n",
    "\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        n_gpu_layers = kwargs.get('n_gpu_layers', 1)\n",
    "        n_batch = kwargs.get('n_batch', 512)  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "        url = kwargs.get('gguf_url', 'https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q5_K_M.gguf')\n",
    "        local_dir = os.environ.get('model_dir', None)\n",
    "        model_path = get_cached_gguf(url, local_dir)\n",
    "        n_ctx = kwargs.get('n_ctx', 4096)\n",
    "\n",
    "        model = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_ctx=n_ctx,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_batch=n_batch,\n",
    "            callback_manager=callback_manager,\n",
    "            f16_kv=True,\n",
    "            verbose=True, # Verbose is required to pass to the callback manager\n",
    "        )        \n",
    "\n",
    "    elif langchain_model == \"GPT3.5\":\n",
    "\n",
    "        model = OpenAI(openai_api_key=os.environ['OPEN_API_KEY'])\n",
    " \n",
    "    else:\n",
    "\n",
    "        raise ValueError(f\"Unknown model {langchain_model}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
