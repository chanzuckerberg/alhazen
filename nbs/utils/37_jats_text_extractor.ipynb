{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JATS Text Extractor Utility\n",
    "\n",
    "> Extracts structured text from JATS XML Files with offset annotations for sections, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.jats_text_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from collections import namedtuple\n",
    "from lxml import etree\n",
    "from lxml.etree import ElementTree\n",
    "from nxml2txt import rewritetex\n",
    "from nxml2txt import rewritemmla\n",
    "from nxml2txt import respace\n",
    "from nxml2txt import rewriteu2a\n",
    "from nxml2txt import standoff\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from urllib.request import urlopen\n",
    "from requests.utils import requote_uri\n",
    "\n",
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import csv\n",
    "import html\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from databricks import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_ft_url_from_doi(doi, file_path):\n",
    "    \n",
    "    if os.environ.get('DB_TOKEN') is None:  \n",
    "        msg = 'Error attempting to query Databricks for URL data, did you set the DB_TOKEN environment variable?'\n",
    "        raise Exception(msg)\n",
    "\n",
    "    get_ft_url_from_doi_sql = '''\n",
    "        SELECT DISTINCT p.pmc_id, p.doi, YEAR(p.publication_date) as year, p.title, p.abstract, p.full_text_format, p.full_text_url, a.last_name\n",
    "        FROM scipubstore.ingestion.papers as p \n",
    "            JOIN scipubstore.ingestion.authors as a on (p.paper_id=a.paper_id) \n",
    "        WHERE p.doi = '{}' and a.author_index=1\n",
    "        ORDER BY p.full_text_url DESC\n",
    "    '''.format(doi)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with sql.connect(server_hostname = 'czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com',\n",
    "                        http_path = '/sql/1.0/warehouses/1c4df94f2f1a6305',\n",
    "                        access_token = os.getenv(\"DB_TOKEN\")) as connection:\n",
    "\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(get_ft_url_from_doi_sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "    df = pd.DataFrame([row.asDict() for row in result])\n",
    "    if df.shape[0] == 0:\n",
    "        return('No paper found with that DOI')\n",
    "    \n",
    "    title = df['title'].values[0]\n",
    "    first_author = df['last_name'].values[0]\n",
    "    year = df['year'].values[0]  \n",
    "    url = df['full_text_url'].values[0]\n",
    "    xml = requests.get(url).text\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'namedtuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#| export\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m TexOptions \u001b[39m=\u001b[39m namedtuple(\u001b[39m'\u001b[39m\u001b[39mTexOptions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m U2aOptions \u001b[39m=\u001b[39m namedtuple(\u001b[39m'\u001b[39m\u001b[39mU2aOptions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhex keep_missing stdout directory overwrite\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m@dataclasses\u001b[39m\u001b[39m.\u001b[39mdataclass\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gburns/Documents/Coding/ChatGPT_etc/alzhazen/nbs/12_jats_text_extractor.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNxmlDoc\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'namedtuple' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "TexOptions = namedtuple('TexOptions', 'verbose')\n",
    "U2aOptions = namedtuple('U2aOptions', 'hex keep_missing stdout directory overwrite')\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class NxmlDoc:\n",
    "    \"\"\"A class that provides structure for full text papers specified under the JATS 'nxml' format.\"\"\"\n",
    "\n",
    "    # The XML content of the file\n",
    "    xml: str\n",
    "\n",
    "    # The identifier of the paper\n",
    "    ft_id: str\n",
    "\n",
    "    # Plain text of the paper's contents\n",
    "    text: str\n",
    "\n",
    "    # Standoff annotations superimposed over the  prompts\n",
    "    standoffs: str\n",
    "      \n",
    "    def __init__(self, ft_id, xml):\n",
    "        self.ft_id = ft_id\n",
    "\n",
    "        # HTML entities kill the XML parse\n",
    "        # but any '<' characters must be replaced with &lt; in XML (and '& with &amp;)\n",
    "        xml = xml.replace('<', '__less_than__')\n",
    "        xml = html.unescape(xml)\n",
    "        xml = xml.replace('&', '&amp;')\n",
    "        xml = xml.replace('<', '&lt;')\n",
    "        xml = xml.replace('__less_than__', '<')\n",
    "        xml = xml.encode('utf-8')\n",
    "        self.xml = xml\n",
    "        \n",
    "        tree = ElementTree( etree.fromstring(xml) )\n",
    "        tex_options = TexOptions(verbose=True)\n",
    "        rewritetex.process_tree(tree, options=tex_options)\n",
    "        \n",
    "        # process MathML annotations\n",
    "        rewritemmla.process_tree(tree)\n",
    "        \n",
    "        # normalize whitespace\n",
    "        #respace.process_tree(tree)\n",
    "        \n",
    "        # map unicodoffs = nxml2txt(nxmlfne to ASCII)\n",
    "        u2a_options = U2aOptions(keep_missing=True, hex=False, stdout=False, directory=None, overwrite=False)\n",
    "        rewriteu2a.process_tree(tree, options=u2a_options)\n",
    "        \n",
    "        # convert to text and standoffs\n",
    "        text, standoffs = standoff.convert_tree(tree)   \n",
    "        self.text = text\n",
    "        self.standoffs = standoffs\n",
    "\n",
    "        self.to_exclude = ['table-wrap-foot']\n",
    "        self.text_tag_types = ['front/article-title', 'front/abstract', \n",
    "                               'body/p', 'body/title', 'body/label',\n",
    "                               'back/p', 'back/title']\n",
    "        self.section_tag_types = ['front/article-title', 'front/abstract', \n",
    "                               'body/sec']\n",
    "\n",
    "        self.tag_types = {'text':['article-title', 'abstract', 'p', 'title', 'label', 'caption'],\n",
    "                'structure':['front', 'body', 'back', 'ref-list', 'sec', 'fig', 'supplementary-material'],\n",
    "                'xref': ['xref', 'ref', 'label', 'name', 'surname', 'year', 'pub-id', 'fpage']}\n",
    "\n",
    "    def get_figure_reference(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        for t in hits:\n",
    "          if t.element.tag == 'fig':      \n",
    "            return t.element.get('id','')\n",
    "        return ''\n",
    "\n",
    "    def get_sec_tree(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        sec_tree = ''\n",
    "        for t in hits:\n",
    "            if t.element.tag == 'sec':      \n",
    "                if len(sec_tree) > 0:\n",
    "                    sec_tree += ' >> ' \n",
    "                if t.element.find('title') is not None and \\\n",
    "                        t.element.find('title').text is not None :\n",
    "                    sec_tree += t.element.find('title').text\n",
    "                else:\n",
    "                    sec_tree += ' ??? ' \n",
    "        return sec_tree\n",
    "    \n",
    "    def get_sec_tag(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        sec = None\n",
    "        for t in hits:\n",
    "          if t.element.tag == 'sec':      \n",
    "            sec = t\n",
    "        if sec is None:\n",
    "            return ''\n",
    "        elif sec.element.find('title') is not None:\n",
    "            return sec.element.find('title').text\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def get_top_level_sec_tag(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        for t in hits:\n",
    "          if t.element.tag == 'sec':      \n",
    "            if t.element.get('sec-type', None):\n",
    "              return t.element.get('sec-type')\n",
    "            elif t.element.find('title') is not None:\n",
    "              return t.element.find('title').text\n",
    "        return ''\n",
    "\n",
    "    def generate_tag_tree(self, t):\n",
    "        pos = t.start\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if pos>=s.start and pos<s.end and s!=t: \n",
    "            hits.append(s)\n",
    "        #tag_tree = '|'.join(['%s[%s...]'%(t.element.tag,self.text[t.start:t.start+8]) if t.element.tag=='sec' else t.element.tag for t in hits])\n",
    "        tag_tree = '|'.join([t.element.tag for t in hits])\n",
    "        tag_tree = tag_tree+'.'+t.element.tag\n",
    "        return tag_tree\n",
    "    \n",
    "    def list_section_titles(self):\n",
    "        secs = [t for t in self.standoffs if t.element.tag=='sec']\n",
    "        titles = [t.element.find('title').text.strip() for t in secs]\n",
    "        levels = [self.generate_tag_tree(t).count('|') for t in secs]\n",
    "        return (titles, levels)\n",
    "    \n",
    "    def search_section_titles(self, query):\n",
    "        standoffs = []\n",
    "        for s in self.standoffs:\n",
    "            if s.element.tag=='sec':\n",
    "                title_tag = s.element.find('title')\n",
    "                if (title_tag and title_tag.text and query in title_tag.text.lower()) or \\\n",
    "                        query in s.element.get('sec-type', ''):\n",
    "                    standoffs.append(s)\n",
    "        return standoffs\n",
    "\n",
    "    def read_section_text(self, t):\n",
    "        hits = []\n",
    "        for s in sorted(self.standoffs, key=lambda x: x.start):\n",
    "          if s.start>t.start and s.end<t.end and s!=t and (s.element.tag=='p' or s.element.tag=='title'): \n",
    "            hits.append(s)\n",
    "        return '\\n'.join([re.sub('\\s+', ' ', self.text[t.start:t.end]) for t in hits])\n",
    "\n",
    "    def build_simple_document_dataframe(self):\n",
    "\n",
    "        text_tuples = []\n",
    "\n",
    "        try:\n",
    "\n",
    "            # two stage process - build a lookup list of all relevant tags \n",
    "            # - then use the tags start/end properties to identify text portions of the paper and render those.\n",
    "\n",
    "            this_doc_standoffs = {t:[] for tt in self.tag_types.keys() for t in self.tag_types[tt]}\n",
    "            \n",
    "            all_xrefs = []\n",
    "            for s in self.standoffs:\n",
    "                if this_doc_standoffs.get(s.element.tag) is not None:\n",
    "                    this_doc_standoffs.get(s.element.tag).append(s)\n",
    "                if s.element.tag == 'xref':\n",
    "                    all_xrefs.append(s)\n",
    "            #\n",
    "            # skip the whole file if there's no body tag.\n",
    "            #\n",
    "            if len(this_doc_standoffs.get('body')) == 0:\n",
    "                return None\n",
    "            \n",
    "            text_so_list = []\n",
    "            for ttt in self.text_tag_types:\n",
    "                part,tag = ttt.split('/')\n",
    "                if len(this_doc_standoffs.get(part)) == 0:\n",
    "                    continue\n",
    "                part_so = this_doc_standoffs.get(part)[0]\n",
    "                for so in this_doc_standoffs.get(tag):\n",
    "                    if so.start < part_so.start or so.end > part_so.end:\n",
    "                        continue\n",
    "                    text_so_list.append(so)\n",
    "                    #print((row.PMID, local_id, so.element.tag, query_document_standoffs(so, text, standoffs), so.start, (so.end-so.start), text[so.start:so.end]))\n",
    "              \n",
    "            # Manipulate standoff annotations so that titles and labels fall naturally in the text \n",
    "            # and paragraph tags that hold other paragraphs (as is the case with pmid:26791617) don't trigger repeating text.\n",
    "            # Make sure the SOs only tile the document and do not overlap.\n",
    "            text_so_list = sorted(text_so_list, key=lambda x: x.start)\n",
    "            last_so = None\n",
    "            for so in text_so_list:\n",
    "                if last_so:\n",
    "                    if last_so.end > so.start:\n",
    "                        last_so.end = so.start-1\n",
    "                last_so = so\n",
    "              \n",
    "            sent_id = 0\n",
    "            for local_id, so in enumerate(text_so_list):\n",
    "                sec_tree = self.get_sec_tree(so)\n",
    "                sec_title = self.get_sec_tag(so) \n",
    "                top_sec_title = self.get_top_level_sec_tag(so) \n",
    "                figure_reference = self.get_figure_reference(so) \n",
    "                so_text = self.text[so.start:so.end]\n",
    "                if( so.element.tag == 'article-title' or so.element.tag == 'abstract' ):\n",
    "                    sec_title = 'TIAB'\n",
    "                    top_sec_title = 'TIAB'         \n",
    "                    sec_tree = 'TIAB'                        \n",
    "                tuple = (self.ft_id, local_id, so.element.tag, top_sec_title, sec_tree, sec_title, so.start, (so.end-so.start), figure_reference, so_text)\n",
    "                text_tuples.append(tuple)\n",
    "              \n",
    "        except etree.XMLSyntaxError as xmlErr:\n",
    "            print(\"XML Syntax Error: {0}\".format(xmlErr))\n",
    "        except UnicodeDecodeError as unicodeErr:\n",
    "            print(\"Unicode parsing Error: {0}\".format(unicodeErr))\n",
    "        #except TypeError as typeErr:\n",
    "        #  print(\"Type Error: {0}\".format(typeErr))  \n",
    "        #    print(\"ValueError: {0}\".format(valErr))\n",
    "        #    return None\n",
    "\n",
    "        text_df = pd.DataFrame(text_tuples, columns=['PMID', 'PARAGRAPH_ID', 'TAG', 'TOP_SECTION', 'SECTION_TREE', 'SECTION', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'])\n",
    "        return text_df\n",
    "\n",
    "    def build_enhanced_document_dataframe(self):\n",
    "        '''This method processes the JATS file and returns a dataframe with the following columns:\n",
    "        PMID: the paper's identifier\n",
    "        PARAGRAPH_ID: the paragraph's identifier\n",
    "        TAG: the tag type\n",
    "        TAG_TREE: the tag tree\n",
    "        OFFSET: the offset of the text in the overall document \n",
    "                (note that these values are derived from the text generated by \n",
    "                 the nxml2txt package - which differs from the text we are generating \n",
    "                 here).\n",
    "        LENGTH: the length of the text\n",
    "        FIG_REF: Whether the text is a reference to a figure (or table)\n",
    "        PLAIN_TEXT: the text itself\n",
    "        '''\n",
    "\n",
    "        text_tuples = []\n",
    "\n",
    "        try:\n",
    "\n",
    "            # two stage process - build a lookup list of all relevant tags \n",
    "            # - then use the tags start/end properties to identify text portions of the paper and render those.\n",
    "\n",
    "            this_doc_standoffs = {t:[] for tt in self.tag_types.keys() for t in self.tag_types[tt]}\n",
    "            \n",
    "            all_xrefs = []\n",
    "            for s in self.standoffs:\n",
    "                if this_doc_standoffs.get(s.element.tag) is not None:\n",
    "                    this_doc_standoffs.get(s.element.tag).append(s)\n",
    "                if s.element.tag == 'xref':\n",
    "                    all_xrefs.append(s)\n",
    "            #\n",
    "            # skip the whole file if there's no body tag.\n",
    "            #\n",
    "            if len(this_doc_standoffs.get('body')) == 0:\n",
    "                return None\n",
    "            \n",
    "            ref_dict = self.extract_ref_dict_from_nxml()\n",
    "\n",
    "            text_so_list = []\n",
    "            for ttt in self.text_tag_types:\n",
    "                part,tag = ttt.split('/')\n",
    "                part_so = this_doc_standoffs.get(part)[0]\n",
    "                for so in this_doc_standoffs.get(tag):\n",
    "                    if so.start < part_so.start or so.end > part_so.end:\n",
    "                        continue\n",
    "                    text_so_list.append(so)\n",
    "                    #print((row.PMID, local_id, so.element.tag, query_document_standoffs(so, text, standoffs), so.start, (so.end-so.start), text[so.start:so.end]))\n",
    "              \n",
    "            # Manipulate standoff annotations so that titles and labels fall naturally in the text \n",
    "            # and paragraph tags that hold other paragraphs (as is the case with pmid:26791617) don't trigger repeating text.\n",
    "            # Make sure the SOs only tile the document and do not overlap.\n",
    "            text_so_list = sorted(text_so_list, key=lambda x: x.start)\n",
    "            last_so = None\n",
    "            for so in text_so_list:\n",
    "                if last_so:\n",
    "                    if last_so.end > so.start:\n",
    "                        last_so.end = so.start-1\n",
    "                last_so = so\n",
    "              \n",
    "            sent_id = 0\n",
    "            for local_id, so in enumerate(text_so_list):\n",
    "                sec_tree = self.get_sec_tree(so)\n",
    "                sec_title = self.get_sec_tag(so) \n",
    "                top_sec_title = self.get_top_level_sec_tag(so) \n",
    "                figure_reference = self.get_figure_reference(so) \n",
    "                \n",
    "                # ANY EXCLUSION CRITERIA FOR TAGS PUT IT HERE\n",
    "                \n",
    "                # SEARCH FOR XREFS IN THIS TEXT BLOCK - AND SUB THEM INTO THE TEXT.\n",
    "                so_text = ''\n",
    "                prev_end = so.start\n",
    "                ref_xrefs = [x for x in all_xrefs if x.start>=so.start and x.end<=so.end and x.element.attrib['ref-type']=='bibr']\n",
    "                #print(ref_xrefs)\n",
    "                \n",
    "                if len(ref_xrefs) > 0:\n",
    "                    refbib_xrefs = [x for x in all_xrefs if x.start>=so.start and x.end<=so.end and \n",
    "                                (x.element.attrib['ref-type']=='bibr' or x.element.attrib['ref-type']=='fig')] \n",
    "                    for x in refbib_xrefs:\n",
    "                        if x.element.attrib['ref-type']=='bibr':\n",
    "                            ref_id = x.element.attrib['rid']\n",
    "                            ref = ref_dict.get(ref_id, None)\n",
    "                            if ref and ref.get('pmid'):\n",
    "                                ref_text= '<<REF:%s>>'%(ref.get('pmid'))\n",
    "                            elif ref:\n",
    "                                ref_text= '<<REF:%s-%s-%s-%s>>'%(ref.get('first_author','???'),ref.get('year','?'),ref.get('vol','?'),ref.get('page','?'))\n",
    "                            else:\n",
    "                                ref_text = '<<REF>>'\n",
    "                            so_text += self.text[prev_end:x.start] + ref_text\n",
    "                        else: \n",
    "                            fig_id = x.element.attrib['rid']\n",
    "                            fig_text = '%s <<FIG:%s>>'%(self.text[x.start:x.end],fig_id)\n",
    "                            so_text += self.text[prev_end:x.start] + fig_text\n",
    "                        #print(pmid, ref_id, ref_text)\n",
    "                        prev_end = x.end\n",
    "                    \n",
    "                    #if len(so_text)>0:\n",
    "                    #  print(so_text)\n",
    "                    so_text += self.text[prev_end:so.end]  \n",
    "                    so_text = html.unescape(so_text)\n",
    "                #__________________________________________________________________\n",
    "                else: # TRY TO USE REGEXES TO SUBSTITUTE REFERENCES IN PASSAGE TEXT\n",
    "                    fig_xrefs = [x for x in all_xrefs if x.start>=so.start and x.end<=so.end and x.element.attrib['ref-type']=='fig']\n",
    "                    so_text = ''\n",
    "                    prev_end = so.start\n",
    "                    for x in fig_xrefs:\n",
    "                        fig_id = x.element.attrib['rid']\n",
    "                        fig_text = '%s <<FIG:%s>>'%(self.text[x.start:x.end],fig_id)\n",
    "                        so_text += self.text[prev_end:x.start] + fig_text\n",
    "                        prev_end = x.end\n",
    "                    so_text += self.text[prev_end:so.end]  \n",
    "                    so_text = html.unescape(so_text)\n",
    "\n",
    "                    #print(so_text)\n",
    "                    for key in ref_dict:\n",
    "                        ref = ref_dict[key]\n",
    "                        if ref.get('pmid'):  \n",
    "                            ref_text= ' <<REF:%s>> '%(ref.get('pmid'))\n",
    "                        else:\n",
    "                            ref_text= ' <<REF:%s-%s-%s-%s>> '%(ref.get('first_author','???'),ref.get('year','?'),ref.get('vol','?'),ref.get('page','?'))\n",
    "                        if ref.get('year') and ref.get('second_author'):\n",
    "                            regex = '%s( and %s,|,|\\\\s+et al\\\\.\\\\,|\\\\s+et al){0,1}\\\\s+%s'%(ref.get('first_author',''),ref.get('second_author',''),ref.get('year',''))\n",
    "                        elif ref.get('year') and len(ref.get('first_author',''))>0:\n",
    "                            regex = '%s(,|\\\\s+et al\\\\.\\\\,|\\\\s+et al){0,1}\\\\s+%s'%(ref.get('first_author',''),ref.get('year',''))\n",
    "                        else:\n",
    "                            regex = '%s( and [A-Za-z]+|,|\\\\s+et al\\\\.\\\\,|\\\\s+et al){0,1}\\\\s+(19|20)[0-9][0-9]'%(ref.get('first_author',''))\n",
    "                        pattern = re.compile(regex)\n",
    "                        if pattern.search(so_text):\n",
    "                            so_text = pattern.sub(ref_text, so_text)\n",
    "                        #print( pattern.sub(ref_text,so_text))\n",
    "                \n",
    "                tuple = (self.ft_id, local_id, so.element.tag, top_sec_title, sec_tree, sec_title, so.start, (so.end-so.start), figure_reference, so_text)\n",
    "                text_tuples.append(tuple)\n",
    "              \n",
    "        except etree.XMLSyntaxError as xmlErr:\n",
    "            print(\"XML Syntax Error: {0}\".format(xmlErr))\n",
    "        except UnicodeDecodeError as unicodeErr:\n",
    "            print(\"Unicode parsing Error: {0}\".format(unicodeErr))\n",
    "        #except TypeError as typeErr:\n",
    "        #  print(\"Type Error: {0}\".format(typeErr))  \n",
    "        #    print(\"ValueError: {0}\".format(valErr))\n",
    "        #    return None\n",
    "        \n",
    "        text_df = pd.DataFrame(text_tuples, columns=['PMID', 'PARAGRAPH_ID', 'TAG', 'TOP_SECTION', 'SECTION_TREE', 'SECTION', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'])\n",
    "        return text_df\n",
    "    \n",
    "    def extract_ref_dict_from_nxml(self, search_pubmed=False):\n",
    "    \n",
    "        if self.xml is None:\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(self.xml, \"lxml-xml\")\n",
    "\n",
    "        references = soup.find_all('ref')\n",
    "        all_ref_dict = {}\n",
    "        for r in references:\n",
    "            ref_dict = {}\n",
    "            ref_dict['ref'] = r.attrs.get('id')\n",
    "\n",
    "            ref_dict['author'] = ''\n",
    "            for t in r.descendants:\n",
    "                if type(t) is Tag and t.name == 'surname' and ref_dict.get('first_author', None) is None:\n",
    "                    ref_dict['first_author'] = re.sub(\"'\",\"''\",t.text)\n",
    "                if type(t) is Tag and t.name == 'surname' and ref_dict.get('first_author', None) is not None:\n",
    "                    ref_dict['second_author'] = re.sub(\"'\",\"''\",t.text)\n",
    "                if type(t) is Tag and t.name == 'name' and len(ref_dict.get('author')) > 0:\n",
    "                    ref_dict['author'] += ', '\n",
    "                if type(t) is Tag and t.name == 'surname':\n",
    "                    ref_dict['author'] += re.sub(\"'\",\"''\",t.text)\n",
    "                if type(t) is Tag and t.name == 'given-names':\n",
    "                    ref_dict['author'] += ' ' + re.sub(\"'\",\"''\",t.text)\n",
    "                elif type(t) is Tag and t.name == 'article-title':\n",
    "                    ref_dict['title'] = re.sub(\"'\",\"''\",t.text)\n",
    "                elif type(t) is Tag and t.name == 'source':\n",
    "                    ref_dict['journal'] = t.text\n",
    "                elif type(t) is Tag and t.name == 'year':\n",
    "                    m = re.match('(\\\\d\\\\d\\\\d\\\\d)', t.text)\n",
    "                    if m:\n",
    "                        ref_dict['year'] = m.group(1)\n",
    "                elif type(t) is Tag and t.name == 'volume':\n",
    "                    ref_dict['vol'] = t.text\n",
    "                elif type(t) is Tag and t.name == 'fpage':\n",
    "                    ref_dict['page'] = t.text\n",
    "                    \n",
    "            all_ref_dict[ref_dict.get('ref')] = ref_dict\n",
    "        \n",
    "        # Search pubmed for the PMIDs        \n",
    "        if search_pubmed:\n",
    "            if os.environ.get('NCBI_API_KEY') is None:\n",
    "                raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "            pubmed_api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "            clauses = []\n",
    "            for r in all_ref_dict: \n",
    "                ref_dict = all_ref_dict[r]\n",
    "                if ref_dict.get('first_author', None) is not None and \\\n",
    "                        ref_dict.get('year', None) is not None and \\\n",
    "                        ref_dict.get('vol', None) is not None and \\\n",
    "                        ref_dict.get('page', None) is not None:\n",
    "                    c = \"(%s[au]+AND+%s[dp]+AND+%s[vi]+AND+%s[pg]')\"%(\n",
    "                            ref_dict.get('first_author'),\n",
    "                            ref_dict.get('year'), \n",
    "                            ref_dict.get('vol'), \n",
    "                            ref_dict.get('page')\n",
    "                        )     \n",
    "                    clauses.append(c)\n",
    "            \n",
    "            if len(clauses)==0:\n",
    "                return all_ref_dict\n",
    "\n",
    "            stem1 = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&api_key='+pubmed_api_key\n",
    "            pmids = []\n",
    "            for i in range(0, len(clauses), 50):\n",
    "                query1 = '+OR+'.join(clauses[i:i+50])\n",
    "                r1 = requests.get(stem1+'&db=pubmed&term='+query1)\n",
    "                soup2 = BeautifulSoup(r1.text, \"lxml-xml\")\n",
    "                        \n",
    "                for id in soup2.find_all('id'):\n",
    "                    pmids.append(id.text)\n",
    "\n",
    "            stem2 = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&api_key='+pubmed_api_key\n",
    "            for i in range(0, len(pmids), 50):\n",
    "                query2 = ','.join(clauses[i:i+50])\n",
    "                r2 = requests.get(stem2+'&db=pubmed&id='+query2)\n",
    "                soup2 = BeautifulSoup(r2.text, \"lxml-xml\")\n",
    "                for article_tag in soup2.find_all('PubmedArticle'):\n",
    "                    first_author = article_tag.find('Author').find('LastName').text\n",
    "                    year = article_tag.find('PubDate').find('Year').text\n",
    "                    vol = article_tag.find('Volume').text\n",
    "                    page = article_tag.find('StartPage').text\n",
    "                    pmid = article_tag.find('PMID').text\n",
    "                    all_ref_dict[('%s-%s-%s-%s'%(first_author, year, vol, page)).lower()] = pmid\n",
    "  \n",
    "        return all_ref_dict\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
