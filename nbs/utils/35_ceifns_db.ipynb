{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database for Scientific Knowledge \n",
    "\n",
    ">  A local Postgresql database of scientific content with associated information generated by the agent. The intent is to use this repository as 'memory' in a Langchain-enabled agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.ceifns_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a local database for use with the Alhazen schema conforming to the following schema:\n",
    "\n",
    "![Schema](../pics/schema/Alhazen_Schema_v0.0.3.png) \n",
    "\n",
    "The key elements of the schema are:\n",
    "\n",
    "**Abstract Parent Class**\n",
    "\n",
    "* *InformationContentEntity* - a named piece of information (all other entities inherit from this)\n",
    "  \n",
    "**Classes Denoting Extant Scientific Knowledge from External Sources**\n",
    "\n",
    "* *ScientificKnowledgeCollection* - a collection of scientific knowledge expressions\n",
    "* *ScientificKnowledgeExpression* - an expression of scientific knowledge (e.g., a paper, a database record, a blog post, etc.) \n",
    "* *ScientificKnowledgeItem* - the data item that manifests the expression (e.g., a pdf, an html data file, a yaml file, etc.) \n",
    "* *ScientificKnowledgeFragment* - a relevant fragment of the item (e.g., a paragraph, a table, a figure, etc.)            \n",
    "\n",
    "**Classes Denoting Information Generated by Alhazen**\n",
    "\n",
    "* *Note* - an annotation placed on any InformationContentEntity  (e.g., a comment, a question, a link to another fragment, etc.)\n",
    "\n",
    "> Our definitions are inspired by [FABIO](http://www.sparontologies.net/ontologies/fabio), but only differentiate between '*expression*' (as the reference to an scientific underlying work product, e.g., a paper or database record) and '*item*' (as the actual data that delivers on that expression). \n",
    "> The system is coded in LinkML and generated as a Postgresql database and a SQLAlchemy ORM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.aliases import *\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery, load_paper_from_openalex, read_references_from_openalex \n",
    "from alhazen.utils.pdf_research_article_text_extractor import LAPDFBlockLoader, HuridocsPDFLoader\n",
    "from alhazen.utils.html_research_article_text_extractor import TrafilaturaSectionLoader\n",
    "\n",
    "#from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "from alhazen.schema_sqla import *\n",
    "import alhazen.schema_python as linkml_py\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import fitz\n",
    "from importlib_resources import files\n",
    "import json\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Extra, Field, root_validator\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "import local_resources.data_files.world_bank_wdi as wdi\n",
    "\n",
    "import nltk.data\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from sqlalchemy import create_engine, exists, Engine, text, func, or_, desc\n",
    "from sqlalchemy.orm import sessionmaker, aliased, Session\n",
    "import sqlite3  \n",
    "import subprocess\n",
    "import sys\n",
    "from time import time,sleep\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import uuid\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_ceifns_database(db_name):\n",
    "  \"\"\"Use this function to create a CEIFNS database within the local postgres server.\"\"\"\n",
    "  conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "      user=os.environ['PGUSER'],\n",
    "      password=os.environ['PGPASSWORD'],\n",
    "      host=os.environ['PGHOST'],\n",
    "      port= '5432'\n",
    "  )\n",
    "  conn.autocommit = True\n",
    "  cursor = conn.cursor()\n",
    "  cursor.execute('CREATE database ' + db_name)\n",
    "  conn.close()\n",
    "  \n",
    "  conn = psycopg2.connect(\n",
    "    database=db_name,\n",
    "    user=os.environ['PGUSER'],\n",
    "    password=os.environ['PGPASSWORD'],\n",
    "    host=os.environ['PGHOST'],\n",
    "    port= '5432'\n",
    "  )\n",
    "  conn.autocommit = True\n",
    "  cursor = conn.cursor()\n",
    "  with open(files(linkml).joinpath('schema.sql'), 'r') as f:\n",
    "    for sql in tqdm(f.read().split(';')):\n",
    "        if len(sql.strip()) == 0:\n",
    "          continue\n",
    "        cursor.execute(sql)\n",
    "\n",
    "  engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:5432/%s\"%(os.environ['PGUSER'], os.environ['PGPASSWORD'], os.environ['PGHOST'], db_name))\n",
    "  session_class = sessionmaker(bind=engine)\n",
    "  session = session_class()\n",
    "\n",
    "  # add tabular data about countries\n",
    "  url = 'https://www.iban.com/country-codes'\n",
    "  page = requests.get(url)\n",
    "  soup = BeautifulSoup(page.content, 'html.parser')\n",
    "  table = soup.find('table')\n",
    "  rows = table.find_all('tr')\n",
    "  countries = []\n",
    "  for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols)==0:\n",
    "        continue\n",
    "    country = cols[0].text.strip()\n",
    "    code3 = cols[1].text.strip()\n",
    "    code2 = cols[2].text.strip()\n",
    "    countries.append((country, code3, code2))\n",
    "  country_df = pd.DataFrame(countries, columns=['country', 'code2', 'code3'])\n",
    "  wdi_classes_path = files(wdi).joinpath('CLASS.xlsx')\n",
    "  wdi_xls = pd.ExcelFile(wdi_classes_path)\n",
    "  wdi_df = wdi_xls.parse('List of economies')\n",
    "  for i, crow in country_df.iterrows():\n",
    "    \n",
    "    crow2 = wdi_df[wdi_df['Code']==crow.code3]\n",
    "    if len(crow2)>0:\n",
    "      country_region = str(crow2['Region'].values[0])\n",
    "      country_income = str(crow2['Income group'].values[0])\n",
    "      c = Country(id=uuid.uuid4().hex[0:16],\n",
    "                  name = crow.country, \n",
    "                  type='Country', \n",
    "                  code2 = crow.code2,\n",
    "                  code3 = crow.code3,\n",
    "                  region = country_region,\n",
    "                  income = country_income)\n",
    "    else: \n",
    "      c = Country(id=uuid.uuid4().hex[0:16],\n",
    "            name = crow.country, \n",
    "            type='Country', \n",
    "            code2 = crow.code2,\n",
    "            code3 = crow.code3)\n",
    "    session.add(c)\n",
    "  session.commit()\n",
    "      \n",
    "  conn.close()\n",
    "\n",
    "def drop_ceifns_database(db_name, backupFirst=True):\n",
    "  \"\"\"Use this function to delete a CEIFNS database from the local postgres server. Set the backupFirst flag to True to backup the database before deletion.\"\"\"\n",
    "  if backupFirst:\n",
    "    loc = os.environ['LOCAL_FILE_PATH']\n",
    "    current_date_time = datetime.now()\n",
    "    formatted_date_time = f'{current_date_time:%Y-%m-%d-%H-%M-%S}'\n",
    "    backup_path = loc+db_name+'/backup'+formatted_date_time+'.sql'\n",
    "    backup_ceifns_database(db_name, backup_path)\n",
    "    print(\"Database has been backed up to %s\"%(backup_path));\n",
    "  conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "      user=os.environ['PGUSER'],\n",
    "      password=os.environ['PGPASSWORD'],\n",
    "      host=os.environ['PGHOST'],\n",
    "      port= '5432'\n",
    "  )\n",
    "  conn.autocommit = True\n",
    "  cursor = conn.cursor()\n",
    "  cursor.execute(\"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='\"+db_name+\"' AND leader_pid IS NULL;\")\n",
    "  cursor.execute('DROP database ' + db_name)\n",
    "  print(\"Database has been dropped successfully !!\");\n",
    "  conn.close()\n",
    "\n",
    "# ADAPTED FROM https://gist.github.com/valferon/4d6ebfa8a7f3d4e84085183609d10f14\n",
    "def backup_ceifns_database(db_name, dest_file, verbose=False):\n",
    "  \"\"\"\n",
    "  Backup postgres db to a local file. Note that this\n",
    "  \"\"\"\n",
    "  \"postgresql://localhost:5432/\"+db_name\n",
    "\n",
    "  if verbose:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_dump',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            '-Fc',\n",
    "            '-f', dest_file,\n",
    "            '-v'],\n",
    "          stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if int(process.returncode) != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "        exit(1)\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      exit(1)\n",
    "  else:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_dump',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            '-f', dest_file],\n",
    "          stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if process.returncode != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "        exit(1)\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      exit(1)\n",
    "\n",
    "# ADAPTED FROM https://gist.github.com/valferon/4d6ebfa8a7f3d4e84085183609d10f14\n",
    "def restore_ceifns_database(db_name, backup_file, verbose=False):\n",
    "  \"\"\"\n",
    "  Restore postgres db from a file.\n",
    "  \"\"\"\n",
    "  if verbose:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_restore',\n",
    "            '--no-owner',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            '-v',\n",
    "            backup_file],\n",
    "          stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if int(process.returncode) != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(\"Issue with the db restore : {}\".format(e))\n",
    "  else:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_restore',\n",
    "            '--no-owner',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            backup_file],\n",
    "        stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if int(process.returncode) != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(\"Issue with the db restore : {}\".format(e))\n",
    "\n",
    "def list_databases():\n",
    "  \"\"\"\n",
    "  List all CEIFNS databases in the postgres server\n",
    "  \"\"\"\n",
    "  engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:5432/%s\"%(os.environ['POSTGRES_USER'], os.environ['POSTGRES_PASSWORD'], os.environ['POSTGRES_HOST'], 'postgres'))\n",
    "  connection = engine.connect()\n",
    "  result = connection.execute(text(\"SELECT datname FROM pg_database;\"))\n",
    "  dbn = [row[0] for row in result if row[0] != 'postgres']\n",
    "  connection.close()\n",
    "\n",
    "  ah_dbs = []\n",
    "  for db in dbn:\n",
    "    engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:5432/%s\"%(os.environ['POSTGRES_USER'], os.environ['POSTGRES_PASSWORD'], os.environ['POSTGRES_HOST'], db))\n",
    "    try: \n",
    "        connection = engine.connect()\n",
    "        result = connection.execute(text('SELECT * FROM \"ScientificKnowledgeCollection\" LIMIT 1;'))\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    connection.close()    \n",
    "    ah_dbs.append(db)\n",
    "\n",
    "  return ah_dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class Ceifns_LiteratureDb(BaseModel):\n",
    "  \"\"\"This class runs a set of queries on external literature databases to build a local database of linked corpora and papers.\n",
    "\n",
    "  Functionality includes:\n",
    "\n",
    "    * Executes queries over European PMC \n",
    "    * Can run combinatorial sets of queries using a dataframe structure\n",
    "        * Requires a column of queries expressed in boolean logic\n",
    "        * Optional to define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n",
    "    * Has capability to run boolean logic over sources (currently only European PMC, but possibly others) \n",
    "    * Builds a local Postgresql database with tables for collections, expressions, items, fragments, and notes. \n",
    "    * Provides an API for querying the database and returning results as sqlAlchemy objects.\n",
    "    * Permits user to download a local copy of full text papers in NXML(JATS), PDF, and HTML format.\n",
    "  \"\"\"\n",
    "  loc: str = Field()\n",
    "  name: str = Field()\n",
    "  engine: Engine = Field(default=None, init=False)\n",
    "  session: Session = Field(default=None, init=False)\n",
    "  sent_detector: nltk.tokenize.punkt.PunktSentenceTokenizer = Field(default=None, init=False)\n",
    "  embed_model_name: str = Field(default='BAAI/bge-large-en-v1.5', init=False)\n",
    "  embed_model_device: str = Field(default='cpu', init=False)\n",
    "  embed_model: Embeddings = Field(default=None, init=False)\n",
    "\n",
    "\n",
    "  class Config:\n",
    "    \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    \n",
    "    if self.loc[-1] != '/':\n",
    "      self.loc = self.loc + '/' \n",
    "    if os.path.exists(self.loc) is False:\n",
    "      os.makedirs(self.loc)\n",
    "\n",
    "    db_dir = Path(self.loc + self.name)\n",
    "    if db_dir.exists() is False:\n",
    "      os.makedirs(db_dir)\n",
    "\n",
    "    self.engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:5432/%s\"%(os.environ['POSTGRES_USER'], os.environ['POSTGRES_PASSWORD'], os.environ['POSTGRES_HOST'], self.name))\n",
    "\n",
    "    self.start_session() # instantiate session \n",
    "\n",
    "    log_path = '%s%s_log.txt'%(self.loc, self.name)\n",
    "    if os.path.exists(log_path) is False:\n",
    "      Path(log_path).touch()\n",
    "    \n",
    "    self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    #\n",
    "    # hard coded embedding model at present.\n",
    "    #\n",
    "    model_kwargs = {\"device\": self.embed_model_device}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    self.embed_model = HuggingFaceBgeEmbeddings(\n",
    "      model_name=self.embed_model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "    # PGVECTOR representation for embedding uses environmental variables to set the right database \n",
    "    os.environ['PGVECTOR_CONNECTION_STRING'] = \"postgresql+psycopg2://%s:%s@%s:5432/%s\"%(os.environ['POSTGRES_USER'], os.environ['POSTGRES_PASSWORD'], os.environ['POSTGRES_HOST'], self.name)\n",
    "\n",
    "  def start_session(self):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    return self.session\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Add Collections, Expressions, Items, and Fragments \n",
    "  # by running queries on external databases\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  #def add_corpus_from_epmc(self, qt, qt2, \n",
    "  #                         sections=['paper_title', 'ABSTRACT'], \n",
    "  #                         sections2=['paper_title', 'ABSTRACT'], \n",
    "  #                         page_size=1000):\n",
    "  #  \"\"\"Adds corpora based on coupled QueryTranslator objects.\"\"\"\n",
    "  #  \n",
    "  #  if self.session is None:\n",
    "  #    session_class = sessionmaker(bind=self.engine)\n",
    "  #    self.session = session_class()\n",
    "  #  \n",
    "  #  if self.session.query(exists().where(InformationResource.name=='EPMC')).scalar():\n",
    "  #    info_resource = self.session.query(InformationResource) \\\n",
    "  #        .filter(InformationResource.name=='EPMC').first()\n",
    "  #  else:\n",
    "  #    info_resource = InformationResource(id='skem:EPMC', \n",
    "  #                                        iri=['skem:EPMC'], \n",
    "  #                                        name='European PubMed Central', \n",
    "  #                                        type='skem:InformationResource',\n",
    "  #                                        xref=['https://europepmc.org/'])\n",
    "  #  \n",
    "  #  (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
    "  #  if qt2:\n",
    "  #    (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections2)\n",
    "  #  else: \n",
    "  #    (subset_ids, epmc_subset_queries) = ([0],[''])\n",
    "  #  for (i, q) in zip(corpus_ids, epmc_queries):\n",
    "  #    for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "  #      query = q\n",
    "  #      corpus_id = str(i)\n",
    "  #      corpus_name = qt.df.loc[qt.df['ID']==i][qt.name_col].values[0]\n",
    "  #      if query is None or query=='nan' or len(query)==0: \n",
    "  #        continue\n",
    "  #      if len(sq) > 0 and sq != 'nan':\n",
    "  #        query = '(%s) AND (%s)'%(q, sq)\n",
    "  #        if j is not None:\n",
    "  #          corpus_id = str(i)+'.'+str(j)\n",
    "  #          corpus_name2 = qt2.df.loc[qt2.df['ID']==j][qt2.name_col].values[0]\n",
    "  #          corpus_name = corpus_name + '/'+ corpus_name2\n",
    "  #      try:\n",
    "  #        self.add_corpus_from_epmc_query(corpus_id, corpus_name, query)\n",
    "  #      except Exception as ex:\n",
    "  #        print(ex)\n",
    "  #        print('skipping %s'%(corpus_name) )\n",
    "  #        continue\n",
    "  #  self.session.commit()\n",
    "  \n",
    "  def add_corpus_from_epmc_query(self, \n",
    "                                 corpus_id, \n",
    "                                 corpus_name, \n",
    "                                 query, \n",
    "                                 commit_this=True,\n",
    "                                 page_size=1000):\n",
    "    \"\"\"Adds corpus to database based on terms constructed in a direct EPMC query.\"\"\"\n",
    "    \n",
    "    # does this collection already exist?  \n",
    "    corpus_id = str(corpus_id)\n",
    "    all_existing_query = self.session.query(SKC).filter(SKC.id==corpus_id)\n",
    "    \n",
    "    corpus = None\n",
    "    for c in all_existing_query.all():\n",
    "      corpus = c\n",
    "    if corpus is None:      \n",
    "      corpus = ScientificKnowledgeCollection(id=corpus_id,\n",
    "                                           type='skem:ScientificKnowledgeCollection',\n",
    "                                           name=corpus_name,\n",
    "                                           has_members=[])\n",
    "    self.session.add(corpus)\n",
    "\n",
    "    epmcq = EuroPMCQuery()\n",
    "    numFound, pubs = epmcq.run_empc_query(query, page_size=page_size)\n",
    "    for p in tqdm(pubs):\n",
    "      p_id = str(p.id)\n",
    "      p_check = self.session.query(SKE) \\\n",
    "          .filter(SKE.id==p_id).first()\n",
    "      if p_check is not None:\n",
    "        p = p_check\n",
    "      corpus.has_members.append(p)\n",
    "      p.member_of.append(corpus)\n",
    "      for item in p.has_representation:\n",
    "        for f in item.has_part:\n",
    "          #f.content = '\\n'.join(self.sent_detector.tokenize(f.content))\n",
    "          f.part_of = item.id\n",
    "          self.session.add(f)\n",
    "        item.represented_by = p.id\n",
    "        self.session.add(item)\n",
    "      self.session.add(p)\n",
    "    \n",
    "    skes = self.list_unindexed_skes(corpus_id)\n",
    "    self.embed_expression_list(skes)\n",
    "\n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "\n",
    "    return corpus\n",
    "\n",
    "  def check_query_terms(self, qt, qt2=None, pubmed_api_key=''):\n",
    "    pmq = ESearchQuery(api_key=pubmed_api_key)\n",
    "    terms = set()\n",
    "    for t in qt.terms2id.keys():\n",
    "        terms.add(t)\n",
    "    if qt2 is not None:\n",
    "        for t2 in qt2.terms2id.keys():\n",
    "            terms.add(t2)\n",
    "    check_table = {} \n",
    "    for t in tqdm(terms):\n",
    "        (is_ok, t2, c) = pmq._check_query_phrase(t)\n",
    "        check_table[t] = (is_ok, c)\n",
    "    return check_table\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Add corpus based on a list of dois using OpenAlex\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def add_collection_from_dois_using_openalex(self, corpus_id, corpus_name, dois, commit_this=True):\n",
    "    \n",
    "    # does this collection already exist?  \n",
    "    corpus_id = str(corpus_id)\n",
    "    all_existing_query = self.session.query(SKC).filter(SKC.id==corpus_id)\n",
    "    \n",
    "    corpus = None\n",
    "    for c in all_existing_query.all():\n",
    "      corpus = c\n",
    "    if corpus is None:      \n",
    "      corpus = ScientificKnowledgeCollection(id=corpus_id,\n",
    "                                           type='skem:ScientificKnowledgeCollection',\n",
    "                                           name=corpus_name,\n",
    "                                           has_members=[])\n",
    "    self.session.add(corpus)\n",
    "\n",
    "    papers_to_index = []\n",
    "    for doi in tqdm(dois):\n",
    "        p = load_paper_from_openalex(doi)\n",
    "        if p is None:\n",
    "          continue\n",
    "        p_id = str(p.id)\n",
    "        p_check = self.session.query(SKE) \\\n",
    "            .filter(SKE.id==p_id).first()\n",
    "        if p_check is not None:\n",
    "          p = p_check\n",
    "\n",
    "        self.session.add(p)\n",
    "        corpus.has_members.append(p)\n",
    "        p.member_of.append(corpus)\n",
    "        for item in p.has_representation:\n",
    "            for f in item.has_part:\n",
    "                #f.content = '\\n'.join(self.sent_detector.tokenize(f.content))\n",
    "                f.part_of = item.id\n",
    "                self.session.add(f)\n",
    "            item.represented_by = p.id\n",
    "            self.session.add(item)\n",
    "        papers_to_index.append(p)\n",
    "        self.session.flush()\n",
    "\n",
    "    self.embed_expression_list(papers_to_index)\n",
    "\n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Manipulating + Combining Collections\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def rename_collection(self, collection_id, new_collection_name, commit_this=True):\n",
    "    \"\"\"Renames a collection.\"\"\"\n",
    "    \n",
    "    c = self.session.query(SKC).filter(SKC.id==collection_id).first()\n",
    "    if c is None:\n",
    "      raise Exception('Collection does not exist')\n",
    "    c.name = new_collection_name\n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "    return c\n",
    "\n",
    "  def create_new_collection_from_intersection(self, collection_id, collection_name, c1_id, c2_id, commit_this=True):\n",
    "    \"\"\"Creates a new collection based on the intersection of two existing collections.\"\"\"\n",
    "    \n",
    "    c1 = self.session.query(SKC).filter(SKC.id==c1_id).first()\n",
    "    c2 = self.session.query(SKC).filter(SKC.id==c2_id).first()\n",
    "    \n",
    "    if c1 is None or c2 is None:\n",
    "      raise Exception('One or both of the collections do not exist')\n",
    "    \n",
    "    c1_items = set([m.id for m in c1.has_members])\n",
    "    c2_items = set([m.id for m in c2.has_members])\n",
    "    \n",
    "    intersection = c1_items.intersection(c2_items)\n",
    "    \n",
    "    new_collection = ScientificKnowledgeCollection(id=collection_id,\n",
    "                                                   type='skem:ScientificKnowledgeCollection',\n",
    "                                                   name=collection_name,\n",
    "                                                   has_members=[])\n",
    "    self.session.add(new_collection)\n",
    "    \n",
    "    for i in intersection:\n",
    "      item = self.session.query(SKE).filter(SKE.id==i).first()\n",
    "      new_collection.has_members.append(item)\n",
    "      item.member_of.append(new_collection)\n",
    "    \n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "\n",
    "    return new_collection\n",
    "\n",
    "  def create_new_collection_from_sample(self, collection_id, collection_name, c_id, count, subtypes=[], commit_this=True):\n",
    "    \"\"\"Creates a new collection based on the intersection of two existing collections.\"\"\"\n",
    "    \n",
    "    q = self.session.query(SKE) \\\n",
    "      .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "      .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "      .filter(SKC.id==c_id)\n",
    "    \n",
    "    if len(subtypes)>0:\n",
    "      q = q.filter(SKE.type.in_(subtypes))\n",
    "\n",
    "    items = q.all()\n",
    "    sample = random.sample(items, count)\n",
    "        \n",
    "    new_collection = ScientificKnowledgeCollection(id=collection_id,\n",
    "                                                   type='skem:ScientificKnowledgeCollection',\n",
    "                                                   name=collection_name,\n",
    "                                                   has_members=[])\n",
    "    self.session.add(new_collection)\n",
    "    \n",
    "    for e in sample:\n",
    "      new_collection.has_members.append(e)\n",
    "      e.member_of.append(new_collection)\n",
    "    \n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "\n",
    "    return new_collection\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Deleting Collections, Expressions, Items, and Fragments.\n",
    "  #     Need to adjust SQLAlchemy methods to permit cascades.\n",
    "  #     This is more precise and careful (but need shortcuts)\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def delete_collection(self, collection_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified collection from the database, leave all expressions and items intact\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKC.id==collection_id)).scalar():\n",
    "\n",
    "      # remove collection - expressions relations\n",
    "      q2 = self.session.query(SKC_HM) \\\n",
    "          .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "          .filter(SKC.id==collection_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove expression -> collection relations\n",
    "      q3 = self.session.query(SKE_MO) \\\n",
    "          .filter(SKE_MO.member_of_id==collection_id)\n",
    "      for cp in q3.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKC) \\\n",
    "          .filter(SKC.id==collection_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKC_HN, N) \\\n",
    "        .filter(SKC_HN.ScientificKnowledgeCollection_id==collection_id) \\\n",
    "        .filter(SKC_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('skc:'+collection_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')\n",
    "\n",
    "  def delete_expression(self, expression_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified collection from the database, leave all expressions and items intact\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKE.id==expression_id)).scalar():\n",
    "\n",
    "      q = self.session.query(SKI) \\\n",
    "          .filter(SKE_HR.ScientificKnowledgeExpression_id==expression_id) \\\n",
    "          .filter(SKE_HR.has_representation_id==SKI.id)\n",
    "      for item in q.all():\n",
    "        self.delete_item(item.id, commit_this=False)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove collection - expressions relations\n",
    "      q2 = self.session.query(SKC_HM) \\\n",
    "          .filter(SKC_HM.has_members_id==expression_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove expression -> collection relations\n",
    "      q3 = self.session.query(SKE_MO) \\\n",
    "          .filter(SKE_MO.ScientificKnowledgeExpression_id==expression_id)\n",
    "      for cp in q3.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKE_HN, N) \\\n",
    "        .filter(SKE_HN.ScientificKnowledgeExpression_id==expression_id) \\\n",
    "        .filter(SKC_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('ske:'+expression_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      q7 = self.session.query(SKE_IRI) \\\n",
    "        .filter(SKE_IRI.ScientificKnowledgeExpression_id==expression_id)\n",
    "      for iri in q7.all():\n",
    "        self.session.delete(iri)\n",
    "        self.session.flush()\n",
    "\n",
    "      q8 = self.session.query(SKE_XREF) \\\n",
    "        .filter(SKE_XREF.ScientificKnowledgeExpression_id==expression_id)\n",
    "      for xref in q8.all():\n",
    "        self.session.delete(xref)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKE) \\\n",
    "          .filter(SKE.id==expression_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')\n",
    "\n",
    "  def delete_item(self, item_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified item from the database, cascade the deletion to all attached fragments and notes.\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKI.id==item_id)).scalar():\n",
    "\n",
    "      q = self.session.query(SKF) \\\n",
    "          .filter(SKI_HP.ScientificKnowledgeItem_id==item_id) \\\n",
    "          .filter(SKI_HP.has_part_id==SKF.id)\n",
    "      for fragment in q.all():\n",
    "        self.delete_fragment(fragment.id, commit_this=False)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove expression -> item relations\n",
    "      q2 = self.session.query(SKE_HR) \\\n",
    "          .filter(SKE_HR.has_representation_id==item_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKI) \\\n",
    "          .filter(SKI.id==item_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKI_HN, N) \\\n",
    "        .filter(SKI_HN.ScientificKnowledgeItem_id==item_id) \\\n",
    "        .filter(SKI_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('ski:'+item_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')\n",
    "\n",
    "  def delete_fragment(self, fragment_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified fragment from the database, cascade the deletion to all associated notes.\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKF.id==fragment_id)).scalar():\n",
    "\n",
    "      # remove item -> fragment relations\n",
    "      q2 = self.session.query(SKI_HP) \\\n",
    "          .filter(SKI_HP.has_part_id==fragment_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKF) \\\n",
    "          .filter(SKF.id==fragment_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKF_HN, N) \\\n",
    "        .filter(SKF_HN.ScientificKnowledgeFragment_id==fragment_id) \\\n",
    "        .filter(SKF_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('skf:'+fragment_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')  \n",
    "\n",
    "  def delete_note(self, note_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified note from the database, cascade the deletion to all associated entities.\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(Note.id==note_id)).scalar():\n",
    "\n",
    "      # remove item -> fragment relations\n",
    "      q1 = self.session.query(NoteIsAbout) \\\n",
    "          .filter(NoteIsAbout.Note_id==note_id) \n",
    "      for cp in q1.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q2 = self.session.query(SKF_HN) \\\n",
    "          .filter(SKF_HN.has_notes_id==note_id) \n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    " \n",
    "      q3 = self.session.query(SKC_HN) \\\n",
    "          .filter(SKC_HN.has_notes_id==note_id) \n",
    "      for cp in q3.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKE_HN) \\\n",
    "          .filter(SKE_HN.has_notes_id==note_id) \n",
    "      for cp in q4.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKI_HN) \\\n",
    "          .filter(SKI_HN.has_notes_id==note_id) \n",
    "      for cp in q5.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(N_HN) \\\n",
    "          .filter(N_HN.has_notes_id==note_id) \n",
    "      for cp in q6.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(Note) \\\n",
    "        .filter(Note.id==note_id)\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Note does not exist')  \n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Analyze the structure / composition of a collection\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def build_full_text_report_on_collection(self, collection_id):\n",
    "    q = self.session.query(SKE) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKC.id==str(collection_id)) \\\n",
    "        .order_by(desc(SKE.publication_date))\n",
    "    return pd.DataFrame([self.check_full_text(e) for e in tqdm(q.all())])    \n",
    "\n",
    "  def build_item_type_pivot_table(self):\n",
    "    q = self.session.query(SKC.name, SKE.id, SKI.type) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id==SKI.id) \n",
    "    df = pd.DataFrame(q.all(), columns=['collection', 'doi', 'type'])    \n",
    "    return df.pivot_table(index='collection', columns='type', values='doi', aggfunc=lambda x: len(x.unique()))\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Add full text elements to database based on dowloaded files\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def add_full_text_for_expression(self, e):\n",
    "    \"\"\"Adds Full Text Item data to given Expression \n",
    "    if full text is already downloaded in the `{loc}/ft/{doi}` directory.\"\"\"\n",
    "\n",
    "    doi = re.sub('doi:','',e.id)\n",
    "    \n",
    "    # Check local directory for files\n",
    "    base_file_path = '%s%s/ft/'%(self.loc, self.name)\n",
    "    nxml_file_path = base_file_path+doi+'.nxml'\n",
    "    pdf_file_path = base_file_path+doi+'.pdf'\n",
    "    html_file_path = base_file_path+doi+'.html'\n",
    "\n",
    "    self.add_full_text_for_expression_nxml(e, nxml_file_path)\n",
    "    self.add_full_text_for_expression_pdf(e, pdf_file_path)\n",
    "    self.add_full_text_for_expression_html(e, html_file_path)\n",
    "    self.session.commit() \n",
    "\n",
    "  def add_full_text_for_expression_nxml(self, e, nxml_file_path, update_existing=False):\n",
    "    \"\"\"Adds Full Text Item data to expression if *.nxml full text is already downloaded in the `{loc}/ft/{doi}` directory.\"\"\"\n",
    "\n",
    "    doi = re.sub('doi:','',e.id)\n",
    "\n",
    "    q = self.session.query(SKI) \\\n",
    "        .filter(SKE_HR.ScientificKnowledgeExpression_id == e.id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(or_(SKI.type == 'JATSFullText'))\n",
    "    if update_existing is False:\n",
    "      for i in q.all():\n",
    "        return\n",
    "    else:\n",
    "      for i in q.all():\n",
    "        self.delete_item(i.id, commit_this=False)\n",
    "        self.session.commit()\n",
    "\n",
    "    if os.path.exists(nxml_file_path):\n",
    "      with open(nxml_file_path, 'r') as f:\n",
    "        xml = f.read()\n",
    "        try:\n",
    "          d = NxmlDoc(doi, xml)\n",
    "        except Exception as ex:\n",
    "          return\n",
    "        doc_text = d.text \n",
    "        if 'body' in [so.element.tag for so in d.standoffs] is False:\n",
    "          return\n",
    "        ski_type = 'JATSFullText'\n",
    "                \n",
    "        ski_id = str(uuid.uuid4().hex)[:10]\n",
    "        ski = ScientificKnowledgeItem(id=ski_id, \n",
    "                                      content=doc_text,\n",
    "                                      xref=['file:'+nxml_file_path],\n",
    "                                      type=ski_type,)\n",
    "        e.has_representation.append(ski)\n",
    "        self.session.add(ski)\n",
    "        self.session.flush()\n",
    "        self.add_sections_as_fragments_from_nxmldoc(ski, d)\n",
    "        self.session.commit()\n",
    "\n",
    "  def add_sections_as_fragments_from_nxmldoc(self, item, d):\n",
    "    \"\"\"Adds fragments from an NxmlDoc object to a ScientificKnowledgeItem object.\"\"\"\n",
    "\n",
    "    # ['PMID', 'PARAGRAPH_ID', 'TAG', 'TAG_TREE', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'\n",
    "    fragments = []\n",
    "    df = d.build_enhanced_document_dataframe()\n",
    "    # df = df[df.SECTION_TREE!='']\n",
    "    # Can search for a substring in top section headers\n",
    "    # df3 = df[df.TOP_SECTION.str.contains('method', case=False)]\n",
    "\n",
    "    try: \n",
    "      df3 = df[df.TOP_SECTION!='']\n",
    "      df4 = df3.groupby('SECTION_TREE').agg({'OFFSET': 'min', 'LENGTH': 'sum', 'PLAIN_TEXT':lambda x: '\\n'.join(x)}).sort_values('OFFSET')\n",
    "      df4 = df4.reset_index(drop=False)\n",
    "    \n",
    "      if df4 is None:\n",
    "        return\n",
    "    \n",
    "      for i, tup in df4.iterrows():      \n",
    "        #sentence_split_text = '\\n'.join(self.sent_detector.tokenize(tup.PLAIN_TEXT))\n",
    "        sentence_split_text = tup.PLAIN_TEXT\n",
    "        fragment = ScientificKnowledgeFragment(id=item.id[:10]+'.'+str(i), \n",
    "                                              type='section',\n",
    "                                              offset=tup.OFFSET,\n",
    "                                              length=tup.LENGTH,\n",
    "                                              name=tup.SECTION_TREE,\n",
    "                                              content=sentence_split_text)\n",
    "        self.session.add(fragment)\n",
    "        self.session.flush()\n",
    "        item.has_part.append(fragment)\n",
    "        fragment.part_of = item.id\n",
    "        fragments.append(fragment)\n",
    "        self.session.flush() \n",
    "      self.session.commit()\n",
    "    except Exception as e:\n",
    "      self.session.rollback()    \n",
    "      \n",
    "  def add_full_text_for_expression_pdf(self, e, pdf_file_path, update_existing=False):\n",
    "    \"\"\"Adds Full Text Item data to expression if *.pdf full text is already downloaded in the `{loc}/ft/{doi}` directory.\"\"\"\n",
    "\n",
    "    doi = re.sub('doi:','',e.id)\n",
    "\n",
    "    q = self.session.query(SKI) \\\n",
    "        .filter(SKE_HR.ScientificKnowledgeExpression_id == e.id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(or_(SKI.type == 'PDFFullText'))\n",
    "    if update_existing is False:\n",
    "      for i in q.all():\n",
    "        return\n",
    "    else:\n",
    "      for i in q.all():\n",
    "        self.delete_item(i.id, commit_this=False)\n",
    "        self.session.commit()\n",
    "\n",
    "    if os.path.exists(pdf_file_path):\n",
    "      loader = HuridocsPDFLoader(pdf_file_path)\n",
    "      #loader = LAPDFBlockLoader(pdf_file_path)\n",
    "      docs = loader.load(curi_id='doi:'+doi)\n",
    "      skfs = []\n",
    "      doc_text = ''\n",
    "      ski_type = 'PDFFullText'\n",
    "\n",
    "      ski_id = str(uuid.uuid4().hex)[:10]\n",
    "      for i, d in enumerate(docs):\n",
    "        #sentence_split_text = '\\n'.join(self.sent_detector.tokenize(d.page_content))\n",
    "        sentence_split_text = d.page_content\n",
    "\n",
    "        skf = ScientificKnowledgeFragment(id=ski_id+'.'+str(i), \n",
    "                                             type='section',\n",
    "                                             offset=len(doc_text),\n",
    "                                             length=len(sentence_split_text),\n",
    "                                             name=sentence_split_text.split('\\n')[0],\n",
    "                                             content=sentence_split_text)\n",
    "        if len(doc_text) > 0:\n",
    "          doc_text += '\\n\\n'\n",
    "        doc_text += sentence_split_text\n",
    "        self.session.add(skf)\n",
    "        self.session.flush()\n",
    "        skfs.append(skf)\n",
    "      ski = ScientificKnowledgeItem(id=ski_id, \n",
    "                                    content=doc_text,\n",
    "                                    xref=['file:'+pdf_file_path],\n",
    "                                    type=ski_type,)\n",
    "      e.has_representation.append(ski)\n",
    "      self.session.add(ski)\n",
    "      self.session.flush()\n",
    "      for skf in skfs:\n",
    "        ski.has_part.append(skf)\n",
    "        skf.part_of = ski.id\n",
    "        self.session.flush()\n",
    "  \n",
    "  def add_full_text_for_expression_html(self, e, html_file_path, update_existing=False):\n",
    "    \"\"\"Adds Full Text Item data to expression if *.html full text is already downloaded in the `{loc}/ft/{doi}` directory.\"\"\"\n",
    "\n",
    "    doi = re.sub('doi:','',e.id)\n",
    "\n",
    "    q = self.session.query(SKI) \\\n",
    "        .filter(SKE_HR.ScientificKnowledgeExpression_id == e.id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(or_(SKI.type == 'HTMLFullText'))\n",
    "    if update_existing is False:\n",
    "      for i in q.all():\n",
    "        return\n",
    "    else:\n",
    "      for i in q.all():\n",
    "        self.delete_item(i.id, commit_this=False)\n",
    "        self.session.commit()\n",
    "\n",
    "    if os.path.exists(html_file_path):\n",
    "      loader = TrafilaturaSectionLoader(html_file_path)\n",
    "      #loader = LAPDFBlockLoader(pdf_file_path)\n",
    "      try: \n",
    "        docs = loader.load()\n",
    "      except Exception as ex:\n",
    "        print('Error loading HTML file: %s'%(html_file_path))\n",
    "        return\n",
    "      skfs = []\n",
    "      doc_text = ''\n",
    "      ski_type = 'HTMLFullText'\n",
    "\n",
    "      ski_id = str(uuid.uuid4().hex)[:10]\n",
    "      for i, d in enumerate(docs):\n",
    "        #sentence_split_text = '\\n'.join(self.sent_detector.tokenize(d.page_content))\n",
    "        sentence_split_text = d.page_content\n",
    "\n",
    "        skf = ScientificKnowledgeFragment(id=ski_id+'.'+str(i), \n",
    "                                             type='section',\n",
    "                                             offset=len(doc_text),\n",
    "                                             length=len(sentence_split_text),\n",
    "                                             name=sentence_split_text.split('\\n')[0],\n",
    "                                             content=sentence_split_text)\n",
    "        if len(doc_text) > 0:\n",
    "          doc_text += '\\n\\n'\n",
    "        doc_text += sentence_split_text\n",
    "        self.session.add(skf)\n",
    "        self.session.flush()\n",
    "        skfs.append(skf)\n",
    "      ski = ScientificKnowledgeItem(id=ski_id, \n",
    "                                    content=doc_text,\n",
    "                                    xref=['file:'+html_file_path],\n",
    "                                    type=ski_type,)\n",
    "      e.has_representation.append(ski)\n",
    "      self.session.add(ski)\n",
    "      self.session.flush()\n",
    "      for skf in skfs:\n",
    "        ski.has_part.append(skf)\n",
    "        skf.part_of = ski.id\n",
    "        self.session.flush()\n",
    "        \n",
    "  def add_full_text_for_collection(self, collection_id, \n",
    "                                   get_nxml=True, get_pdf=True, get_html=True):\n",
    "    for e in self.list_expressions(collection_id=collection_id):\n",
    "      self.add_full_text_for_expression(e, get_nxml, get_pdf, get_html)  \n",
    "  \n",
    "  def check_full_text(self, e):\n",
    "      \"\"\"Checks if full text is available and readable for a given expression.\"\"\"\n",
    "\n",
    "      doi = re.sub('doi:','',e.id)\n",
    "      file_path = self.loc + self.name + '/ft/'\n",
    "      nxml_file_path = file_path+doi+'.nxml'\n",
    "      pdf_file_path = file_path+doi+'.pdf'\n",
    "      html_file_path = file_path+doi+'.html'\n",
    "      \n",
    "      nxml_item_present = False\n",
    "      q = self.session.query(SKI) \\\n",
    "          .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "          .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "          .filter(SKI.type == 'JATSFullText') \\\n",
    "          .filter(SKE.id==e.id)\n",
    "      if q.count() > 0:\n",
    "          nxml_item_present = True\n",
    "      nxml_present = os.path.exists(nxml_file_path)\n",
    "      nxml_has_body = False\n",
    "      if os.path.exists(nxml_file_path):\n",
    "          with open(nxml_file_path, 'r') as file:\n",
    "              soup = BeautifulSoup(file, \"lxml-xml\")\n",
    "              if soup.body is not None:\n",
    "                  nxml_has_body = True\n",
    "      \n",
    "      pdf_item_present = False\n",
    "      q2 = self.session.query(SKI) \\\n",
    "          .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "          .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "          .filter(SKI.type == 'PDFFullText') \\\n",
    "          .filter(SKE.id==e.id)\n",
    "      if q2.count() > 0:\n",
    "          pdf_item_present = True\n",
    "      pdf_present = os.path.exists(pdf_file_path)\n",
    "      pdf_readable = False\n",
    "      if pdf_present:\n",
    "          try: \n",
    "              with fitz.open(pdf_file_path) as doc:\n",
    "                  pdf_readable = True\n",
    "                  #print('PDF: %s, Page Count: %d'%(doi, doc.page_count))\n",
    "          except Exception as ex:\n",
    "              pdf_readable = False\n",
    "                      \n",
    "      if os.path.exists(html_file_path):\n",
    "          html_present = True\n",
    "      else:\n",
    "          html_present = False\n",
    "\n",
    "      html_readable = False\n",
    "      if html_present:\n",
    "        loader = TrafilaturaSectionLoader(html_file_path)\n",
    "        try: \n",
    "          docs = loader.load()\n",
    "          html_readable = True\n",
    "        except Exception as ex:\n",
    "          html_readable = False\n",
    "\n",
    "      prior_attempts = False\n",
    "      for n in self.read_notes_about_x(e):\n",
    "        if 'download' in n.name:\n",
    "          prior_attempts = True\n",
    "          break\n",
    "\n",
    "      report = {'doi': doi, \n",
    "                'nxml_item_present': nxml_item_present,\n",
    "                'nxml_present': nxml_present,\n",
    "                'nxml_has_body': nxml_has_body,\n",
    "                'pdf_item_present': pdf_item_present,\n",
    "                'pdf_present': pdf_present,\n",
    "                'pdf_readable': pdf_readable,\n",
    "                'html_present': html_present,\n",
    "                'html_readable': html_readable,\n",
    "                'prior_attempts': prior_attempts}\n",
    "      \n",
    "      return report\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Read and Write Notes for Entities\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def write_note_about_x(self, x, note_name, note_content, note_format, note_type, commit_this=False):\n",
    "    \"\"\"Writes a note about an entity in the database.\"\"\"\n",
    "    note_id = str(uuid.uuid4().hex)[:10]\n",
    "    creation_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "    note = Note(id=note_id, \n",
    "             name=note_name, \n",
    "             creation_date=creation_date,\n",
    "             format=note_format,\n",
    "             type=note_type, \n",
    "             content=note_content)\n",
    "    self.session.add(note)\n",
    "    x.has_notes.append(note)\n",
    "    #note.is_about.append(x)\n",
    "    self.session.flush()\n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "    return note\n",
    "  \n",
    "  def read_notes_about_x(self, x):\n",
    "    \"\"\"Reads notes about an entity in the database.\"\"\"\n",
    "    q = self.session.query(N) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == x.id)\n",
    "    for n in q.all():\n",
    "      yield(n)\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Quick reports\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def report_collection_composition(self):\n",
    "    \"\"\"Returns a DataFrame with the composition of all collections in the database.\"\"\"\n",
    "    q = self.session.query(SKC.id, SKC.name, SKE.id, SKI.type) \\\n",
    "            .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "            .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "            .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(SKE_HR.has_representation_id==SKI.id) \n",
    "    df = pd.DataFrame(q.all(), columns=['id', 'collection name', 'doi', 'item type'])\n",
    "    piv_df = df.pivot_table(index=['id', 'collection name'], columns='item type', values='doi', aggfunc=lambda x: len(x.unique())).fillna(0)\n",
    "    return piv_df\n",
    "\n",
    "  def report_non_full_text_for_collection(self, collection_id):\n",
    "    \"\"\"Returns a DataFrame listing all papers without PDF or JATS full text versions in the database.\"\"\"\n",
    "    q = self.session.query(SKC.id, SKE.id, SKI.type) \\\n",
    "            .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "            .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "            .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "            .filter(SKC.id==str(collection_id))\n",
    "    df = pd.DataFrame(q.all(), columns=['collection_id', 'doi', 'item type'])\n",
    "    piv_df = df.pivot_table(index=['doi'], \n",
    "                        columns='item type', values='collection_id', \n",
    "                        aggfunc=lambda x: len(x.unique()))\n",
    "    piv_df.fillna(0, inplace=True)\n",
    "    piv_df = piv_df.reset_index(drop=False).sort_values(by='doi')\n",
    "    return piv_df[(piv_df['JATSFullText']==0.0) & (piv_df['PDFFullText']==0.0)]\n",
    "\n",
    "  def report_all_notes_by_type_name(self, note_type, name_pattern):\n",
    "    \"\"\"Returns a DataFrame listing all note contents with full text versions in the database.\"\"\"\n",
    "    q = self.session.query(N) \\\n",
    "      .filter(N.id == NIA.Note_id) \\\n",
    "      .filter(N.type == note_type) \\\n",
    "      .filter(N.name.like(name_pattern)) \n",
    "    l = []\n",
    "    for n in q.all():\n",
    "        tup = json.loads(n.content)\n",
    "        tup = json.loads(n.provenance)\n",
    "        tup['note_id'] = n.name\n",
    "        l.append(tup)\n",
    "    report_df = pd.DataFrame(l).set_index('note_id')\n",
    "    return report_df\n",
    "\n",
    "  def create_zip_archive_of_full_text_files(self, collection_id, zip_file_path):\n",
    "    \"\"\"Creates a zip archive of all full text files for a given collection.\"\"\"\n",
    "    file_path = self.loc + self.name + '/ft/'\n",
    "    with ZipFile(zip_file_path, 'w') as myzip:\n",
    "      for e in self.list_expressions(collection_id=collection_id):\n",
    "        doi = re.sub('doi:','',e.id)\n",
    "        nxml_file_path = file_path+doi+'.nxml'\n",
    "        pdf_file_path = file_path+doi+'.pdf'\n",
    "        html_file_path = file_path+doi+'.html'\n",
    "        if os.path.exists(nxml_file_path):\n",
    "          myzip.write(nxml_file_path, arcname=doi+'.nxml')\n",
    "        if os.path.exists(pdf_file_path):\n",
    "          myzip.write(pdf_file_path, arcname=doi+'.pdf')\n",
    "        if os.path.exists(html_file_path):\n",
    "          myzip.write(html_file_path, arcname=doi+'.html')\n",
    "          \n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Query Collections, Expressions, Items, and Fragments\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def get_expression_by_doi(self, doi):\n",
    "    \"\"\"Returns a ScientificKnowledgeExpression object for a given DOI.\"\"\"\n",
    "    self.start_session()\n",
    "    query = self.session.query(SKE) \\\n",
    "        .filter(SKE.id==SKE_XREF.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_XREF.xref=='doi:'+doi)\n",
    "    ske = query.first()\n",
    "    return ske\n",
    "\n",
    "  def list_items_for_expression(self, doi):\n",
    "    \"\"\"Returns all items for an expression for a given DOI.\"\"\"\n",
    "    self.start_session()\n",
    "    if doi[:4] != 'doi:':\n",
    "      doi = 'doi:'+doi\n",
    "    query = self.session.query(SKE, SKI) \\\n",
    "          .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "          .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "          .filter(SKE.id == doi)\n",
    "    l_it = []\n",
    "    for (ex, it) in query.all():\n",
    "      l_it.append(it)\n",
    "    return l_it\n",
    "\n",
    "  def list_collections(self, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if search_term:\n",
    "      q = (self.session.query(SKC) \n",
    "          .filter(SKC.name == search_term))\n",
    "    else:\n",
    "      q = self.session.query(SKC)\n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_expressions(self, collection_id=None, search_term=None): \n",
    "    '''Lists expressions in database. \n",
    "    Including the `collection_id` parameter limits the returned list to the specified collection. \n",
    "    Including the `search_term` parameter searches the citation string for the specified term.'''\n",
    "\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if collection_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKE) \n",
    "            .filter(SKC_HM.ScientificKnowledgeCollection_id==collection_id)\n",
    "            .filter(SKC_HM.has_members_id == SKE.id)\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "         q = (self.session.query(SKE) \n",
    "            .filter(SKC.id == SKC_HM.ScientificKnowledgeCollection_id)\n",
    "            .filter(SKC_HM.has_members_id == SKE.id)\n",
    "            .filter(SKC.id == collection_id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKE) \n",
    "            .filter(SKC_HM.has_members_id == SKE.id)\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(SKE)\n",
    "      \n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_notes_for_fragments_in_paper(self, run_name, paper_id, item_type='FullTextPaper'):\n",
    "    '''returns notes of a specific type associated with fragments from a given paper .'''\n",
    "    q1 = self.session.query(SKI) \\\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "            .filter(SKI.type == item_type) \\\n",
    "            .filter(SKE.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "      return []\n",
    "    for f in i.has_part:\n",
    "      for n in f.has_notes:\n",
    "        if n.name == run_name:\n",
    "          yield(n)\n",
    "         \n",
    "  def list_fragments_for_paper(self, paper_id, item_type, fragment_types=['title', 'abstract']):\n",
    "    '''Loads fragments from a given paper sections of a specified paper from the local database.'''\n",
    "    q1 = self.session.query(SKI) \\\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "            .filter(SKI.type == item_type) \\\n",
    "            .filter(SKE.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "      return []\n",
    "    fragments = []\n",
    "    for f in i.has_part:\n",
    "      if f.type in fragment_types:\n",
    "        fragments.append(f)\n",
    "    return sorted(fragments, key=lambda f: f.offset)\n",
    "  \n",
    "  def list_fragments(self, expression_id=None, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if expression_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKF) \n",
    "            .filter(SKE_HR.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id)\n",
    "            .filter(SKI_HP.has_part_id == SKF.id)\n",
    "            .filter(SKF.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = (self.session.query(SKF) \n",
    "            .filter(SKE_HR.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id)\n",
    "            .filter(SKI_HP.has_part_id == SKF.id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKF) \n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id)\n",
    "            .filter(SKI_HP.has_part_id == SKF.id)\n",
    "            .filter(SKF.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(SKF)\n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Build RAG Indexes of documents\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "  def embed_expression_list(self, explist):\n",
    "    '''Runs through the list of expressions, generates embeddings, and stores them in the database'''\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for e in tqdm(explist):\n",
    "      coll_ids = ','.join([c.id for c in e.member_of])\n",
    "\n",
    "      t, a = self.list_fragments_for_paper(e.id, 'CitationRecord')\n",
    "      i = t.part_of\n",
    "      tiab_text = t.content+'\\n\\n'+a.content\n",
    "\n",
    "      if len(tiab_text) > 0:\n",
    "        texts.append(tiab_text)\n",
    "        metadatas.append({'c_ids': coll_ids, \\\n",
    "                                  'e_id': e.id, \\\n",
    "                                  'e_type': e.type, \\\n",
    "                                  'i_id': t.part_of, \\\n",
    "                                  'i_type': 'CitationRecord', \\\n",
    "                                  'f_id': t.id, \\\n",
    "                                  'f_type': t.type+','+a.type, \\\n",
    "                                  'citation': e.content})\n",
    "\n",
    "      jats_frgs = self.list_fragments_for_paper(e.id, 'JATSFullText', ['section'])\n",
    "      for f in jats_frgs:\n",
    "        texts.append(f.content)\n",
    "        metadatas.append({'c_ids': coll_ids, \\\n",
    "                               'e_id': e.id, \\\n",
    "                               'e_type': e.type, \\\n",
    "                               'i_id': f.part_of, \\\n",
    "                               'i_type': 'JATSFullText', \\\n",
    "                               'f_id': f.id, \\\n",
    "                               'citation': e.content})        \n",
    "\n",
    "      pdf_frgs = self.list_fragments_for_paper(e.id, 'PDFFullText', ['section'])\n",
    "      for f in pdf_frgs:\n",
    "        texts.append(f.content)\n",
    "        metadatas.append({'c_ids': coll_ids, \\\n",
    "                              'e_id': e.id, \\\n",
    "                              'e_type': e.type, \\\n",
    "                              'i_id': f.part_of, \\\n",
    "                              'i_type': 'PDFFullText', \\\n",
    "                              'f_id': f.id, \\\n",
    "                              'citation': e.content})\n",
    "\n",
    "    # Use this when we want to index text on individual sentences       \n",
    "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size = 3000, chunk_overlap=150)\n",
    "    \n",
    "    # build vector indexes from citation record     \n",
    "    #tiab_docs = text_splitter.create_documents(tiab_texts, metadatas=tiab_metadatas)\n",
    "    docs = []\n",
    "    for t,m in zip(texts, metadatas):\n",
    "      docs.append(Document(page_content=t, metadata=m))\n",
    "    \n",
    "    print('Indexing %d documents'%(len(docs)))\n",
    "    db = PGVector.from_documents(\n",
    "        embedding=self.embed_model,\n",
    "        documents=docs,\n",
    "        collection_name=\"ScienceKnowledgeItem\"\n",
    "    )\n",
    "\n",
    "  def query_vectorindex(self, query_string, entity_type='ScienceKnowledgeItem', k=4):\n",
    "    db2 = PGVector.from_existing_index(\n",
    "      embedding=self.embed_model, \n",
    "      collection_name=entity_type) \n",
    "    docs = db2.similarity_search_with_score(query_string, k=k)\n",
    "    return docs\n",
    "  \n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Retrieve and manipulate raw vectors\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def list_unindexed_skes(self, collection_id):\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT ske.id\n",
    "        FROM \"ScientificKnowledgeExpression\" as ske\n",
    "            JOIN \"ScientificKnowledgeCollection_has_members\" as skc_hm on ske.id = skc_hm.has_members_id\n",
    "            LEFT JOIN langchain_pg_embedding as emb on emb.cmetadata->>'e_id' = ske.id\n",
    "        WHERE emb.cmetadata->>'e_id' IS NULL AND\n",
    "            skc_hm.\"ScientificKnowledgeCollection_id\"='{}';\n",
    "        \"\"\".format(collection_id)\n",
    "    skes = []\n",
    "    for ske_id in self.session.execute(text(query)).fetchall():\n",
    "      skes.extend(self.session.query(SKE).filter(SKE.id==ske_id[0]).all())\n",
    "    return skes\n",
    "  \n",
    "  def list_embedded_skes_for_collection_by_fragment_type(self, collection_id, f_type='title,abstract'):\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT ske.id, emb.uid, emb.cmetadata->>'f_id'\n",
    "        FROM langchain_pg_embedding as emb, \n",
    "            \"ScientificKnowledgeExpression\" as ske,\n",
    "            \"ScientificKnowledgeCollection_has_members\" as skc_hm\n",
    "        WHERE emb.cmetadata->>'f_type' = '{}}' AND\n",
    "            emb.cmetadata->>'e_id' = ske.id AND \n",
    "            ske.id = skc_hm.has_members_id AND\n",
    "            skc_hm.\"ScientificKnowledgeCollection_id\"='{}';\n",
    "        \"\"\".format(f_type, collection_id)\n",
    "    rag_data = self.session.execute(text(query)).fetchall()\n",
    "    return rag_data\n",
    "\n",
    "  def delete_repeated_embeddings_for_collection_by_fragment_type(self, collection_id, f_type='title,abstract'):\n",
    "    rag_data = self.list_embedded_skes_for_collection_by_fragment_type(collection_id, f_type=f_type)\n",
    "    checked_dois = []\n",
    "    to_remove = []\n",
    "    for tup in rag_data:\n",
    "        if tup[2] not in checked_dois:\n",
    "            checked_dois.append(tup[2])\n",
    "        else:\n",
    "            to_remove.append(tup[1])\n",
    "    for Uuid in to_remove:\n",
    "      self.session.execute(text('DELETE FROM langchain_pg_embedding WHERE uuid=:uuid'), {'uuid':Uuid})\n",
    "    self.session.commit()\n",
    "\n",
    "  def retrieve_vectors_for_collection_by_item_type(self, collection_id, i_type='CitationRecord'):\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT ske.id, emb.embedding \n",
    "        FROM langchain_pg_embedding as emb, \n",
    "            \"ScientificKnowledgeExpression\" as ske,\n",
    "            \"ScientificKnowledgeCollection_has_members\" as skc_hm\n",
    "        WHERE emb.cmetadata->>'i_type' = '{}}' AND\n",
    "            emb.cmetadata->>'e_id' = ske.id AND \n",
    "            ske.id = skc_hm.has_members_id AND\n",
    "            skc_hm.\"ScientificKnowledgeCollection_id\"='{}';\n",
    "        \"\"\".format(i_type, collection_id)\n",
    "    rag_data = self.session.execute(text(query)).fetchall()\n",
    "    return rag_data\n",
    "\n",
    "  def retrieve_vectors_for_collection_by_fragment_type(self, collection_id, f_type='title,abstract'):\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT ske.id, ske.content, emb.embedding \n",
    "        FROM langchain_pg_embedding as emb, \n",
    "            \"ScientificKnowledgeExpression\" as ske,\n",
    "            \"ScientificKnowledgeCollection_has_members\" as skc_hm\n",
    "        WHERE emb.cmetadata->>'f_type' = '{}}' AND\n",
    "            emb.cmetadata->>'e_id' = ske.id AND \n",
    "            ske.id = skc_hm.has_members_id AND\n",
    "            skc_hm.\"ScientificKnowledgeCollection_id\"='{}';\n",
    "        \"\"\".format(f_type, collection_id)\n",
    "    rag_data = self.session.execute(text(query)).fetchall()\n",
    "    return rag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_information_content_entity_iri(ice, id_prefix):\n",
    "    \"\"\"Reads an identifier for a given prefix\"\"\"\n",
    "    idmap = {k[:k.find(':')]:k[k.find(':')+1:] for k in ice.xref} \n",
    "    return idmap.get(id_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
