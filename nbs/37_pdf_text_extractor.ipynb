{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Text Extractor Utility\n",
    "\n",
    "> Extracts unstructured text from scientific papers published as PDF files ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.pdf_research_article_text_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import dataclasses\n",
    "from langchain.document_loaders.pdf import BasePDFLoader\n",
    "from langchain.document_loaders.base import BaseBlobParser\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.blob_loaders import Blob\n",
    "import re\n",
    "from typing import Optional, List, Iterator, Mapping, Any, Dict\n",
    "from dataclasses import field\n",
    "import requests\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class CumulativeTextFeature:\n",
    "    name:str = ''\n",
    "    d:dict = field(default_factory=dict)\n",
    "\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "        self.d = {}\n",
    "\n",
    "    def add(self, feat:str, txt:str):\n",
    "        if self.d.get(feat) is None:\n",
    "            self.d[feat] = len(txt)\n",
    "        else:\n",
    "            self.d[feat] += len(txt)   \n",
    "\n",
    "    def read_most_common(self):\n",
    "        return max(self.d, key=self.d.get)\n",
    "\n",
    "    def read_next_common(self):\n",
    "        keys = sorted(self.d, key=self.d.get, reverse=True)\n",
    "        if len(keys) > 1:\n",
    "            return keys[1]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def read_distribution(self):\n",
    "        return self.d\n",
    "\n",
    "    def read_total_length(self):\n",
    "        return sum(self.d.values())\n",
    "\n",
    "    def read_normalized_distribution(self):\n",
    "        tl = self.read_total_length()\n",
    "        return {k:self.d[k]/tl for k in self.d.keys()}\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LAPDF_FeatureBlock():\n",
    "    \"\"\"A block of text with spatial features occuring in a PDF full-text article.\"\"\"\n",
    "\n",
    "    page: int\n",
    "\n",
    "    x0:float\n",
    "    x1:float\n",
    "    midx:float\n",
    "    width:float\n",
    "\n",
    "    y0:float\n",
    "    y1:float\n",
    "    midy:float\n",
    "    height:float\n",
    "    \n",
    "    nlines:int\n",
    "    sizes:CumulativeTextFeature = field(default_factory=CumulativeTextFeature)\n",
    "    fonts:CumulativeTextFeature = field(default_factory=CumulativeTextFeature)\n",
    "    \n",
    "    text:str = ''\n",
    "    t:str = ''\n",
    "\n",
    "    def __init__(self, p:int, x0:float, y0:float, x1:float, y1:float, text:str, \n",
    "                 nlines:int, sizes:dict, fonts:dict, pos_err:float=0.05):\n",
    "        self.x0 = x0\n",
    "        self.y0 = y0\n",
    "        self.x1 = x1\n",
    "        self.y1 = y1\n",
    "        self.midx = (x0 + x1)/2.0\n",
    "        self.width = x1 - x0\n",
    "        self.midy = (y0 + y1)/2.0\n",
    "        self.height = y1 - y0\n",
    "        self.page = p\n",
    "        self.text = text\n",
    "        self.nlines = nlines\n",
    "        self.t = re.sub('\\-\\n','',self.text)\n",
    "        self.t = re.sub('\\n',' ',self.text)\n",
    "        self.t = re.sub('\\s+',' ',self.text)\n",
    "        self.sizes = sizes\n",
    "        self.fonts = fonts\n",
    "        self.pos_err = pos_err\n",
    "\n",
    "    def generate_layout_features(self, min_x0:float, min_y0:float, max_x1:float, max_y1:float):\n",
    "        d_center = (min_x0 + max_x1)/2.0 - self.midx\n",
    "        d_rcenter = (min_x0 + max_x1)/2.0 - self.x1\n",
    "        d_lcenter = self.x0 - (min_x0 + max_x1)/2.0 \n",
    "        d_lmargin = min_x0 - self.x0\n",
    "        d_rmargin = self.x1 - max_x1\n",
    "        d_top = self.x0 - min_y0 \n",
    "        d_bottom = max_y1 - self.x0\n",
    "        top_size = self.sizes.read_most_common()\n",
    "        dist = self.sizes.read_normalized_distribution()\n",
    "        size_prop = dist.get(top_size)\n",
    "        top_font = self.fonts.read_most_common()\n",
    "        dist2 = self.fonts.read_normalized_distribution()\n",
    "        font_prop = dist2.get(top_font)\n",
    "        return {\n",
    "            'page': self.page,\n",
    "            'd_center': d_center,\n",
    "            'd_rcenter': d_rcenter,\n",
    "            'd_lcenter': d_lcenter, \n",
    "            'd_lmargin': d_lmargin,\n",
    "            'd_rmargin': d_rmargin,\n",
    "            'd_top': d_top,\n",
    "            'd_bottom': d_bottom,\n",
    "            'top_size': top_size,\n",
    "            'size_prop': size_prop,\n",
    "            'width': self.width,\n",
    "            'height': self.height,\n",
    "            'nlines': self.nlines,\n",
    "            'top_font': top_font,\n",
    "            'font_prop': font_prop\n",
    "        }\n",
    "           \n",
    "class LAPDFBlockLoader(BasePDFLoader):\n",
    "    \"\"\"Load `PDF` files using `PyMuPDF` into representative .\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize with a file path.\"\"\"\n",
    "        try:\n",
    "            import fitz  # noqa:F401\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"`PyMuPDF` package not found, please install it with \"\n",
    "                \"`pip install pymupdf`\"\n",
    "            )\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def load(self, **kwargs: Optional[Any]) -> List[Document]:\n",
    "        \"\"\"Load file.\"\"\"\n",
    "\n",
    "        parser = LAPDFBlockParser(text_kwargs=kwargs)\n",
    "        blob = Blob.from_path(self.file_path)\n",
    "        blocks = parser.parse(blob)\n",
    "        return blocks\n",
    "\n",
    "class LAPDFBlockParser(BaseBlobParser):\n",
    "    \"\"\"Parse `PDF` using `PyMuPDF`.\"\"\"    \n",
    "\n",
    "    def __init__(self, text_kwargs: Optional[Mapping[str, Any]] = None) -> None:\n",
    "        \"\"\"Initialize the parser.\n",
    "\n",
    "        Args:\n",
    "            text_kwargs: Keyword arguments to pass to ``fitz.Page.get_text()``.\n",
    "        \"\"\"\n",
    "        self.text_kwargs = text_kwargs or {}\n",
    "\n",
    "    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n",
    "        \"\"\"Lazily parse the blob.\"\"\"\n",
    "        import fitz\n",
    "        #sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        pos_err = 0.05\n",
    "        \n",
    "        with blob.as_bytes_io() as file_path:\n",
    "\n",
    "            doc = fitz.open(file_path)  # open document\n",
    "\n",
    "            feature_blocks = []\n",
    "            dsizes = CumulativeTextFeature('font_sizes')\n",
    "            dfonts = CumulativeTextFeature('fonts')\n",
    "            all_spans = []\n",
    "            for page in doc:\n",
    "                d = page.get_textpage().extractDICT()\n",
    "                for block in d.get('blocks'):\n",
    "                    bfonts = CumulativeTextFeature('block_font')\n",
    "                    bsizes = CumulativeTextFeature('block_font_size')\n",
    "                    btext = ''\n",
    "                    last_size = 0\n",
    "                    for line in block.get('lines'):\n",
    "                        for span in line.get('spans'):\n",
    "                            all_spans.append(span)\n",
    "\n",
    "                            text = span.get('text','')\n",
    "                            text_size = span.get('size',0)\n",
    "                            dsizes.add(text_size, text)\n",
    "                            bsizes.add(text_size, text)\n",
    "\n",
    "                            # if the text size changes more than 5%, add a space \n",
    "                            if abs(text_size - last_size) > pos_err:\n",
    "                                btext += ' '\n",
    "                            btext += text\n",
    "                            last_size = text_size                             \n",
    "                            \n",
    "                            text_font = span.get('font',0)\n",
    "                            dfonts.add(text_font, text)\n",
    "                            bfonts.add(text_font, text)\n",
    "\n",
    "                    pfb = LAPDF_FeatureBlock(\n",
    "                        p=page.number,\n",
    "                        x0=block.get('bbox')[0],\n",
    "                        y0=block.get('bbox')[1],\n",
    "                        x1=block.get('bbox')[2],\n",
    "                        y1=block.get('bbox')[3],\n",
    "                        text=btext,\n",
    "                        nlines=len(block.get('lines')),\n",
    "                        sizes=bsizes,\n",
    "                        fonts=bfonts\n",
    "                    )\n",
    "                    feature_blocks.append(pfb)\n",
    "                    \n",
    "            # VERY SIMPLE APPROACH TO START WITH\n",
    "            # Find the most common font size and use that to estimate the \n",
    "            # bounds of the page to exclude headers and footers\n",
    "            #\n",
    "            # Want to add a structured model to predict type of \n",
    "            # text blocks in the document  \n",
    "\n",
    "            most_common_font = dfonts.read_most_common()\n",
    "            most_common_size = dsizes.read_most_common()\n",
    "\n",
    "            min_x0 = min([sp.get('bbox')[0]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "            min_y0 = min([sp.get('bbox')[1]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "            max_x1 = max([sp.get('bbox')[2]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "            max_y1 = max([sp.get('bbox')[3]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "\n",
    "            # Ideally, we would like to be able to classify the blocks into different types\n",
    "            # For now, we will read the blocks in order unless their text seems to confirm to \n",
    "            # regexes to detect figure + table captions\n",
    "\n",
    "            text = ''\n",
    "            last_block = None   \n",
    "            figure_regex = re.compile(r'^Fig(ure){0,1}\\s*\\d+\\s*:', re.IGNORECASE)\n",
    "            table_regex = re.compile(r'^Tab(le){0,1}\\s*\\d+\\s*:', re.IGNORECASE)\n",
    "            figs = []\n",
    "            tables = []\n",
    "\n",
    "            for pfb in feature_blocks:\n",
    "\n",
    "                if pfb.x0 < min_x0 or pfb.y0 < min_y0 or pfb.x1 > max_x1 or pfb.y1 > max_y1:\n",
    "                    continue    \n",
    "\n",
    "                if figure_regex.match(pfb.t):\n",
    "                    figs.append(pfb)\n",
    "                    continue\n",
    "\n",
    "                if table_regex.match(pfb.t):\n",
    "                    tables.append(pfb)\n",
    "                    continue\n",
    "                \n",
    "                if last_block is not None and \\\n",
    "                        (last_block.page != pfb.page or last_block.y1 < pfb.y0 - pos_err):\n",
    "                    text += '\\n\\n'\n",
    "                \n",
    "                text += pfb.t\n",
    "                last_block = pfb\n",
    "\n",
    "            for pfb in figs:\n",
    "                text += '\\n\\n' + pfb.t\n",
    "\n",
    "            for pfb in tables:\n",
    "                text += '\\n\\n' + pfb.t\n",
    "\n",
    "        return Document(page_content=text, metadata=self.text_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class HuridocsPDFLoader(BasePDFLoader):\n",
    "    \"\"\"Load `PDF` files using `Huridocs` (https://github.com/huridocs/pdf_paragraphs_extraction).\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize with a file path.\"\"\"\n",
    "        try:\n",
    "            # check that the server's up and running\n",
    "            url = 'http://localhost:5051/info'\n",
    "            r = requests.get(url)\n",
    "        except Exception as e:\n",
    "            raise Exception(\n",
    "                \"Huridocs `pdf_paragraphs_extraction` service not running. \" + \\\n",
    "                \"Please download it and run it from `https://github.com/GullyBurns/pdf_paragraphs_extraction`\"\n",
    "            )\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def load(self, **kwargs: Optional[Any]) -> List[Document]:\n",
    "        \"\"\"Load file.\"\"\"\n",
    "\n",
    "        parser = HuridocsPDFParser(text_kwargs=kwargs)\n",
    "        blob = Blob.from_path(self.file_path)\n",
    "        blocks = parser.parse(blob)\n",
    "        return blocks\n",
    "\n",
    "class HuridocsPDFParser(BaseBlobParser):\n",
    "    \"\"\"Parse `PDF` using `Huridocs` (https://github.com/huridocs/pdf_paragraphs_extraction).\"\"\"    \n",
    "\n",
    "    def __init__(self, text_kwargs: Optional[Mapping[str, Any]] = None) -> None:\n",
    "        \"\"\"Initialize the parser.\"\"\"\n",
    "        self.text_kwargs = text_kwargs or {}\n",
    "\n",
    "    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n",
    "        \"\"\"Lazily parse the blob.\"\"\"\n",
    "        import fitz\n",
    "        #sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "        with blob.as_bytes_io() as file_path:\n",
    "\n",
    "            # Use fitz to compute the document margins to exclude \n",
    "            # headers, footnotes, and page numbers\n",
    "            try:\n",
    "                doc = fitz.open(file_path)  # open document\n",
    "\n",
    "                dsizes = CumulativeTextFeature('font_sizes')\n",
    "                all_spans = []\n",
    "                for page in doc:\n",
    "                    d = page.get_textpage().extractDICT()\n",
    "                    for block in d.get('blocks'):\n",
    "                        for line in block.get('lines'):\n",
    "                            for span in line.get('spans'):\n",
    "                                text = span.get('text','')\n",
    "                                if re.match('^[\\d\\s]+$', text): \n",
    "                                    continue\n",
    "                                all_spans.append(span)\n",
    "                                text_size = span.get('size',0)\n",
    "                                dsizes.add(text_size, text)\n",
    "                        \n",
    "                most_common_size = dsizes.read_most_common()\n",
    "                min_x0 = min([sp.get('bbox')[0]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "                min_y0 = min([sp.get('bbox')[1]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "                max_x1 = max([sp.get('bbox')[2]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "                max_y1 = max([sp.get('bbox')[3]  for sp in all_spans if sp.get('size') == most_common_size])\n",
    "            except Exception as e:\n",
    "                # If fitz fails, use the Huridocs service to extract the text\n",
    "                min_x0 = 0\n",
    "                min_y0 = 0\n",
    "                max_x1 = 10000\n",
    "                max_y1 = 10000 \n",
    "\n",
    "            url = 'http://localhost:5051'\n",
    "            files = {'file': file_path}\n",
    "            r = requests.post(url, files=files)\n",
    "            paragraphs = r.json().get('paragraphs', [])\n",
    "            main_types = ['LIST', 'TITLE', 'TEXT', 'FORMULA']\n",
    "            title = None\n",
    "            docs = []\n",
    "            text = ''\n",
    "            for p in paragraphs:\n",
    "                pg = p.get('page_number', 0)\n",
    "                x0 = p.get('left',0)\n",
    "                x1 = x0 + p.get('width',0)\n",
    "                y0 = p.get('top',0)\n",
    "                y1 = y0 + p.get('height',0)\n",
    "                if y0<min_y0-5.0 or y1>max_y1+5.0:\n",
    "                    continue\n",
    "                if p.get('type') not in main_types:\n",
    "                    continue\n",
    "                if p.get('type') == 'TITLE':\n",
    "                    if title is not None:\n",
    "                        docs.append(Document(page_content=text, metadata=self.text_kwargs))\n",
    "                        text = ''\n",
    "                    title = p.get('text')\n",
    "                text += p.get('text') + '\\n'\n",
    "            for p in paragraphs:\n",
    "                if p.get('type') != 'FIGURE':\n",
    "                    continue\n",
    "                docs.append(Document(page_content=p.get('text')+'\\n', metadata=self.text_kwargs))\n",
    "            for p in paragraphs:\n",
    "                if p.get('type') != 'TABLE':\n",
    "                    continue\n",
    "                docs.append(Document(page_content=p.get('text')+'\\n', metadata=self.text_kwargs))\n",
    "            return docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
