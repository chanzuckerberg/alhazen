{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Tools  \n",
    "\n",
    " > A library of classes that provide query access to a number of online academic search services including (Pubmed, PMC, and European PMC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.searchEngineUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import alhazen.schema_python as linkml_py\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.utils.web_robot import retrieve_full_text_links_from_biorxiv, \\\n",
    "                retrieve_pdf_from_doidotorg, \\\n",
    "                get_html_from_pmc_doi\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "import datetime\n",
    "from enum import Enum\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pyalex import Works, Authors, Sources, Institutions, Concepts, Publishers, Funders, config\n",
    "import re\n",
    "import requests\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "PAGE_SIZE = 10000\n",
    "TIME_THRESHOLD = 0.3333334\n",
    "\n",
    "class NCBI_Database_Type(Enum):\n",
    "  \"\"\"\n",
    "  Simple enumeration of the different NCBI databases supported by this tool\n",
    "  \"\"\"\n",
    "  pubmed = 'pubmed'\n",
    "  PMC = 'PMC'\n",
    "\n",
    "class ESearchQuery:\n",
    "  \"\"\"\n",
    "  Class to provide query interface for ESearch (i.e., query terms in elaborate ways, return a list of ids)\n",
    "  Each instance of this class executes queries of a given type\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, api_key=None, oa=False, db='pubmed'):\n",
    "    \"\"\"\n",
    "    :param api_key: API Key for NCBI EUtil Services \n",
    "    :param oa: Is this query searching for open access papers? \n",
    "    :param db: The database being queried\n",
    "    \"\"\"\n",
    "    self.api_key = api_key\n",
    "    self.idPrefix = ''\n",
    "    self.oa = oa\n",
    "    self.db = db\n",
    "  \n",
    "  def build_query_tuples(self, df, check_threshold, name_col, terms_col, sep):\n",
    "    '''\n",
    "    Given a dataframe defining a set of 'OR' queries (i.e., entirely broken into | clauses), \n",
    "    check each indvidual term in Pubmed and return complete queries with problematic terms \n",
    "    stripped.\n",
    "    '''\n",
    "    query_tuples = []\n",
    "    phrase_counts = []\n",
    "    for ind in df.index:\n",
    "      search_l = []\n",
    "      terms_to_check = [df[name_col][ind]]\n",
    "      terms_to_check.extend(df[terms_col][ind].split('|'))\n",
    "      for s in terms_to_check: \n",
    "        go_no_go, phrase, count = _check_query_phrase(self, s.strip())\n",
    "        print(go_no_go, phrase, count)\n",
    "        if go_no_go:\n",
    "          search_l.append(phrase)\n",
    "          sleep(0.10)\n",
    "          if count>check_threshold:\n",
    "            phrase_counts.append((phrase, count))\n",
    "      query_tuples.append( (ind, df[name_col][ind], ' OR '.join(search_l)) )\n",
    "    return query_tuples, phrase_counts\n",
    "  \n",
    "  def _check_query_phrase(self, phrase):\n",
    "    \"\"\"\n",
    "    Checks whether a phrase would work on Pubmed or would be expanded (which can lead to unpredictable errors). \n",
    "    Use this as a check for synonyms.   \n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    m1 = re.match('^[a-zA-Z0-9]{1,5}$', phrase)\n",
    "    if m1 is not None:\n",
    "      return False, 'Abbreviation', 0\n",
    "\n",
    "    m2 = re.search('[(\\)]', phrase)\n",
    "    if m2 is not None:\n",
    "      return False, 'Brackets', 0\n",
    "\n",
    "    m3 = re.search('[\\,\\;]', phrase)\n",
    "    if m3 is not None:\n",
    "      phrase = '(\"' + '\" AND \"'.join(re.split('[\\,\\;]', phrase.strip()))+'\")'\n",
    "\n",
    "    if self.api_key is not None: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else:\n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db='+self.db + '&term='\n",
    "    url =  esearch_stem + quote('\"'+phrase+'\"')\n",
    "\n",
    "    esearch_response = urlopen(url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count = int(esearch_soup.find('Count').string)\n",
    "    #n_translations = len(esearch_soup.find('TranslationStack').findAll('TermSet'))\n",
    "    phrase_not_found = esearch_soup.find('PhraseNotFound')\n",
    "    quoted_phrase_not_found = esearch_soup.find('QuotedPhraseNotFound')\n",
    "    if phrase_not_found is not None or quoted_phrase_not_found is not None:\n",
    "      return False, '\"'+phrase+'\" not found', 0\n",
    "    if count == 0:\n",
    "      return False, phrase, count\n",
    "    return True, phrase, count        \n",
    "\n",
    "  def execute_count_query(self, query):\n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    if self.oa:\n",
    "      if self.db == NCBI_Database_Type.PMC:\n",
    "        query = '\"open access\"[filter] AND (' + query + ')'\n",
    "        idPrefix = 'PMC'\n",
    "      elif self.db == NCBI_Database_Type.pubmed:\n",
    "        query = '\"loattrfree full text\"[sb] AND (' + query + ')'\n",
    "    if self.api_key: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else:\n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db='+self.db + '&term='\n",
    "    query = quote_plus(query)\n",
    "    esearch_response = urlopen(esearch_stem + query)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count_tag = esearch_soup.find('Count')\n",
    "    if count_tag is None:\n",
    "      raise Exception('No Data returned from \"' + self.query + '\"')\n",
    "    return int(count_tag.string)\n",
    "\n",
    "  def execute_query_on_website(self, q, pm_order='relevance'):\n",
    "    \"\"\"\n",
    "    Executes a query on the Pubmed database and returns papers in order of relevance or date.\n",
    "    This is important to determine accuracy of complex queries are based on the composition of the first page of results.  \n",
    "    \"\"\"\n",
    "    query = 'https://pubmed.ncbi.nlm.nih.gov/?format=pmid&size=10&term='+re.sub('\\s+','+',q)\n",
    "    if pm_order == 'date':\n",
    "      query += '&sort=date'\n",
    "    response = urlopen(query)\n",
    "    data = response.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(data, \"lxml-xml\")\n",
    "    pmids = re.split('\\s+', soup.find('body').text.strip())\n",
    "    return pmids\n",
    "\n",
    "  def execute_query(self, query):\n",
    "    \"\"\"\n",
    "    Executes a query on the eutils service and returns data as a Pandas Dataframe\n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    if self.oa:\n",
    "      if self.db == NCBI_Database_Type.PMC:\n",
    "        query = '\"open access\"[filter] AND (' + query + ')'\n",
    "        idPrefix = 'PMC'\n",
    "      elif self.db == NCBI_Database_Type.pubmed:\n",
    "        query = '\"loattrfree full text\"[sb] AND (' + query + ')'\n",
    "\n",
    "    if self.api_key: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=' + self.db + '&term='\n",
    "\n",
    "    query = quote_plus(query)\n",
    "    print(esearch_stem + query)\n",
    "    esearch_response = urlopen(esearch_stem + query)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count_tag = esearch_soup.find('Count')\n",
    "    if count_tag is None:\n",
    "        raise Exception('No Data returned from \"' + query + '\"')\n",
    "    count = int(count_tag.string)\n",
    "\n",
    "    latest_time = time()\n",
    "\n",
    "    ids = []\n",
    "    for i in tqdm(range(0,count,PAGE_SIZE)):\n",
    "      full_query = esearch_stem + query + '&retstart=' + str(i)+ '&retmax=' + str(PAGE_SIZE)\n",
    "      esearch_response = urlopen(full_query)\n",
    "      esearch_data = esearch_response.read().decode('utf-8')\n",
    "      esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "      for pmid_tag in esearch_soup.find_all('Id') :\n",
    "        ids.append(self.idPrefix + pmid_tag.text)\n",
    "      delta_time = time() - latest_time\n",
    "      if delta_time < TIME_THRESHOLD :\n",
    "        sleep(TIME_THRESHOLD - delta_time)\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EFetchQuery:\n",
    "    \"\"\"\n",
    "    Class to provide query interface for EFetch (i.e., query based on a list of ids)\n",
    "    Each instance of this class executes queries of a given type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key=None, db='pubmed'):\n",
    "        \"\"\"\n",
    "        :param api_key: API Key for NCBI EUtil Services \n",
    "        :param oa: Is this query searching for open access papers? \n",
    "        :param db: The database being queried\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.db = db\n",
    "\n",
    "    def execute_efetch(self, id):\n",
    "        \"\"\"\n",
    "        Executes a query for a single specific identifier\n",
    "        \"\"\"\n",
    "        if self.api_key:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+self.api_key+'&db=pubmed&retmode=xml&id='\n",
    "        else:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id='\n",
    "        efetch_response = urlopen(efetch_stem + str(id))\n",
    "        return self._generate_rows_from_medline_records(efetch_response.read().decode('utf-8'))\n",
    "\n",
    "    def generate_data_frame_from_id_list(self, id_list):\n",
    "        \"\"\"\n",
    "        Executes a query for a list of ID values\n",
    "        \"\"\"\n",
    "        if self.api_key:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+self.api_key+'&db=pubmed&retmode=xml&id='\n",
    "        else:\n",
    "          efetch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id='\n",
    "        page_size = 100\n",
    "        i = 0\n",
    "        url = efetch_stem\n",
    "        efetch_df = pd.DataFrame()\n",
    "        for line in tqdm(id_list):\n",
    "            try:\n",
    "                l = re.split('\\s+', str(line))\n",
    "                pmid = l[0]\n",
    "                i += 1\n",
    "                if i >= page_size:\n",
    "                    efetch_response = urlopen(url)\n",
    "                    df = self._generate_rows_from_medline_records(efetch_response.read().decode('utf-8'))\n",
    "                    efetch_df = efetch_df.append(df)\n",
    "                    url = efetch_stem\n",
    "                    i = 0\n",
    "\n",
    "                if re.search('\\d$', url):\n",
    "                    url += ','\n",
    "                url += pmid.strip()\n",
    "            except URLError as e:\n",
    "                sleep(10)\n",
    "                print(\"URLError({0}): {1}\".format(e.errno, e.strerror))\n",
    "            except TypeError as e2:\n",
    "                pause = 1\n",
    "\n",
    "        if url != efetch_stem:\n",
    "            efetch_response = urlopen(url)\n",
    "            df = self._generate_rows_from_medline_records(efetch_response.read().decode('utf-8'))\n",
    "            efetch_df = efetch_df.append(df)\n",
    "        return efetch_df\n",
    "\n",
    "    def generate_mesh_data_frame_from_id_list(self, id_list):\n",
    "        \"\"\"\n",
    "        Executes a query for MeSH data from a list of ID values\n",
    "        \"\"\"\n",
    "        url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "        if self.api_key:\n",
    "          payload = 'api_key='+self.api_key+'&db=pubmed&retmode=xml&id='\n",
    "        else:\n",
    "          payload = 'db=pubmed&retmode=xml&id='\n",
    "          \n",
    "        headers = {'content-type': 'application/xml'}\n",
    "\n",
    "        page_size = 10000\n",
    "        i = 0\n",
    "        efetch_df = pd.DataFrame()\n",
    "        for line in tqdm(id_list):\n",
    "            try:\n",
    "                l = re.split('\\s+', line)\n",
    "                pmid = l[0]\n",
    "                i += 1\n",
    "                if i >= page_size:\n",
    "                    print('   running query')\n",
    "                    r = requests.post(url, data=payload)\n",
    "                    efetch_data = r.content.decode('utf-8')\n",
    "                    df = self._generate_mesh_rows_from_medline_records(efetch_data)\n",
    "                    efetch_df = efetch_df.append(df)\n",
    "                    payload = 'db=pubmed&retmode=xml&id='\n",
    "                    i = 0\n",
    "\n",
    "                if re.search('\\d$', payload):\n",
    "                    payload += ','\n",
    "                payload += pmid.strip()\n",
    "            except URLError as e:\n",
    "                sleep(10)\n",
    "                print(\"URLError({0}): {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "        if payload[-1] != '=':\n",
    "            r = requests.post(url, data=payload)\n",
    "            efetch_data = r.content.decode('utf-8')\n",
    "            df = self._generate_mesh_rows_from_medline_records(efetch_data)\n",
    "            efetch_df = efetch_df.append(df)\n",
    "\n",
    "        return efetch_df\n",
    "\n",
    "    def _generate_mesh_rows_from_medline_records(self, record):\n",
    "\n",
    "        soup2 = BeautifulSoup(record, \"lxml-xml\")\n",
    "\n",
    "        rows = []\n",
    "        cols = ['PMID','MESH']\n",
    "        for citation_tag in tqdm(soup2.find_all('MedlineCitation')):\n",
    "\n",
    "            pmid_tag = citation_tag.find('PMID')\n",
    "\n",
    "            mesh_labels = []\n",
    "\n",
    "            if pmid_tag is None:\n",
    "                continue\n",
    "\n",
    "            for meshTag in citation_tag.findAll('MeshHeading'):\n",
    "                desc = meshTag.find('DescriptorName')\n",
    "                qual_list = meshTag.findAll('QualifierName')\n",
    "                if len(qual_list)>0:\n",
    "                    mesh_labels.append('%s/%s'%(desc.text.replace('\\n', ' '),'/'.join(q.text.replace('\\n', ' ') for q in qual_list)))\n",
    "                else:\n",
    "                    mesh_labels.append(desc.text.replace('\\n', ' '))\n",
    "            mesh_data = \",\".join(mesh_labels)\n",
    "\n",
    "            rows.append((pmid_tag.text, mesh_data))\n",
    "\n",
    "        df = pd.DataFrame(data=rows, columns=cols)\n",
    "        return df\n",
    "\n",
    "    def _generate_rows_from_medline_records(self, record):\n",
    "\n",
    "        soup2 = BeautifulSoup(record, \"lxml-xml\")\n",
    "\n",
    "        rows = []\n",
    "        cols = ['PMID', 'YEAR', 'PUBLICATION_TYPE', 'TITLE', 'ABSTRACT','MESH','KEYWORDS']\n",
    "        for citation_tag in soup2.find_all('MedlineCitation'):\n",
    "\n",
    "            pmid_tag = citation_tag.find('PMID')\n",
    "            title_tag = citation_tag.find('ArticleTitle')\n",
    "            abstract_txt = ''\n",
    "            for x in citation_tag.findAll('AbstractText'):\n",
    "                if x.label is not None:\n",
    "                    abstract_txt += ' ' + x.label + ': '\n",
    "                abstract_txt += x.text\n",
    "\n",
    "            mesh_labels = []\n",
    "            for meshTag in citation_tag.findAll('MeshHeading'):\n",
    "                desc = meshTag.find('DescriptorName')\n",
    "                qual_list = meshTag.findAll('QualifierName')\n",
    "                if len(qual_list)>0:\n",
    "                    mesh_labels.append('%s/%s'%(desc.text.replace('\\n', ' '),'/'.join(q.text.replace('\\n', ' ') for q in qual_list)))\n",
    "                else:\n",
    "                    mesh_labels.append(desc.text.replace('\\n', ' '))\n",
    "            mesh_data = \",\".join(mesh_labels)\n",
    "\n",
    "            if pmid_tag is None or title_tag is None or abstract_txt == '':\n",
    "                continue\n",
    "\n",
    "            year_tag = citation_tag.find('PubDate').find('Year')\n",
    "\n",
    "            year = ''\n",
    "            if year_tag is not None:\n",
    "                year = year_tag.text\n",
    "\n",
    "            is_review = '|'.join([x.text for x in citation_tag.findAll('PublicationType')])\n",
    "\n",
    "            keyword_data = '|'.join([meshTag.text.replace('\\n', ' ') for meshTag in citation_tag.findAll('Keyword')])\n",
    "\n",
    "            rows.append((pmid_tag.text, year, is_review, title_tag.text, abstract_txt, mesh_data, keyword_data))\n",
    "\n",
    "        df = pd.DataFrame(data=rows, columns=cols)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"\"\"Downloads a file from an URL to a local disk\"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def get_nxml_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    #print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "      print('\\t{} does not exist'.format(doi))\n",
    "      return\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "    \n",
    "    efetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+api_key+'&db=pmc&id='+pmc_id+'&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    #print(efetch_url)\n",
    "    efetch_response = urlopen(efetch_url)\n",
    "    efetch_data = efetch_response.read().decode('utf-8')\n",
    "    xml = BeautifulSoup(efetch_data, \"lxml-xml\")\n",
    "    body_tag = xml.findAll('body')\n",
    "    if body_tag is None:\n",
    "        print('\\t{} no full text as NXML file.'.format(doi))\n",
    "        return    \n",
    "    \n",
    "    file_path = Path(base_file_path + '/' + doi + '.nxml')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(xml))\n",
    "\n",
    "def get_pdf_from_pubmed_doi(doi, base_file_path):    \n",
    "    \"\"\"\n",
    "    Executes a query on the target database and returns a count of papers \n",
    "    \"\"\"\n",
    "    if os.environ.get('NCBI_API_KEY') is None:\n",
    "        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')\n",
    "    api_key = os.environ.get('NCBI_API_KEY')\n",
    "\n",
    "    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'\n",
    "    sleep(0.1)\n",
    "    #print(esearch_url)\n",
    "    esearch_response = urlopen(esearch_url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    id_tag = esearch_soup.find('Id')    \n",
    "    if id_tag is None:\n",
    "        print('\\t{} does not exist'.format(doi))\n",
    "        return\n",
    "      # raise Exception('Could not find \"' + doi + '\" in PMC')\n",
    "    pmc_id = id_tag.string\n",
    "\n",
    "    # OA Dataset \n",
    "    oapi_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id='+pmc_id+'&format=pdf'\n",
    "    sleep(0.1)\n",
    "    print(oapi_url)\n",
    "    oapi_response = urlopen(oapi_url)\n",
    "    oapi_data = oapi_response.read().decode('utf-8')    \n",
    "    oapi_soup = BeautifulSoup(oapi_data, \"lxml-xml\")\n",
    "    \n",
    "    pdf_link_tag = oapi_soup.find('link')\n",
    "    if(pdf_link_tag is None):\n",
    "        print('\\t{} no full text as PDF file.'.format(doi))\n",
    "        return\n",
    "    \n",
    "    pdf_url = pdf_link_tag['href']\n",
    "    if pdf_url.startswith('ftp:'):\n",
    "        pdf_url = pdf_url.replace('ftp:','https:') \n",
    "    file_path = Path(base_file_path + '/' + doi + '.pdf')\n",
    "    parent_dir = file_path.parent\n",
    "    if os.path.exists(parent_dir) is False:\n",
    "        os.makedirs(parent_dir)\n",
    "    download_file(pdf_url, file_path)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuroPMCQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EuroPMCQuery():\n",
    "    \"\"\"\n",
    "    A class that executes search queries on the European PMC API \n",
    "    \"\"\"\n",
    "    def __init__(self, oa=False, db='pubmed'):\n",
    "        \"\"\"\n",
    "        Initialization of the class\n",
    "        :param oa:\n",
    "        \"\"\"\n",
    "        self.oa = oa\n",
    "\n",
    "    def run_empc_query(self, q, page_size=1000, timeout=60, \n",
    "                       extra_columns=[\"id\", \"source\", \"doi\", 'title', 'pubYear', 'abstractText', 'pubType']):\n",
    "        EMPC_API_URL = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize='+str(page_size)+'&synonym=TRUE'\n",
    "        if len(extra_columns)>0:\n",
    "            EMPC_API_URL += '&resultType=core'\n",
    "        url = EMPC_API_URL + '&query=' + q\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        data = json.loads(r.text)\n",
    "        numFound = data['hitCount']\n",
    "        print(url + ', ' + str(numFound) + ' European PMC PAPERS FOUND')\n",
    "        publications = []\n",
    "        fragments = []\n",
    "        selectors = []\n",
    "        cursorMark = '*'\n",
    "        for i in tqdm(range(0, numFound, page_size)):\n",
    "            url = EMPC_API_URL + '&cursorMark=' + cursorMark + '&query=' + q\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            data = json.loads(r.text)\n",
    "            date_format = '%Y-%m-%d'\n",
    "            #print(data)\n",
    "            if data.get('nextCursorMark'):\n",
    "                cursorMark = data['nextCursorMark']\n",
    "            for d in data['resultList']['result']:\n",
    "                pTypes = [t.lower() for t in d.get('pubTypeList',{}).get('pubType',[])]\n",
    "                if d.get('firstPublicationDate'):\n",
    "                    date_obj = datetime.datetime.strptime(d.get('firstPublicationDate'), date_format)\n",
    "                elif d.get('dateOfCreation'):\n",
    "                    date_obj = datetime.strptime(d.get('dateOfCreation'), date_format)\n",
    "                else:\n",
    "                    date_obj = None\n",
    "                xref = ['epmid:%s'%(str(d['id']))]\n",
    "                if d.get('doi'):\n",
    "                    xref.append('doi:'+d.get('doi'))\n",
    "                else:\n",
    "                    continue # skip if no DOI\n",
    "                title = d.get('title','')\n",
    "                abstract = d.get('abstractText','')\n",
    "                content = title+'\\n'+abstract\n",
    "                if 'patent' in pTypes:\n",
    "                    continue\n",
    "                elif 'clinical trial' in pTypes: \n",
    "                    ptype='ClinicalTrial'\n",
    "                elif 'review' in pTypes or 'systematic review' in pTypes or 'systematic-review' in pTypes or 'meta-analysis' in pTypes or 'review-article' in pTypes:\n",
    "                    ptype='ScientificReviewArticle'\n",
    "                elif 'preprint' in pTypes:\n",
    "                    ptype='ScientificPrimaryResearchPreprint'\n",
    "                elif 'journal article' in pTypes or 'research-article' in pTypes:\n",
    "                    ptype='ScientificPrimaryResearchArticle'\n",
    "                elif 'case-report' in pTypes or 'case reports' in pTypes:\n",
    "                    ptype='ClinicalCaseReport'\n",
    "                elif 'practice guideline' in pTypes:\n",
    "                    ptype='ClinicalGuidelines'\n",
    "                elif 'letter' in pTypes or 'comment' in pTypes or 'editorial' in pTypes:\n",
    "                    ptype='ScientificComment'\n",
    "                elif 'published erratum' in pTypes or 'correction' in pTypes or 'retraction of publication' in pTypes: \n",
    "                    ptype='ScientificErrata'\n",
    "                else:\n",
    "                    #print('|'.join(pTypes))\n",
    "                    continue \n",
    "                human_readable_reference = d.get('authorString','') + ' (' + str(date_obj.year) + ') ' + title\n",
    "                p = ScientificKnowledgeExpression(\n",
    "                        id='doi:%s'%(str(d['doi'])), \n",
    "                        iri=['doi:%s'%(str(d['doi']))], \n",
    "                        xref=xref, \n",
    "                        publication_date=date_obj, \n",
    "                        content=human_readable_reference,\n",
    "                        type=ptype)\n",
    "                publications.append(p)\n",
    "                item = ScientificKnowledgeItem(\n",
    "                        id=uuid.uuid4().hex[0:10], \n",
    "                        content=content, \n",
    "                        type='CitationRecord')\n",
    "                p.has_representation.append(item)\n",
    "                item.representation_of = p.id\n",
    "                frg1 = ScientificKnowledgeFragment( \n",
    "                    id=item.id+'.0',\n",
    "                    content=title,\n",
    "                    offset=0,\n",
    "                    length=len(title),\n",
    "                    type='title')\n",
    "                frg1.part_of = item\n",
    "                frg2 = ScientificKnowledgeFragment( \n",
    "                    id=item.id+'.1',\n",
    "                    content=abstract,\n",
    "                    offset=len(title)+1,\n",
    "                    length=len(abstract),\n",
    "                    type='abstract')\n",
    "                frg2.part_of = item\n",
    "                item.has_part = [frg1, frg2]\n",
    "\n",
    "        print(' Returning '+str(len(publications)))\n",
    "        return (numFound, publications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_authors_and_institutions_from_openalex(doi_list):\n",
    "    oa_works = []\n",
    "    for doi in doi_list:\n",
    "        w = Works()[doi]        \n",
    "        oa_works.append(w)\n",
    "    return oa_works\n",
    "\n",
    "# Map the OpenAlex data to the Alhazen schema\n",
    "def load_paper_from_openalex(doi_or_oa_id):\n",
    "\n",
    "    if doi_or_oa_id.startswith('https://doi.org/'):\n",
    "        doi_or_oa_id = re.sub('https://doi.org/', 'doi:', doi_or_oa_id)\n",
    "    elif doi_or_oa_id.startswith('doi:') is False:\n",
    "        doi_or_oa_id = 'doi:'+ doi_or_oa_id\n",
    "    elif doi_or_oa_id.startswith('https://openalex.org/'):\n",
    "        doi_or_oa_id = re.sub('https://openalex.org/', '', doi_or_oa_id)\n",
    "\n",
    "    w = Works()[doi_or_oa_id]\n",
    "\n",
    "    if w is None:\n",
    "        return None\n",
    "\n",
    "    doi = re.sub('https://doi.org/', 'doi:', w.get('doi'))\n",
    "    \n",
    "    authors = [] \n",
    "    orgs = set()\n",
    "\n",
    "    content = ''\n",
    "    first_author = None\n",
    "    last_author = None\n",
    "    middle_author = None\n",
    "\n",
    "    for aa in w['authorships']:\n",
    "        if aa.get('author_position') == 'first':\n",
    "            first_author = aa\n",
    "        elif aa.get('author_position') == 'last':\n",
    "            last_author = aa\n",
    "        elif aa.get('author_position') == 'middle':\n",
    "            middle_author = aa\n",
    "\n",
    "        a = aa.get('author', {})\n",
    "        a_id = a.get('id', '')\n",
    "        a_name = a.get('display_name', '')\n",
    "        orcid = a.get('orcid')\n",
    "        author_ice = Author(id=a_id, name=a_name, type='Author')\n",
    "        authors.append(author_ice)\n",
    "        if orcid:\n",
    "            author_ice.xref.append(orcid)\n",
    "\n",
    "        for inst in aa.get('institutions', []):\n",
    "            inst_id = inst.get('id', '')\n",
    "            inst_country = inst.get('country_code', '')\n",
    "            inst_name = inst.get('display_name', '')\n",
    "            ror = inst.get('ror')        \n",
    "            org_ice = Organization(id=inst_id, \n",
    "                                    name=inst_name,\n",
    "                                    type='Organization',\n",
    "                                    country=[inst_country])\n",
    "            if ror:\n",
    "                org_ice.xref.append(ror)\n",
    "            author_ice.affiliations.append(org_ice)\n",
    "            orgs.add(org_ice)\n",
    "\n",
    "    # First author only    \n",
    "    if first_author and middle_author is None and last_author is None:\n",
    "        content = first_author.get('raw_author_name') + ' (' + str(w.get('publication_year')) + ') ' + w.get('title')\n",
    "    # First and second author only    \n",
    "    elif first_author and middle_author is None and last_author:\n",
    "        content = first_author.get('raw_author_name')+' and '+last_author.get('raw_author_name')+' ('+str(w.get('publication_year'))+') '+w.get('title')\n",
    "    # 'et al.'    \n",
    "    else:\n",
    "        content = first_author.get('raw_author_name')+' et al. ('+str(w.get('publication_year'))+') '+w.get('title')\n",
    "    \n",
    "    xrefs = set()\n",
    "    oa = w.get('ids',{}).get('openalex')\n",
    "    if oa:\n",
    "        xrefs.add(oa)\n",
    "    doi2 = w.get('ids',{}).get('doi')\n",
    "    if doi2:\n",
    "        xrefs.add(doi2)\n",
    "    pmid = w.get('ids',{}).get('pmid')\n",
    "    if pmid:\n",
    "        xrefs.add(pmid)        \n",
    "    pmcid = w.get('ids',{}).get('pmcid')\n",
    "    if pmcid:\n",
    "        xrefs.add(pmcid)\n",
    "\n",
    "    title = w.get('title')\n",
    "    abstract = w['abstract']\n",
    "\n",
    "    # How to match the type of the paper to the Alhazen schema? \n",
    "    e = ScientificKnowledgeExpression(id=doi,    \n",
    "                                 xref=list(xrefs), \n",
    "                                 publication_date=w.get('publication_date'), \n",
    "                                 content=content,\n",
    "                                 type='ScientificPrimaryResearchArticle')\n",
    "    \n",
    "    for a in authors:\n",
    "        e.has_authors.append(a)\n",
    "        a.is_author_of.append(e)\n",
    "\n",
    "    item = ScientificKnowledgeItem(\n",
    "            id=uuid.uuid4().hex[0:10], \n",
    "            content=content, \n",
    "            type='CitationRecord')\n",
    "    e.has_representation.append(item)\n",
    "    item.representation_of = e.id\n",
    "    frg1 = ScientificKnowledgeFragment( \n",
    "        id=item.id+'.0',\n",
    "        content=title,\n",
    "        offset=0,\n",
    "        length=len(title),\n",
    "        type='title')\n",
    "    frg1.part_of = item\n",
    "    frg2 = ScientificKnowledgeFragment( \n",
    "        id=item.id+'.1',\n",
    "        content=abstract,\n",
    "        offset=len(title)+1,\n",
    "        length=len(abstract),\n",
    "        type='abstract')\n",
    "    frg2.part_of = item\n",
    "    item.has_part = [frg1, frg2]\n",
    "\n",
    "    return e\n",
    "\n",
    "# Read a papers references fromv OpenAlex\n",
    "def read_references_from_openalex(doi_or_oa_id):\n",
    "\n",
    "    if doi_or_oa_id.startswith('https://doi.org/'):\n",
    "        doi_or_oa_id = re.sub('https://doi.org/', 'doi:', doi_or_oa_id)\n",
    "    elif doi_or_oa_id.startswith('doi:') is False:\n",
    "        doi_or_oa_id = 'doi:'+ doi_or_oa_id\n",
    "    elif doi_or_oa_id.startswith('https://openalex.org/'):\n",
    "        doi_or_oa_id = re.sub('https://openalex.org/', '', doi_or_oa_id)\n",
    "\n",
    "    w = Works()[doi_or_oa_id]\n",
    "\n",
    "    if w is None:\n",
    "        return None\n",
    "    \n",
    "    referenced_works = w.get('referenced_works')\n",
    "\n",
    "    return referenced_works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full text downloads from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_full_text_paper_for_doi(doi, path, headless=True): \n",
    "    \"\"\"Attempts to download full text for a given DOI if the paper is listed as open access.\"\"\"\n",
    "\n",
    "    if path[-1:] != '/':\n",
    "        path += '/'\n",
    "    nxml_file_path = path+doi+'.nxml'\n",
    "    pdf_file_path = path+doi+'.pdf'\n",
    "    html_file_path = path+doi+'.html'\n",
    "    \n",
    "    if os.path.exists(nxml_file_path) or os.path.exists(pdf_file_path) or os.path.exists(html_file_path):\n",
    "        return True\n",
    "    \n",
    "    # Check if the paper is open access from UnPayWall\n",
    "    dev_email = os.environ.get('DEV_EMAIL', 'gully.burns@chanzuckerberg.com') \n",
    "    stem_url = 'https://api.unpaywall.org/v2/'\n",
    "    resp = requests.get(stem_url+doi+'?email='+dev_email).json()\n",
    "    if resp.get('is_oa', False) is False:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Try NCBI open access NXML first\n",
    "        ft_path = get_nxml_from_pubmed_doi(doi, path)\n",
    "        if ft_path is not None:\n",
    "            return True\n",
    "\n",
    "        # If the DOI starts with, '10.1101/', it's a bioRxiv paper, so go get it there. \n",
    "        if doi.startswith('10.1101/'):\n",
    "            ft_path = retrieve_full_text_links_from_biorxiv(doi, path)\n",
    "            if ft_path is not None:\n",
    "                return True\n",
    "            \n",
    "        # Try NCBI open access PDF\n",
    "        ft_path = get_pdf_from_pubmed_doi(doi, path)\n",
    "        if ft_path is not None:\n",
    "            return True\n",
    "\n",
    "        # Try to get the pdf from the doi\n",
    "        ft_path = retrieve_pdf_from_doidotorg(doi, path, headless=headless)\n",
    "        if ft_path is not None:\n",
    "            return True\n",
    "        \n",
    "        # Finally, try to find an NCBI HTML page for the paper \n",
    "        ft_path = get_html_from_pmc_doi(doi, path)\n",
    "        if ft_path is not None:\n",
    "            return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
