{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated Dataframe Utilities\n",
    "\n",
    " > Library to provide functions to compute statistics and summary information on a dataframe that has been curated by multiple people. This library provides tools for computing Krippendorf Alpha stats and computing merged dataframes across multiple curators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.curatedDataUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.metrics import agreement\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import masi_distance, binary_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CuratedDataUtils:\n",
    "  \"\"\"This class permits generation of curation statistics and merged, consensus dataframes. \n",
    "  \n",
    "  Attributes:\n",
    "    * df: The dataframe being processed\n",
    "    * doc_id_column: column in df that denotes document IDs\n",
    "    * category_column: column in df that denotes curated category\n",
    "    * curator_column: column in df that denotes curator\n",
    "    * docs: the document set being curated\n",
    "    * curators: the curators performing the curation work\n",
    "    * categories: the set of categories being used to annotate the documents \n",
    "    * doc_task: a low-level `nltk` task object\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, df, doc_id_column, category_column, curator_column, distance_function=masi_distance):\n",
    "    self.df = df\n",
    "    self.doc_id_column = doc_id_column\n",
    "    self.category_column = category_column\n",
    "    self.curator_column = curator_column\n",
    "    document_task_data = []\n",
    "    curators = {}\n",
    "    docs = {}\n",
    "    categories = {}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "      category_array = str(row[category_column]).split(',')\n",
    "      item = str(row[doc_id_column])\n",
    "      td_row = (row[curator_column], item, frozenset(category_array))\n",
    "      document_task_data.append(td_row)\n",
    "      if docs.get(item) is None:\n",
    "        docs[item] = 1\n",
    "      else:\n",
    "        docs[item] = docs[item] + 1\n",
    "\n",
    "      if curators.get(row[curator_column]) is None:\n",
    "        curators[row[curator_column]] = 1\n",
    "      else:\n",
    "        curators[row[curator_column]] = curators[row[curator_column]] + 1\n",
    "      for c in str(row[category_column]).split(','):\n",
    "        if categories.get(c) is None:\n",
    "          categories[c] = 1\n",
    "        else:\n",
    "          categories[c] = categories[c] + 1\n",
    "\n",
    "    doc_task = AnnotationTask(distance = distance_function)\n",
    "    doc_task.load_array(document_task_data)\n",
    "    self.docs = docs\n",
    "    self.curators = sorted(curators.keys())\n",
    "    self.categories = categories\n",
    "    self.task = doc_task\n",
    "  \n",
    "  def get_avg_doc_agr(self, item):\n",
    "    temp_list = [] \n",
    "    sum = 0.0\n",
    "    cnt = 0.0\n",
    "    for i in range(len(self.curators)):\n",
    "      for j in range(i):\n",
    "        try:\n",
    "          sum += self.task.agr(self.curators[i], self.curators[j], str(item))\n",
    "          cnt += 1.0\n",
    "        except StopIteration:\n",
    "          # No need to do anything - we get this error if attempting to compute agreement \n",
    "          # between curators where one of them never entered a score. \n",
    "          print('', end = '')\n",
    "    if cnt > 0.0:\n",
    "      avg = sum/cnt\n",
    "    else: \n",
    "      avg = 0.0;\n",
    "    return avg\n",
    "\n",
    "  def get_consensus(self, item):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    best = 0.0 \n",
    "    for i in range(len(self.curators)):\n",
    "      for j in range(i):\n",
    "        try:\n",
    "          agr = self.task.agr(self.curators[i], self.curators[j], str(item))\n",
    "          if agr == 1.0:\n",
    "            l = [x for x in self.task.data if x['coder']==self.curators[i] and x['item']==item]\n",
    "            return list(l[0]['labels'])[0]\n",
    "        except StopIteration:\n",
    "          # No need to do anything - we get this error if attempting to compute agreement \n",
    "          # between curators where one of them never entered a score. \n",
    "          print('', end = '')\n",
    "    return '-'\n",
    "\n",
    "  def get_consensus_per_doc(self):\n",
    "    \"\"\"\n",
    "    Generates DataFrame with new columns for 'AVG_AGREEMENT' and 'CONSENSUS' for each document\n",
    "    \"\"\"\n",
    "    cat_list = sorted(list({c:0 for cc in self.df[self.category_column] for c in str(cc).split(',')}.keys()))\n",
    "    curators = self.df[self.curator_column].unique()\n",
    "\n",
    "    unused_columns = [c for c in self.df.columns if c != self.doc_id_column]\n",
    "    sdf = self.df.drop([c for c in self.df.columns if c != self.doc_id_column], axis=1).drop_duplicates()\n",
    "    sdf = sdf.reset_index(drop=True)\n",
    "\n",
    "    #cat_count_dict = {cc: {c: 0 for c in cat_list} for cc in df.ID_PAPER}\n",
    "    #for row in df.itertuples():\n",
    "    #  for t in row.CATEGORIES.split(','):\n",
    "    #    cat_count_dict[row.ID_PAPER][t] = cat_count_dict.get(row.ID_PAPER).get(t) + 1\n",
    "    #cat_counts = [[cat_count_dict[row.ID_PAPER][c] for c in cat_list ] for row in sdf.itertuples()]\n",
    "    #sdf['CATEGORY_COUNTS'] = cat_counts\n",
    "\n",
    "    sdf['AVG_AGREEMENT'] = [self.get_avg_doc_agr(str(sdf.iloc[i][self.doc_id_column])) for i in range(len(sdf))]\n",
    "    sdf['CONSENSUS'] = [self.get_consensus(str(sdf.iloc[i][self.doc_id_column])) for i in range(len(sdf))]\n",
    "\n",
    "    return sdf\n",
    "\n",
    "  def get_cross_curator_comparison(self):\n",
    "    \"\"\"\n",
    "    Generates table with 1 row per doc + extra columns for each curator\n",
    "    \"\"\"\n",
    "    unused_columns = [c for c in self.df.columns if c != self.doc_id_column]\n",
    "    sdf = df.drop(unused_columns, axis=1).drop_duplicates()\n",
    "    sdf = sdf.reset_index(drop=True)\n",
    "    for c in self.curators:\n",
    "      sdf_temp = self.df.query(self.curator_column+'==\\''+c+'\\'')\n",
    "      sdf_temp[c] = sdf_temp[self.category_column]\n",
    "      sdf_temp = sdf_temp.reset_index(drop=True)\n",
    "      sdf_temp = sdf_temp.drop(unused_columns, axis=1)\n",
    "      sdf = sdf.join(sdf_temp.set_index(self.doc_id_column), \n",
    "                     lsuffix='_1', rsuffix='_2', on=self.doc_id_column, how='outer') \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Some additional distance functions \n",
    "def ordinal_distance(label1, label2):\n",
    "  \"\"\"Krippendorff's ordinal distance metric\n",
    "  Modified from Wikipedia page: https://en.wikipedia.org/wiki/Krippendorff%27s_alpha#Difference_functions\n",
    "  \"\"\"\n",
    "  try:\n",
    "    return pow(sum([g for g in range(label1, label2+1)]) - (label1+label2)/2, 2)\n",
    "  #        return pow(list(label1)[0]-list(label2)[0],2)\n",
    "  except:\n",
    "    print(\"non-numeric labels not supported with ordinal distance\")\n",
    "        \n",
    "def no_maybe_yes_distance(label1, label2):\n",
    "  \"\"\"Simple distance for no / maybe / yes scale used to denote curation task. \n",
    "  \n",
    "  Lookup table\n",
    "  d(0,0) = 0.0\n",
    "  d(1,1) = 0.0\n",
    "  d(2,2) = 0.0\n",
    "  d(0,1) = 3.0\n",
    "  d(1,2) = 1.0\n",
    "  d(0,2) = 5.0\n",
    "  \"\"\"\n",
    "  try:\n",
    "    l1, l2 = sorted([int(list(label1)[0]), int(list(label2)[0])])\n",
    "    if l1 == l2: \n",
    "      return 0.0 \n",
    "    elif (l1==0 and l2==1) or (l2==0 and l1==1):\n",
    "      return 3.0\n",
    "    elif (l1==1 and l2==2) or (l2==1 and l1==2):\n",
    "      return 1.0\n",
    "    elif (l1==0 and l2==2) or (l2==0 and l1==2):\n",
    "      return 5.0\n",
    "  except:\n",
    "    print(\"error\")\n",
    "    print(label1,label2)\n",
    "  print(\"error\")\n",
    "  print(label1,label2)\n",
    "  return 1000.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
