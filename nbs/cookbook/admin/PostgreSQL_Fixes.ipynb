{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL Fixes   \n",
    "\n",
    "> Like any DBMS, PostgreSQL requires careful administration. Our implementation is a little quick-and-dirty, so here, we include functions to fix errors as they arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports\n",
    "\n",
    "Setting python imports, environment variables, and other crucial set up parameters here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen.aliases import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.agent import AlhazenAgent\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.protocol_extraction_tool import *\n",
    "from alhazen.tools.tiab_classifier_tool import *\n",
    "from alhazen.tools.tiab_extraction_tool import *\n",
    "from alhazen.tools.tiab_mapping_tool import *\n",
    "from alhazen.toolkit import *\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database, list_databases\n",
    "\n",
    "from alhazen.utils.searchEngineUtils import *\n",
    "\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import uuid\n",
    "import yaml\n",
    "\n",
    "import local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\n",
    "from rapidfuzz import fuzz\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import local_resources.queries.em_tech as em_tech_queries\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the PostGresQL database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])  \n",
    "loc = os.environ.get('LOCAL_FILE_PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup all databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = os.environ['LOCAL_FILE_PATH']\n",
    "current_date_time = datetime.now()\n",
    "formatted_date_time = f'{current_date_time:%Y-%m-%d-%H-%M-%S}'\n",
    "formatted_date = f'{current_date_time:%Y-%m-%d}'\n",
    "if os.path.exists(loc+'/backups') is False:\n",
    "    os.makedirs(loc+'/backups')\n",
    "for db in list_databases():\n",
    "    print('Backing up database:',db)\n",
    "    backup_path = loc+'/backups/'+db+'_backup_'+formatted_date_time+'.sql'\n",
    "    backup_ceifns_database(db, backup_path)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'em_tech'\n",
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for papers using embeddings\n",
    "q = ldb.session.query(SKE.id) \\\n",
    "        .distinct() \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "        .filter(SKC.id == '3') \\\n",
    "        .filter(or_(SKI.type == 'JATSFullText', SKI.type == 'PDFFullText')) \n",
    "for e_id in q.all():\n",
    "    print(e_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ids = ['2']\n",
    "collections_clause = ' OR '.join(['skc_hm.\"ScientificKnowledgeCollection_id\"=\\'{}\\''.format(coll_id) for coll_id in c_ids])\n",
    "ldb.session.rollback()\n",
    "q2_all = \"\"\"\n",
    "    SELECT DISTINCT ske.id, ske.content, ske.publication_date as pub_date, ske.type as pub_type, emb.embedding, skf.content \n",
    "    FROM langchain_pg_embedding as emb, \n",
    "        \"ScientificKnowledgeExpression\" as ske,\n",
    "        \"ScientificKnowledgeCollection_has_members\" as skc_hm, \n",
    "        \"ScientificKnowledgeFragment\" as skf\n",
    "    WHERE skc_hm.has_members_id = ske.id AND \n",
    "        emb.cmetadata->>'i_type' = 'CitationRecord' AND\n",
    "        emb.cmetadata->>'e_id' = ske.id AND \n",
    "        emb.cmetadata->>'f_id' = skf.id AND ({})\n",
    "    ORDER BY pub_date DESC;\n",
    "    \"\"\".format(collections_clause)\n",
    "\n",
    "ldb.session.execute(text(q2_all)).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a collection based on EMPIAR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "step = 20\n",
    "for start_i in range(0, len(empiar_dois), step):\n",
    "    query = ' OR '.join(['doi:\"'+empiar_dois[i]+'\"' for i in range(start_i, start_i+step)])\n",
    "    addEMPCCollection_tool.run({'id': '3', 'name':'EMPIAR Papers', 'query':query, 'full_text':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_set(x):\n",
    "    out = ''\n",
    "    try:\n",
    "        out = ' '.join(set(x))\n",
    "    except:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "# identify papers that we have full text for in EMPIAR\n",
    "q = ldb.session.query(SKE.id) \\\n",
    "        .distinct() \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "        .filter(SKC.id == '3') \\\n",
    "        .filter(or_(SKI.type == 'JATSFullText', SKI.type == 'PDFFullText')) \n",
    "dois_to_include = [d[0][4:] for d in q.all()]    \n",
    "\n",
    "empiar_gold_standard = []\n",
    "for i, row in empiar_df.iterrows():\n",
    "    if row.doi in dois_to_include:\n",
    "        empiar_gold_standard.append( row.to_dict() )\n",
    "empiar_gold_standard_df = pd.DataFrame(empiar_gold_standard)\n",
    "\n",
    "empiar_gs_df = empiar_gold_standard_df.groupby(['doi']).agg({'sample_preparation_type': join_set, 'agg_state': join_set, 'sample_preparation_buffer_ph': join_set, \n",
    "                                              'grid_model': join_set, 'grid_material': join_set, 'grid_mesh': join_set, \n",
    "                                              'grid_support_topology': join_set, 'grid_pretreatment': join_set, 'grid_vitrification_cryogen': join_set, \n",
    "                                              'grid_vit_ctemp': join_set, 'grid_vit_chumid': join_set}).reset_index()\n",
    "empiar_gs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import papers from DOIs pertaining to CryoET-Portal records `10000-10010`\n",
    "\n",
    "The [CryoET Data portal](https://chanzuckerberg.github.io/cryoet-data-portal/python-api.html) system is based on submitted data to our curation team, accompanied by papers referenced by DOIs. Each dataset is assigned an ID value associated with DOIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the EMPCSearchTool to run a query for the dois mentioned\n",
    "query = ' OR '.join(['doi:\"'+d+'\"' for d_id in dois for d in dois[d_id] ])\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "addEMPCCollection_tool.run(tool_input={'id': '0', 'name':'CryoET Portal (10000-10010)', 'query':query, 'full_text':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Database to include all CryoET papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cols_to_include = ['ID', 'CORPUS_NAME', 'QUERY']\n",
    "df = pd.read_csv(files(em_tech_queries).joinpath('EM_Methods.tsv'), sep='\\t')\n",
    "df = df.drop(columns=[c for c in df.columns if c not in cols_to_include])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QueryTranslator(df.sort_values('ID'), 'ID', 'QUERY', 'CORPUS_NAME')\n",
    "(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=['TITLE_ABS', 'METHODS'])\n",
    "corpus_names = df['CORPUS_NAME']\n",
    "\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "for (id, name, query) in zip(corpus_ids, corpus_names, epmc_queries):\n",
    "    if id != 2:\n",
    "        continue\n",
    "    addEMPCCollection_tool.run(tool_input={'id': id, 'name':name, 'query':query, 'full_text':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine + Sample CryoET + EMPIAR Collections to provide a test set of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_new_collection_from_intersection('4', 'EMPIAR CryoET Papers', '2', '3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_query = '''\n",
    "(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n",
    "    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n",
    "    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet ) AND \n",
    "(\"Machine Learning\" OR \"Artificial Intelligence\" OR \"Deep Learning\" OR \"Neural Networks\")\n",
    "'''\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "addEMPCCollection_tool.run(tool_input={'id': '6', \n",
    "                                       'name': 'Machine Learning in CryoET', \n",
    "                                       'query': ml_query, \n",
    "                                       'full_text': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, DeleteCollectionTool)][0]\n",
    "delCollection_tool.run(tool_input={'collection_id': '6'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break up TIAB of papers into sentences + classify by discourse\n",
    "\n",
    "NOTE - HUGGING FACE MODELS DO NOT WORK WELL ON THIS CORPUS. \n",
    "(NOT SURPRISINGLY - THEY WERE TRAINED ON MEDICAL PAPERS WHERE THE DIFFERENT SECTIONS OF THE PAPER WERE EXPLICITLY LABELED)\n",
    "\n",
    "USE LLMS TO DO THE EXTRACTION - GPT3.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\n",
    "t2.run(tool_input={'collection_id': '5', 'run_label': 'dev'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = '''{\n",
    "\"Background\": \"Eps15-homology domain containing proteins (EHDs) are eukaryotic, dynamin-related ATPases involved in cellular membrane trafficking. They oligomerize on membranes into filaments that induce membrane tubulation. While EHD crystal structures in open and closed conformations were previously reported, little structural information is available for the membrane-bound oligomeric form. Consequently, mechanistic insights into the membrane remodeling mechanism have remained sparse.\",\n",
    "\"Objectives_Methods\": \"Here, by using cryo-electron tomography and subtomogram averaging, we determined structures of nucleotide-bound EHD4 filaments on membrane tubes of various diameters at an average resolution of 7.6 Ã….\",\n",
    "\"Results_Conclusions\": \"Assembly of EHD4 is mediated via interfaces in the G-domain and the helical domain. The oligomerized EHD4 structure resembles the closed conformation, where the tips of the helical domains protrude into the membrane. The variation in filament geometry and tube radius suggests a spontaneous filament curvature of approximately 1/70 nm<sup>-1</sup>. Combining the available structural and functional data, we suggest a model for EHD-mediated membrane remodeling.\"\n",
    "}'''\n",
    "\n",
    "json.loads(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the metadata extraction tool\n",
    "models = ['databricks_dbrx']\n",
    "for m in models:\n",
    "    llm = llms_lookup.get(m)\n",
    "    cb = AlhazenAgent(llm, llm, db_name=db_name)\n",
    "    t2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\n",
    "    t2.run(tool_input={'collection_id': '2', 'run_label': m, 'repeat_run': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\"doi:10.1101/2024.03.04.583254\", \n",
    "             \"doi:10.1101/2023.11.21.567712\",\n",
    "\"doi:10.3791/6515\",\n",
    "\"doi:10.1101/2023.07.28.550950\",\n",
    "\"doi:10.1093/micmic/ozad067.483\",\n",
    "\"doi:10.1007/978-1-0716-2639-9_20\",\n",
    "\"doi:10.1016/j.yjsbx.2022.100076\",\n",
    "\"doi:10.1016/j.xpro.2022.101658\",\n",
    "\"doi:10.1016/j.cell.2022.06.034\",\n",
    "\"doi:10.1093/plphys/kiab449\",\n",
    "\"doi:10.1073/pnas.2118020118\",\n",
    "\"doi:10.3791/62886\",\n",
    "\"doi:10.20944/preprints202105.0098.v1\",\n",
    "\"doi:10.1016/bs.mcb.2020.12.009\",\n",
    "\"doi:10.1007/978-1-0716-0966-8_1\",\n",
    "\"doi:10.1007/978-1-0716-0966-8_2\",\n",
    "\"doi:10.21769/bioprotoc.3768\",\n",
    "\"doi:10.1371/journal.ppat.1008883\",\n",
    "\"doi:10.1101/2020.05.19.104828\",\n",
    "\"doi:10.1073/pnas.1916331116\",\n",
    "\"doi:10.1042/bst20170351_cor\",\n",
    "\"doi:10.1038/s41594-018-0043-7\",\n",
    "\"doi:10.1007/978-1-4939-8585-2_4\",\n",
    "\"doi:10.1007/s41048-017-0040-0\",\n",
    "\"doi:10.1007/978-1-4939-6927-2_20\",\n",
    "\"doi:10.1016/j.str.2015.03.008\",\n",
    "\"doi:10.1007/978-1-62703-227-8_4\",\n",
    "\"doi:10.1016/b978-0-12-397945-2.00017-2\",\n",
    "\"doi:10.1016/j.jmb.2010.10.021\",\n",
    "\"doi:10.1186/1757-5036-3-6\",\n",
    "\"doi:10.1016/j.jmb.2008.03.014\",\n",
    "\"doi:10.1007/978-1-59745-294-6_20\"]\n",
    "\n",
    "\n",
    "for d in to_remove:\n",
    "    q = \"\"\"\n",
    "    SELECT DISTINCT n.id FROM langchain_pg_embedding as emb, \"Note\" as n\n",
    "    WHERE emb.cmetadata->>'n_type' = 'TiAbMappingNote__Discourse' AND\n",
    "        emb.cmetadata->>'about_id' = '{}' AND \n",
    "        emb.cmetadata->>'discourse_type' = 'ResultsConclusions' AND \n",
    "        emb.cmetadata->>'n_id' = n.id;\"\"\".format(d)\n",
    "    for row in ldb.session.execute(text(q)).all():\n",
    "        ldb.delete_note(row[0], commit_this=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "exp_q = ldb.session.query(SKE) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.type=='CitationRecord') \\\n",
    "        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n",
    "        .order_by(desc(SKE.publication_date))\n",
    "\n",
    "count = 0\n",
    "for e in tqdm(exp_q.all()):\n",
    "    q = ldb.session.query(N) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == e.id) \\\n",
    "        .filter(N.type =='TiAbMappingNote__Discourse')\n",
    "    for n in q.all():\n",
    "        dmap = json.loads(n.content) \n",
    "        if 'Objectives_Methods' in dmap.keys():\n",
    "            print('beep')\n",
    "            #new_dmap = {'Background': dmap.get('Background'), 'ObjectivesMethods': dmap.get('Objectives_Methods'), 'ResultsConclusions': dmap.get('Results_Conclusions')}\n",
    "            #n.content = json.dumps(new_dmap, indent=4)\n",
    "            #ldb.session.flush()\n",
    "#ldb.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "exp_q = ldb.session.query(SKE) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.type=='CitationRecord') \\\n",
    "        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n",
    "        .order_by(desc(SKE.publication_date))\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "count = 0\n",
    "for e in tqdm(exp_q.all()):\n",
    "    q = ldb.session.query(N) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == e.id) \\\n",
    "        .filter(N.type =='TiAbMappingNote__Discourse')\n",
    "\n",
    "\n",
    "    n = q.first()\n",
    "    dmap = json.loads(n.content)\n",
    "    '''Runs through the list of expressions, generates embeddings, and stores them in the database'''\n",
    "\n",
    "    for dtype in ['Background', 'ObjectivesMethods', 'ResultsConclusions']:\n",
    "        t = dmap.get(dtype)\n",
    "        if t is None:\n",
    "            continue\n",
    "        texts.append(t)\n",
    "        metadatas.append({'about_id': e.id, \\\n",
    "                        'about_type': 'ScientificKnowledgeExpression', \\\n",
    "                        'n_id': n.id, \\\n",
    "                        'n_type': 'TiAbMappingNote__Discourse', \\\n",
    "                        'discourse_type': dtype})\n",
    "\n",
    "docs = []\n",
    "for t,m in zip(texts, metadatas):\n",
    "    docs.append(Document(page_content=t, metadata=m))\n",
    "    \n",
    "db = PGVector.from_documents(\n",
    "    embedding=ldb.embed_model,\n",
    "    documents=docs,\n",
    "    collection_name=\"Note\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = '/Users/gully.burns/Documents/2024H1/models/discourse_tagger'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\", \n",
    "                                          truncation=True, \n",
    "                                          max_length=512)\n",
    "labels = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
    "lookup = {'LABEL_%d'%(i):l for i, l in enumerate(labels)}\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model = model_path, \n",
    "                      tokenizer=tokenizer, \n",
    "                      truncation=True,\n",
    "                      batch_size=8,\n",
    "                      device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = ldb\n",
    "collection_id = '2'\n",
    "\n",
    "q1 = self.session.query(SKE, SKI) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "        .filter(SKI_HP.has_part_id == SKF.id) \\\n",
    "        .filter(SKI.type == 'CitationRecord') \\\n",
    "        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \n",
    "\n",
    "for ske, ski in tqdm(q1.all()):\n",
    "    b = ''\n",
    "    om = ''\n",
    "    rc = ''  \n",
    "\n",
    "    fragments = []\n",
    "    for f in ski.has_part:\n",
    "      if f.type in ['title', 'abstract']:\n",
    "        fragments.append(f)\n",
    "\n",
    "    # USE AN LLM HERE INSTEAD OF A DEEP LEARNING CLASSIFER\n",
    "\n",
    "\n",
    "    for skf in sorted(fragments, key=lambda f: f.offset):\n",
    "        for s in self.sent_detector.tokenize(skf.content):\n",
    "            m = classifier(skf.content)\n",
    "            l = lookup.get(m[0].get('label'))\n",
    "            if l == 'BACKGROUND':\n",
    "                if len(b) > 0:\n",
    "                    b += '\\n'\n",
    "                b += s\n",
    "            elif l == 'OBJECTIVE' or l == 'METHODS':\n",
    "                if len(om) > 0:\n",
    "                    om += '\\n'\n",
    "                om += s\n",
    "            else: \n",
    "                if len(rc) > 0:\n",
    "                    rc += '\\n'\n",
    "                rc += s\n",
    "    skf_stem = ske.id+'.'+ski.type+'.'\n",
    "    if len(b) > 0:\n",
    "        f_b = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n",
    "                type='background_sentences', offset=-1, length=len(b),\n",
    "                name=skf_stem+'background', content=b)\n",
    "        self.session.add(f_b)\n",
    "        ski.has_part.append(f_b)\n",
    "        f_b.part_of = ski.id    \n",
    "    if len(om) > 0:\n",
    "        f_om = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n",
    "                type='objective_methods_sentences', offset=-1, length=len(om),\n",
    "                name=skf_stem+'objective_methods', content=om)\n",
    "        self.session.add(f_om)\n",
    "        ski.has_part.append(f_om)\n",
    "        f_om.part_of = ski.id\n",
    "    if len(rc) > 0:\n",
    "        f_rc = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n",
    "                type='results_conclusions_sentences', offset=-1, length=len(rc),\n",
    "                name=skf_stem+'results_conclusions', content=rc)\n",
    "        self.session.add(f_rc)\n",
    "        ski.has_part.append(f_rc)\n",
    "        f_rc.part_of = ski.id\n",
    "    self.session.flush()\n",
    "self.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = ldb\n",
    "collection_id = '2'\n",
    "#self.session.rollback()\n",
    "q2 = self.session.query(SKF) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "        .filter(SKI_HP.has_part_id == SKF.id) \\\n",
    "        .filter(SKI.type == 'CitationRecord') \\\n",
    "        .filter(or_(SKF.type == 'results_conclusions_sentences', \\\n",
    "                SKF.type == 'objective_methods_sentences', \\\n",
    "                SKF.type == 'background_sentences'))\n",
    "for skf in tqdm(q2.all()):\n",
    "    self.delete_fragment(skf.id)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = ldb\n",
    "collection_id = '2'\n",
    "#self.session.rollback()\n",
    "q2 = self.session.query(SKE, SKF) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "        .filter(SKI_HP.has_part_id == SKF.id) \\\n",
    "        .filter(SKI.type == 'CitationRecord') \\\n",
    "        .filter(SKF.type == 'objective_methods_sentences') \\\n",
    "        .order_by(desc(SKE.publication_date)) \\\n",
    "        .order_by(SKF.name)\n",
    "\n",
    "for ske, skf in tqdm(q2.all()):\n",
    "    print(skf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get full text copies of all the papers about CryoET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Get full text copies of all papers in the collection with id=\"2\".'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_new_collection_from_sample('5', 'EMPIAR CryoET Papers Tests', '4', 20, ['ScientificPrimaryResearchArticle', 'ScientificPrimaryResearchPreprint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKC.id, SKC.name, SKE.id, SKI.type) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id==SKI.id) \n",
    "df = pd.DataFrame(q.all(), columns=['id', 'collection name', 'doi', 'item type'])\n",
    "df.pivot_table(index=['id', 'collection name'], columns='item type', values='doi', aggfunc=lambda x: len(x.unique())).fillna(0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey + Run Classifications over Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "l = []\n",
    "q = ldb.session.query(N, SKE) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n",
    "\n",
    "output = []        \n",
    "print(len(q.all()))\n",
    "for n, ske in q.all():\n",
    "    ldb.delete_note(n.id)    \n",
    "print(len(q.all()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\n",
    "t.run({'collection_id': '5', 'classification_type':'cryoet_study_types', 'repeat_run':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\n",
    "t.run({'collection_id': '2', 'classification_type':'cryoet_study_types'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "q = ldb.session.query(N, SKE) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n",
    "\n",
    "output = []        \n",
    "for n, ske in q.all():\n",
    "        tup = json.loads(n.content)\n",
    "        tup['doi'] = 'http://doi.org/'+re.sub('doi:', '', ske.id)\n",
    "        tup['year'] = ske.publication_date.year\n",
    "        tup['month'] = ske.publication_date.month\n",
    "        tup['ref'] = ske.content\n",
    "        output.append(tup)\n",
    "df = pd.DataFrame(output).sort_values(['year', 'month'], ascending=[False, False])\n",
    "df.to_csv(loc+'/'+db_name+'/cryoet_study_types.tsv', sep='\\t')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey + Run Extractions over Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractExtraction_OneDocAtATime_Tool)][0]\n",
    "t.run({'collection_id': '5', 'extraction_type':'cryoet'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests + Checks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent tool selection + execution + interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Hi who are you and what can you do?'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MetaData Extraction Chain over listed papers\n",
    "\n",
    "Here, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKE.id) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKC.id=='2')  \n",
    "dois = [e.id for e in q.all()]\n",
    "dois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "metadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\n",
    "t2.compile_answers('cryoet', metadata_dir)\n",
    "t2.write_answers_as_notes('cryoet', metadata_dir)\n",
    "#sorted(list(set([doi for q in t2.examples for doi in t2.examples[q]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "\n",
    "# Hack to get the path to the metadata directory as a string\n",
    "#metadata_dir = str(files(cryoet_portal_metadata).joinpath('temp'))[0:-4]\n",
    "metadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\n",
    "\n",
    "# Compile the answers from the metadata directory\n",
    "t2.compile_answers('cryoet', metadata_dir)\n",
    "\n",
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "df = pd.DataFrame()\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df = pd.concat([df, pd.DataFrame(l)]) \n",
    "     \n",
    "# Iterate over papers to run the metadata extraction tool\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "\n",
    "    # Skip if the doi is already in the database\n",
    "    if len(df)>0 and d in df.doi.unique():\n",
    "        continue\n",
    "\n",
    "    # Run the metadata extraction tool on the doi\n",
    "    t2.run(tool_input={'paper_id': d, 'extraction_type': 'cryoet', 'run_label': 'test'})\n",
    "\n",
    "    # Add the results to the dataframe    \n",
    "    l2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df = pd.concat([df, pd.DataFrame(l2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "df_final = pd.DataFrame()\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df_final = pd.concat([df_final, pd.DataFrame(l)]) \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "l = []\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "    pred1 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    pred2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test_dbrx')\n",
    "    gold = t2.read_metadata_extraction_notes(d, 'cryoet', 'gold') \n",
    "    if pred1 is None or pred2 is None or gold is None or \\\n",
    "            len(pred1)==0 or len(pred2)==0 or len(gold)!=1:\n",
    "        continue\n",
    "    for k in gold[0]:\n",
    "        g_case = gold[0][k]\n",
    "        if g_case=='' or g_case is None:\n",
    "            continue    \n",
    "        for j, p_case in enumerate(pred1):\n",
    "            sim = fuzz.ratio(str(g_case), str(p_case.get(k,''))) / 100.0\n",
    "            print(k, str(g_case), str(p_case.get(k,'')), sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "\n",
    "df = t2.report_metadata_extraction_for_collection('5', 'cryoet', 'test').set_index('doi')\n",
    "df.to_csv(loc+'/'+db_name+'/reports/cryoet_metadata_gpt4.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_zip_archive_of_full_text_files('5', loc+'/'+db_name+'/full_text_files.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'MetadataExtractionNote') \n",
    "l = []\n",
    "for row in q3.all():\n",
    "    paper = row[0]\n",
    "    name = row[1]\n",
    "#    provenance = json.loads(row[2])\n",
    "    result = json.loads(row[3])\n",
    "    kv = {k:result[k] for k in result}\n",
    "    kv['DOI'] = paper\n",
    "    kv['run'] = name\n",
    "    l.append(kv)\n",
    "# create a dataframe from the list of dictionaries with DOI as the index column\n",
    "if len(l)>0:\n",
    "    df = pd.DataFrame(l).set_index(['DOI', 'run'])\n",
    "else: \n",
    "    df = pd.DataFrame()\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "for row in q3.all():\n",
    "    d_id = row[0]\n",
    "    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n",
    "    notes_to_delete = []\n",
    "    for n in ldb.read_notes_about_x(e):\n",
    "        notes_to_delete.append(n.id)\n",
    "    for n in notes_to_delete:\n",
    "        ldb.delete_note(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol Modeling + Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "slm = ChatOllama(model='stablelm-zephyr') \n",
    "llm = ChatOllama(model='mixtral:instruct') \n",
    "llm2 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "llm3 = ChatOpenAI(model='gpt-3.5-turbo') \n",
    "d = (\"This tool attempts to draw a protocol design from the description of a scientific paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ProcotolEntitiesExtractionTool(db=ldb, llm=llm3, description=d)\n",
    "entities = t1.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = ProcotolProcessesExtractionTool(db=ldb, llm=llm3, description=d)\n",
    "processes = t2.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\n",
    "processes.get('data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "alhazen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
