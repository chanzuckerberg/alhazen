{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Disease Literature.  \n",
    "\n",
    "> Applying AI to understand trends of research in rare disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Here we set up libraries and methods to create and query the local Postgres database we will be using to store our information from the Alhazen tools and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen.aliases import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.agent import AlhazenAgent\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool\n",
    "from alhazen.tools.paperqa_emulation_tool import *\n",
    "from alhazen.toolkit import *\n",
    "\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\n",
    "from alhazen.utils.searchEngineUtils import *\n",
    "\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import func, text\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import get_buffer_string, OutputParserException, format_document\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "\n",
    "#from paperqa.prompts import summary_prompt as paperqa_summary_prompt, qa_prompt as paperqa_qa_prompt, select_paper_prompt, citation_prompt, default_system_prompt\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import local_resources.queries.rao_grantees as rao_files\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the Postgres database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save files for your digital library, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n",
    "\n",
    "loc = os.environ['LOCAL_FILE_PATH']\n",
    "db_name = 'rare_as_one_diseases'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to destroy your current database \n",
    "\n",
    "**USE WITH CAUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to create a new, empty database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command lists all the tools the Alhazen agent system has access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "\n",
    "llms = lookup_chat_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_databricks_llama3 = ChatOpenAI(base_url='https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/serving-endpoints', \n",
    "                api_key=os.environ['DATABRICKS_API_KEY'], \n",
    "                model='databricks-meta-llama-3-70b-instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_dbrx = llms.get('gpt4_1106')\n",
    "\n",
    "cb = AlhazenAgent(db_name=db_name, agent_llm=llm_databricks_llama3, tool_llm=llm_databricks_llama3)\n",
    "print('AGENT TOOLS')\n",
    "for t in cb.tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build paper collections\n",
    "\n",
    "This section will build a literature collection across each of the diseases in the Rare As One Cohorts for cycle 1 and 2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What diseases are we querying the literature for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cols_to_include = ['ID', 'CORPUS_NAME', 'TERMS']\n",
    "df = pd.read_csv(files(rao_files).joinpath('CZI_RAO_diseases.tsv'), sep='\\t')\n",
    "df = df.drop(columns=[c for c in df.columns if c not in cols_to_include])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command iterates over the list of different collections and runs a query for each one on the European website by processing the `TERMS` column from the  dataframe with the `QueryTranslator` utility. This generates a search query in boolean logic that searches the `TITLE_ABS` field in the remote database (See https://www.ebi.ac.uk/europepmc/webservices/rest/fields for possible fields to search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QueryTranslator(df.sort_values('ID'), 'ID', 'TERMS', 'CORPUS_NAME')\n",
    "(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=['TITLE_ABS'])\n",
    "corpus_names = df['CORPUS_NAME']\n",
    "\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "for (id, name, query) in zip(corpus_ids, corpus_names, epmc_queries):\n",
    "    if id < 60:\n",
    "        continue\n",
    "    addEMPCCollection_tool.run(tool_input={'id': id, 'name':name, 'query':query, 'full_text':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Note - create a new corpus for collaborative discussions with CellXGene team (in particular Maximillian L.)\n",
    "#\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "id = '83'\n",
    "name = 'Diffuse Midline Glioma'\n",
    "query = '\"diffuse midline glioma\" OR \"diffuse intrinsic pontine glioma\" OR \"brainstem glioma\" OR \"diffuse intrinsic pontine glioma\"'\n",
    "addEMPCCollection_tool.run(tool_input={'id': id, 'name':name, 'query':query, 'full_text':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Note - create a new corpus for collaborative discussions with CellXGene team (in particular Maximillian L.)\n",
    "#\n",
    "PaperQA_tool = [t for t in cb.tk.get_tools() if isinstance(t, PaperQAEmulationTool)][0]\n",
    "question = 'Write an essay to answer the question: \"What is the connection between SEC61B and the unfolded protein response?'\n",
    "PaperQA_tool.run(tool_input={'question': question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Write an essay to answer the question: \"What known gene variants are associated with Primary Ciliary Dyskinesia?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crazy Bug\n",
    "\n",
    "Running the `skc_id=83` query below takes 6 minutes and `skc_id=79` takes less than 1 sec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "ldb.session.execute(text(\n",
    "    '''SELECT DISTINCT skc.name, ske.id, ske.content, ske.publication_date as pub_date, ske.type as pub_type, emb.embedding, skf.content \n",
    "    FROM langchain_pg_embedding as emb, \n",
    "        \"ScientificKnowledgeExpression\" as ske,\n",
    "        \"ScientificKnowledgeCollection_has_members\" as skc_hm, \n",
    "        \"ScientificKnowledgeCollection\" as skc, \n",
    "        \"ScientificKnowledgeFragment\" as skf\n",
    "    WHERE emb.cmetadata->>'i_type' = 'CitationRecord' AND\n",
    "        emb.cmetadata->>'e_id' = ske.id AND \n",
    "        emb.cmetadata->>'f_id' = skf.id AND\n",
    "        skc_hm.\"ScientificKnowledgeCollection_id\" = skc.id AND\n",
    "        ske.id = skc_hm.has_members_id AND (skc.id='79')\n",
    "    ORDER BY pub_date DESC;''')).fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "ldb.session.execute(text(\n",
    "    '''SELECT DISTINCT skc.name, ske.id, ske.content, ske.publication_date as pub_date, ske.type as pub_type, emb.embedding, skf.content \n",
    "    FROM langchain_pg_embedding as emb, \n",
    "        \"ScientificKnowledgeExpression\" as ske,\n",
    "        \"ScientificKnowledgeCollection_has_members\" as skc_hm, \n",
    "        \"ScientificKnowledgeCollection\" as skc, \n",
    "        \"ScientificKnowledgeFragment\" as skf\n",
    "    WHERE emb.cmetadata->>'i_type' = 'CitationRecord' AND\n",
    "        emb.cmetadata->>'e_id' = ske.id AND \n",
    "        emb.cmetadata->>'f_id' = skf.id AND\n",
    "        skc_hm.\"ScientificKnowledgeCollection_id\" = skc.id AND\n",
    "        ske.id = skc_hm.has_members_id AND (skc.id='83')\n",
    "    ORDER BY pub_date DESC;''')).fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKE).distinct(SKE.id)\n",
    "print(q.count())\n",
    "ldb.embed_expression_list(q.all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Get full text copies of all papers in the collection with id=\"0\".'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKC.id, SKC.name, func.count(SKC_HM.has_members_id)) \\\n",
    "    .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "    .group_by(SKC.id, SKC.name) \\\n",
    "    .order_by(SKC.id.cast(Integer))\n",
    "corpora_df = pd.DataFrame(q.all(), columns=['Corpus ID', 'Corpus Name', 'Paper Count'])\n",
    "\n",
    "paper_count = ldb.session.query(func.count(SKE.id)).first()\n",
    "print('Count of all papers in database: %d'%(paper_count[0]))\n",
    "\n",
    "corpora_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q3 = ldb.session.query(N) \\\n",
    "        .filter(N.type == 'NoteAboutFragment') \n",
    "\n",
    "for n in q3.all():\n",
    "    n_content = json.loads(n.content)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(n.id)\n",
    "    print(n_content.get('response')) \n",
    "    print(n_content.get('data')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skes = ldb.session.query(SKE).all()\n",
    "#ldb.embed_expression_list(skes)\n",
    "print(len(skes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_retriever  = [t for t in tk.get_tools() if isinstance(t, RetrieveFullTextTool)][0]\n",
    "\n",
    "for i, c in corpora_df.iterrows():\n",
    "    if c['Corpus ID'] != '81':\n",
    "        continue\n",
    "    print(c['Corpus Name'])\n",
    "    ft_count = 0\n",
    "    no_ft_count = 0\n",
    "    doi_list = [e.id for e in ldb.list_expressions(collection_id=c['Corpus ID'])]\n",
    "    for doi in doi_list:\n",
    "        d2 = doi.replace('doi:', '')\n",
    "        path = loc+db_name+'/ft/'\n",
    "        nxml_file_path = path+'/'+d2+'.nxml'\n",
    "        pdf_file_path = path+'/'+d2+'.pdf'\n",
    "        html_file_path = path+'/'+d2+'.html'\n",
    "        if os.path.exists(nxml_file_path) or  \\\n",
    "                os.path.exists(pdf_file_path) or \\\n",
    "                os.path.exists(html_file_path):\n",
    "            ft_count += 1\n",
    "        try: \n",
    "            no_ft_count += 1\n",
    "            #print('\\t'+doi)\n",
    "            ft_retriever.run(tool_input={'paper_id': doi})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print(ft_count)\n",
    "    print(no_ft_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKE.id, SKI.id, SKI.type, SKF.id, SKF.type, SKF.offset, SKF.content) \\\n",
    "    .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "    .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "    .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "    .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "    .filter(SKI.id==SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "    .filter(SKI_HP.has_part_id==SKF.id) \\\n",
    "    .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "    .filter(SKF.type=='section') \\\n",
    "    .filter(SKI.type.like('%FullText')) \\\n",
    "    .order_by(SKE.id, SKF.offset)\n",
    "items_df = pd.DataFrame(q.all(), columns=['doi', 'item_id', 'item_type', 'fragment_id', 'fragment_type', 'offset', 'content'])\n",
    "\n",
    "items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index the abstracts and run some simple semantic queries\n",
    "\n",
    "Here we index each paper's title and abstract to build a simple question / answer interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in tqdm(corpora_df.iterrows()):\n",
    "    if c['Corpus ID'] != '81':\n",
    "        continue\n",
    "    expressions = ldb.list_expressions(collection_id=c['Corpus ID'])    \n",
    "    ldb.embed_expression_list(expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is known about genetics underlying Stiff Person Syndrome?'\n",
    "\n",
    "ldb.query_vectorindex(question, k=10, collection_name='ScienceKnowledgeItem_FullText')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTEMPTING TO RECONSTRUCT PAPER-QA PIPELINE IN OUR SYSTEM.\n",
    "\n",
    "1. Embed paper sections + question\n",
    "2. Given the question, summarize the retrieved paper sections relative to the question\n",
    "3. Score and select relevant passages\n",
    "4. Put summaries into prompt\n",
    "5. Generate answer with prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "os.environ['PGVECTOR_CONNECTION_STRING'] = \"postgresql+psycopg2:///\"+ldb.name\n",
    "vectorstore = PGVector.from_existing_index(\n",
    "        embedding = ldb.embed_model, \n",
    "        collection_name = 'ScienceKnowledgeItem') \n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':15, 'filter': {'skc_ids': 81}})\n",
    "#retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hum_p = '''First, read through the following JSON encoding of {k} research articles: \n",
    "\n",
    "Each document has three attributes: (A) a digital object identifier ('DOI') code, (B) a CITATION string containing the authors, publication year, title and publication location, and the (C) CONTENT field with the title and abstract of the paper.  \n",
    "\n",
    "```json:{context}```\n",
    "\n",
    "Then, generate a JSON list of summaries of each article in order to help answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Do NOT directly answer the question, instead summarize to give evidence to help answer the question. \n",
    "Focus on specific details, including numbers, equations, or specific quotes. \n",
    "Reply \"Not applicable\" if text is irrelevant. \n",
    "Restrict each summary to {summary_length} words. \n",
    "Also, provide a score from 1-10 indicating relevance to question. Do not explain your score. \n",
    "\n",
    "Write this answer as JSON formatted output. Provide a list of {k} dict objects with the following fields: DOI, SUMMARY, RELEVANCE SCORE. \n",
    "\n",
    "Do not provide additional explanation for the answer.\n",
    "Do not include any other response other than a JSON object.\n",
    "'''\n",
    "sys_p = '''Answer in a direct and concise tone. Your audience is an expert, so be highly specific. If there are ambiguous terms or acronyms, first define them.'''\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"'DOI': '{ske_id}', CITATION: '{citation}', CONTENT:'{page_content}'\")\n",
    "def combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"},{\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return '[{'+document_separator.join(doc_strings)+'}]'\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", sys_p),\n",
    "            (\"human\", hum_p)])\n",
    "\n",
    "qa_chain = (\n",
    "    RunnableParallel({\n",
    "        \"k\": itemgetter(\"k\"),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"summary_length\": itemgetter(\"summary_length\"),\n",
    "        \"context\": itemgetter(\"question\") | retriever | combine_documents,\n",
    "    })\n",
    "    | {\n",
    "        \"summary\": template | ChatOllama(model='mixtral') | JsonEnclosedByTextOutputParser(),\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "input = {'question': question, 'summary_length': 1000, 'k':5}    \n",
    "out = qa_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "print(json.dumps(out, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discourse Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = '/Users/gully.burns/Documents/2024H1/models/discourse_tagger'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\", \n",
    "                                          truncation=True, \n",
    "                                          max_length=512)\n",
    "labels = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
    "lookup = {'LABEL_%d'%(i):l for i, l in enumerate(labels)}\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model = model_path, \n",
    "                      tokenizer=tokenizer, \n",
    "                      truncation=True,\n",
    "                      batch_size=8,\n",
    "                      device='mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an out-of-the-box classifier on the data for discourse tagging.\n",
    "\n",
    "ldb.session.rollback()\n",
    "one_year_ago = (datetime.now() - timedelta(days=1*365))\n",
    "\n",
    "q = ldb.session.query(SKE, SKF) \\\n",
    "    .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "    .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "    .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "    .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "    .filter(SKI.id==SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "    .filter(SKI_HP.has_part_id==SKF.id) \\\n",
    "    .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "    .filter(SKI.type == 'CitationRecord' ) \\\n",
    "    .order_by(SKE.id)\n",
    "\n",
    "#   .filter(SKC.name == 'The Stiff Person Syndrome' ) \\\n",
    "#   .filter(SKE.publication_date >= one_year_ago) \\\n",
    "\n",
    "s_list = []\n",
    "for e, f in q.all():\n",
    "    for i, s in enumerate(ldb.sent_detector.tokenize(f.content)):\n",
    "        s_list.append([e.id, f.id, i, s])\n",
    "sent_df = pd.DataFrame(s_list, columns=['doi', 'f_id', 's_id', 'text'])\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict multipe texts on single CPU and time the inference duration\n",
    "start = time()\n",
    "\n",
    "df = sent_df\n",
    "\n",
    "preds = classifier([row.text for i, row in df.iterrows()])\n",
    "pred_df = pd.DataFrame(preds)\n",
    "df['label'] = [lookup[row.label] for i, row in pred_df.iterrows()]\n",
    "df['score'] = [row.score for i, row in pred_df.iterrows()]\n",
    "\n",
    "end = time()\n",
    "\n",
    "print('Prediction time:', str(timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fragment sentences and add them as Notes\n",
    "ldb.session.rollback()\n",
    "for i, row in df.iterrows():\n",
    "    f_q = ldb.session.query(SKF).filter(SKF.id == row.f_id).first()\n",
    "    i_q = ldb.session.query(SKI).filter(SKI.id == row.f_id.split('.')[0]).first()\n",
    "    o = i_q.content.find(row.text)\n",
    "    l = len(row.text)\n",
    "    sentence_fragment = ScientificKnowledgeFragment(id=f_q.id+'.'+str(row.s_id), \\\n",
    "                                                    content=row.text, \\\n",
    "                                                    offset=o, \\\n",
    "                                                    length=l, \\\n",
    "                                                    type='sentence')\n",
    "    i_q.has_part.append(sentence_fragment)\n",
    "    note_content = {'discourse_label': row.label, 'score': row.score}\n",
    "    n = Note(id=f_q.id+'.'+str(row.s_id)+'.discourse_type',\n",
    "             content=json.dumps(note_content, indent=4),\n",
    "             format='json',\n",
    "             type='NoteAboutFragment')\n",
    "    sentence_fragment.has_notes.append(n)\n",
    "    ldb.session.flush()\n",
    "ldb.session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running DRSM Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = '/Users/gully.burns/Documents/2024H1/models/drsm_classifier'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\", \n",
    "                                          truncation=True, \n",
    "                                          max_length=512)\n",
    "labels = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
    "lookup = {'LABEL_%d'%(i):l for i, l in enumerate(labels)}\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model = model_path, \n",
    "                      tokenizer=tokenizer, \n",
    "                      truncation=True,\n",
    "                      batch_size=8,\n",
    "                      device='mps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling over the corpus. \n",
    "\n",
    "What are the main topics being discussed in each paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = '/Users/gully.burns/Documents/2024H1/models/drsm_classifier'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\", \n",
    "                                          truncation=True, \n",
    "                                          max_length=512)\n",
    "labels = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
    "lookup = {'LABEL_%d'%(i):l for i, l in enumerate(labels)}\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model = model_path, \n",
    "                      tokenizer=tokenizer, \n",
    "                      truncation=True,\n",
    "                      batch_size=8,\n",
    "                      device='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for and download Full Text Papers.\n",
    "\n",
    "Can we search for all Stiff Person Syndrome papers published in the last 10 years?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "\n",
    "ten_years_ago = (datetime.now() - timedelta(days=10*365))\n",
    "print(ten_years_ago)\n",
    "\n",
    "q = ldb.session.query(func.extract('year', SKE.publication_date.cast(Date)), func.count(SKE.id) ) \\\n",
    "    .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "    .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "    .filter(SKE.publication_date >= ten_years_ago) \\\n",
    "    .filter(SKC.name == 'The Stiff Person Syndrome' ) \\\n",
    "    .group_by(func.extract('year', SKE.publication_date.cast(Date))) \\\n",
    "    .order_by(func.extract('year', SKE.publication_date.cast(Date)))\n",
    "sps_pubcount_df = pd.DataFrame(q.all(), columns=['doi', 'date'])\n",
    "sps_pubcount_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PaperQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Write a short essay on \"What connections between primary ciliary diskinesia and primary cilia have been studied?\" based on the collection with ID=\"70\".'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
