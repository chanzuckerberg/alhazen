{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Workbook  \n",
    "\n",
    "> Using Alhazen in my general day-to-day work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen.aliases import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.agent import AlhazenAgent\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.protocol_extraction_tool import *\n",
    "from alhazen.tools.tiab_classifier_tool import *\n",
    "from alhazen.tools.tiab_mapping_tool import *\n",
    "from alhazen.toolkit import *\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database\n",
    "from alhazen.utils.searchEngineUtils import *\n",
    "\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import uuid\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the PostGresQL database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n",
    "\n",
    "loc = os.environ['LOCAL_FILE_PATH']\n",
    "db_name = 'open_science_best_practices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Database has been backed up to /users/gully.burns/alhazen/alhazen_workbooks/backup2024-02-14-09-25-50.sql\n",
      "Database has been dropped successfully !!\n"
     ]
    }
   ],
   "source": [
    "drop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [00:00<00:00, 2249.83it/s]\n"
     ]
    }
   ],
   "source": [
    "create_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ollama_mixtral', 'gpt4.0', 'gpt3.5', 'gemini1.0', 'databricks_dbrx', 'databricks_mixtral'])\n"
     ]
    }
   ],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "llms_lookup = lookup_chat_models()\n",
    "print(llms_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT TOOLS\n",
      "\tAddCollectionFromEPMCTool\n",
      "\tAddAuthorsToCollectionTool\n",
      "\tDescribeCollectionCompositionTool\n",
      "\tDeleteCollectionTool\n",
      "\tRetrieveFullTextTool\n",
      "\tRetrieveFullTextToolForACollection\n",
      "\tMetadataExtraction_EverythingEverywhere_Tool\n",
      "\tSimpleExtractionWithRAGTool\n",
      "\tPaperQAEmulationTool\n",
      "\tProcotolEntitiesExtractionTool\n",
      "\tCheckExpressionTool\n",
      "\tTitleAbstractClassifier_OneDocAtATime_Tool\n",
      "\tTitleAbstractDiscourseMappingTool\n"
     ]
    }
   ],
   "source": [
    "llm_gpt4 = llms_lookup.get('gpt4.0')\n",
    "llm_mixt = llms_lookup.get('ollama_mixtral')\n",
    "llm_dbrx = llms_lookup.get('databricks_dbrx')\n",
    "\n",
    "cb = AlhazenAgent(llm_mixt, llm_gpt4, db_name=db_name)\n",
    "print('AGENT TOOLS')\n",
    "for t in cb.tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize=1000&synonym=TRUE&resultType=core&query=\"best practices\" AND \"model sharing\" AND \"machine learning\", 33 European PMC PAPERS FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:19<00:00, 19.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Returning 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:17<00:00,  1.56it/s]\n",
      "/Users/gully.burns/miniconda3/envs/alhazen/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:293: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'We added a collection to the database called `Best Practices for Machine Learning Model Sharing` containing 0 papers from this query: `\"best practices\" AND \"model sharing\" AND \"machine learning\"`.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "#t.run({'id': '0', \n",
    "#       'name':'Best Practices for Data and Model Sharing', \n",
    "#       'query':'\\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\")'})\n",
    "t.run({'id': '1', \n",
    "       'name':'Best Practices for Machine Learning Model Sharing', \n",
    "       'query':'\"best practices\" AND \"model sharing\" AND \"machine learning\"'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "alhazen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
