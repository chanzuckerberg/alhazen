{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landscaping Open Science Best Practices  \n",
    "\n",
    "> Building a landscaping analysis of best practices around data and model sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen.apps.chat import  AlhazenAgentChatBot\n",
    "from alhazen.core import get_langchain_chatmodel, MODEL_TYPE\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \n",
    "from alhazen.toolkit import AlhazenToolkit\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import create_engine, exists, func\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skc = aliased(ScientificKnowledgeCollection)\n",
    "skc_p = aliased(ScientificKnowledgeCollectionProvenance)\n",
    "skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "ske = aliased(ScientificKnowledgeExpression)\n",
    "ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "ski = aliased(ScientificKnowledgeItem)\n",
    "ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "skf = aliased(ScientificKnowledgeFragment)\n",
    "\n",
    "n = aliased(Note)\n",
    "skc_hn = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "ske_hn = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "ski_hn = aliased(ScientificKnowledgeItemHasNotes)\n",
    "skf_hn = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the PostGresQL database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ALHAZEN_DB_NAME'] = 'open_science_best_practices'\n",
    "os.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n",
    "\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n",
    "\n",
    "if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "    raise Exception('Which database do you want to use for this application?')\n",
    "db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "loc = os.environ['LOCAL_FILE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:00<00:00, 3507.90it/s]\n"
     ]
    }
   ],
   "source": [
    "create_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database has been dropped successfully !!\n"
     ]
    }
   ],
   "source": [
    "drop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE TOOLS\n",
      "\tAddCollectionFromEPMCTool\n",
      "\tDescribeCollectionCompositionTool\n",
      "\tDeleteCollectionTool\n",
      "\tRetrieveFullTextTool\n",
      "\tMetadataExtractionTool\n",
      "\tSimpleExtractionWithRAGTool\n",
      "\tPaperQAEmulationTool\n",
      "\tCheckExpressionTool\n"
     ]
    }
   ],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "llm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\n",
    "cb = AlhazenAgentChatBot()\n",
    "\n",
    "print('AVAILABLE TOOLS')\n",
    "for t in cb.tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CEIFNS database from queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a basic query to build up corpus. \n",
    "\n",
    "Use the following general query to generate a corpus to work with: `\"best practices\" AND (\"data sharing\" OR \"model sharing\")`, but frame it as a question to the Alhazen agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Build a collection using the query string \\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\") and set the collection_id to \\\"best_practices_models_or_data\\\"\"\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Build a collection using the query string \\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\") and set the collection_id to \\\"best_practices_models_or_data\\\"\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Build a collection using the query string \\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\") and set the collection_id to \\\"best_practices_models_or_data\\\"\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Your name is \\\"Alhazen\\\".  \\nRespond to the human as helpfully and accurately as possible. Either follow the user's instructions or answer the users' questions using the tools provided. \\nYou have access to the following tools:\\nadd_collection_from_epmc_query: This tool executes a search for scientific papers in the EPMC database based on a query and then builds a collection out of the papers returned. Input to this tool has three parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\n- 'query' which is string that defines a query using Boolean logic for search terms.\\n- 'name' which is a string that defines a descriptive name for the collection.\\nThe tool will execute an query over the remote database, create the collection and add papers to the collection to our local database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'A code that serves as the id of the collection we will add papers to.', 'type': 'string'}, 'name': {'title': 'Name', 'description': 'A human-readable name of the collection we will add papers to.', 'type': 'string'}, 'query': {'title': 'Query', 'description': 'A search query (using AND/OR/NOT statements) to be executed on the European PMC online literature repository for scientific publications.', 'type': 'string'}}\\ndescribe_collection_in_local_database: This tool describes the contents of a collection in the database. Input to this tool has one parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\nIf successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'should be the id of the collection we wish to describe', 'type': 'string'}}\\ndelete_collection: This tool deletes a collection from the database., args: {'collection_id': {'title': 'Collection Id', 'description': 'should be the collection_id of the collection we will add papers to', 'type': 'string'}}\\nretrieve_full_text_for_paper_id: This tool invokes a web search for single full text paper from the web given a doi identifier. Input to this tool has one parameter: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'\\nThe tool will search online for the paper, return it and add it's text to the database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'paper_id': {'title': 'Paper Id', 'description': 'the digitial objecty identifier (doi) of the paper being retrieved from external sources', 'type': 'string'}}\\nmetadata_extraction: This tool extracts experimental metadata for an experiment from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using thisInput to this tool has two parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'extraction_type' which denotes the type of experiment being analysed. This should be set to the string 'cryoet'.\\nThe tool will execute an LLM over the paper to extract metadata from available text  and then insert the metadata into the database. The output is a string that returns a completion message (either positive or an error report)., args: {'paper_id': {'title': 'Paper Id'}, 'extraction_type': {'title': 'Extraction Type'}, 'item_type': {'title': 'Item Type', 'default': 'JATSFullText'}}\\nsimple_extraction: This tool extracts information from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using this.Input to this tool has three parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'variable_name' which denotes the name of the variable that the question is attempting to describe (e.g. 'species_name', 'sample_preparation_method').\\n- 'question' which is the question that must be answered to provide a value for the specified variable (e.g., 'Rat', 'Plunge Vitrificiation').\\nThe tool will execute an LLM over the paper to extract answers to the specified question. Output from the tool will be a JSON-formatted string with two fields: 'report' and 'data'.Text in the report field specifies how the tool performed this task., args: {'paper_id': {'title': 'Paper Id'}, 'variable_name': {'title': 'Variable Name'}, 'question': {'title': 'Question'}}\\nsimple_qa_over_papers: This tool answers scientific questions by searching for papers from Alhazen's database, reading a small subset of them and then synthesizing a response in the form of an essay. Only use this tool to respond to scientific questions that you already have built a database for.Input to this tool is a question, the ID value for the collection that the question will be asked over (collection_id), the number of documents to be sampled (n_sample_size), the number of documents to be synthesized in the final analysis.The tool will execute an Map-Reduce RAG pipeline to query the vector store, and then summarize the information returned to write a response to the question., args: {'question': {'title': 'Question'}, 'n_sample_size': {'title': 'N Sample Size', 'default': 15}, 'n_summary_size': {'title': 'N Summary Size', 'default': 5}, 'collection_id': {'title': 'Collection Id', 'default': -1}}\\ndescribe_expression_in_local_database: This tool checks if the database contains a paper. Input to this tool has one parameter: \\n- 'query' which is a search string for the paper (based on either the id or title).\\nThe tool will query the database and report its findings., args: {'query': {'title': 'Query', 'description': 'a search query for the paper in the database', 'type': 'string'}}\\nUse a json blob to specify a tool by providing an action key (tool name)  and an action_input key (tool input).\\nValid \\\"action\\\" values: \\\"Final Answer\\\" or add_collection_from_epmc_query, describe_collection_in_local_database, delete_collection, retrieve_full_text_for_paper_id, metadata_extraction, simple_extraction, simple_qa_over_papers, describe_expression_in_local_database\\nProvide only ONE action per $JSON_BLOB, as shown:\\n``` {\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n} ```\\nFollow this format:\\nQuestion: input question to answer Thought: consider previous and subsequent steps Action: ``` $JSON_BLOB ``` Observation: action result ... (repeat Thought/Action/Observation N times) Thought: I know what to respond Action: ``` {\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Final response to human\\\"\\n}\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. \\nUse tools if necessary.  Respond directly if appropriate. \\nFormat is Action:```$JSON_BLOB```then Observation\\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Build a collection using the query string \\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\") and set the collection_id to \\\"best_practices_models_or_data\\\"\\n\\n(reminder to respond in a JSON blob no matter what)\\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Your name is \\\"Alhazen\\\".  \\nRespond to the human as helpfully and accurately as possible. Either follow the user's instructions or answer the users' questions using the tools provided. \\nYou have access to the following tools:\\nadd_collection_from_epmc_query: This tool executes a search for scientific papers in the EPMC database based on a query and then builds a collection out of the papers returned. Input to this tool has three parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\n- 'query' which is string that defines a query using Boolean logic for search terms.\\n- 'name' which is a string that defines a descriptive name for the collection.\\nThe tool will execute an query over the remote database, create the collection and add papers to the collection to our local database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'A code that serves as the id of the collection we will add papers to.', 'type': 'string'}, 'name': {'title': 'Name', 'description': 'A human-readable name of the collection we will add papers to.', 'type': 'string'}, 'query': {'title': 'Query', 'description': 'A search query (using AND/OR/NOT statements) to be executed on the European PMC online literature repository for scientific publications.', 'type': 'string'}}\\ndescribe_collection_in_local_database: This tool describes the contents of a collection in the database. Input to this tool has one parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\nIf successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'should be the id of the collection we wish to describe', 'type': 'string'}}\\ndelete_collection: This tool deletes a collection from the database., args: {'collection_id': {'title': 'Collection Id', 'description': 'should be the collection_id of the collection we will add papers to', 'type': 'string'}}\\nretrieve_full_text_for_paper_id: This tool invokes a web search for single full text paper from the web given a doi identifier. Input to this tool has one parameter: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'\\nThe tool will search online for the paper, return it and add it's text to the database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'paper_id': {'title': 'Paper Id', 'description': 'the digitial objecty identifier (doi) of the paper being retrieved from external sources', 'type': 'string'}}\\nmetadata_extraction: This tool extracts experimental metadata for an experiment from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using thisInput to this tool has two parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'extraction_type' which denotes the type of experiment being analysed. This should be set to the string 'cryoet'.\\nThe tool will execute an LLM over the paper to extract metadata from available text  and then insert the metadata into the database. The output is a string that returns a completion message (either positive or an error report)., args: {'paper_id': {'title': 'Paper Id'}, 'extraction_type': {'title': 'Extraction Type'}, 'item_type': {'title': 'Item Type', 'default': 'JATSFullText'}}\\nsimple_extraction: This tool extracts information from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using this.Input to this tool has three parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'variable_name' which denotes the name of the variable that the question is attempting to describe (e.g. 'species_name', 'sample_preparation_method').\\n- 'question' which is the question that must be answered to provide a value for the specified variable (e.g., 'Rat', 'Plunge Vitrificiation').\\nThe tool will execute an LLM over the paper to extract answers to the specified question. Output from the tool will be a JSON-formatted string with two fields: 'report' and 'data'.Text in the report field specifies how the tool performed this task., args: {'paper_id': {'title': 'Paper Id'}, 'variable_name': {'title': 'Variable Name'}, 'question': {'title': 'Question'}}\\nsimple_qa_over_papers: This tool answers scientific questions by searching for papers from Alhazen's database, reading a small subset of them and then synthesizing a response in the form of an essay. Only use this tool to respond to scientific questions that you already have built a database for.Input to this tool is a question, the ID value for the collection that the question will be asked over (collection_id), the number of documents to be sampled (n_sample_size), the number of documents to be synthesized in the final analysis.The tool will execute an Map-Reduce RAG pipeline to query the vector store, and then summarize the information returned to write a response to the question., args: {'question': {'title': 'Question'}, 'n_sample_size': {'title': 'N Sample Size', 'default': 15}, 'n_summary_size': {'title': 'N Summary Size', 'default': 5}, 'collection_id': {'title': 'Collection Id', 'default': -1}}\\ndescribe_expression_in_local_database: This tool checks if the database contains a paper. Input to this tool has one parameter: \\n- 'query' which is a search string for the paper (based on either the id or title).\\nThe tool will query the database and report its findings., args: {'query': {'title': 'Query', 'description': 'a search query for the paper in the database', 'type': 'string'}}\\nUse a json blob to specify a tool by providing an action key (tool name)  and an action_input key (tool input).\\nValid \\\"action\\\" values: \\\"Final Answer\\\" or add_collection_from_epmc_query, describe_collection_in_local_database, delete_collection, retrieve_full_text_for_paper_id, metadata_extraction, simple_extraction, simple_qa_over_papers, describe_expression_in_local_database\\nProvide only ONE action per $JSON_BLOB, as shown:\\n``` {\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n} ```\\nFollow this format:\\nQuestion: input question to answer Thought: consider previous and subsequent steps Action: ``` $JSON_BLOB ``` Observation: action result ... (repeat Thought/Action/Observation N times) Thought: I know what to respond Action: ``` {\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Final response to human\\\"\\n}\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. \\nUse tools if necessary.  Respond directly if appropriate. \\nFormat is Action:```$JSON_BLOB```then Observation\\n\\nHuman: Build a collection using the query string \\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\") and set the collection_id to \\\"best_practices_models_or_data\\\"\\n\\n(reminder to respond in a JSON blob no matter what)\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOllama] [3.92s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" {\\n  \\\"action\\\": \\\"add\\\\_collection\\\\_from\\\\_epmc\\\\_query\\\",\\n  \\\"action\\\\_input\\\": {\\n    \\\"id\\\": \\\"best\\\\_practices\\\\_models\\\\_or\\\\_data\\\",\\n    \\\"name\\\": \\\"Best Practices for Data and Model Sharing\\\",\\n    \\\"query\\\": \\\"\\\\\\\"best practices\\\\\\\" AND (\\\\\\\"data sharing\\\\\\\" OR \\\\\\\"model sharing\\\\\\\")\\\"\\n  }\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mixtral:instruct\",\n",
      "          \"created_at\": \"2024-02-07T23:29:06.348931Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3915649083,\n",
      "          \"load_duration\": 3405042,\n",
      "          \"prompt_eval_duration\": 789770000,\n",
      "          \"eval_count\": 89,\n",
      "          \"eval_duration\": 3121413000\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \" {\\n  \\\"action\\\": \\\"add\\\\_collection\\\\_from\\\\_epmc\\\\_query\\\",\\n  \\\"action\\\\_input\\\": {\\n    \\\"id\\\": \\\"best\\\\_practices\\\\_models\\\\_or\\\\_data\\\",\\n    \\\"name\\\": \\\"Best Practices for Data and Model Sharing\\\",\\n    \\\"query\\\": \\\"\\\\\\\"best practices\\\\\\\" AND (\\\\\\\"data sharing\\\\\\\" OR \\\\\\\"model sharing\\\\\\\")\\\"\\n  }\\n}\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:JSONAgentOutputParser_withFixes] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:JSONAgentOutputParser_withFixes] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"schema\",\n",
      "    \"agent\",\n",
      "    \"AgentAction\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"tool\": \"add_collection_from_epmc_query\",\n",
      "    \"tool_input\": {\n",
      "      \"id\": \"best_practices_models_or_data\",\n",
      "      \"name\": \"Best Practices for Data and Model Sharing\",\n",
      "      \"query\": \"\\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\")\"\n",
      "    },\n",
      "    \"log\": \" {\\n  \\\"action\\\": \\\"add_collection_from_epmc_query\\\",\\n  \\\"action_input\\\": {\\n    \\\"id\\\": \\\"best_practices_models_or_data\\\",\\n    \\\"name\\\": \\\"Best Practices for Data and Model Sharing\\\",\\n    \\\"query\\\": \\\"\\\\\\\"best practices\\\\\\\" AND (\\\\\\\"data sharing\\\\\\\" OR \\\\\\\"model sharing\\\\\\\")\\\"\\n  }\\n}\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] [3.92s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m {\n",
      "  \"action\": \"add_collection_from_epmc_query\",\n",
      "  \"action_input\": {\n",
      "    \"id\": \"best_practices_models_or_data\",\n",
      "    \"name\": \"Best Practices for Data and Model Sharing\",\n",
      "    \"query\": \"\\\"best practices\\\" AND (\\\"data sharing\\\" OR \\\"model sharing\\\")\"\n",
      "  }\n",
      "}\u001b[0m\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:add_collection_from_epmc_query] Entering Tool run with input:\n",
      "\u001b[0m\"{'id': 'best_practices_models_or_data', 'name': 'Best Practices for Data and Model Sharing', 'query': '\"best practices\" AND (\"data sharing\" OR \"model sharing\")'}\"\n",
      "https://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize=1000&synonym=TRUE&resultType=core&query=\"best practices\" AND (\"data sharing\" OR \"model sharing\"), 7016 European PMC PAPERS FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:53<00:00, 14.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Returning 6626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6626/6626 [00:37<00:00, 177.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:add_collection_from_epmc_query] [452.28s] Exiting Tool run with output:\n",
      "\u001b[0m\"{'report': 'We added a collection to the database called `Best Practices for Data and Model Sharing` containing 0 papers from this query: `\"best practices\" AND (\"data sharing\" OR \"model sharing\")`.'}\"\n",
      "\u001b[36;1m\u001b[1;3m{'report': 'We added a collection to the database called `Best Practices for Data and Model Sharing` containing 0 papers from this query: `\"best practices\" AND (\"data sharing\" OR \"model sharing\")`.'}\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [456.21s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out = cb.agent_executor.invoke({'input':'Build a collection using the query string \"best practices\" AND (\"data sharing\" OR \"model sharing\") and set the collection_id to \"best_practices_models_or_data\"'},\n",
    "                               config={'callbacks': [ConsoleCallbackHandler()]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of all papers in database: 6618\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus ID</th>\n",
       "      <th>Corpus Name</th>\n",
       "      <th>Provenance</th>\n",
       "      <th>Paper Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best_practices_models_or_data</td>\n",
       "      <td>Best Practices for Data and Model Sharing</td>\n",
       "      <td>{\"action_type\": \"\", \"action\": {\"id\": \"add_coll...</td>\n",
       "      <td>6618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Corpus ID                                Corpus Name  \\\n",
       "0  best_practices_models_or_data  Best Practices for Data and Model Sharing   \n",
       "\n",
       "                                          Provenance  Paper Count  \n",
       "0  {\"action_type\": \"\", \"action\": {\"id\": \"add_coll...         6618  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = ldb.session.query(skc.id, skc.name, func.aggregate_strings(skc_p.provenance, separator=', '), func.count(skc_hm.has_members_id)) \\\n",
    "    .filter(skc.id==skc_hm.ScientificKnowledgeCollection_id) \\\n",
    "    .filter(skc_p.ScientificKnowledgeCollection_id==skc.id) \\\n",
    "    .group_by(skc.id, skc.name) \\\n",
    "    .order_by(skc.id)\n",
    "corpora_df = pd.DataFrame(q.all(), columns=['Corpus ID', 'Corpus Name', 'Provenance', 'Paper Count'])\n",
    "\n",
    "paper_count = ldb.session.query(func.count(ske.id)).first()\n",
    "print('Count of all papers in database: %d'%(paper_count[0]))\n",
    "\n",
    "corpora_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
