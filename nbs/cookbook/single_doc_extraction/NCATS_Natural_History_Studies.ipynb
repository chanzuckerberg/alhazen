{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCATS Natural History Studies  \n",
    "\n",
    "> Using Alhazen to extract information from Natural History Studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen.agent import AlhazenAgent\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.protocol_extraction_tool import *\n",
    "from alhazen.toolkit import *\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database\n",
    "from alhazen.utils.searchEngineUtils import *\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import create_engine, exists, func, or_, and_, not_, desc, asc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Aliases like this massively simplifies the use of SQLAlchemy\n",
    "IR = aliased(InformationResource)\n",
    "\n",
    "SKC = aliased(ScientificKnowledgeCollection)\n",
    "SKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "SKE = aliased(ScientificKnowledgeExpression)\n",
    "SKE_XREF = aliased(ScientificKnowledgeExpressionXref)\n",
    "SKE_IRI = aliased(ScientificKnowledgeExpressionIri)\n",
    "SKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "SKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\n",
    "SKI = aliased(ScientificKnowledgeItem)\n",
    "SKI_HP = aliased(ScientificKnowledgeItemHasPart)\n",
    "SKF = aliased(ScientificKnowledgeFragment)\n",
    "\n",
    "N = aliased(Note)\n",
    "NIA = aliased(NoteIsAbout)\n",
    "SKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "SKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "SKI_HN = aliased(ScientificKnowledgeItemHasNotes)\n",
    "SKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the PostGresQL database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ALHAZEN_DB_NAME'] = 'natural_history_studies'\n",
    "os.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n",
    "\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n",
    "\n",
    "if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "    raise Exception('Which database do you want to use for this application?')\n",
    "db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "loc = os.environ['LOCAL_FILE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_ceifns_database('natural_history_studies', loc+'/natural_history_studies.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "llm = ChatOllama(model='mixtral:instruct') \n",
    "llm2 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "llm2 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "\n",
    "cb = AlhazenAgent(llm2, llm2)\n",
    "print('AGENT TOOLS')\n",
    "for t in cb.tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dois = ['10.1007/s40123-019-00218-9', \n",
    "                    '10.1136/heartjnl-2013-304920',\n",
    "                    '10.21037/cdt.2018.09.18',\n",
    "                    '10.1038/sc.2013.170',\n",
    "                    '10.1016/j.jacc.2006.07.053',\n",
    "                    '10.1186/s12884-016-1076-8',\n",
    "                    '10.1200/PO.20.00218',\n",
    "                    '10.1056/NEJMoa021736',\n",
    "                    '10.1093/europace/euw067',\n",
    "                    '10.7150/jca.32579']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "step = 40\n",
    "for start_i in range(0, len(dois), step):\n",
    "    query = ' OR '.join(['doi:\\\"'+dois[i]+'\\\"' for i in range(start_i, start_i+step) if i < len(dois)])\n",
    "    addEMPCCollection_tool.run({'id': '0', 'name':'Basic Extraction Demo', 'query':query, 'full_text':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Download all available full text for papers in the collection with id=\"0\"'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.report_collection_composition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_type = 'natural history studies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in cb.tk.get_tools() if isinstance(t, MetadataExtraction_EverythingEverywhere_Tool)][0]\n",
    "\n",
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "df = pd.DataFrame()\n",
    "for d in dois:\n",
    "    item_types = set()\n",
    "    d_id = 'doi:'+d\n",
    "    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, study_type))\n",
    "    df = pd.concat([df, df2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "# Iterate over papers to run the metadata extraction tool\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "\n",
    "    d_id = 'doi:'+d\n",
    "\n",
    "    # Skip if the doi is already in the database\n",
    "    #if len(df)>0 and d_id in df.doi.unique():\n",
    "    #    continue\n",
    "\n",
    "    # Run the metadata extraction tool on the doi\n",
    "    t2.run(tool_input={'paper_id': d_id, 'extraction_type': study_type})\n",
    "\n",
    "    # Add the results to the dataframe\n",
    "    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, study_type))\n",
    "    df = pd.concat([df, df2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for d in [d for d in dois]:\n",
    "    d_id = 'doi:'+d\n",
    "    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, study_type))\n",
    "    df = pd.concat([df, df2]) \n",
    "df.to_csv(loc+'/nhs_metadata_extraction.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "q3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'MetadataExtractionNote') \n",
    "for row in q3.all():\n",
    "    d_id = row[0]\n",
    "    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n",
    "    notes_to_delete = []\n",
    "    for n in ldb.read_notes_about_x(e):\n",
    "        notes_to_delete.append(n.id)\n",
    "    for n in notes_to_delete:\n",
    "        ldb.delete_note(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_ceifns_database(os.environ['ALHAZEN_DB_NAME'], loc+'/nhs_metadata_extraction.db.backup2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
