{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CryoET  \n",
    "\n",
    "> Methods to extract metadata and study the structure of scientific protocols based on all available online data and knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CryoET\n",
    "\n",
    "Cryo-electron Tomography (CryoET) involves rapidly freezing biological samples in their natural state to preserve their three-dimensional structure without the need for staining or crystallization. This methodology allows researchers to visualize proteins and other biomolecules at near-atomic resolution.\n",
    "\n",
    "This digital library is based on capturing all papers that mention the technique in their titles, abstracts, or methods sections and then analyzing the various methods used and their applications. Our focus is on supporting the work of the Chan Zuckerberg Imaging Institute, [CZII](https://www.czimaginginstitute.org/) on developing [the CryoET data portal](https://cryoetdataportal.czscience.com/), an open source repository for CryoET-based data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports\n",
    "\n",
    "Setting python imports, environment variables, and other crucial set up parameters here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen.aliases import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.agent import AlhazenAgent\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.core import lookup_chat_models\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.protocol_extraction_tool import *\n",
    "from alhazen.tools.tiab_classifier_tool import *\n",
    "from alhazen.tools.tiab_extraction_tool import *\n",
    "from alhazen.tools.tiab_mapping_tool import *\n",
    "from alhazen.toolkit import *\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database\n",
    "\n",
    "from alhazen.utils.searchEngineUtils import *\n",
    "\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import uuid\n",
    "import yaml\n",
    "\n",
    "import local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Plot the distribution of the lengths of the methods sections \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import local_resources.queries.em_tech as em_tech_queries\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "import json\n",
    "from jsonpath_ng import jsonpath, parse\n",
    "from langchain_community.chat_models.openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the PostGresQL database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n",
    "\n",
    "loc = os.environ['LOCAL_FILE_PATH']\n",
    "db_name = 'em_tech'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup utils, agents, and tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "llms_lookup = lookup_chat_models()\n",
    "print(llms_lookup.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gpt4_1106 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "llm_gpt35 = ChatOpenAI(model='gpt-3.5-turbo')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm_gpt4_1106 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "llm_gpt35 = ChatOpenAI(model='gpt-3.5-turbo')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> 75dc97aff9dc932799675920edfaed089bf106a3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = llms_lookup.get('databricks_llama3')\n",
    "\n",
    "cb = AlhazenAgent(llm_gpt35, llm_gpt35, db_name=db_name)\n",
    "print('AGENT TOOLS')\n",
    "for t in cb.tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)\n",
    "\n",
    "test_tk = MetadataExtractionToolkit(db=ldb, llm=llm_gpt35)\n",
    "print('\\nTESTING TOOLS')\n",
    "for t in test_tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Evaluation Dataset\n",
    "\n",
    "These are cases directly taken from `*.yaml` files that  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify cases from the CZI CryoET Portal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois = {10000: ['10.1101/2022.04.12.488077'], \n",
    "        10001: ['10.1101/2022.04.12.488077'], \n",
    "        10003: ['10.1038/s41586-022-05255-2', '10.1038/s41592-020-01054-7'], \n",
    "        10004: ['10.1101/2023.04.28.538734'], \n",
    "        10005: ['10.1038/s41594-022-00861-0'], \n",
    "        10006: ['10.1038/s41586-020-2665-2'], \n",
    "        10007: [], \n",
    "        10008: ['10.1038/s41586-022-04971-z'], \n",
    "        10009: ['10.1126/science.abm6704'], \n",
    "        10010: ['10.1083/jcb.202204093', '10.1101/2022.01.23.477440']}\n",
    "dois_flattened = [doi for doi_list in dois.values() for doi in doi_list]\n",
    "dois_flattened = list(set(dois_flattened))\n",
    "dois_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Gold Standard experimental metadata from EMPIAR database.\n",
    "\n",
    "1. Download the entire database to a local file from: `https://www.ebi.ac.uk/emdb/search/database:EMPIAR`\n",
    "2. Save the location in a temporary variable: `empiar_metadata_path`\n",
    "3. Process the downloaded file for (A) EMDB ids, (B) DOI values for publications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# local_path to the file downloaded from the EMPIAR search results: \n",
    "# https://www.ebi.ac.uk/emdb/api/empiar/search/database:EMPIAR?wt=json&download=true\n",
    "# download the file and save it to a local path\n",
    "url = \"https://www.ebi.ac.uk/emdb/api/empiar/search/database:EMPIAR?wt=json&download=true\"\n",
    "empiar_metadata_path = loc+db_name+'/EMPIAR_search_results.json'\n",
    "response = requests.get(url, stream=True)\n",
    "with open(empiar_metadata_path, \"wb\") as handle:\n",
    "    for data in response.iter_content():\n",
    "        handle.write(data)\n",
    "\n",
    "with open(empiar_metadata_path, 'r') as f:\n",
    "    empiar_metadata = json.load(f)\n",
    "empiar_dataset_ids = list(empiar_metadata.keys())\n",
    "d = {}\n",
    "for empiar_id in empiar_dataset_ids:\n",
    "    d[empiar_id] = {'dois':[], 'emd_ids': []}\n",
    "    for citation in empiar_metadata.get(empiar_id, {}).get('citation', []):\n",
    "        if citation.get('doi') is not None:\n",
    "            d[empiar_id]['dois'].append(citation.get('doi'))\n",
    "    for emd_id in empiar_metadata.get(empiar_id, {}).get('cross_references'):\n",
    "        d[empiar_id]['emd_ids'].append(emd_id.get('name'))    \n",
    "\n",
    "def get_nested(data, *args):\n",
    "    if args and data:\n",
    "        element  = args[0]\n",
    "        if element:\n",
    "            value = data.get(element)\n",
    "            return value if len(args) == 1 else get_nested(value, *args[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get metadata from the EMDB entries for each case\n",
    "metadlist = []\n",
    "\n",
    "# jsonpath expressions to identify specific metadata from the EMDB entries\n",
    "# focus mainly on the specimen preparation (grids, buffers, vitrification, etc.)\n",
    "sd_jp = 'structure_determination_list.structure_determination[*]'\n",
    "sample_preparation_type_jp = parse(sd_jp + '.method')\n",
    "agg_state_jp = parse(sd_jp + '.aggregation_state')\n",
    "specprep_list_jp = sd_jp + '.specimen_preparation_list.specimen_preparation[*]'\n",
    "buffer_jp = parse(specprep_list_jp + '.buffer.ph') \n",
    "grid_model_jp = parse(specprep_list_jp + '.grid.model')\n",
    "grid_material_jp = parse(specprep_list_jp + '.grid.material') \n",
    "grid_mesh_jp = parse(specprep_list_jp + '.grid.mesh')\n",
    "grid_support_topology_jp = parse(specprep_list_jp + '.grid.support_film[*].film_topology')\n",
    "grid_pretreatment_jp = parse(specprep_list_jp + '.grid.pretreatment.type_')\n",
    "grid_vitrification_cryogen_jp = parse(specprep_list_jp + '.vitrification.cryogen_name')\n",
    "grid_vit_ctemp_jp = specprep_list_jp + '.vitrification.chamber_temperature.'\n",
    "grid_vit_chumid_jp = specprep_list_jp + '.vitrification.chamber_humidity'\n",
    "\n",
    "jp_method = parse('structure_determination_list.structure_determination[*]')\n",
    "i = 0\n",
    "for k,v in d.items():\n",
    "    #i += 1\n",
    "    #if i > 10:\n",
    "    #    break\n",
    "    print(k,v)\n",
    "    for emd_id in v['emd_ids']:\n",
    "        emd_exp = requests.get('https://www.ebi.ac.uk/emdb/api/entry/experiment/'+emd_id)\n",
    "        if emd_exp.status_code == 200:\n",
    "            emd = emd_exp.json()\n",
    "            sample_preparation_type = ', '.join([m.value for m in sample_preparation_type_jp.find(emd)])\n",
    "            agg_state = ', '.join([m.value for m in agg_state_jp.find(emd)])\n",
    "            buffer = ', '.join([str(m.value) for m in buffer_jp.find(emd)])\n",
    "            grid_model = ', '.join([m.value for m in grid_model_jp.find(emd)])\n",
    "            grid_material = ', '.join([m.value for m in grid_material_jp.find(emd)])\n",
    "            grid_mesh = ', '.join([str(m.value) for m in grid_mesh_jp.find(emd)])\n",
    "            grid_support_topology = ', '.join([m.value for m in grid_support_topology_jp.find(emd)])\n",
    "            grid_pretreatment = ', '.join([m.value for m in grid_pretreatment_jp.find(emd)])\n",
    "            grid_vitrification_cryogen = ', '.join([m.value for m in grid_vitrification_cryogen_jp.find(emd)])\n",
    "            grid_support_topology = ', '.join([m.value for m in grid_support_topology_jp.find(emd)])\n",
    "\n",
    "            grid_vit_ctemp_units = [m.value for m in parse(grid_vit_ctemp_jp+'.units').find(emd)]\n",
    "            grid_vit_ctemp_values = [str(m.value) for m in parse(grid_vit_ctemp_jp+'.valueOf_').find(emd)]\n",
    "            grid_vit_ctemp = ','.join([t[0]+' '+t[1] for t in zip(grid_vit_ctemp_values, grid_vit_ctemp_units)])\n",
    "\n",
    "            grid_vit_chumid_units = [m.value for m in parse(grid_vit_chumid_jp+'.units').find(emd)]\n",
    "            grid_vit_chumid_values = [str(m.value) for m in parse(grid_vit_chumid_jp+'.valueOf_').find(emd)]\n",
    "            grid_vit_chumid = ', '.join([t[0]+' '+t[1] for t in zip(grid_vit_chumid_values, grid_vit_chumid_units)])\n",
    "\n",
    "            for doi in v['dois']:\n",
    "                metadlist.append({'doi':doi, \n",
    "                                  'emd_id': emd_id, \n",
    "                                  'sample_preparation_type': sample_preparation_type, \n",
    "                                  'agg_state': agg_state, \n",
    "                                  'sample_preparation_buffer_ph': buffer, \n",
    "                                  'grid_model': grid_model, \n",
    "                                  'grid_material': grid_material, \n",
    "                                  'grid_mesh': grid_mesh, \n",
    "                                  'grid_support_topology': grid_support_topology, \n",
    "                                  'grid_pretreatment': grid_pretreatment, \n",
    "                                  'grid_vitrification_cryogen': grid_vitrification_cryogen, \n",
    "                                  'grid_vit_ctemp': grid_vit_ctemp, \n",
    "                                  'grid_vit_chumid': grid_vit_chumid})\n",
    "        else:\n",
    "            print('ERROR: ', emd_exp.status_code)\n",
    "\n",
    "empiar_df = pd.DataFrame(metadlist)\n",
    "empiar_df.to_csv(loc+db_name+'/empiar_metadata.tsv', sep='\\t', index=False)\n",
    "empiar_dois = sorted(empiar_df['doi'].unique())\n",
    "empiar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the EMPIAR data from disk\n",
    "\n",
    "This is from local directory that we just created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empiar_df = pd.read_csv(loc+db_name+'/empiar/empiar_metadata.tsv', sep='\\t')\n",
    "empiar_dois = sorted(empiar_df['doi'].unique())\n",
    "empiar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts to Build / Delete the database\n",
    "\n",
    "If you need to restore a deleted database from backup, use the following shell commands:\n",
    "\n",
    "```\n",
    "$ createdb em_tech\n",
    "$ psql -d em_tech -f /local/file/path/em_tech/backup<date_time>.sql\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = os.environ['LOCAL_FILE_PATH']\n",
    "current_date_time = datetime.now()\n",
    "formatted_date_time = f'{current_date_time:%Y-%m-%d-%H-%M-%S}'\n",
    "backup_path = loc+'/'+db_name+'/backup'+formatted_date_time+'.sql'\n",
    "backup_ceifns_database(db_name, backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CEIFNS database from queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a collection based on EMPIAR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "step = 20\n",
    "for start_i in range(0, len(empiar_dois), step):\n",
    "    query = ' OR '.join(['doi:\"'+empiar_dois[i]+'\"' for i in range(start_i, start_i+step)])\n",
    "    addEMPCCollection_tool.run({'id': '3', 'name':'EMPIAR Papers', 'query':query, 'full_text':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_set(x):\n",
    "    out = ''\n",
    "    try:\n",
    "        out = ' '.join(set(x))\n",
    "    except:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "# identify papers that we have full text for in EMPIAR\n",
    "q = ldb.session.query(SKE.id) \\\n",
    "        .distinct() \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id==SKI.id) \\\n",
    "        .filter(SKC.id == '3') \\\n",
    "        .filter(or_(SKI.type == 'JATSFullText', SKI.type == 'PDFFullText')) \n",
    "dois_to_include = [d[0][4:] for d in q.all()]    \n",
    "\n",
    "empiar_gold_standard = []\n",
    "for i, row in empiar_df.iterrows():\n",
    "    if row.doi in dois_to_include:\n",
    "        empiar_gold_standard.append( row.to_dict() )\n",
    "empiar_gold_standard_df = pd.DataFrame(empiar_gold_standard)\n",
    "\n",
    "empiar_gs_df = empiar_gold_standard_df.groupby(['doi']).agg({'sample_preparation_type': join_set, \n",
    "                                                             'agg_state': join_set, \n",
    "                                                             'sample_preparation_buffer_ph': join_set, \n",
    "                                                             'grid_model': join_set, \n",
    "                                                             'grid_material': join_set, \n",
    "                                                             'grid_mesh': join_set, \n",
    "                                                             'grid_support_topology': join_set, \n",
    "                                                             'grid_pretreatment': join_set, \n",
    "                                                             'grid_vitrification_cryogen': join_set, \n",
    "                                                             'grid_vit_ctemp': join_set, \n",
    "                                                             'grid_vit_chumid': join_set}).reset_index()\n",
    "empiar_gs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import papers from DOIs pertaining to CryoET-Portal records `10000-10010`\n",
    "\n",
    "The [CryoET Data portal](https://chanzuckerberg.github.io/cryoet-data-portal/python-api.html) system is based on submitted data to our curation team, accompanied by papers referenced by DOIs. Each dataset is assigned an ID value associated with DOIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the EMPCSearchTool to run a query for the dois mentioned\n",
    "query = ' OR '.join(['doi:\"'+d+'\"' for d_id in dois for d in dois[d_id] ])\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "addEMPCCollection_tool.run(tool_input={'id': '0', 'name':'CryoET Portal (10000-10010)', 'query':query, 'full_text':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Database to include all CryoET papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_include = ['ID', 'CORPUS_NAME', 'QUERY']\n",
    "df = pd.read_csv(files(em_tech_queries).joinpath('EM_Methods.tsv'), sep='\\t')\n",
    "df = df.drop(columns=[c for c in df.columns if c not in cols_to_include])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QueryTranslator(df.sort_values('ID'), 'ID', 'QUERY', 'CORPUS_NAME')\n",
    "(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=['TITLE_ABS', 'METHODS'])\n",
    "corpus_names = df['CORPUS_NAME']\n",
    "\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "for (id, name, query) in zip(corpus_ids, corpus_names, epmc_queries):\n",
    "    if id != 2:\n",
    "        continue\n",
    "    addEMPCCollection_tool.run(tool_input={'id': id, 'name':name, 'query':query, 'full_text':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine + Sample CryoET + EMPIAR Collections to provide a test set of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_new_collection_from_intersection('4', 'EMPIAR CryoET Papers', '2', '3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_query = '''\n",
    "(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n",
    "    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n",
    "    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet ) AND \n",
    "(\"Machine Learning\" OR \"Artificial Intelligence\" OR \"Deep Learning\" OR \"Neural Networks\")\n",
    "'''\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "addEMPCCollection_tool.run(tool_input={'id': '6', \n",
    "                                       'name': 'Machine Learning in CryoET', \n",
    "                                       'query': ml_query, \n",
    "                                       'full_text': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, DeleteCollectionTool)][0]\n",
    "delCollection_tool.run(tool_input={'collection_id': '6'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break up TIAB of papers into sentences + classify by discourse\n",
    "\n",
    "NOTE - HUGGING FACE MODELS DO NOT WORK WELL ON THIS CORPUS. \n",
    "(NOT SURPRISINGLY - THEY WERE TRAINED ON MEDICAL PAPERS WHERE THE DIFFERENT SECTIONS OF THE PAPER WERE EXPLICITLY LABELED)\n",
    "\n",
    "USE LLMS TO DO THE EXTRACTION - GPT3.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\n",
    "t2.run(tool_input={'collection_id': '5', 'run_label': 'dev'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = '''{\n",
    "\"Background\": \"Eps15-homology domain containing proteins (EHDs) are eukaryotic, dynamin-related ATPases involved in cellular membrane trafficking. They oligomerize on membranes into filaments that induce membrane tubulation. While EHD crystal structures in open and closed conformations were previously reported, little structural information is available for the membrane-bound oligomeric form. Consequently, mechanistic insights into the membrane remodeling mechanism have remained sparse.\",\n",
    "\"Objectives_Methods\": \"Here, by using cryo-electron tomography and subtomogram averaging, we determined structures of nucleotide-bound EHD4 filaments on membrane tubes of various diameters at an average resolution of 7.6 Ã….\",\n",
    "\"Results_Conclusions\": \"Assembly of EHD4 is mediated via interfaces in the G-domain and the helical domain. The oligomerized EHD4 structure resembles the closed conformation, where the tips of the helical domains protrude into the membrane. The variation in filament geometry and tube radius suggests a spontaneous filament curvature of approximately 1/70 nm<sup>-1</sup>. Combining the available structural and functional data, we suggest a model for EHD-mediated membrane remodeling.\"\n",
    "}'''\n",
    "json.loads(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata extraction tool\n",
    "models = ['databricks_dbrx']\n",
    "for m in models:\n",
    "    llm = llms_lookup.get(m)\n",
    "    cb = AlhazenAgent(llm, llm, db_name=db_name)\n",
    "    t2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\n",
    "    t2.run(tool_input={'collection_id': '2', 'run_label': m, 'repeat_run': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\"doi:10.1101/2024.03.04.583254\", \"doi:10.1101/2023.11.21.567712\",\n",
    "            \"doi:10.3791/6515\", \"doi:10.1101/2023.07.28.550950\",\n",
    "            \"doi:10.1093/micmic/ozad067.483\", \"doi:10.1007/978-1-0716-2639-9_20\",\n",
    "            \"doi:10.1016/j.yjsbx.2022.100076\", \"doi:10.1016/j.xpro.2022.101658\",\n",
    "            \"doi:10.1016/j.cell.2022.06.034\", \"doi:10.1093/plphys/kiab449\",\n",
    "            \"doi:10.1073/pnas.2118020118\", \"doi:10.3791/62886\",\n",
    "            \"doi:10.20944/preprints202105.0098.v1\", \"doi:10.1016/bs.mcb.2020.12.009\",\n",
    "            \"doi:10.1007/978-1-0716-0966-8_1\", \"doi:10.1007/978-1-0716-0966-8_2\",\n",
    "            \"doi:10.21769/bioprotoc.3768\", \"doi:10.1371/journal.ppat.1008883\",\n",
    "            \"doi:10.1101/2020.05.19.104828\", \"doi:10.1073/pnas.1916331116\",\n",
    "            \"doi:10.1042/bst20170351_cor\", \"doi:10.1038/s41594-018-0043-7\",\n",
    "            \"doi:10.1007/978-1-4939-8585-2_4\", \"doi:10.1007/s41048-017-0040-0\",\n",
    "            \"doi:10.1007/978-1-4939-6927-2_20\", \"doi:10.1016/j.str.2015.03.008\",\n",
    "            \"doi:10.1007/978-1-62703-227-8_4\", \"doi:10.1016/b978-0-12-397945-2.00017-2\",\n",
    "            \"doi:10.1016/j.jmb.2010.10.021\", \"doi:10.1186/1757-5036-3-6\",\n",
    "            \"doi:10.1016/j.jmb.2008.03.014\", \"doi:10.1007/978-1-59745-294-6_20\"]\n",
    "\n",
    "for d in to_remove:\n",
    "    q = \"\"\"\n",
    "    SELECT DISTINCT n.id FROM langchain_pg_embedding as emb, \"Note\" as n\n",
    "    WHERE emb.cmetadata->>'n_type' = 'TiAbMappingNote__Discourse' AND\n",
    "        emb.cmetadata->>'about_id' = '{}' AND \n",
    "        emb.cmetadata->>'discourse_type' = 'ResultsConclusions' AND \n",
    "        emb.cmetadata->>'n_id' = n.id;\"\"\".format(d)\n",
    "    for row in ldb.session.execute(text(q)).all():\n",
    "        ldb.delete_note(row[0], commit_this=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "exp_q = ldb.session.query(SKE) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.type=='CitationRecord') \\\n",
    "        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n",
    "        .order_by(desc(SKE.publication_date))\n",
    "\n",
    "count = 0\n",
    "for e in tqdm(exp_q.all()):\n",
    "    q = ldb.session.query(N) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == e.id) \\\n",
    "        .filter(N.type =='TiAbMappingNote__Discourse')\n",
    "    for n in q.all():\n",
    "        dmap = json.loads(n.content) \n",
    "        if 'Objectives_Methods' in dmap.keys():\n",
    "            print('beep')\n",
    "            #new_dmap = {'Background': dmap.get('Background'), 'ObjectivesMethods': dmap.get('Objectives_Methods'), 'ResultsConclusions': dmap.get('Results_Conclusions')}\n",
    "            #n.content = json.dumps(new_dmap, indent=4)\n",
    "            #ldb.session.flush()\n",
    "#ldb.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "exp_q = ldb.session.query(SKE) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.type=='CitationRecord') \\\n",
    "        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n",
    "        .order_by(desc(SKE.publication_date))\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "count = 0\n",
    "for e in tqdm(exp_q.all()):\n",
    "    q = ldb.session.query(N) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == e.id) \\\n",
    "        .filter(N.type =='TiAbMappingNote__Discourse')\n",
    "\n",
    "\n",
    "    n = q.first()\n",
    "    dmap = json.loads(n.content)\n",
    "    '''Runs through the list of expressions, generates embeddings, and stores them in the database'''\n",
    "\n",
    "    for dtype in ['Background', 'ObjectivesMethods', 'ResultsConclusions']:\n",
    "        t = dmap.get(dtype)\n",
    "        if t is None:\n",
    "            continue\n",
    "        texts.append(t)\n",
    "        metadatas.append({'about_id': e.id, \\\n",
    "                        'about_type': 'ScientificKnowledgeExpression', \\\n",
    "                        'n_id': n.id, \\\n",
    "                        'n_type': 'TiAbMappingNote__Discourse', \\\n",
    "                        'discourse_type': dtype})\n",
    "\n",
    "docs = []\n",
    "for t,m in zip(texts, metadatas):\n",
    "    docs.append(Document(page_content=t, metadata=m))\n",
    "    \n",
    "db = PGVector.from_documents(\n",
    "    embedding=ldb.embed_model,\n",
    "    documents=docs,\n",
    "    collection_name=\"Note\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = '/Users/gully.burns/Documents/2024H1/models/discourse_tagger'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\", \n",
    "                                          truncation=True, \n",
    "                                          max_length=512)\n",
    "labels = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
    "lookup = {'LABEL_%d'%(i):l for i, l in enumerate(labels)}\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model = model_path, \n",
    "                      tokenizer=tokenizer, \n",
    "                      truncation=True,\n",
    "                      batch_size=8,\n",
    "                      device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = ldb\n",
    "collection_id = '2'\n",
    "\n",
    "q1 = self.session.query(SKE, SKI) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "        .filter(SKI_HP.has_part_id == SKF.id) \\\n",
    "        .filter(SKI.type == 'CitationRecord') \\\n",
    "        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \n",
    "\n",
    "for ske, ski in tqdm(q1.all()):\n",
    "    b = ''\n",
    "    om = ''\n",
    "    rc = ''  \n",
    "\n",
    "    fragments = []\n",
    "    for f in ski.has_part:\n",
    "      if f.type in ['title', 'abstract']:\n",
    "        fragments.append(f)\n",
    "\n",
    "    # USE AN LLM HERE INSTEAD OF A DEEP LEARNING CLASSIFER\n",
    "\n",
    "\n",
    "    for skf in sorted(fragments, key=lambda f: f.offset):\n",
    "        for s in self.sent_detector.tokenize(skf.content):\n",
    "            m = classifier(skf.content)\n",
    "            l = lookup.get(m[0].get('label'))\n",
    "            if l == 'BACKGROUND':\n",
    "                if len(b) > 0:\n",
    "                    b += '\\n'\n",
    "                b += s\n",
    "            elif l == 'OBJECTIVE' or l == 'METHODS':\n",
    "                if len(om) > 0:\n",
    "                    om += '\\n'\n",
    "                om += s\n",
    "            else: \n",
    "                if len(rc) > 0:\n",
    "                    rc += '\\n'\n",
    "                rc += s\n",
    "    skf_stem = ske.id+'.'+ski.type+'.'\n",
    "    if len(b) > 0:\n",
    "        f_b = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n",
    "                type='background_sentences', offset=-1, length=len(b),\n",
    "                name=skf_stem+'background', content=b)\n",
    "        self.session.add(f_b)\n",
    "        ski.has_part.append(f_b)\n",
    "        f_b.part_of = ski.id    \n",
    "    if len(om) > 0:\n",
    "        f_om = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n",
    "                type='objective_methods_sentences', offset=-1, length=len(om),\n",
    "                name=skf_stem+'objective_methods', content=om)\n",
    "        self.session.add(f_om)\n",
    "        ski.has_part.append(f_om)\n",
    "        f_om.part_of = ski.id\n",
    "    if len(rc) > 0:\n",
    "        f_rc = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n",
    "                type='results_conclusions_sentences', offset=-1, length=len(rc),\n",
    "                name=skf_stem+'results_conclusions', content=rc)\n",
    "        self.session.add(f_rc)\n",
    "        ski.has_part.append(f_rc)\n",
    "        f_rc.part_of = ski.id\n",
    "    self.session.flush()\n",
    "self.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = ldb\n",
    "collection_id = '2'\n",
    "#self.session.rollback()\n",
    "q2 = self.session.query(SKF) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "        .filter(SKI_HP.has_part_id == SKF.id) \\\n",
    "        .filter(SKI.type == 'CitationRecord') \\\n",
    "        .filter(or_(SKF.type == 'results_conclusions_sentences', \\\n",
    "                SKF.type == 'objective_methods_sentences', \\\n",
    "                SKF.type == 'background_sentences'))\n",
    "for skf in tqdm(q2.all()):\n",
    "    self.delete_fragment(skf.id)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = ldb\n",
    "collection_id = '2'\n",
    "#self.session.rollback()\n",
    "q2 = self.session.query(SKE, SKF) \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n",
    "        .filter(SKC_HM.has_members_id == SKE.id) \\\n",
    "        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n",
    "        .filter(SKI_HP.has_part_id == SKF.id) \\\n",
    "        .filter(SKI.type == 'CitationRecord') \\\n",
    "        .filter(SKF.type == 'objective_methods_sentences') \\\n",
    "        .order_by(desc(SKE.publication_date)) \\\n",
    "        .order_by(SKF.name)\n",
    "\n",
    "for ske, skf in tqdm(q2.all()):\n",
    "    print(skf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get full text copies of all the papers about CryoET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Get full text copies of all papers in the collection with id=\"2\".'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_new_collection_from_sample('5', 'EMPIAR CryoET Papers Tests', '4', 20, ['ScientificPrimaryResearchArticle', 'ScientificPrimaryResearchPreprint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKC.id, SKC.name, SKE.id, SKI.type) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_HR.has_representation_id==SKI.id) \n",
    "df = pd.DataFrame(q.all(), columns=['id', 'collection name', 'doi', 'item type'])\n",
    "df.pivot_table(index=['id', 'collection name'], columns='item type', values='doi', aggfunc=lambda x: len(x.unique())).fillna(0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey + Run Classifications over Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "l = []\n",
    "q = ldb.session.query(N, SKE) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n",
    "\n",
    "output = []        \n",
    "print(len(q.all()))\n",
    "for n, ske in q.all():\n",
    "    ldb.delete_note(n.id)    \n",
    "print(len(q.all()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\n",
    "t.run({'collection_id': '5', 'classification_type':'cryoet_study_types', 'repeat_run':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\n",
    "t.run({'collection_id': '2', 'classification_type':'cryoet_study_types'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "ldb.session.rollback()\n",
    "q = ldb.session.query(N, SKE) \\\n",
    "        .join(NIA, NIA.Note_id == N.id) \\\n",
    "        .join(SKE, SKE.id == NIA.is_about_id) \\\n",
    "        .join(SKC_HM, SKE.id == SKC_HM.has_members_id) \\\n",
    "        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n",
    "        .filter(SKC_HM.ScientificKnowledgeCollection_id == '5') \\\n",
    "        .order_by(SKE.id, N.provenance)\n",
    "\n",
    "output = []        \n",
    "for n, ske in q.all():\n",
    "        tup = json.loads(n.content)\n",
    "        tup['doi'] = 'http://doi.org/'+re.sub('doi:', '', ske.id)\n",
    "        tup['year'] = ske.publication_date.year\n",
    "        tup['month'] = ske.publication_date.month\n",
    "        tup['ref'] = ske.content\n",
    "        output.append(tup)\n",
    "df = pd.DataFrame(output).sort_values(['year', 'month'], ascending=[False, False])\n",
    "df.to_csv(loc+'/'+db_name+'/cryoet_study_types.tsv', sep='\\t')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_type_lookup = {'A': 'Viral Pathogens', \n",
    "                     'B': \"Mutated protein structure\", \n",
    "                     'C': 'Bacterial pathogens', \n",
    "                     'D': 'Plant cells', \n",
    "                     'E': 'Material science', \n",
    "                     'F': 'Intracellular Transport Structure', \n",
    "                     'G': 'Synapses or Vesicle Release', \n",
    "                     'H': 'Other Intracellular Structure', \n",
    "                     'I': 'Cellular Processes',\n",
    "                     'J': 'Dynamics of molecular interactions',    \n",
    "                     'K': 'New CryoET imaging methods', \n",
    "                     'L': 'New data analysis methods'}\n",
    "\n",
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "step = 20\n",
    "\n",
    "for k in study_type_lookup.keys():\n",
    "    df1 = df[df['cryoet_study_type_code'] == k]\n",
    "    dois_to_add = [re.sub('http://doi.org/', 'doi:', r.doi) for i, r in df1.iterrows()]\n",
    "\n",
    "    c_id = '2.'+k\n",
    "    c_name = 'CryoET - ' + study_type_lookup[k]\n",
    "\n",
    "    corpus = None\n",
    "    all_existing_query = ldb.session.query(SKC).filter(SKC.id==c_id)\n",
    "    for c in all_existing_query.all():\n",
    "      corpus = c\n",
    "    if corpus is None:      \n",
    "      corpus = ScientificKnowledgeCollection(id=c_id,\n",
    "                                           type='skem:ScientificKnowledgeCollection',\n",
    "                                           name=c_name,\n",
    "                                           has_members=[])\n",
    "    ldb.session.add(corpus)\n",
    "    ldb.session.flush()\n",
    "\n",
    "    for doi in tqdm(dois_to_add):\n",
    "        p = ldb.session.query(SKE) \\\n",
    "            .filter(SKE.id==doi).first()\n",
    "        if p is None:\n",
    "          continue\n",
    "        ldb.session.add(p)\n",
    "        corpus.has_members.append(p)\n",
    "        p.member_of.append(corpus)\n",
    "        ldb.session.flush()\n",
    "ldb.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_collection_tool = [t for t in cb.tk.get_tools() if isinstance(t, DeleteCollectionTool)][0]\n",
    " \n",
    "for k in study_type_lookup.keys():\n",
    "    print(k)\n",
    "    delete_collection_tool.run({'collection_id': '2.'+k})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey + Run Extractions over Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractExtraction_OneDocAtATime_Tool)][0]\n",
    "t.run({'collection_id': '5', 'extraction_type':'cryoet'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests + Checks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent tool selection + execution + interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.agent_executor.invoke({'input':'Hi who are you and what can you do?'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MetaData Extraction Chain over listed papers\n",
    "\n",
    "Here, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ldb.session.query(SKE.id) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKC.id=='5')  \n",
    "dois = [e.id for e in q.all()]\n",
    "dois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to count tokens submitted to the server as a way of tracking usage. \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device='mps', token=os.environ['HF_API_KEY'])\n",
    "prompt = \"The methods section of the paper is as follows:\"\n",
    "tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(len(tokenized[\"input_ids\"][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How long are methods sections in the CryoET papers?\n",
    "ldb.session.rollback()\n",
    "q = ldb.session.query(SKE.id) \\\n",
    "        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "        .filter(SKC_HM.has_members_id==SKE.id) \\\n",
    "        .filter(SKC.id=='2') \\\n",
    "        .filter(or_(SKE.type=='ScientificPrimaryResearchArticle', SKE.type=='ScientificPrimaryResearchPreprint'))\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "tups = []\n",
    "for e in tqdm(q.all()):\n",
    "    item_types = set()\n",
    "    item_type = None\n",
    "    for i in ldb.list_items_for_expression(e.id):\n",
    "        item_types.add(i.type)\n",
    "    for i_type in item_types:\n",
    "        if i_type == 'CitationRecord':\n",
    "            continue\n",
    "        item_type = i_type\n",
    "        break\n",
    "    if item_type is None:\n",
    "        continue\n",
    "\n",
    "    fragments = [f.content for f in ldb.list_fragments_for_paper(e.id, item_type, fragment_types=['section'])]\n",
    "    on_off = False\n",
    "    text = ''\n",
    "    all_text = ''\n",
    "    for t in fragments:\n",
    "        all_text += t\n",
    "        l1 = t.split('\\n')[0].lower()\n",
    "        if 'method' in l1:\n",
    "            on_off = True\n",
    "        elif 'results' in l1 or 'discussion' in l1 or 'conclusion' in l1 or 'acknowledgements' in l1 \\\n",
    "                or 'references' in l1 or 'supplementary' in l1 or 'appendix' in l1 or 'introduction' in l1 or 'abstract' in l1 or 'cited' in l1:\n",
    "            on_off = False\n",
    "        if on_off:\n",
    "            if len(text) > 0:\n",
    "                text += '\\n\\n'\n",
    "            text += t\n",
    "\n",
    "    all_text_length = len(tokenizer(all_text, return_tensors=\"pt\")['input_ids'][0])\n",
    "    text_length = len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n",
    "    tups.append({'doi':e.id, 'doc_length': all_text_length, 'method_length': text_length})\n",
    "df_length = pd.DataFrame(tups)\n",
    "df_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_length[df_length['method_length']>8000]))\n",
    "print(len(df_length[df_length['method_length']<8000]))\n",
    "\n",
    "\n",
    "def plot_length_distribution(df_length):\n",
    "    plt.hist(df_length, bins=10)\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths')\n",
    "    plt.show()\n",
    "\n",
    "plot_length_distribution(df_length['method_length'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, r in tqdm(df_length.iterrows()):\n",
    "    if len(df[df['doi']==r['doi']]) > 0:\n",
    "        continue\n",
    "    # Run the metadata extraction tool on the doi\n",
    "    try: \n",
    "        t2.run(tool_input={'paper_id': r['doi'], 'extraction_type': 'cryoet', 'run_label': 'test_llama3'})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "df2 = pd.DataFrame()\n",
    "for i, r in tqdm(df_length.iterrows()):\n",
    "        item_types = set()\n",
    "        l = t2.read_metadata_extraction_notes(r['doi'], 'cryoet', 'test')\n",
    "        if(len(l) == 0):\n",
    "            continue\n",
    "        df2 = pd.concat([df2, pd.DataFrame(l)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "df = pd.DataFrame()\n",
    "for i, r in tqdm(df_length.iterrows()):\n",
    "    if r['method_length'] < 8000:\n",
    "        item_types = set()\n",
    "        l = t2.read_metadata_extraction_notes(r['doi'], 'cryoet', 'test_llama3')\n",
    "        if(len(l) == 0):\n",
    "            continue\n",
    "        df = pd.concat([df, pd.DataFrame(l)]) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['doi']=='doi:10.1101/2022.04.12.488077']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['doi']=='doi:10.1101/2022.04.12.488077']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "metadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\n",
    "t2.compile_answers('cryoet', metadata_dir)\n",
    "t2.write_answers_as_notes('cryoet', metadata_dir)\n",
    "#sorted(list(set([doi for q in t2.examples for doi in t2.examples[q]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "\n",
    "# Hack to get the path to the metadata directory as a string\n",
    "#metadata_dir = str(files(cryoet_portal_metadata).joinpath('temp'))[0:-4]\n",
    "metadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\n",
    "\n",
    "# Compile the answers from the metadata directory\n",
    "t2.compile_answers('cryoet', metadata_dir)\n",
    "\n",
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "df = pd.DataFrame()\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df = pd.concat([df, pd.DataFrame(l)]) \n",
    "     \n",
    "# Iterate over papers to run the metadata extraction tool\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "\n",
    "    # Skip if the doi is already in the database\n",
    "    if len(df)>0 and d in df.doi.unique():\n",
    "        continue\n",
    "\n",
    "    # Run the metadata extraction tool on the doi\n",
    "    t2.run(tool_input={'paper_id': d, 'extraction_type': 'cryoet', 'run_label': 'test_llama3'})\n",
    "\n",
    "    # Add the results to the dataframe    \n",
    "    l2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df = pd.concat([df, pd.DataFrame(l2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dataframe to store previously extracted metadata\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "df_final = pd.DataFrame()\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    df_final = pd.concat([df_final, pd.DataFrame(l)]) \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "\n",
    "#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\n",
    "l = []\n",
    "for d in [d for d in dois]:\n",
    "    item_types = set()\n",
    "    pred1 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n",
    "    pred2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test_dbrx')\n",
    "    gold = t2.read_metadata_extraction_notes(d, 'cryoet', 'gold') \n",
    "    if pred1 is None or pred2 is None or gold is None or \\\n",
    "            len(pred1)==0 or len(pred2)==0 or len(gold)!=1:\n",
    "        continue\n",
    "    for k in gold[0]:\n",
    "        g_case = gold[0][k]\n",
    "        if g_case=='' or g_case is None:\n",
    "            continue    \n",
    "        for j, p_case in enumerate(pred1):\n",
    "            sim = fuzz.ratio(str(g_case), str(p_case.get(k,''))) / 100.0\n",
    "            print(k, str(g_case), str(p_case.get(k,'')), sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n",
    "\n",
    "df = t2.report_metadata_extraction_for_collection('5', 'cryoet', 'test').set_index('doi')\n",
    "df.to_csv(loc+'/'+db_name+'/reports/cryoet_metadata_gpt4.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.create_zip_archive_of_full_text_files('5', loc+'/'+db_name+'/full_text_files.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n",
    "        .filter(N.id == NIA.Note_id) \\\n",
    "        .filter(NIA.is_about_id == SKE.id) \\\n",
    "        .filter(N.type == 'MetadataExtractionNote') \n",
    "l = []\n",
    "for row in q3.all():\n",
    "    paper = row[0]\n",
    "    name = row[1]\n",
    "#    provenance = json.loads(row[2])\n",
    "    result = json.loads(row[3])\n",
    "    kv = {k:result[k] for k in result}\n",
    "    kv['DOI'] = paper\n",
    "    kv['run'] = name\n",
    "    l.append(kv)\n",
    "# create a dataframe from the list of dictionaries with DOI as the index column\n",
    "if len(l)>0:\n",
    "    df = pd.DataFrame(l).set_index(['DOI', 'run'])\n",
    "else: \n",
    "    df = pd.DataFrame()\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "for row in q3.all():\n",
    "    d_id = row[0]\n",
    "    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n",
    "    notes_to_delete = []\n",
    "    for n in ldb.read_notes_about_x(e):\n",
    "        notes_to_delete.append(n.id)\n",
    "    for n in notes_to_delete:\n",
    "        ldb.delete_note(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol Modeling + Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "slm = ChatOllama(model='stablelm-zephyr') \n",
    "llm = ChatOllama(model='mixtral:instruct') \n",
    "llm2 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "llm3 = ChatOpenAI(model='gpt-3.5-turbo') \n",
    "d = (\"This tool attempts to draw a protocol design from the description of a scientific paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ProcotolEntitiesExtractionTool(db=ldb, llm=llm3, description=d)\n",
    "entities = t1.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = ProcotolProcessesExtractionTool(db=ldb, llm=llm3, description=d)\n",
    "processes = t2.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\n",
    "processes.get('data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
