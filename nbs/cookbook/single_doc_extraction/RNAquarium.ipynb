{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNAquarium  \n",
    "\n",
    "> Using LLMs to extract information from RNA studies in Zebrafish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports\n",
    "\n",
    "Setting python imports, environment variables, and other crucial set up parameters here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen.aliases import *\n",
    "from alhazen.core import get_langchain_chatmodel, MODEL_TYPE\n",
    "from alhazen.agent import AlhazenAgent\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.core import get_langchain_chatmodel, MODEL_TYPE\n",
    "from alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import * \n",
    "from alhazen.tools.protocol_extraction_tool import *\n",
    "from alhazen.toolkit import *\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\n",
    "from alhazen.utils.searchEngineUtils import *\n",
    "\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import create_engine, text, exists, func, or_, and_, not_, desc, asc\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import yaml\n",
    "\n",
    "import pymde\n",
    "import torch\n",
    "import local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\n",
    "\n",
    "from alhazen.utils.searchEngineUtils import load_paper_from_openalex, read_references_from_openalex \n",
    "from pyalex import config, Works, Work\n",
    "config.email = \"gully.burns@chanzuckerberg.com\"\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import local_resources.data_files.rnaquarium as rnaquarium\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the PostGresQL database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ALHAZEN_DB_NAME'] = 'rnaquarium'\n",
    "os.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n",
    "    \n",
    "if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "    raise Exception('Which database do you want to use for this application?')\n",
    "db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "loc = os.environ['LOCAL_FILE_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup utils, agents, and tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "llm = ChatOllama(model='mixtral:instruct') \n",
    "llm2 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "llm2 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "\n",
    "cb = AlhazenAgent(llm2, llm2)\n",
    "print('AGENT TOOLS')\n",
    "for t in cb.tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)\n",
    "\n",
    "test_tk = MetadataExtractionToolkit(db=ldb, llm=llm2)\n",
    "print('\\nTESTING TOOLS')\n",
    "for t in test_tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts to Build / Delete the database\n",
    "\n",
    "If you need to restore a deleted database from backup, use the following shell commands:\n",
    "\n",
    "```\n",
    "$ createdb em_tech\n",
    "$ psql -d em_tech -f /local/file/path/em_tech/backup<date_time>.sql\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ceifns_database(os.environ['ALHAZEN_DB_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CEIFNS database from 900 dois in database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from the spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(files(rnaquarium).joinpath('RNAquarium_paper_list.tsv'), sep='\\t')\n",
    "dois = df['DOI'].to_list()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to execute paged queries (length 40) over the European PMC for each of the DOIs mentioned in the spreadsheet loaded above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n",
    "step = 40\n",
    "for start_i in range(0, len(dois), step):\n",
    "    query = ' OR '.join(['doi:\\\"'+dois[i]+'\\\"' for i in range(start_i, start_i+step) if i < len(dois)])\n",
    "    addEMPCCollection_tool.run({'id': '0', 'name':'RNAquarium Papers', 'query':query, 'full_text':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to check how many papers from the list are loaded in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare contents of database to the list of dois\n",
    "missing_list = []\n",
    "titles = []\n",
    "for doi in dois:\n",
    "    row = df[df['DOI']==doi]\n",
    "    doi_in_db = ldb.session.query(SKE).filter(SKE.id=='doi:'+doi.lower()).all()\n",
    "    if len(doi_in_db) == 0:\n",
    "        print('DOI: '+doi)\n",
    "        print('\\t%s (%d) %s %s'%(row['Author'].iloc[0],row['Publication Year'].iloc[0],row['Title'].iloc[0],row['Journal Abbreviation'].iloc[0]))\n",
    "        missing_list.append(doi)\n",
    "        titles.append(row['Title'].iloc[0])\n",
    "print('%d Missing DOIs'%(len(missing_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OpenAlex as filler to add papers that were missed on EPMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n",
    "corpus = ldb.session.query(SKC).filter(SKC.id=='0').first()\n",
    "count = 0\n",
    "print(len(corpus.has_members))\n",
    "\n",
    "papers_to_index = []\n",
    "for i, doi in enumerate(missing_list):\n",
    "    p = load_paper_from_openalex(doi)\n",
    "    ldb.session.add(p)\n",
    "    corpus.has_members.append(p)\n",
    "    p.member_of.append(corpus)\n",
    "    for item in p.has_representation:\n",
    "        for f in item.has_part:\n",
    "            #f.content = '\\n'.join(self.sent_detector.tokenize(f.content))\n",
    "            f.part_of = item.id\n",
    "            ldb.session.add(f)\n",
    "        item.represented_by = p.id\n",
    "        ldb.session.add(item)\n",
    "    papers_to_index.append(p)\n",
    "    ldb.session.flush()\n",
    "\n",
    "ldb.embed_expression_list(papers_to_index)\n",
    "\n",
    "ldb.session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get full text copies of all the papers about CryoET\n",
    "\n",
    "This invokes the agent directly to make it easy to run the retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.db.session.rollback()\n",
    "cb.agent_executor.invoke({'input':'Retrieve full text for the collection with id=\"0\".'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a basic report of the composition over all collections in the database (listed by types of items).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.db.report_collection_composition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.db.report_non_full_text_for_collection(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests + Checks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent tool selection + execution + interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to test the agent's \n",
    "cb.agent_executor.invoke({'input':'Hi who are you and what can you do?'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MetaData Extraction Chain over listed papers\n",
    "\n",
    "Here, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(files(cryoet_portal_metadata).joinpath('temp'))[0:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata extraction tool\n",
    "t2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_EverythingEverywhere_Tool)][0]\n",
    "\n",
    "# Hack to get the path to the metadata directory as a string\n",
    "metadata_dir = str(files(rnaquarium).joinpath('temp'))[0:-4]\n",
    "\n",
    "# Compile the answers from the metadata directory\n",
    "#t2.compile_answers('cryoet', metadata_dir)\n",
    "\n",
    "# Create a dataframe to store previously extracted metadata\n",
    "df = pd.DataFrame()\n",
    "for d_id in dois:\n",
    "    item_types = set()\n",
    "    #d_id = 'doi:'+d\n",
    "    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, 'rnaquarium')) \n",
    "    df = pd.concat([df, df2]) \n",
    "     \n",
    "# Iterate over papers to run the metadata extraction tool\n",
    "for d_id in dois[0:10]:\n",
    "    item_types = set()\n",
    "    #d_id = 'doi:'+d\n",
    "\n",
    "    # Skip if the doi is already in the database\n",
    "    if len(df)>0 and d_id in df.doi.unique():\n",
    "        continue\n",
    "\n",
    "    # Run the metadata extraction tool on the doi\n",
    "    t2.run(tool_input={'paper_id': d_id, 'extraction_type': 'rnaquarium'})\n",
    "\n",
    "    # Add the results to the dataframe\n",
    "    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, 'rnaquarium')) \n",
    "    df = pd.concat([df, df2]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = cb.db.session.query(N) \\\n",
    "    .filter(N.id == NIA.Note_id) \\\n",
    "    .filter(N.type == 'MetadataExtractionNote') \\\n",
    "    .filter(N.name.like('rnaquarium_%')) \n",
    "l = []\n",
    "for n in q.all():\n",
    "    tup = json.loads(n.content)\n",
    "    t, doi, label = n.name.split('__')\n",
    "    tup['doi'] = 'doi:'+doi\n",
    "    tup['extraction_type'] = t\n",
    "    tup['run_label'] = label\n",
    "    l.append(tup)\n",
    "report_df = pd.DataFrame(l).set_index('doi')\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_csv(loc+'/rnaquarium_metadata_extraction_report.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store previously extracted metadata\n",
    "df = pd.DataFrame()\n",
    "for d_id in dois:\n",
    "    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, 'rnaquarium')) \n",
    "    df = pd.concat([df, df2]) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n",
    "# clear all notes across papers listed in `dois` list\n",
    "for d in list(set(dois[0:10])):\n",
    "    d_id = 'doi:'+d\n",
    "    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n",
    "    notes_to_delete = []\n",
    "    if e is None:\n",
    "        continue\n",
    "    for n in ldb.read_notes_about_x(e):\n",
    "        notes_to_delete.append(n.id)\n",
    "    for n in notes_to_delete:\n",
    "        ldb.delete_note(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol Modeling + Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "slm = ChatOllama(model='stablelm-zephyr') \n",
    "llm = ChatOllama(model='mixtral:instruct') \n",
    "llm2 = ChatOpenAI(model='gpt-4-1106-preview') \n",
    "d = (\"This tool attempts to draw a protocol design from the description of a scientific paper.\")\n",
    "t = ProcotolExtractionTool(db=ldb, llm=llm2, description=d)\n",
    "t.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077', 'extraction_type': 'cryoet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ldb.session.rollback()\n",
    "rag_embeddings_list = [json.loads(e[0]) for e in ldb.session.execute(text(\"\"\"\n",
    "                        SELECT DISTINCT emb.embedding \n",
    "                         FROM langchain_pg_embedding as emb, \n",
    "                            \"ScientificKnowledgeExpression\" as ske,\n",
    "                            \"ScientificKnowledgeCollection_has_members\" as skc_hm\n",
    "                         WHERE cmetadata->>'i_type' = 'CitationRecord' AND\n",
    "                            cmetadata->>'e_id' = ske.id AND \n",
    "                            ske.id = skc_hm.has_members_id AND\n",
    "                            skc_hm.\"ScientificKnowledgeCollection_id\"='0';\n",
    "                        \"\"\")).fetchall()]\n",
    "rag_embeddings_tensor = torch.FloatTensor(rag_embeddings_list)\n",
    "\n",
    "proj_embeddings = pymde.preserve_neighbors(rag_embeddings_tensor, constraint=pymde.Standardized()).embed()\n",
    "pymde.plot(proj_embeddings) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "alhazen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
