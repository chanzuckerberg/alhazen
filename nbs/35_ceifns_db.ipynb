{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database for Scientific Knowledge \n",
    "\n",
    ">  A local Postgresql database of scientific content with associated information generated by the agent. The intent is to use this repository as 'memory' in a Langchain-enabled agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.ceifns_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a local database for use with the Alhazen schema conforming to the following schema:\n",
    "\n",
    "![Schema](pics/schema/Alhazen_Schema_v0.0.2.png) \n",
    "\n",
    "The key elements of the schema are:\n",
    "\n",
    "**Abstract Parent Class**\n",
    "\n",
    "* *InformationContentEntity* - a named piece of information (all other entities inherit from this)\n",
    "  \n",
    "**Classes Denoting Extant Scientific Knowledge from External Sources**\n",
    "\n",
    "* *ScientificKnowledgeCollection* - a collection of scientific knowledge expressions\n",
    "* *ScientificKnowledgeExpression* - an expression of scientific knowledge (e.g., a paper, a database record, a blog post, etc.) \n",
    "* *ScientificKnowledgeItem* - the data item that manifests the expression (e.g., a pdf, an html data file, a yaml file, etc.) \n",
    "* *ScientificKnowledgeFragment* - a relevant fragment of the item (e.g., a paragraph, a table, a figure, etc.)            \n",
    "\n",
    "**Classes Denoting Information Generated by Alhazen**\n",
    "\n",
    "* *Note* - an annotation of the fragment (e.g., a comment, a question, a link to another fragment, etc.)\n",
    "\n",
    "> Our definitions are inspired by [FABIO](http://www.sparontologies.net/ontologies/fabio), but only differentiate between '*expression*' (as the reference to an scientific underlying work product, e.g., a paper or database record) and '*item*' (as the actual data that delivers on that expression). \n",
    "> The system is coded in LinkML and generated as a Postgresql database and a SQLAlchemy ORM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.pdf_research_article_text_extractor import LAPDFBlockLoader, HuridocsPDFLoader\n",
    "\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "from alhazen.schema_sqla import ScientificKnowledgeCollection, \\\n",
    "    ScientificKnowledgeExpression, ScientificKnowledgeExpressionXref, ScientificKnowledgeCollectionHasMembers, \\\n",
    "    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \\\n",
    "    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \\\n",
    "    InformationResource, Note, NoteIsAbout\n",
    "import alhazen.schema_python as linkml_py\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "from langchain.pydantic_v1 import BaseModel, Extra, Field, root_validator\n",
    "import local_resources.linkml as linkml\n",
    "import nltk.data\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sqlalchemy import create_engine, exists, Engine\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "import sqlite3  \n",
    "import sys\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import uuid\n",
    "from langchain.vectorstores.pgvector import PGVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class QuerySpec:\n",
    "    \n",
    "    \"\"\"Class that permits definition of a query specification for a literature database .\"\"\"\n",
    "    name: str\n",
    "    id_col: str\n",
    "    query_col: str\n",
    "    name_col: str\n",
    "    col_map: dict[str, str] = field(default_factory=dict)\n",
    "    sections: list[str] = field(default_factory=list)\n",
    "\n",
    "    def process_cdf(self, cdf: pd.DataFrame):\n",
    "        cdf = cdf.rename(columns={self.id_col:'ID', self.query_col:'QUERY'})\n",
    "        cdf = cdf.rename(columns=self.col_map)\n",
    "        cdf = cdf.fillna('').rename(\n",
    "            columns={c:re.sub('[\\s\\(\\)]','_', c.upper()) for c in cdf.columns}\n",
    "            )\n",
    "        cdf.QUERY = [re.sub('^http[s]*://', '', r.QUERY) if r.QUERY[:4]=='http' else r.QUERY \n",
    "                     for i,r in cdf.iterrows()] \n",
    "        cdf.QUERY = [re.sub('/$', '', r.QUERY.strip())  \n",
    "                        for i,r in cdf.iterrows()]\n",
    "        return cdf\n",
    "\n",
    "class Ceifns_LiteratureDb(BaseModel):\n",
    "  \"\"\"This class runs a set of queries on external literature databases to build a local database of linked corpora and papers.\n",
    "\n",
    "  Functionality includes:\n",
    "\n",
    "    * Executes queries over European PMC \n",
    "    * Can run combinatorial sets of queries using a dataframe structure\n",
    "        * Requires a column of queries expressed in boolean logic\n",
    "        * Optional to define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n",
    "    * Has capability to run boolean logic over sources (currently only European PMC, but possibly others) \n",
    "    * Builds a local Postgresql database with tables for collections, expressions, items, fragments, and notes. \n",
    "    * Provides an API for querying the database and returning results as sqlAlchemy objects.\n",
    "    * Permits user to download a local copy of full text papers in NXML(JATS), PDF, and HTML format.\n",
    "  \"\"\"\n",
    "  loc: str = Field()\n",
    "  name: str = Field()\n",
    "  engine: Engine = Field(default=None, init=False)\n",
    "  session: Session = Field(default=None, init=False)\n",
    "  sent_detector: nltk.tokenize.punkt.PunktSentenceTokenizer = Field(default=None, init=False)\n",
    "\n",
    "  class Config:\n",
    "    \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    \n",
    "    if self.loc[-1] != '/':\n",
    "      self.loc = self.loc + '/' \n",
    "    if os.path.exists(self.loc) is False:\n",
    "      os.makedirs(self.loc)\n",
    "\n",
    "    db_dir = Path(self.loc + self.name)\n",
    "    if db_dir.exists() is False:\n",
    "      os.makedirs(db_dir)\n",
    "\n",
    "    self.engine = create_engine('postgresql+psycopg2:///'+self.name)\n",
    "\n",
    "    self.start_session() # instantiate session \n",
    "\n",
    "    log_path = '%s%s_log.txt'%(self.loc, self.name)\n",
    "    if os.path.exists(log_path) is False:\n",
    "      Path(log_path).touch()\n",
    "    \n",
    "    self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "  def start_session(self):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    return self.session\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Add Collections, Expressions, Items, and Fragments\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def add_full_text_for_expression(self, e,  \n",
    "                                   get_nxml=True, \n",
    "                                   get_pdf=False, \n",
    "                                   get_html=False):\n",
    "    \"\"\"Adds Full Text ScientificKnowledgeItem data to given ScientificKnowledgeExpression if full text is already downloaded in the `{loc}/ft/{doi}` directory.\"\"\"\n",
    "\n",
    "    doi = read_information_content_entity_iri(e, 'doi')\n",
    "    if doi is None:\n",
    "      return  \n",
    "    \n",
    "    # Check local directory for nxml file\n",
    "    base_file_path = '%s%s/ft/'%(self.loc, self.name)\n",
    "    if get_nxml:\n",
    "      nxml_file_path = base_file_path+doi+'.nxml'\n",
    "      if os.path.exists(nxml_file_path):\n",
    "        with open(nxml_file_path, 'r') as f:\n",
    "          xml = f.read()\n",
    "          try:\n",
    "            d = NxmlDoc(doi, xml)\n",
    "          except Exception as ex:\n",
    "            return\n",
    "          doc_text = d.text \n",
    "          if 'body' in [so.element.tag for so in d.standoffs]:\n",
    "            ski_type = 'JATSFullText'\n",
    "          else:\n",
    "            ski_type = 'CitationRecord'     \n",
    "          ski_id = str(uuid.uuid4().hex)[:10]\n",
    "          ski = ScientificKnowledgeItem(id=ski_id, \n",
    "                                        content=doc_text,\n",
    "                                        xref=['file:'+nxml_file_path],\n",
    "                                        type=ski_type,)\n",
    "          e.has_representation.append(ski)\n",
    "          self.session.add(ski)\n",
    "          self.session.flush()\n",
    "          self.add_sections_as_fragments_from_nxmldoc(ski, d)\n",
    "\n",
    "    # Check local directory for pdf file\n",
    "    if get_pdf:\n",
    "      pdf_path = base_file_path+doi+'.pdf'\n",
    "      loader = HuridocsPDFLoader(pdf_path)\n",
    "      docs = loader.load(curi_id='doi:'+doi) \n",
    "      skfs = []\n",
    "      doc_text = ''\n",
    "      ski_id = str(uuid.uuid4().hex)[:10]\n",
    "      for i, d in enumerate(docs):\n",
    "        sentence_split_text = '\\n'.join(self.sent_detector.tokenize(d.page_content))\n",
    "\n",
    "        skf = ScientificKnowledgeFragment(id=ski_id+'.'+str(i), \n",
    "                                             type='section',\n",
    "                                             offset=len(doc_text),\n",
    "                                             length=len(sentence_split_text),\n",
    "                                             name=sentence_split_text.split('\\n')[0],\n",
    "                                             content=sentence_split_text)\n",
    "        if len(doc_text) > 0:\n",
    "          doc_text += '\\n\\n'\n",
    "        doc_text += sentence_split_text\n",
    "        self.session.add(skf)\n",
    "        self.session.flush()\n",
    "        skfs.append(skf)\n",
    "      ski_type = 'PDFFullText'\n",
    "      ski = ScientificKnowledgeItem(id=ski_id, \n",
    "                                    content=doc_text,\n",
    "                                    xref=['file:'+pdf_path],\n",
    "                                    type=ski_type,)\n",
    "      e.has_representation.append(ski)\n",
    "      self.session.add(ski)\n",
    "      self.session.flush()\n",
    "      for skf in skfs:\n",
    "        ski.has_part.append(skf)\n",
    "        skf.part_of = ski.id\n",
    "        self.session.flush()\n",
    "        \n",
    "    if get_html:\n",
    "      placeholder = 1  \n",
    "\n",
    "  def add_sections_as_fragments_from_nxmldoc(self, item, d):\n",
    "    \"\"\"Adds fragments from an NxmlDoc object to a ScientificKnowledgeItem object.\"\"\"\n",
    "\n",
    "    # ['PMID', 'PARAGRAPH_ID', 'TAG', 'TAG_TREE', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'\n",
    "    fragments = []\n",
    "    df = d.build_enhanced_document_dataframe()\n",
    "    df = df[df.TAG_TREE!='']\n",
    "    # Can search for a substring in top section headers\n",
    "    # df3 = df[df.TOP_SECTION.str.contains('method', case=False)]\n",
    "\n",
    "    df3 = df[df.TOP_SECTION!='']\n",
    "    df4 = df3.groupby('SECTION_TREE').agg({'OFFSET': 'min', 'LENGTH': 'sum', 'PLAIN_TEXT':lambda x: '\\n'.join(x)}).sort_values('OFFSET')\n",
    "    df4 = df4.reset_index(drop=False)\n",
    "    \n",
    "    if df4 is None:\n",
    "      return\n",
    "    \n",
    "    for i, tup in df4.iterrows():      \n",
    "      sentence_split_text = '\\n'.join(self.sent_detector.tokenize(tup.PLAIN_TEXT))\n",
    "      fragment = ScientificKnowledgeFragment(id=item.id[:10]+'.'+str(i), \n",
    "                                             type='section',\n",
    "                                             offset=tup.OFFSET,\n",
    "                                             length=tup.LENGTH,\n",
    "                                             name=tup.SECTION_TREE,\n",
    "                                             content=sentence_split_text)\n",
    "      self.session.add(fragment)\n",
    "      self.session.flush()\n",
    "      item.has_part.append(fragment)\n",
    "      fragment.part_of = item.id\n",
    "      fragments.append(fragment)\n",
    "      self.session.flush()     \n",
    "\n",
    "  def add_full_text_for_collection(self, collection_id, \n",
    "                                   get_nxml=True, get_pdf=True, get_html=True):\n",
    "    for e in self.list_expressions(collection_id=collection_id):\n",
    "      doi = read_information_content_entity_iri(e, 'doi')\n",
    "      if doi is None:\n",
    "        continue  \n",
    "      self.add_full_text_for_expression(e, get_nxml, get_pdf, get_html)      \n",
    "\n",
    "  def add_corpus_from_epmc(self, qt, qt2, \n",
    "                           sections=['paper_title', 'ABSTRACT'], \n",
    "                           sections2=['paper_title', 'ABSTRACT'], \n",
    "                           page_size=1000):\n",
    "    \"\"\"Adds corpora based on coupled QueryTranslator objects.\"\"\"\n",
    "    \n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "\n",
    "    if self.session.query(exists().where(InformationResource.name=='EPMC')).scalar():\n",
    "      info_resource = self.session.query(InformationResource) \\\n",
    "          .filter(InformationResource.name=='EPMC').first()\n",
    "    else:\n",
    "      info_resource = InformationResource(id='skem:EPMC', \n",
    "                                          iri=['skem:EPMC'], \n",
    "                                          name='European PubMed Central', \n",
    "                                          type='skem:InformationResource',\n",
    "                                          xref=['https://europepmc.org/'])\n",
    "\n",
    "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
    "    if qt2:\n",
    "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections2)\n",
    "    else: \n",
    "      (subset_ids, epmc_subset_queries) = ([0],[''])\n",
    "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
    "      for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "        query = q\n",
    "        corpus_id = str(i)\n",
    "        corpus_name = qt.df.loc[qt.df['ID']==i][qt.name_col].values[0]\n",
    "        if query is None or query=='nan' or len(query)==0: \n",
    "          continue\n",
    "        if len(sq) > 0:\n",
    "          query = '(%s) AND (%s)'%(q, sq)\n",
    "          if j is not None:\n",
    "            corpus_id = str(i)+'.'+str(j)\n",
    "            corpus_name2 = qt2.df.loc[qt2.df['ID']==j][qt2.name_col].values[0]\n",
    "            corpus_name = corpus_name + '/'+ corpus_name2\n",
    "        self.add_corpus_from_epmc_query(corpus_id, corpus_name, query)\n",
    "    self.session.commit()\n",
    "  \n",
    "  def add_corpus_from_epmc_query(self, \n",
    "                                 corpus_id, \n",
    "                                 corpus_name, \n",
    "                                 query, \n",
    "                                 commit_this=True,\n",
    "                                 page_size=1000):\n",
    "    \"\"\"Adds corpus to database based on terms constructed in a direct EPMC query.\"\"\"\n",
    "    \n",
    "    # does this collection already exist?  \n",
    "    corpus_id = str(corpus_id)\n",
    "    if self.session.query(exists().where(ScientificKnowledgeCollection.id==corpus_id)).scalar():\n",
    "      raise Exception('Collection already exists')\n",
    "\n",
    "    info_resource = InformationResource(id='skem:EPMC', \n",
    "                                          iri=['skem:EPMC'], \n",
    "                                          name='European PubMed Central', \n",
    "                                          type='skem:InformationResource',\n",
    "                                          xref=['https://europepmc.org/'])\n",
    "    if self.session.query(exists().where(InformationResource.name=='EPMC')).scalar():\n",
    "      info_resource = self.session.query(InformationResource) \\\n",
    "          .filter(InformationResource.name=='EPMC').first()\n",
    "    \n",
    "    corpus = ScientificKnowledgeCollection(id=corpus_id,\n",
    "                                           type='skem:ScientificKnowledgeCollection',\n",
    "                                           provenance=[query], \n",
    "                                           name=corpus_name,\n",
    "                                           has_members=[])\n",
    "    self.session.add(corpus)\n",
    "\n",
    "    epmcq = EuroPMCQuery()\n",
    "    numFound, pubs = epmcq.run_empc_query(query, page_size=page_size)\n",
    "    for p in tqdm(pubs):\n",
    "      p_id = str(p.id)\n",
    "      p_check = self.session.query(ScientificKnowledgeExpression) \\\n",
    "          .filter(ScientificKnowledgeExpression.id==p_id).first()\n",
    "      if p_check is not None:\n",
    "        p = p_check\n",
    "      corpus.has_members.append(p)\n",
    "      for item in p.has_representation:\n",
    "        for f in item.has_part:          \n",
    "          f.part_of = item.id\n",
    "          self.session.add(f)\n",
    "        item.represented_by = p.id\n",
    "        self.session.add(item)\n",
    "      self.session.add(p)\n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "\n",
    "  def delete_collection(self, collection_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified collection from the database, leave all expressions and items intact\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(ScientificKnowledgeCollection.id==collection_id)).scalar():\n",
    "      cp = self.session.query(ScientificKnowledgeCollection) \\\n",
    "          .join(ScientificKnowledgeCollection.has_members, isouter=True) \\\n",
    "          .filter(ScientificKnowledgeCollection.id==collection_id).first()\n",
    "      if cp is not None:\n",
    "        for p in cp.has_members:\n",
    "          cp.has_members.remove(p)\n",
    "        self.session.flush()\n",
    "        self.session.delete(cp)\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    else:\n",
    "      raise Exception('Collection does not exist')\n",
    "\n",
    "  def check_query_terms(self, qt, qt2=None, pubmed_api_key=''):\n",
    "    pmq = ESearchQuery(api_key=pubmed_api_key)\n",
    "    terms = set()\n",
    "    for t in qt.terms2id.keys():\n",
    "        terms.add(t)\n",
    "    if qt2 is not None:\n",
    "        for t2 in qt2.terms2id.keys():\n",
    "            terms.add(t2)\n",
    "    check_table = {} \n",
    "    for t in tqdm(terms):\n",
    "        (is_ok, t2, c) = pmq._check_query_phrase(t)\n",
    "        check_table[t] = (is_ok, c)\n",
    "    return check_table\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Query Collections, Expressions, Items, and Fragments\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "  def get_expression_by_doi(self, doi):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    \"\"\"Returns a ScientificKnowledgeExpression object for a given DOI.\"\"\"\n",
    "    query = self.session.query(ScientificKnowledgeExpression) \\\n",
    "        .filter(ScientificKnowledgeExpression.id==ScientificKnowledgeExpressionXref.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(ScientificKnowledgeExpressionXref.xref=='doi:'+doi)\n",
    "    ske = query.first()\n",
    "    return ske\n",
    "\n",
    "  def list_collections(self, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if search_term:\n",
    "      q = (self.session.query(ScientificKnowledgeCollection) \n",
    "          .filter(ScientificKnowledgeCollection.name == search_term))\n",
    "    else:\n",
    "      q = self.session.query(ScientificKnowledgeCollection)\n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_expressions(self, collection_id=None, search_term=None): \n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if collection_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeCollectionHasMembers,\n",
    "                              ScientificKnowledgeExpression,\n",
    "                              ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem) \n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id==collection_id)\n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id)\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "         q = (self.session.query(ScientificKnowledgeCollection, \n",
    "                              ScientificKnowledgeCollectionHasMembers,\n",
    "                              ScientificKnowledgeExpression ) \n",
    "            .filter(ScientificKnowledgeCollection.id == ScientificKnowledgeCollectionHasMembers.ScientificKnowledgeCollection_id)\n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id)\n",
    "            .filter(ScientificKnowledgeCollection.id == collection_id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeExpression,\n",
    "                              ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem) \n",
    "            .filter(ScientificKnowledgeCollectionHasMembers.has_members_id == ScientificKnowledgeExpression.id)\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(ScientificKnowledgeExpression)\n",
    "      \n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_notes_for_fragments_in_paper(self, run_name, paper_id, item_type='FullTextPaper'):\n",
    "    '''returns notes of a specific type associated with fragments from a given paper .'''\n",
    "    q1 = self.session.query(ScientificKnowledgeItem) \\\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id) \\\n",
    "            .filter(ScientificKnowledgeItem.type == item_type) \\\n",
    "            .filter(ScientificKnowledgeExpression.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "      return []\n",
    "    for f in i.has_part:\n",
    "      for n in f.has_notes:\n",
    "        if n.name == run_name:\n",
    "          yield(n)\n",
    "         \n",
    "  def list_fragments_for_paper(self, paper_id, item_type):\n",
    "    '''Loads fragments from a given paper sections of a specified paper from the local database.'''\n",
    "    q1 = self.session.query(ScientificKnowledgeItem) \\\n",
    "            .filter(ScientificKnowledgeExpression.id == ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id) \\\n",
    "            .filter(ScientificKnowledgeItem.type == item_type) \\\n",
    "            .filter(ScientificKnowledgeExpression.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "        raise Exception('No items of format: %s found for expression: %s'%(item_type, str(paper_id)))\n",
    "    fragments = []\n",
    "    for f in i.has_part:\n",
    "      fragments.append(f)\n",
    "    return sorted(fragments, key=lambda f: f.offset)\n",
    "  \n",
    "  def list_fragments(self, expression_id=None, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if expression_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem,\n",
    "                              ScientificKnowledgeItemHasPart,\n",
    "                              ScientificKnowledgeFragment) \n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.id == ScientificKnowledgeItemHasPart.ScientificKnowledgeItem_id)\n",
    "            .filter(ScientificKnowledgeItemHasPart.has_part_id == ScientificKnowledgeFragment.id)\n",
    "            .filter(ScientificKnowledgeFragment.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = (self.session.query(ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem,\n",
    "                              ScientificKnowledgeItemHasPart,\n",
    "                              ScientificKnowledgeFragment) \n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.id == ScientificKnowledgeItemHasPart.ScientificKnowledgeItem_id)\n",
    "            .filter(ScientificKnowledgeItemHasPart.has_part_id == ScientificKnowledgeFragment.id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(ScientificKnowledgeExpressionHasRepresentation,\n",
    "                              ScientificKnowledgeItem,\n",
    "                              ScientificKnowledgeItemHasPart,\n",
    "                              ScientificKnowledgeFragment) \n",
    "            .filter(ScientificKnowledgeExpressionHasRepresentation.has_representation_id == ScientificKnowledgeItem.id)\n",
    "            .filter(ScientificKnowledgeItem.id == ScientificKnowledgeItemHasPart.ScientificKnowledgeItem_id)\n",
    "            .filter(ScientificKnowledgeItemHasPart.has_part_id == ScientificKnowledgeFragment.id)\n",
    "            .filter(ScientificKnowledgeFragment.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(ScientificKnowledgeFragment)\n",
    "    for c in q.all():\n",
    "      yield(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_information_content_entity_iri(ice, id_prefix):\n",
    "    \"\"\"Reads an identifier for a given prefix\"\"\"\n",
    "    idmap = {k[:k.find(':')]:k[k.find(':')+1:] for k in ice.xref} \n",
    "    return idmap.get(id_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
