{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database for Scientific Knowledge \n",
    "\n",
    ">  A local Postgresql database of scientific content with associated information generated by the agent. The intent is to use this repository as 'memory' in a Langchain-enabled agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.ceifns_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a local database for use with the Alhazen schema conforming to the following schema:\n",
    "\n",
    "![Schema](pics/schema/Alhazen_Schema_v0.0.2.png) \n",
    "\n",
    "The key elements of the schema are:\n",
    "\n",
    "**Abstract Parent Class**\n",
    "\n",
    "* *InformationContentEntity* - a named piece of information (all other entities inherit from this)\n",
    "  \n",
    "**Classes Denoting Extant Scientific Knowledge from External Sources**\n",
    "\n",
    "* *ScientificKnowledgeCollection* - a collection of scientific knowledge expressions\n",
    "* *ScientificKnowledgeExpression* - an expression of scientific knowledge (e.g., a paper, a database record, a blog post, etc.) \n",
    "* *ScientificKnowledgeItem* - the data item that manifests the expression (e.g., a pdf, an html data file, a yaml file, etc.) \n",
    "* *ScientificKnowledgeFragment* - a relevant fragment of the item (e.g., a paragraph, a table, a figure, etc.)            \n",
    "\n",
    "**Classes Denoting Information Generated by Alhazen**\n",
    "\n",
    "* *Note* - an annotation placed on any InformationContentEntity  (e.g., a comment, a question, a link to another fragment, etc.)\n",
    "\n",
    "> Our definitions are inspired by [FABIO](http://www.sparontologies.net/ontologies/fabio), but only differentiate between '*expression*' (as the reference to an scientific underlying work product, e.g., a paper or database record) and '*item*' (as the actual data that delivers on that expression). \n",
    "> The system is coded in LinkML and generated as a Postgresql database and a SQLAlchemy ORM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from alhazen.utils.airtableUtils import AirtableUtils\n",
    "from alhazen.utils.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from alhazen.utils.pdf_research_article_text_extractor import LAPDFBlockLoader, HuridocsPDFLoader\n",
    "\n",
    "from alhazen.utils.queryTranslator import QueryTranslator, QueryType\n",
    "from alhazen.schema_sqla import *\n",
    "import alhazen.schema_python as linkml_py\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Extra, Field, root_validator\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import local_resources.linkml as linkml\n",
    "import nltk.data\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import re\n",
    "from sqlalchemy import create_engine, exists, Engine, text, func, or_\n",
    "from sqlalchemy.orm import sessionmaker, aliased, Session\n",
    "import sqlite3  \n",
    "import subprocess\n",
    "import sys\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import uuid\n",
    "from langchain.vectorstores.pgvector import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Using Aliases like this massively simplifies the use of SQLAlchemy\n",
    "IR = aliased(InformationResource)\n",
    "\n",
    "SKC = aliased(ScientificKnowledgeCollection)\n",
    "SKC_PROV = aliased(ScientificKnowledgeCollectionProvenance)\n",
    "SKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "SKE = aliased(ScientificKnowledgeExpression)\n",
    "SKE_XREF = aliased(ScientificKnowledgeExpressionXref)\n",
    "SKE_IRI = aliased(ScientificKnowledgeExpressionIri)\n",
    "SKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "SKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\n",
    "SKI = aliased(ScientificKnowledgeItem)\n",
    "SKI_HP = aliased(ScientificKnowledgeItemHasPart)\n",
    "SKF = aliased(ScientificKnowledgeFragment)\n",
    "\n",
    "\n",
    "N = aliased(Note)\n",
    "SKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "SKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "SKI_HN = aliased(ScientificKnowledgeItemHasNotes)\n",
    "SKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase has been dropped successfully !!\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n\u001b[1;32m     50\u001b[0m   conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCeifns_LiteratureDb\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"This class runs a set of queries on external literature databases to build a local database of linked corpora and papers.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m  Functionality includes:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    * Permits user to download a local copy of full text papers in NXML(JATS), PDF, and HTML format.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   loc: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "def create_ceifns_database(db_name):\n",
    "  conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "      user='postgres',\n",
    "      password='password',\n",
    "      host='localhost',\n",
    "      port= '5432'\n",
    "  )\n",
    "  conn.autocommit = True\n",
    "  cursor = conn.cursor()\n",
    "  cursor.execute('CREATE database ' + db_name)\n",
    "  conn.close()\n",
    "  \n",
    "  conn = psycopg2.connect(\n",
    "    database=db_name,\n",
    "    host='localhost',\n",
    "    port= '5432'\n",
    "  )\n",
    "  conn.autocommit = True\n",
    "  cursor = conn.cursor()\n",
    "  with open(files(linkml).joinpath('schema.sql'), 'r') as f:\n",
    "    for sql in tqdm(f.read().split(';')):\n",
    "        if len(sql.strip()) == 0:\n",
    "          continue\n",
    "        cursor.execute(sql)\n",
    "\n",
    "  ## frontload installation of the pgvector extensions.\n",
    "  #statement = text(\"BEGIN;\"\n",
    "  #                 \"SELECT pg_advisory_xact_lock(1573678846307946496);\"\n",
    "  #                 \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
    "  #                 \"COMMIT;\")\n",
    "  #cursor.execute(statement)\n",
    "  conn.close()\n",
    "\n",
    "def drop_ceifns_database(db_name):\n",
    "  conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "      user='postgres',\n",
    "      password='password',\n",
    "      host='localhost',\n",
    "      port= '5432'\n",
    "  )\n",
    "  conn.autocommit = True\n",
    "  cursor = conn.cursor()\n",
    "  cursor.execute(\"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='\"+db_name+\"' AND leader_pid IS NULL;\")\n",
    "  cursor.execute('DROP database ' + db_name)\n",
    "  print(\"Database has been dropped successfully !!\");\n",
    "  conn.close()\n",
    "\n",
    "# ADAPTED FROM https://gist.github.com/valferon/4d6ebfa8a7f3d4e84085183609d10f14\n",
    "def backup_ceifns_database(db_name, dest_file, verbose=False):\n",
    "  \"\"\"\n",
    "  Backup postgres db to a file.\n",
    "  \"\"\"\n",
    "  \"postgresql://localhost:5432/\"+db_name\n",
    "\n",
    "  if verbose:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_dump',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            '-Fc',\n",
    "            '-f', dest_file,\n",
    "            '-v'],\n",
    "          stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if int(process.returncode) != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "        exit(1)\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      exit(1)\n",
    "  else:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_dump',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            '-f', dest_file],\n",
    "          stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if process.returncode != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "        exit(1)\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      exit(1)\n",
    "\n",
    "# ADAPTED FROM https://gist.github.com/valferon/4d6ebfa8a7f3d4e84085183609d10f14\n",
    "def restore_ceifns_database(db_name, backup_file, verbose=False):\n",
    "  \"\"\"\n",
    "  Restore postgres db from a file.\n",
    "  \"\"\"\n",
    "  if verbose:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_restore',\n",
    "            '--no-owner',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            '-v',\n",
    "            backup_file],\n",
    "          stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if int(process.returncode) != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(\"Issue with the db restore : {}\".format(e))\n",
    "  else:\n",
    "    try:\n",
    "      process = subprocess.Popen(\n",
    "          ['pg_restore',\n",
    "            '--no-owner',\n",
    "            '--dbname=postgresql://localhost:5432/{}'.format(db_name),\n",
    "            backup_file],\n",
    "        stdout=subprocess.PIPE\n",
    "      )\n",
    "      output = process.communicate()[0]\n",
    "      if int(process.returncode) != 0:\n",
    "        print('Command failed. Return code : {}'.format(process.returncode))\n",
    "      return output\n",
    "    except Exception as e:\n",
    "      print(\"Issue with the db restore : {}\".format(e))\n",
    "\n",
    "class Ceifns_LiteratureDb(BaseModel):\n",
    "  \"\"\"This class runs a set of queries on external literature databases to build a local database of linked corpora and papers.\n",
    "\n",
    "  Functionality includes:\n",
    "\n",
    "    * Executes queries over European PMC \n",
    "    * Can run combinatorial sets of queries using a dataframe structure\n",
    "        * Requires a column of queries expressed in boolean logic\n",
    "        * Optional to define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n",
    "    * Has capability to run boolean logic over sources (currently only European PMC, but possibly others) \n",
    "    * Builds a local Postgresql database with tables for collections, expressions, items, fragments, and notes. \n",
    "    * Provides an API for querying the database and returning results as sqlAlchemy objects.\n",
    "    * Permits user to download a local copy of full text papers in NXML(JATS), PDF, and HTML format.\n",
    "  \"\"\"\n",
    "  loc: str = Field()\n",
    "  name: str = Field()\n",
    "  engine: Engine = Field(default=None, init=False)\n",
    "  session: Session = Field(default=None, init=False)\n",
    "  sent_detector: nltk.tokenize.punkt.PunktSentenceTokenizer = Field(default=None, init=False)\n",
    "  embed_model: Embeddings = Field(default=None, init=False)\n",
    "\n",
    "  class Config:\n",
    "    \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    \n",
    "    if self.loc[-1] != '/':\n",
    "      self.loc = self.loc + '/' \n",
    "    if os.path.exists(self.loc) is False:\n",
    "      os.makedirs(self.loc)\n",
    "\n",
    "    db_dir = Path(self.loc + self.name)\n",
    "    if db_dir.exists() is False:\n",
    "      os.makedirs(db_dir)\n",
    "\n",
    "    self.engine = create_engine('postgresql+psycopg2:///'+self.name)\n",
    "\n",
    "    self.start_session() # instantiate session \n",
    "\n",
    "    log_path = '%s%s_log.txt'%(self.loc, self.name)\n",
    "    if os.path.exists(log_path) is False:\n",
    "      Path(log_path).touch()\n",
    "    \n",
    "    self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    #\n",
    "    # hard coded embedding model at present.\n",
    "    #\n",
    "    model_name = \"BAAI/bge-large-en\"\n",
    "    model_kwargs = {\"device\": \"mps\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    self.embed_model = HuggingFaceBgeEmbeddings(\n",
    "      model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "    # PGVECTOR representation for embedding uses environmental variables to set the right database \n",
    "    os.environ['PGVECTOR_CONNECTION_STRING'] = \"postgresql+psycopg2:///\"+self.name\n",
    "\n",
    "  def start_session(self):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    return self.session\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Add Collections, Expressions, Items, and Fragments\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def add_corpus_from_epmc(self, qt, qt2, \n",
    "                           sections=['paper_title', 'ABSTRACT'], \n",
    "                           sections2=['paper_title', 'ABSTRACT'], \n",
    "                           page_size=1000):\n",
    "    \"\"\"Adds corpora based on coupled QueryTranslator objects.\"\"\"\n",
    "    \n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "\n",
    "    if self.session.query(exists().where(InformationResource.name=='EPMC')).scalar():\n",
    "      info_resource = self.session.query(InformationResource) \\\n",
    "          .filter(InformationResource.name=='EPMC').first()\n",
    "    else:\n",
    "      info_resource = InformationResource(id='skem:EPMC', \n",
    "                                          iri=['skem:EPMC'], \n",
    "                                          name='European PubMed Central', \n",
    "                                          type='skem:InformationResource',\n",
    "                                          xref=['https://europepmc.org/'])\n",
    "\n",
    "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
    "    if qt2:\n",
    "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections2)\n",
    "    else: \n",
    "      (subset_ids, epmc_subset_queries) = ([0],[''])\n",
    "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
    "      for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "        query = q\n",
    "        corpus_id = str(i)\n",
    "        corpus_name = qt.df.loc[qt.df['ID']==i][qt.name_col].values[0]\n",
    "        if query is None or query=='nan' or len(query)==0: \n",
    "          continue\n",
    "        if len(sq) > 0 and sq != 'nan':\n",
    "          query = '(%s) AND (%s)'%(q, sq)\n",
    "          if j is not None:\n",
    "            corpus_id = str(i)+'.'+str(j)\n",
    "            corpus_name2 = qt2.df.loc[qt2.df['ID']==j][qt2.name_col].values[0]\n",
    "            corpus_name = corpus_name + '/'+ corpus_name2\n",
    "        try:\n",
    "          self.add_corpus_from_epmc_query(corpus_id, corpus_name, query)\n",
    "        except Exception as ex:\n",
    "          print(ex)\n",
    "          print('skipping %s'%(corpus_name) )\n",
    "          continue\n",
    "    self.session.commit()\n",
    "  \n",
    "  def add_corpus_from_epmc_query(self, \n",
    "                                 corpus_id, \n",
    "                                 corpus_name, \n",
    "                                 query, \n",
    "                                 commit_this=True,\n",
    "                                 page_size=1000):\n",
    "    \"\"\"Adds corpus to database based on terms constructed in a direct EPMC query.\"\"\"\n",
    "    \n",
    "    # does this collection already exist?  \n",
    "    corpus_id = str(corpus_id)\n",
    "    all_existing_query = self.session.query(SKC).filter(SKC.id==corpus_id)\n",
    "    for c in all_existing_query.all():\n",
    "      return c\n",
    "        \n",
    "    corpus = ScientificKnowledgeCollection(id=corpus_id,\n",
    "                                           type='skem:ScientificKnowledgeCollection',\n",
    "                                           provenance=[query], \n",
    "                                           name=corpus_name,\n",
    "                                           has_members=[])\n",
    "    self.session.add(corpus)\n",
    "\n",
    "    epmcq = EuroPMCQuery()\n",
    "    numFound, pubs = epmcq.run_empc_query(query, page_size=page_size)\n",
    "    for p in tqdm(pubs):\n",
    "      p_id = str(p.id)\n",
    "      p_check = self.session.query(SKE) \\\n",
    "          .filter(SKE.id==p_id).first()\n",
    "      if p_check is not None:\n",
    "        p = p_check\n",
    "      corpus.has_members.append(p)\n",
    "      p.member_of.append(corpus)\n",
    "      for item in p.has_representation:\n",
    "        for f in item.has_part:\n",
    "          #f.content = '\\n'.join(self.sent_detector.tokenize(f.content))\n",
    "          f.part_of = item.id\n",
    "          self.session.add(f)\n",
    "        item.represented_by = p.id\n",
    "        self.session.add(item)\n",
    "      self.session.add(p)\n",
    "    \n",
    "    skes = (self.session.query(SKE) \n",
    "          .filter(SKC.id == SKC_HM.ScientificKnowledgeCollection_id)\n",
    "          .filter(SKC_HM.has_members_id == SKE.id)\n",
    "          .filter(SKC.id == corpus_id)).all()\n",
    "    self.embed_expression_list(skes)\n",
    "\n",
    "    if commit_this:\n",
    "      self.session.commit()\n",
    "\n",
    "    return corpus\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Super cumbersome methods for deletion\n",
    "  #     Need to adjust SQLAlchemy methods to permit cascades.\n",
    "  #     This is more precise and careful (but need shortcuts)\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def delete_collection(self, collection_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified collection from the database, leave all expressions and items intact\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKC.id==collection_id)).scalar():\n",
    "\n",
    "      # remove provenance relationships\n",
    "      q = self.session.query(SKC_PROV) \\\n",
    "          .filter(SKC.id==SKC_PROV.ScientificKnowledgeCollection_id) \\\n",
    "          .filter(SKC.id==collection_id)\n",
    "      for p in q.all():\n",
    "          self.session.delete(p)\n",
    "      self.session.flush()\n",
    "\n",
    "      # remove collection - expressions relations\n",
    "      q2 = self.session.query(SKC_HM) \\\n",
    "          .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n",
    "          .filter(SKC.id==collection_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove expression -> collection relations\n",
    "      q3 = self.session.query(SKE_MO) \\\n",
    "          .filter(SKE_MO.member_of_id==collection_id)\n",
    "      for cp in q3.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKC) \\\n",
    "          .filter(SKC.id==collection_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKC_HN, N) \\\n",
    "        .filter(SKC_HN.ScientificKnowledgeCollection_id==collection_id) \\\n",
    "        .filter(SKC_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('skc:'+collection_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')\n",
    "\n",
    "  #def delete_expressions_with_no_collection(self):\n",
    "  #    q = self.session.query(SKI.id) \\\n",
    "  #        .filter(SKE_HR.ScientificKnowledgeExpression_id==expression_id) \\\n",
    "  #        .filter(SKE_HR.has_representation_id==SKI.id)\n",
    "  #    for item_id in q.all():\n",
    "  #      self.delete_item(item_id, commit_this=False)\n",
    "  #      self.session.flush()\n",
    "\n",
    "  def delete_expression(self, expression_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified collection from the database, leave all expressions and items intact\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKE.id==expression_id)).scalar():\n",
    "\n",
    "      q = self.session.query(SKI) \\\n",
    "          .filter(SKE_HR.ScientificKnowledgeExpression_id==expression_id) \\\n",
    "          .filter(SKE_HR.has_representation_id==SKI.id)\n",
    "      for item in q.all():\n",
    "        self.delete_item(item.id, commit_this=False)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove collection - expressions relations\n",
    "      q2 = self.session.query(SKC_HM) \\\n",
    "          .filter(SKC_HM.has_members_id==expression_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove expression -> collection relations\n",
    "      q3 = self.session.query(SKE_MO) \\\n",
    "          .filter(SKE_MO.ScientificKnowledgeExpression_id==expression_id)\n",
    "      for cp in q3.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKE_HN, N) \\\n",
    "        .filter(SKE_HN.ScientificKnowledgeExpression_id==expression_id) \\\n",
    "        .filter(SKC_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('ske:'+expression_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      q7 = self.session.query(SKE_IRI) \\\n",
    "        .filter(SKE_IRI.ScientificKnowledgeExpression_id==expression_id)\n",
    "      for iri in q7.all():\n",
    "        self.session.delete(iri)\n",
    "        self.session.flush()\n",
    "\n",
    "      q8 = self.session.query(SKE_XREF) \\\n",
    "        .filter(SKE_XREF.ScientificKnowledgeExpression_id==expression_id)\n",
    "      for xref in q8.all():\n",
    "        self.session.delete(xref)\n",
    "        self.session.flush()\n",
    "\n",
    "\n",
    "      q4 = self.session.query(SKE) \\\n",
    "          .filter(SKE.id==expression_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')\n",
    "\n",
    "  def delete_item(self, item_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified collection from the database, leave all expressions and items intact\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKI.id==item_id)).scalar():\n",
    "\n",
    "      q = self.session.query(SKF) \\\n",
    "          .filter(SKI_HP.ScientificKnowledgeItem_id==item_id) \\\n",
    "          .filter(SKI_HP.has_part_id==SKF.id)\n",
    "      for fragment in q.all():\n",
    "        self.delete_fragment(fragment.id, commit_this=False)\n",
    "        self.session.flush()\n",
    "\n",
    "      # remove expression -> item relations\n",
    "      q2 = self.session.query(SKE_HR) \\\n",
    "          .filter(SKE_HR.has_representation_id==item_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKI) \\\n",
    "          .filter(SKI.id==item_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKI_HN, N) \\\n",
    "        .filter(SKI_HN.ScientificKnowledgeItem_id==item_id) \\\n",
    "        .filter(SKI_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('ski:'+item_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')\n",
    "\n",
    "  def delete_fragment(self, fragment_id, commit_this=True):    \n",
    "    \"\"\"Deletes the specified collection from the database, leave all expressions and items intact\"\"\"  \n",
    "\n",
    "    if self.session.query(exists().where(SKF.id==fragment_id)).scalar():\n",
    "\n",
    "      # remove item -> fragment relations\n",
    "      q2 = self.session.query(SKI_HP) \\\n",
    "          .filter(SKI_HP.has_part_id==fragment_id)\n",
    "      for cp in q2.all():\n",
    "        self.session.delete(cp)\n",
    "        self.session.flush()\n",
    "\n",
    "      q4 = self.session.query(SKF) \\\n",
    "          .filter(SKF.id==fragment_id)\n",
    "      for c in q4.all():\n",
    "        self.session.delete(c)\n",
    "        self.session.flush()\n",
    "\n",
    "      q5 = self.session.query(SKF_HN, N) \\\n",
    "        .filter(SKF_HN.ScientificKnowledgeFragment_id==fragment_id) \\\n",
    "        .filter(SKF_HN.has_notes_id==N.id)\n",
    "      for hn, n in q5.all():\n",
    "        self.session.delete(hn)\n",
    "        self.session.delete(n)\n",
    "        self.session.flush()\n",
    "\n",
    "      q6 = self.session.query(NoteIsAbout) \\\n",
    "        .filter(NoteIsAbout.Note_id.like('skf:'+fragment_id+'.'))\n",
    "      for nia in q6.all():\n",
    "        self.session.delete(nia)\n",
    "        self.session.flush()\n",
    "\n",
    "      if commit_this:\n",
    "        self.session.commit()\n",
    "    \n",
    "    else:\n",
    "      raise Exception('Collection does not exist')  \n",
    "\n",
    "  def check_query_terms(self, qt, qt2=None, pubmed_api_key=''):\n",
    "    pmq = ESearchQuery(api_key=pubmed_api_key)\n",
    "    terms = set()\n",
    "    for t in qt.terms2id.keys():\n",
    "        terms.add(t)\n",
    "    if qt2 is not None:\n",
    "        for t2 in qt2.terms2id.keys():\n",
    "            terms.add(t2)\n",
    "    check_table = {} \n",
    "    for t in tqdm(terms):\n",
    "        (is_ok, t2, c) = pmq._check_query_phrase(t)\n",
    "        check_table[t] = (is_ok, c)\n",
    "    return check_table\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Add full text to database\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  def add_full_text_for_expression(self, e):\n",
    "    \"\"\"Adds Full Text ScientificKnowledgeItem data to given ScientificKnowledgeExpression if full text is already downloaded in the `{loc}/ft/{doi}` directory.\"\"\"\n",
    "\n",
    "    doi = read_information_content_entity_iri(e, 'doi')\n",
    "    if doi is None:\n",
    "      return  \n",
    "    \n",
    "    # Check local directory for files\n",
    "    base_file_path = '%s%s/ft/'%(self.loc, self.name)\n",
    "    nxml_file_path = base_file_path+doi+'.nxml'\n",
    "    pdf_file_path = base_file_path+doi+'.pdf'\n",
    "    \n",
    "    if os.path.exists(nxml_file_path):\n",
    "      with open(nxml_file_path, 'r') as f:\n",
    "        xml = f.read()\n",
    "        try:\n",
    "          d = NxmlDoc(doi, xml)\n",
    "        except Exception as ex:\n",
    "          return\n",
    "        doc_text = d.text \n",
    "        if 'body' in [so.element.tag for so in d.standoffs]:\n",
    "          ski_type = 'JATSFullText'\n",
    "        else:\n",
    "          return\n",
    "        ski_id = str(uuid.uuid4().hex)[:10]\n",
    "        ski = ScientificKnowledgeItem(id=ski_id, \n",
    "                                      content=doc_text,\n",
    "                                      xref=['file:'+nxml_file_path],\n",
    "                                      type=ski_type,)\n",
    "        e.has_representation.append(ski)\n",
    "        self.session.add(ski)\n",
    "        self.session.flush()\n",
    "        self.add_sections_as_fragments_from_nxmldoc(ski, d)\n",
    "    elif os.path.exists(pdf_file_path):\n",
    "      loader = HuridocsPDFLoader(pdf_file_path)\n",
    "      docs = loader.load(curi_id='doi:'+doi) \n",
    "      skfs = []\n",
    "      doc_text = ''\n",
    "      ski_id = str(uuid.uuid4().hex)[:10]\n",
    "      for i, d in enumerate(docs):\n",
    "        #sentence_split_text = '\\n'.join(self.sent_detector.tokenize(d.page_content))\n",
    "        sentence_split_text = d.page_content\n",
    "\n",
    "        skf = ScientificKnowledgeFragment(id=ski_id+'.'+str(i), \n",
    "                                             type='section',\n",
    "                                             offset=len(doc_text),\n",
    "                                             length=len(sentence_split_text),\n",
    "                                             name=sentence_split_text.split('\\n')[0],\n",
    "                                             content=sentence_split_text)\n",
    "        if len(doc_text) > 0:\n",
    "          doc_text += '\\n\\n'\n",
    "        doc_text += sentence_split_text\n",
    "        self.session.add(skf)\n",
    "        self.session.flush()\n",
    "        skfs.append(skf)\n",
    "      ski_type = 'PDFFullText'\n",
    "      ski = ScientificKnowledgeItem(id=ski_id, \n",
    "                                    content=doc_text,\n",
    "                                    xref=['file:'+pdf_file_path],\n",
    "                                    type=ski_type,)\n",
    "      e.has_representation.append(ski)\n",
    "      self.session.add(ski)\n",
    "      self.session.flush()\n",
    "      for skf in skfs:\n",
    "        ski.has_part.append(skf)\n",
    "        skf.part_of = ski.id\n",
    "        self.session.flush()\n",
    "        \n",
    "  def add_sections_as_fragments_from_nxmldoc(self, item, d):\n",
    "    \"\"\"Adds fragments from an NxmlDoc object to a ScientificKnowledgeItem object.\"\"\"\n",
    "\n",
    "    # ['PMID', 'PARAGRAPH_ID', 'TAG', 'TAG_TREE', 'OFFSET', 'LENGTH', 'FIG_REF', 'PLAIN_TEXT'\n",
    "    fragments = []\n",
    "    df = d.build_enhanced_document_dataframe()\n",
    "    # df = df[df.SECTION_TREE!='']\n",
    "    # Can search for a substring in top section headers\n",
    "    # df3 = df[df.TOP_SECTION.str.contains('method', case=False)]\n",
    "\n",
    "    df3 = df[df.TOP_SECTION!='']\n",
    "    df4 = df3.groupby('SECTION_TREE').agg({'OFFSET': 'min', 'LENGTH': 'sum', 'PLAIN_TEXT':lambda x: '\\n'.join(x)}).sort_values('OFFSET')\n",
    "    df4 = df4.reset_index(drop=False)\n",
    "    \n",
    "    if df4 is None:\n",
    "      return\n",
    "    \n",
    "    for i, tup in df4.iterrows():      \n",
    "      #sentence_split_text = '\\n'.join(self.sent_detector.tokenize(tup.PLAIN_TEXT))\n",
    "      sentence_split_text = tup.PLAIN_TEXT\n",
    "      fragment = ScientificKnowledgeFragment(id=item.id[:10]+'.'+str(i), \n",
    "                                             type='section',\n",
    "                                             offset=tup.OFFSET,\n",
    "                                             length=tup.LENGTH,\n",
    "                                             name=tup.SECTION_TREE,\n",
    "                                             content=sentence_split_text)\n",
    "      self.session.add(fragment)\n",
    "      self.session.flush()\n",
    "      item.has_part.append(fragment)\n",
    "      fragment.part_of = item.id\n",
    "      fragments.append(fragment)\n",
    "      self.session.flush()     \n",
    "\n",
    "  def add_full_text_for_collection(self, collection_id, \n",
    "                                   get_nxml=True, get_pdf=True, get_html=True):\n",
    "    for e in self.list_expressions(collection_id=collection_id):\n",
    "      doi = read_information_content_entity_iri(e, 'doi')\n",
    "      if doi is None:\n",
    "        continue  \n",
    "      self.add_full_text_for_expression(e, get_nxml, get_pdf, get_html)  \n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Query Collections, Expressions, Items, and Fragments\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "  def get_expression_by_doi(self, doi):\n",
    "    \"\"\"Returns a ScientificKnowledgeExpression object for a given DOI.\"\"\"\n",
    "    self.start_session()\n",
    "    query = self.session.query(SKE) \\\n",
    "        .filter(SKE.id==SKE_XREF.ScientificKnowledgeExpression_id) \\\n",
    "        .filter(SKE_XREF.xref=='doi:'+doi)\n",
    "    ske = query.first()\n",
    "    return ske\n",
    "\n",
    "  def list_items_for_expression(self, doi):\n",
    "    \"\"\"Returns all items for an expression for a given DOI.\"\"\"\n",
    "    self.start_session()\n",
    "    query = self.db.session.query(SKE, SKI) \\\n",
    "          .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "          .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "          .filter(SKE.id == doi)\n",
    "    l_it = []\n",
    "    for (ex, it) in query.all():\n",
    "      l_it.append(it)\n",
    "    return l_it\n",
    "\n",
    "  def list_collections(self, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if search_term:\n",
    "      q = (self.session.query(SKC) \n",
    "          .filter(SKC.name == search_term))\n",
    "    else:\n",
    "      q = self.session.query(SKC)\n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_expressions(self, collection_id=None, search_term=None): \n",
    "    '''Lists expressions in database. \n",
    "    Including the `collection_id` parameter limits the returned list to the specified collection. \n",
    "    Including the `search_term` parameter searches the citation string for the specified term.'''\n",
    "\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if collection_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKE) \n",
    "            .filter(SKC_HM.ScientificKnowledgeCollection_id==collection_id)\n",
    "            .filter(SKC_HM.has_members_id == SKE.id)\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "         q = (self.session.query(SKE) \n",
    "            .filter(SKC.id == SKC_HM.ScientificKnowledgeCollection_id)\n",
    "            .filter(SKC_HM.has_members_id == SKE.id)\n",
    "            .filter(SKC.id == collection_id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKE) \n",
    "            .filter(SKC_HM.has_members_id == SKE.id)\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.content.like(search))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(SKE)\n",
    "      \n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  def list_notes_for_fragments_in_paper(self, run_name, paper_id, item_type='FullTextPaper'):\n",
    "    '''returns notes of a specific type associated with fragments from a given paper .'''\n",
    "    q1 = self.session.query(SKI) \\\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "            .filter(SKI.type == item_type) \\\n",
    "            .filter(SKE.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "      return []\n",
    "    for f in i.has_part:\n",
    "      for n in f.has_notes:\n",
    "        if n.name == run_name:\n",
    "          yield(n)\n",
    "         \n",
    "  def list_fragments_for_paper(self, paper_id, item_type, fragment_types=['title', 'abstract']):\n",
    "    '''Loads fragments from a given paper sections of a specified paper from the local database.'''\n",
    "    q1 = self.session.query(SKI) \\\n",
    "            .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id) \\\n",
    "            .filter(SKI.type == item_type) \\\n",
    "            .filter(SKE.id.like('%'+str(paper_id)+'%')) \n",
    "    i = q1.first()\n",
    "    if i is None:\n",
    "      return []\n",
    "    fragments = []\n",
    "    for f in i.has_part:\n",
    "      if f.type in fragment_types:\n",
    "        fragments.append(f)\n",
    "    return sorted(fragments, key=lambda f: f.offset)\n",
    "  \n",
    "  def list_fragments(self, expression_id=None, search_term=None):\n",
    "    if self.session is None:\n",
    "      session_class = sessionmaker(bind=self.engine)\n",
    "      self.session = session_class()\n",
    "    if expression_id:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKF) \n",
    "            .filter(SKE_HR.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id)\n",
    "            .filter(SKI_HP.has_part_id == SKF.id)\n",
    "            .filter(SKF.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = (self.session.query(SKF) \n",
    "            .filter(SKE_HR.ScientificKnowledgeExpression_id == expression_id)\n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id)\n",
    "            .filter(SKI_HP.has_part_id == SKF.id)\n",
    "          )\n",
    "    else:\n",
    "      if search_term:\n",
    "        search = \"%{}%\".format(search_term)\n",
    "        q = (self.session.query(SKF) \n",
    "            .filter(SKE_HR.has_representation_id == SKI.id)\n",
    "            .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id)\n",
    "            .filter(SKI_HP.has_part_id == SKF.id)\n",
    "            .filter(SKF.content.ilike('%'+search+'%'))\n",
    "          )\n",
    "      else:\n",
    "        q = self.session.query(SKF)\n",
    "    for c in q.all():\n",
    "      yield(c)\n",
    "\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  # Build RAG Indexes of documents\n",
    "  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "  def embed_expression_list(self, explist):\n",
    "    '''Runs through the list of expressions, generates embeddings, and stores them in the database'''\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for e in explist:\n",
    "      coll_ids = ','.join([c.id for c in e.member_of])\n",
    "\n",
    "      t, a = self.list_fragments_for_paper(e.id, 'CitationRecord')\n",
    "      i = t.part_of\n",
    "      tiab_text = t.content+'\\n\\n'+a.content\n",
    "\n",
    "      if len(tiab_text) > 0:\n",
    "        texts.append(tiab_text)\n",
    "        metadatas.append({'c_ids': coll_ids, \\\n",
    "                                  'e_id': e.id, \\\n",
    "                                  'e_type': e.type, \\\n",
    "                                  'i_id': t.part_of, \\\n",
    "                                  'i_type': 'CitationRecord', \\\n",
    "                                  'f_id': t.id, \\\n",
    "                                  'f_type': t.type+','+a.type, \\\n",
    "                                  'citation': e.content})\n",
    "\n",
    "      jats_frgs = self.list_fragments_for_paper(e.id, 'JATSFullText', ['section'])\n",
    "      for f in jats_frgs:\n",
    "        texts.append(f.content)\n",
    "        metadatas.append({'c_ids': coll_ids, \\\n",
    "                               'e_id': e.id, \\\n",
    "                               'e_type': e.type, \\\n",
    "                               'i_id': f.part_of, \\\n",
    "                               'i_type': 'JATSFullText', \\\n",
    "                               'f_id': f.id, \\\n",
    "                               'citation': e.content})        \n",
    "\n",
    "      pdf_frgs = self.list_fragments_for_paper(e.id, 'PDFFullText', ['section'])\n",
    "      for f in pdf_frgs:\n",
    "        texts.append(f.content)\n",
    "        metadatas.append({'c_ids': coll_ids, \\\n",
    "                              'e_id': e.id, \\\n",
    "                              'e_type': e.type, \\\n",
    "                              'i_id': f.part_of, \\\n",
    "                              'i_type': 'PDFFullText', \\\n",
    "                              'f_id': f.id, \\\n",
    "                              'citation': e.content})\n",
    "\n",
    "    # Use this when we want to index text on individual sentences       \n",
    "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size = 3000, chunk_overlap=150)\n",
    "    \n",
    "    # build vector indexes from citation record     \n",
    "    #tiab_docs = text_splitter.create_documents(tiab_texts, metadatas=tiab_metadatas)\n",
    "    docs = []\n",
    "    for t,m in zip(texts, metadatas):\n",
    "      docs.append(Document(page_content=t, metadata=m))\n",
    "    \n",
    "    db = PGVector.from_documents(\n",
    "        embedding=self.embed_model,\n",
    "        documents=docs,\n",
    "        collection_name=\"ScienceKnowledgeItem\"\n",
    "    )\n",
    "    \n",
    "  def query_vectorindex(self, query_string, collection_name='ScienceKnowledgeItem', k=4):\n",
    "    db2 = PGVector.from_existing_index(\n",
    "      embedding=self.embed_model, \n",
    "      collection_name=collection_name) \n",
    "    \n",
    "    docs = db2.similarity_search_with_score(query_string, k=k)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_information_content_entity_iri(ice, id_prefix):\n",
    "    \"\"\"Reads an identifier for a given prefix\"\"\"\n",
    "    idmap = {k[:k.find(':')]:k[k.find(':')+1:] for k in ice.xref} \n",
    "    return idmap.get(id_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
