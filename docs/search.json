[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home - Alhazen",
    "section": "",
    "text": "Alhazen is an AI agent to help human users understand what is already known. Alhazen uses tools that help you build a local digital library of papers, webpages, database records, etc. by querying online sources and downloading material to your hard drive. Alhazen also uses a range of tools to analyze its digital library contents to answer meaningful questions to human users. It is primarily built with the excellent LangChain system, providing tools that use web-robots, generative AI, deep learning, ontologies and other knowledge-based technologies. It can be run using locally-running Dashboards, Jupyter Notebooks, or command-line function calls.\nThe goals of this work are threefold:\nThe system uses available tools within the rapidly-expanding ecosystem of generative AI models. This includes state-of-the-art commercial APIs (such as OpenAI, Gemini, Mistral, etc) as well as open models that can be run locally (such as Llama-2, Mixtral, Smaug, Olmo, etc.).\nTo use local models, it is recommended that Alhazen be run on a large, high end machine such as an M2 Apple Macbook with 48+GB of memory.",
    "crumbs": [
      "Get Started",
      "Home - Alhazen"
    ]
  },
  {
    "objectID": "index.html#caution-caveats",
    "href": "index.html#caution-caveats",
    "title": "Home - Alhazen",
    "section": "Caution + Caveats",
    "text": "Caution + Caveats\n\nThis toolkit provides functionality to use agents to download information from the web. Care should be taken by users and developers should make sure they abide by data licensing requirements and third party websites terms and conditions and that that they don’t otherwise infringe upon third party privacy or intellectual property rights.\nAll data generated by Large Language Models (LLMs) should be reviewed for accuracy.",
    "crumbs": [
      "Get Started",
      "Home - Alhazen"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Home - Alhazen",
    "section": "Installation",
    "text": "Installation\n\nDocker\nThe preferred method to run Alhazen is through Docker.\nNote, for correct functionality, set the following environment variables for the shell from which you are calling Docker:\nMANDATORY * LOCAL_FILE_PATH - the directory where the system will store full-text files.\nOPTIONAL * OPENAI_API_KEY - if you are using OpenAI large language models.\n* DATABRICKS_API_KEY - if you are using the Databricks AI Playground endpoint as an LLM server. * GROQ_API_KEY - if you are calling LLMs on groq.com\n\nQuickstart\nTo run the system out of the box, run these commands:\n$ git clone https://github.com/chanzuckerberg/alhazen\n$ cd alhazen\n$ docker compose build\n$ docker compose up\nThis should generate the output that includes a link formatted like this one: http://127.0.0.1:8888/lab?token=LONG-ALPHANUMERIC-STRING.\nOpen a browser to that location and you should get access to a juypter lab notebook that provides access to all notebooks in the repo.\nBrowse to nbs/tutorials/CryoET_Tutorial.ipynb to access a walkthrough of an analysis over papers involving CryoET as a demonstration.\n\n\nRun Huridocs as PDF extraction\nTo run the system with support from the Huridocs PDF extraction system (needed for processing full text articles), you must first run the docker container for that system:\n$ git clone https://github.com/huridocs/pdf_paragraphs_extraction\n$ cd pdf_paragraphs_extraction\n$ docker compose build\n$ docker compose up\nThen repeat as before, but with the huridocs alhazen image\n$ cd ..\n$ git clone https://github.com/chanzuckerberg/alhazen\n$ cd alhazen\n$ docker compose build\n$ docker compose -f docker-compose-huridocs.yml up\n\n\n\nInstall dependencies\n\nPostgresql\nAlhazen requires postgresql@14 to run. Homebrew provides an installer:\n$ brew install postgresql@14\nwhich can be run as a service:\n$ brew services start postgresql@14\n$ brew services list\nIf you install Postgresql via homebrew, you will need to create a postgres superuser to run the psql command.\n$ createuser -s postgres\nNote that the Postgres.app system also provides a nice GUI interface for Postgres but installing the pgvector package is a little more involved.\n\n\nOllama\nThe tool uses the Ollama library to execute large language models locally on your machine. Note that to able to run the best performing models on a Apple Mac M1 or M2 machine, you will need at least 48GB of memory.\n\n\nHuridocs\nWe use a PDF document text extraction and classification system called Huridocs. In particular, our PDF processing requires a docker image of their PDF Paragraphs Extraction system. To run this, perform the following steps:\n1. git clone https://github.com/huridocs/pdf_paragraphs_extraction\n2. cd pdf_paragraphs_extraction\n3. docker-compose up\n\n\n\nInstall Alhazen source code\ngit clone https://github.com/chanzuckerberg/alzhazen\nconda create -n alhazen python=3.11\nconda activate alhazen\ncd alhazen\npip install -e .",
    "crumbs": [
      "Get Started",
      "Home - Alhazen"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Home - Alhazen",
    "section": "How to use",
    "text": "How to use\nWe provide a number of low-level interfaces to work with Alhazen.\n\nNotebooks\nWe have developed numerous worked examples of corpora that can generated by running queries on public sources and then processing the results with LLM-enabled workflows. See the nbs/cookbook subdirectory for examples.\n\n\nMarimo Dashboards\nWe provide dashboards using Marimo notebooks. These provide runnable, ‘reactive notebooks’ (similar to the excellent ObservableHQ system but implemented in Python). They provide lightweight dashboards and data visualization.\nFor a dashboard that shows contents of all active databases on the current machine, run\nmarimo run nb/marimo/002_corpora_map.py \n\n\nApplications\nWe use simple python modules to run applications. To generate a simple gradio chatbot to interact with the `` library to create a modular command line interface (CLI) for Alhazen. Use the following command structure to execute specific demo applications:\npython -m alhazen.apps.chat --loc &lt;path/to/location/of/data/files&gt; --db_name &lt;database_name&gt;",
    "crumbs": [
      "Get Started",
      "Home - Alhazen"
    ]
  },
  {
    "objectID": "index.html#environmental-variables",
    "href": "index.html#environmental-variables",
    "title": "Home - Alhazen",
    "section": "Environmental Variables",
    "text": "Environmental Variables\nThe following environment variables will need to be set:\n\nLOCAL_FILE_PATH = /path/to/local/directory/for/full/text/files/\n\nTo use other commercial services, you should also set appropriate environmental variables to gain access. Examples include:\n\nOPENAI_API_KEY\nDB_API_KEY\nVERTEXAI_PROJECT_NAME\nNCBI_API_KEY",
    "crumbs": [
      "Get Started",
      "Home - Alhazen"
    ]
  },
  {
    "objectID": "index.html#code-status-and-capabilities",
    "href": "index.html#code-status-and-capabilities",
    "title": "Home - Alhazen",
    "section": "Code Status and Capabilities",
    "text": "Code Status and Capabilities\nThis project is still very early, but we are attempting to provide access to the full range of capabilities of the project as we develop them.\nThe system is built using the excellent nbdev environment. Jupyter notebooks in the nbs directory are processed based on directive comments contained within notebook cells (see guide) to generate the source code of the library, as well as accompanying documentation.\nExamples of the use of the library to address research / landscaping questions specified in the use cases can be found in the nb_scratchpad/cookbook subdirectory of this github repo.\n\nContributing\nWe warmly welcome contributions from the community! Please see our contributing guide and don’t hesitate to open an issue or send a pull request to improve Alhazen.\nThis project adheres to the Contributor Covenant code of conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to opensource@chanzuckerberg.com.",
    "crumbs": [
      "Get Started",
      "Home - Alhazen"
    ]
  },
  {
    "objectID": "index.html#where-does-the-name-alhazen-come-from",
    "href": "index.html#where-does-the-name-alhazen-come-from",
    "title": "Home - Alhazen",
    "section": "Where does the Name ‘Alhazen’ come from?",
    "text": "Where does the Name ‘Alhazen’ come from?\nOne thousand years ago, Ḥasan Ibn al-Haytham (965-1039 AD) studied optics through experimentation and observation. He advocated that a hypothesis must be supported by experiments based on confirmable procedures or mathematical reasoning — an early pioneer in the scientific method five centuries before Renaissance scientists started following the same paradigm (Website, Wikipedia, Tbakhi & Amir 2007).\nWe use the latinized form of his name (‘Alhazen’) to honor his contribution (which goes largely unrecognized from within non-Islamic communities).\nFamously, he was quoted as saying:\n\nThe duty of the man who investigates the writings of scientists, if learning the truth is his goal, is to make himself an enemy of all that he reads, and, applying his mind to the core and margins of its content, attack it from every side. He should also suspect himself as he performs his critical examination of it, so that he may avoid falling into either prejudice or leniency.\n\nHere, we seek to develop an AI capable of applying scientific knowledge engineering to support CZI’s mission. We seek to honor Ibn al-Haytham’s critical view of published knowledge by creating a AI-powered system for scientific discovery.\nNote - when describing our agent, we will use non-gendered pronouns (they/them/it) to refer to the agent.",
    "crumbs": [
      "Get Started",
      "Home - Alhazen"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Notebooks that provide walkthroughs of functionality to help other users understand how to use and run Alhazen.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nCryoET Tutorial\n\n\nDeveloping a simple tutorial to provide a walkthrough for users attempting to use Alhazen for the first time. This is based on analysis of the Cryo Electron Tomography literature and tools we have developed to analyze that data.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Tutorial Notebooks"
    ]
  },
  {
    "objectID": "utils_undocumented/langchain_output_parsers.html",
    "href": "utils_undocumented/langchain_output_parsers.html",
    "title": "Llama2 LangChain Output Parsers",
    "section": "",
    "text": "source\n\nJsonEnclosedByTextOutputParser\n\n JsonEnclosedByTextOutputParser (name:Optional[str]=None)\n\nParse the output of an LLM call to a JSON object.\n\nsource\n\n\nMermaidExtractionOutputParser\n\n MermaidExtractionOutputParser (name:Optional[str]=None)\n\nParse the output of an LLM call to a JSON object."
  },
  {
    "objectID": "utils_undocumented/curated_data_utils.html",
    "href": "utils_undocumented/curated_data_utils.html",
    "title": "Curated Dataframe Utilities",
    "section": "",
    "text": "source\n\nCuratedDataUtils\n\n CuratedDataUtils (df, doc_id_column, category_column, curator_column,\n                   distance_function=&lt;function masi_distance&gt;)\n\nThis class permits generation of curation statistics and merged, consensus dataframes.\nAttributes: * df: The dataframe being processed * doc_id_column: column in df that denotes document IDs * category_column: column in df that denotes curated category * curator_column: column in df that denotes curator * docs: the document set being curated * curators: the curators performing the curation work * categories: the set of categories being used to annotate the documents * doc_task: a low-level nltk task object\n\nsource\n\n\nno_maybe_yes_distance\n\n no_maybe_yes_distance (label1, label2)\n\nSimple distance for no / maybe / yes scale used to denote curation task.\nLookup table d(0,0) = 0.0 d(1,1) = 0.0 d(2,2) = 0.0 d(0,1) = 3.0 d(1,2) = 1.0 d(0,2) = 5.0\n\nsource\n\n\nordinal_distance\n\n ordinal_distance (label1, label2)\n\nKrippendorff’s ordinal distance metric Modified from Wikipedia page: https://en.wikipedia.org/wiki/Krippendorff%27s_alpha#Difference_functions"
  },
  {
    "objectID": "utils_undocumented/30_ms_nlp_utils.html",
    "href": "utils_undocumented/30_ms_nlp_utils.html",
    "title": "alhazen",
    "section": "",
    "text": "source\n\nget_conv_template\n\n get_conv_template (name:str)\n\nGet a conversation template.\n\nsource\n\n\nregister_conv_template\n\n register_conv_template (template:__main__.Conversation,\n                         override:bool=False)\n\nRegister a new conversation template.\n\nsource\n\n\nConversation\n\n Conversation (name:str, system:str, roles:List[str],\n               messages:List[List[str]], offset:int,\n               sep_style:__main__.SeparatorStyle, sep:str, sep2:str=None,\n               stop_str:str=None, stop_token_ids:List[int]=None,\n               conv_id:Any=None, skip_next:bool=False,\n               model_name:str=None)\n\nA class that keeps all conversation history.\n\nsource\n\n\nSeparatorStyle\n\n SeparatorStyle (value, names=None, module=None, qualname=None, type=None,\n                 start=1, boundary=None)\n\nSeparator styles.\n\nsource\n\n\nget_response\n\n get_response (responses)\n\n\nsource\n\n\npreprocess_instance\n\n preprocess_instance (source)"
  },
  {
    "objectID": "91_ssl_hackfix.html",
    "href": "91_ssl_hackfix.html",
    "title": "alhazen",
    "section": "",
    "text": "# install_certifi.py\n#\n# sample script to install or update a set of default Root Certificates\n# for the ssl module.  Uses the certificates provided by the certifi package:\n#       https://pypi.python.org/pypi/certifi\n\nimport os\nimport os.path\nimport ssl\nimport stat\nimport subprocess\nimport sys\n\nSTAT_0o775 = ( stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\n             | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP\n             | stat.S_IROTH |                stat.S_IXOTH )\n\n\ndef main():\n    openssl_dir, openssl_cafile = os.path.split(\n        ssl.get_default_verify_paths().openssl_cafile)\n\n    print(\" -- pip install --upgrade certifi\")\n    subprocess.check_call([sys.executable,\n        \"-E\", \"-s\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"certifi\"])\n\n    import certifi\n\n    # change working directory to the default SSL directory\n    os.chdir(openssl_dir)\n    relpath_to_certifi_cafile = os.path.relpath(certifi.where())\n    print(\" -- removing any existing file or link\")\n    try:\n        os.remove(openssl_cafile)\n    except FileNotFoundError:\n        pass\n    print(\" -- creating symlink to certifi certificate bundle\")\n    os.symlink(relpath_to_certifi_cafile, openssl_cafile)\n    print(\" -- setting permissions\")\n    os.chmod(openssl_cafile, STAT_0o775)\n    print(\" -- update complete\")\n\nif __name__ == '__main__':\n    main()\n\n -- pip install --upgrade certifi\nRequirement already satisfied: certifi in /Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages (2023.11.17)\n -- removing any existing file or link\n -- creating symlink to certifi certificate bundle\n\n\nDEPRECATION: Loading egg at /Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages/sympy-1.12-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\nDEPRECATION: Loading egg at /Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages/networkx-3.1-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\nDEPRECATION: Loading egg at /Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages/regex-2023.6.3-py3.11-macosx-11.1-arm64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\nDEPRECATION: Loading egg at /Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages/safetensors-0.3.2rc1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\nDEPRECATION: Loading egg at /Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages/mpmath-1.3.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\nDEPRECATION: Loading egg at /Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages/mlc_llm-0.1.dev333+g2440594-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.."
  },
  {
    "objectID": "cookbook/general_workbook.html",
    "href": "cookbook/general_workbook.html",
    "title": "General Workbook",
    "section": "",
    "text": "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to"
  },
  {
    "objectID": "cookbook/general_workbook.html#preliminaries",
    "href": "cookbook/general_workbook.html#preliminaries",
    "title": "General Workbook",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nfrom alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'machine_learning_and_biology'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDatabase has been backed up to /users/gully.burns/alhazen/alhazen_workbooks/backup2024-02-14-09-25-50.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 311/311 [00:00&lt;00:00, 2249.83it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAVAILABLE TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtractionTool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    CheckExpressionTool\n    IntrospectionTool"
  },
  {
    "objectID": "cookbook/trend_analysis/capacity_building.html",
    "href": "cookbook/trend_analysis/capacity_building.html",
    "title": "Capacity Building",
    "section": "",
    "text": "from alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'machine_learning_and_biology'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDatabase has been backed up to /users/gully.burns/alhazen/alhazen_workbooks/backup2024-02-14-09-25-50.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 311/311 [00:00&lt;00:00, 2249.83it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAVAILABLE TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtractionTool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    CheckExpressionTool\n    IntrospectionTool",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Capacity Building"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/capacity_building.html#preliminaries",
    "href": "cookbook/trend_analysis/capacity_building.html#preliminaries",
    "title": "Capacity Building",
    "section": "",
    "text": "from alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'machine_learning_and_biology'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDatabase has been backed up to /users/gully.burns/alhazen/alhazen_workbooks/backup2024-02-14-09-25-50.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 311/311 [00:00&lt;00:00, 2249.83it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAVAILABLE TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtractionTool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    CheckExpressionTool\n    IntrospectionTool",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Capacity Building"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/pathogen_landscaping.html",
    "href": "cookbook/trend_analysis/pathogen_landscaping.html",
    "title": "Pathogen_Landscaping",
    "section": "",
    "text": "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Pathogen_Landscaping"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/pathogen_landscaping.html#preliminaries",
    "href": "cookbook/trend_analysis/pathogen_landscaping.html#preliminaries",
    "title": "Pathogen_Landscaping",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nfrom alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'machine_learning_and_biology'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDatabase has been backed up to /users/gully.burns/alhazen/alhazen_workbooks/backup2024-02-14-09-25-50.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 311/311 [00:00&lt;00:00, 2249.83it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAVAILABLE TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtractionTool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    CheckExpressionTool\n    IntrospectionTool",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Pathogen_Landscaping"
    ]
  },
  {
    "objectID": "cookbook/admin/postgresql_fixes.html",
    "href": "cookbook/admin/postgresql_fixes.html",
    "title": "PostgreSQL Fixes",
    "section": "",
    "text": "Setting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.tools.tiab_classifier_tool import *\nfrom alhazen.tools.tiab_extraction_tool import *\nfrom alhazen.tools.tiab_mapping_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database, list_databases\n\nfrom alhazen.utils.searchEngineUtils import *\n\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport uuid\nimport yaml\n\nimport local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\nfrom rapidfuzz import fuzz\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nimport torch\nimport local_resources.queries.em_tech as em_tech_queries\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\n\n\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])  \nloc = os.environ.get('LOCAL_FILE_PATH')"
  },
  {
    "objectID": "cookbook/admin/postgresql_fixes.html#basics",
    "href": "cookbook/admin/postgresql_fixes.html#basics",
    "title": "PostgreSQL Fixes",
    "section": "",
    "text": "Setting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.tools.tiab_classifier_tool import *\nfrom alhazen.tools.tiab_extraction_tool import *\nfrom alhazen.tools.tiab_mapping_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database, list_databases\n\nfrom alhazen.utils.searchEngineUtils import *\n\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport uuid\nimport yaml\n\nimport local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\nfrom rapidfuzz import fuzz\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nimport torch\nimport local_resources.queries.em_tech as em_tech_queries\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\n\n\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])  \nloc = os.environ.get('LOCAL_FILE_PATH')"
  },
  {
    "objectID": "cookbook/admin/postgresql_fixes.html#backup-all-databases",
    "href": "cookbook/admin/postgresql_fixes.html#backup-all-databases",
    "title": "PostgreSQL Fixes",
    "section": "Backup all databases",
    "text": "Backup all databases\n\nloc = os.environ['LOCAL_FILE_PATH']\ncurrent_date_time = datetime.now()\nformatted_date_time = f'{current_date_time:%Y-%m-%d-%H-%M-%S}'\nformatted_date = f'{current_date_time:%Y-%m-%d}'\nif os.path.exists(loc+'/backups') is False:\n    os.makedirs(loc+'/backups')\nfor db in list_databases():\n    print('Backing up database:',db)\n    backup_path = loc+'/backups/'+db+'_backup_'+formatted_date_time+'.sql'\n    backup_ceifns_database(db, backup_path)\n    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n\n\nTry some queries\n\ndb_name = 'em_tech'\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n\n\n# search for papers using embeddings\nq = ldb.session.query(SKE.id) \\\n        .distinct() \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \\\n        .filter(SKC.id == '3') \\\n        .filter(or_(SKI.type == 'JATSFullText', SKI.type == 'PDFFullText')) \nfor e_id in q.all():\n    print(e_id)\n\n\nc_ids = ['2']\ncollections_clause = ' OR '.join(['skc_hm.\"ScientificKnowledgeCollection_id\"=\\'{}\\''.format(coll_id) for coll_id in c_ids])\nldb.session.rollback()\nq2_all = \"\"\"\n    SELECT DISTINCT ske.id, ske.content, ske.publication_date as pub_date, ske.type as pub_type, emb.embedding, skf.content \n    FROM langchain_pg_embedding as emb, \n        \"ScientificKnowledgeExpression\" as ske,\n        \"ScientificKnowledgeCollection_has_members\" as skc_hm, \n        \"ScientificKnowledgeFragment\" as skf\n    WHERE skc_hm.has_members_id = ske.id AND \n        emb.cmetadata-&gt;&gt;'i_type' = 'CitationRecord' AND\n        emb.cmetadata-&gt;&gt;'e_id' = ske.id AND \n        emb.cmetadata-&gt;&gt;'f_id' = skf.id AND ({})\n    ORDER BY pub_date DESC;\n    \"\"\".format(collections_clause)\n\nldb.session.execute(text(q2_all)).fetchall()\n\n\nAdd a collection based on EMPIAR papers\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nstep = 20\nfor start_i in range(0, len(empiar_dois), step):\n    query = ' OR '.join(['doi:\"'+empiar_dois[i]+'\"' for i in range(start_i, start_i+step)])\n    addEMPCCollection_tool.run({'id': '3', 'name':'EMPIAR Papers', 'query':query, 'full_text':True})\n\n\ndef join_set(x):\n    out = ''\n    try:\n        out = ' '.join(set(x))\n    except:\n        pass\n    return out\n\n# identify papers that we have full text for in EMPIAR\nq = ldb.session.query(SKE.id) \\\n        .distinct() \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \\\n        .filter(SKC.id == '3') \\\n        .filter(or_(SKI.type == 'JATSFullText', SKI.type == 'PDFFullText')) \ndois_to_include = [d[0][4:] for d in q.all()]    \n\nempiar_gold_standard = []\nfor i, row in empiar_df.iterrows():\n    if row.doi in dois_to_include:\n        empiar_gold_standard.append( row.to_dict() )\nempiar_gold_standard_df = pd.DataFrame(empiar_gold_standard)\n\nempiar_gs_df = empiar_gold_standard_df.groupby(['doi']).agg({'sample_preparation_type': join_set, 'agg_state': join_set, 'sample_preparation_buffer_ph': join_set, \n                                              'grid_model': join_set, 'grid_material': join_set, 'grid_mesh': join_set, \n                                              'grid_support_topology': join_set, 'grid_pretreatment': join_set, 'grid_vitrification_cryogen': join_set, \n                                              'grid_vit_ctemp': join_set, 'grid_vit_chumid': join_set}).reset_index()\nempiar_gs_df\n\n\n\nImport papers from DOIs pertaining to CryoET-Portal records 10000-10010\nThe CryoET Data portal system is based on submitted data to our curation team, accompanied by papers referenced by DOIs. Each dataset is assigned an ID value associated with DOIs.\n\n# use the EMPCSearchTool to run a query for the dois mentioned\nquery = ' OR '.join(['doi:\"'+d+'\"' for d_id in dois for d in dois[d_id] ])\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\naddEMPCCollection_tool.run(tool_input={'id': '0', 'name':'CryoET Portal (10000-10010)', 'query':query, 'full_text':True})\n\n\n\nExtend Database to include all CryoET papers\n\ncols_to_include = ['ID', 'CORPUS_NAME', 'QUERY']\ndf = pd.read_csv(files(em_tech_queries).joinpath('EM_Methods.tsv'), sep='\\t')\ndf = df.drop(columns=[c for c in df.columns if c not in cols_to_include])\ndf\n\n\nqt = QueryTranslator(df.sort_values('ID'), 'ID', 'QUERY', 'CORPUS_NAME')\n(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=['TITLE_ABS', 'METHODS'])\ncorpus_names = df['CORPUS_NAME']\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nfor (id, name, query) in zip(corpus_ids, corpus_names, epmc_queries):\n    if id != 2:\n        continue\n    addEMPCCollection_tool.run(tool_input={'id': id, 'name':name, 'query':query, 'full_text':False})\n\n\n\nCombine + Sample CryoET + EMPIAR Collections to provide a test set of papers.\n\nldb.create_new_collection_from_intersection('4', 'EMPIAR CryoET Papers', '2', '3')\n\n\n\nAdding Machine Learning\n\nml_query = '''\n(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet ) AND \n(\"Machine Learning\" OR \"Artificial Intelligence\" OR \"Deep Learning\" OR \"Neural Networks\")\n'''\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\naddEMPCCollection_tool.run(tool_input={'id': '6', \n                                       'name': 'Machine Learning in CryoET', \n                                       'query': ml_query, \n                                       'full_text': False})\n\n\ndelCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, DeleteCollectionTool)][0]\ndelCollection_tool.run(tool_input={'collection_id': '6'})\n\n\n\nBreak up TIAB of papers into sentences + classify by discourse\nNOTE - HUGGING FACE MODELS DO NOT WORK WELL ON THIS CORPUS. (NOT SURPRISINGLY - THEY WERE TRAINED ON MEDICAL PAPERS WHERE THE DIFFERENT SECTIONS OF THE PAPER WERE EXPLICITLY LABELED)\nUSE LLMS TO DO THE EXTRACTION - GPT3.5?\n\n# Get the metadata extraction tool\nt2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\nt2.run(tool_input={'collection_id': '5', 'run_label': 'dev'})\n\n\nj = '''{\n\"Background\": \"Eps15-homology domain containing proteins (EHDs) are eukaryotic, dynamin-related ATPases involved in cellular membrane trafficking. They oligomerize on membranes into filaments that induce membrane tubulation. While EHD crystal structures in open and closed conformations were previously reported, little structural information is available for the membrane-bound oligomeric form. Consequently, mechanistic insights into the membrane remodeling mechanism have remained sparse.\",\n\"Objectives_Methods\": \"Here, by using cryo-electron tomography and subtomogram averaging, we determined structures of nucleotide-bound EHD4 filaments on membrane tubes of various diameters at an average resolution of 7.6 Å.\",\n\"Results_Conclusions\": \"Assembly of EHD4 is mediated via interfaces in the G-domain and the helical domain. The oligomerized EHD4 structure resembles the closed conformation, where the tips of the helical domains protrude into the membrane. The variation in filament geometry and tube radius suggests a spontaneous filament curvature of approximately 1/70 nm&lt;sup&gt;-1&lt;/sup&gt;. Combining the available structural and functional data, we suggest a model for EHD-mediated membrane remodeling.\"\n}'''\n\njson.loads(j)\n\n\n# Get the metadata extraction tool\nmodels = ['databricks_dbrx']\nfor m in models:\n    llm = llms_lookup.get(m)\n    cb = AlhazenAgent(llm, llm, db_name=db_name)\n    t2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\n    t2.run(tool_input={'collection_id': '2', 'run_label': m, 'repeat_run': False})\n\n\nto_remove = [\"doi:10.1101/2024.03.04.583254\", \n             \"doi:10.1101/2023.11.21.567712\",\n\"doi:10.3791/6515\",\n\"doi:10.1101/2023.07.28.550950\",\n\"doi:10.1093/micmic/ozad067.483\",\n\"doi:10.1007/978-1-0716-2639-9_20\",\n\"doi:10.1016/j.yjsbx.2022.100076\",\n\"doi:10.1016/j.xpro.2022.101658\",\n\"doi:10.1016/j.cell.2022.06.034\",\n\"doi:10.1093/plphys/kiab449\",\n\"doi:10.1073/pnas.2118020118\",\n\"doi:10.3791/62886\",\n\"doi:10.20944/preprints202105.0098.v1\",\n\"doi:10.1016/bs.mcb.2020.12.009\",\n\"doi:10.1007/978-1-0716-0966-8_1\",\n\"doi:10.1007/978-1-0716-0966-8_2\",\n\"doi:10.21769/bioprotoc.3768\",\n\"doi:10.1371/journal.ppat.1008883\",\n\"doi:10.1101/2020.05.19.104828\",\n\"doi:10.1073/pnas.1916331116\",\n\"doi:10.1042/bst20170351_cor\",\n\"doi:10.1038/s41594-018-0043-7\",\n\"doi:10.1007/978-1-4939-8585-2_4\",\n\"doi:10.1007/s41048-017-0040-0\",\n\"doi:10.1007/978-1-4939-6927-2_20\",\n\"doi:10.1016/j.str.2015.03.008\",\n\"doi:10.1007/978-1-62703-227-8_4\",\n\"doi:10.1016/b978-0-12-397945-2.00017-2\",\n\"doi:10.1016/j.jmb.2010.10.021\",\n\"doi:10.1186/1757-5036-3-6\",\n\"doi:10.1016/j.jmb.2008.03.014\",\n\"doi:10.1007/978-1-59745-294-6_20\"]\n\n\nfor d in to_remove:\n    q = \"\"\"\n    SELECT DISTINCT n.id FROM langchain_pg_embedding as emb, \"Note\" as n\n    WHERE emb.cmetadata-&gt;&gt;'n_type' = 'TiAbMappingNote__Discourse' AND\n        emb.cmetadata-&gt;&gt;'about_id' = '{}' AND \n        emb.cmetadata-&gt;&gt;'discourse_type' = 'ResultsConclusions' AND \n        emb.cmetadata-&gt;&gt;'n_id' = n.id;\"\"\".format(d)\n    for row in ldb.session.execute(text(q)).all():\n        ldb.delete_note(row[0], commit_this=True)\n\n\nldb.session.rollback()\nexp_q = ldb.session.query(SKE) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.type=='CitationRecord') \\\n        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n        .order_by(desc(SKE.publication_date))\n\ncount = 0\nfor e in tqdm(exp_q.all()):\n    q = ldb.session.query(N) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == e.id) \\\n        .filter(N.type =='TiAbMappingNote__Discourse')\n    for n in q.all():\n        dmap = json.loads(n.content) \n        if 'Objectives_Methods' in dmap.keys():\n            print('beep')\n            #new_dmap = {'Background': dmap.get('Background'), 'ObjectivesMethods': dmap.get('Objectives_Methods'), 'ResultsConclusions': dmap.get('Results_Conclusions')}\n            #n.content = json.dumps(new_dmap, indent=4)\n            #ldb.session.flush()\n#ldb.session.commit()\n\n\nldb.session.rollback()\nexp_q = ldb.session.query(SKE) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.type=='CitationRecord') \\\n        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n        .order_by(desc(SKE.publication_date))\n\ntexts = []\nmetadatas = []\n\ncount = 0\nfor e in tqdm(exp_q.all()):\n    q = ldb.session.query(N) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == e.id) \\\n        .filter(N.type =='TiAbMappingNote__Discourse')\n\n\n    n = q.first()\n    dmap = json.loads(n.content)\n    '''Runs through the list of expressions, generates embeddings, and stores them in the database'''\n\n    for dtype in ['Background', 'ObjectivesMethods', 'ResultsConclusions']:\n        t = dmap.get(dtype)\n        if t is None:\n            continue\n        texts.append(t)\n        metadatas.append({'about_id': e.id, \\\n                        'about_type': 'ScientificKnowledgeExpression', \\\n                        'n_id': n.id, \\\n                        'n_type': 'TiAbMappingNote__Discourse', \\\n                        'discourse_type': dtype})\n\ndocs = []\nfor t,m in zip(texts, metadatas):\n    docs.append(Document(page_content=t, metadata=m))\n    \ndb = PGVector.from_documents(\n    embedding=ldb.embed_model,\n    documents=docs,\n    collection_name=\"Note\"\n)\n\n\nmodel_path = '/Users/gully.burns/Documents/2024H1/models/discourse_tagger'\ntokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\", \n                                          truncation=True, \n                                          max_length=512)\nlabels = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\nlookup = {'LABEL_%d'%(i):l for i, l in enumerate(labels)}\nmodel = AutoModel.from_pretrained(model_path)\nmodel.eval()\n\nclassifier = pipeline(\"text-classification\", \n                      model = model_path, \n                      tokenizer=tokenizer, \n                      truncation=True,\n                      batch_size=8,\n                      device='mps')\n\n\nself = ldb\ncollection_id = '2'\n\nq1 = self.session.query(SKE, SKI) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n        .filter(SKI_HP.has_part_id == SKF.id) \\\n        .filter(SKI.type == 'CitationRecord') \\\n        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \n\nfor ske, ski in tqdm(q1.all()):\n    b = ''\n    om = ''\n    rc = ''  \n\n    fragments = []\n    for f in ski.has_part:\n      if f.type in ['title', 'abstract']:\n        fragments.append(f)\n\n    # USE AN LLM HERE INSTEAD OF A DEEP LEARNING CLASSIFER\n\n\n    for skf in sorted(fragments, key=lambda f: f.offset):\n        for s in self.sent_detector.tokenize(skf.content):\n            m = classifier(skf.content)\n            l = lookup.get(m[0].get('label'))\n            if l == 'BACKGROUND':\n                if len(b) &gt; 0:\n                    b += '\\n'\n                b += s\n            elif l == 'OBJECTIVE' or l == 'METHODS':\n                if len(om) &gt; 0:\n                    om += '\\n'\n                om += s\n            else: \n                if len(rc) &gt; 0:\n                    rc += '\\n'\n                rc += s\n    skf_stem = ske.id+'.'+ski.type+'.'\n    if len(b) &gt; 0:\n        f_b = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n                type='background_sentences', offset=-1, length=len(b),\n                name=skf_stem+'background', content=b)\n        self.session.add(f_b)\n        ski.has_part.append(f_b)\n        f_b.part_of = ski.id    \n    if len(om) &gt; 0:\n        f_om = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n                type='objective_methods_sentences', offset=-1, length=len(om),\n                name=skf_stem+'objective_methods', content=om)\n        self.session.add(f_om)\n        ski.has_part.append(f_om)\n        f_om.part_of = ski.id\n    if len(rc) &gt; 0:\n        f_rc = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n                type='results_conclusions_sentences', offset=-1, length=len(rc),\n                name=skf_stem+'results_conclusions', content=rc)\n        self.session.add(f_rc)\n        ski.has_part.append(f_rc)\n        f_rc.part_of = ski.id\n    self.session.flush()\nself.session.commit()\n\n\nself = ldb\ncollection_id = '2'\n#self.session.rollback()\nq2 = self.session.query(SKF) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n        .filter(SKI_HP.has_part_id == SKF.id) \\\n        .filter(SKI.type == 'CitationRecord') \\\n        .filter(or_(SKF.type == 'results_conclusions_sentences', \\\n                SKF.type == 'objective_methods_sentences', \\\n                SKF.type == 'background_sentences'))\nfor skf in tqdm(q2.all()):\n    self.delete_fragment(skf.id)\n\n\nself = ldb\ncollection_id = '2'\n#self.session.rollback()\nq2 = self.session.query(SKE, SKF) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n        .filter(SKI_HP.has_part_id == SKF.id) \\\n        .filter(SKI.type == 'CitationRecord') \\\n        .filter(SKF.type == 'objective_methods_sentences') \\\n        .order_by(desc(SKE.publication_date)) \\\n        .order_by(SKF.name)\n\nfor ske, skf in tqdm(q2.all()):\n    print(skf)\n\n\n\nGet full text copies of all the papers about CryoET\n\ncb.agent_executor.invoke({'input':'Get full text copies of all papers in the collection with id=\"2\".'})\n\n\nldb.create_new_collection_from_sample('5', 'EMPIAR CryoET Papers Tests', '4', 20, ['ScientificPrimaryResearchArticle', 'ScientificPrimaryResearchPreprint'])"
  },
  {
    "objectID": "cookbook/admin/postgresql_fixes.html#analyze-collections",
    "href": "cookbook/admin/postgresql_fixes.html#analyze-collections",
    "title": "PostgreSQL Fixes",
    "section": "Analyze Collections",
    "text": "Analyze Collections\n\nq = ldb.session.query(SKC.id, SKC.name, SKE.id, SKI.type) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \ndf = pd.DataFrame(q.all(), columns=['id', 'collection name', 'doi', 'item type'])\ndf.pivot_table(index=['id', 'collection name'], columns='item type', values='doi', aggfunc=lambda x: len(x.unique())).fillna(0)\n\n\nSurvey + Run Classifications over Papers\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nl = []\nq = ldb.session.query(N, SKE) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n\noutput = []        \nprint(len(q.all()))\nfor n, ske in q.all():\n    ldb.delete_note(n.id)    \nprint(len(q.all()))\n\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '5', 'classification_type':'cryoet_study_types', 'repeat_run':True})\n\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '2', 'classification_type':'cryoet_study_types'})\n\n\nl = []\nq = ldb.session.query(N, SKE) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n\noutput = []        \nfor n, ske in q.all():\n        tup = json.loads(n.content)\n        tup['doi'] = 'http://doi.org/'+re.sub('doi:', '', ske.id)\n        tup['year'] = ske.publication_date.year\n        tup['month'] = ske.publication_date.month\n        tup['ref'] = ske.content\n        output.append(tup)\ndf = pd.DataFrame(output).sort_values(['year', 'month'], ascending=[False, False])\ndf.to_csv(loc+'/'+db_name+'/cryoet_study_types.tsv', sep='\\t')\ndf\n\n\n\nSurvey + Run Extractions over Papers\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractExtraction_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '5', 'extraction_type':'cryoet'})"
  },
  {
    "objectID": "cookbook/admin/postgresql_fixes.html#tests-checks",
    "href": "cookbook/admin/postgresql_fixes.html#tests-checks",
    "title": "PostgreSQL Fixes",
    "section": "Tests + Checks",
    "text": "Tests + Checks\n\nAgent tool selection + execution + interpretation\n\ncb.agent_executor.invoke({'input':'Hi who are you and what can you do?'})"
  },
  {
    "objectID": "cookbook/admin/postgresql_fixes.html#run-metadata-extraction-chain-over-listed-papers",
    "href": "cookbook/admin/postgresql_fixes.html#run-metadata-extraction-chain-over-listed-papers",
    "title": "PostgreSQL Fixes",
    "section": "Run MetaData Extraction Chain over listed papers",
    "text": "Run MetaData Extraction Chain over listed papers\nHere, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset.\n\nq = ldb.session.query(SKE.id) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKC.id=='2')  \ndois = [e.id for e in q.all()]\ndois\n\n\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\nmetadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\nt2.compile_answers('cryoet', metadata_dir)\nt2.write_answers_as_notes('cryoet', metadata_dir)\n#sorted(list(set([doi for q in t2.examples for doi in t2.examples[q]])))\n\n\n# Get the metadata extraction tool\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\n# Hack to get the path to the metadata directory as a string\n#metadata_dir = str(files(cryoet_portal_metadata).joinpath('temp'))[0:-4]\nmetadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\n\n# Compile the answers from the metadata directory\nt2.compile_answers('cryoet', metadata_dir)\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\ndf = pd.DataFrame()\nfor d in [d for d in dois]:\n    item_types = set()\n    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df = pd.concat([df, pd.DataFrame(l)]) \n     \n# Iterate over papers to run the metadata extraction tool\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nfor d in [d for d in dois]:\n    item_types = set()\n\n    # Skip if the doi is already in the database\n    if len(df)&gt;0 and d in df.doi.unique():\n        continue\n\n    # Run the metadata extraction tool on the doi\n    t2.run(tool_input={'paper_id': d, 'extraction_type': 'cryoet', 'run_label': 'test'})\n\n    # Add the results to the dataframe    \n    l2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df = pd.concat([df, pd.DataFrame(l2)])\n\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\ndf_final = pd.DataFrame()\nfor d in [d for d in dois]:\n    item_types = set()\n    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df_final = pd.concat([df_final, pd.DataFrame(l)]) \ndf_final\n\n\n# Get the metadata extraction tool\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nl = []\nfor d in [d for d in dois]:\n    item_types = set()\n    pred1 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    pred2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test_dbrx')\n    gold = t2.read_metadata_extraction_notes(d, 'cryoet', 'gold') \n    if pred1 is None or pred2 is None or gold is None or \\\n            len(pred1)==0 or len(pred2)==0 or len(gold)!=1:\n        continue\n    for k in gold[0]:\n        g_case = gold[0][k]\n        if g_case=='' or g_case is None:\n            continue    \n        for j, p_case in enumerate(pred1):\n            sim = fuzz.ratio(str(g_case), str(p_case.get(k,''))) / 100.0\n            print(k, str(g_case), str(p_case.get(k,'')), sim)\n\n\n# Get the metadata extraction tool\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\ndf = t2.report_metadata_extraction_for_collection('5', 'cryoet', 'test').set_index('doi')\ndf.to_csv(loc+'/'+db_name+'/reports/cryoet_metadata_gpt4.tsv', sep='\\t')\n\n\nldb.create_zip_archive_of_full_text_files('5', loc+'/'+db_name+'/full_text_files.zip')\n\n\nq3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'MetadataExtractionNote') \nl = []\nfor row in q3.all():\n    paper = row[0]\n    name = row[1]\n#    provenance = json.loads(row[2])\n    result = json.loads(row[3])\n    kv = {k:result[k] for k in result}\n    kv['DOI'] = paper\n    kv['run'] = name\n    l.append(kv)\n# create a dataframe from the list of dictionaries with DOI as the index column\nif len(l)&gt;0:\n    df = pd.DataFrame(l).set_index(['DOI', 'run'])\nelse: \n    df = pd.DataFrame()\ndf\n\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nfor row in q3.all():\n    d_id = row[0]\n    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n    notes_to_delete = []\n    for n in ldb.read_notes_about_x(e):\n        notes_to_delete.append(n.id)\n    for n in notes_to_delete:\n        ldb.delete_note(n)"
  },
  {
    "objectID": "cookbook/admin/postgresql_fixes.html#protocol-modeling-extraction",
    "href": "cookbook/admin/postgresql_fixes.html#protocol-modeling-extraction",
    "title": "PostgreSQL Fixes",
    "section": "Protocol Modeling + Extraction",
    "text": "Protocol Modeling + Extraction\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nslm = ChatOllama(model='stablelm-zephyr') \nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm3 = ChatOpenAI(model='gpt-3.5-turbo') \nd = (\"This tool attempts to draw a protocol design from the description of a scientific paper.\")\n\n\nt1 = ProcotolEntitiesExtractionTool(db=ldb, llm=llm3, description=d)\nentities = t1.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\nentities\n\n\nt2 = ProcotolProcessesExtractionTool(db=ldb, llm=llm3, description=d)\nprocesses = t2.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\nprocesses.get('data')"
  },
  {
    "objectID": "cookbook/key_opinion_leaders/index.html",
    "href": "cookbook/key_opinion_leaders/index.html",
    "title": "Key Opinion Leaders",
    "section": "",
    "text": "Studies that enable analysis of Key Opinion Leaders over a corpus of papers based on citation metrics and underlying searches of researcher-centred resources.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAfrican Microscopy\n\n\nA key effort at CZI is supporting communities of scientists across the world.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Key Opinion Leaders"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cellxgene_datasets.html",
    "href": "cookbook/single_doc_extraction/cellxgene_datasets.html",
    "title": "CellXGene Datasets",
    "section": "",
    "text": "The goal of this work is to survey the literature in RNAseq analysis as it happens. We intend to use this approach to contact researchers publishing these valuable datasets so that we can include them in the CellXGene data portal.",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CellXGene Datasets"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cellxgene_datasets.html#preliminaries",
    "href": "cookbook/single_doc_extraction/cellxgene_datasets.html#preliminaries",
    "title": "CellXGene Datasets",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nfrom alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nfrom alhazen.utils.searchEngineUtils import load_paper_from_openalex, read_references_from_openalex \nfrom pyalex import config, Works, Work\nconfig.email = \"gully.burns@chanzuckerberg.com\"\n\nimport requests\nimport os\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'sc_sequencing'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDatabase has been backed up to /users/gully.burns/alhazen/alhazen_workbooks/backup2024-02-14-09-25-50.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 310/310 [00:00&lt;00:00, 2594.82it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAVAILABLE TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtraction_EverythingEverywhere_Tool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    ProcotolExtractionTool\n    CheckExpressionTool\n    IntrospectionTool\n\n\n\n# Define the API endpoint URL\ndoi = '10.7150/ijbs.82191'\n\ne = load_paper_from_openalex(doi)\n\ndoi = 'doi:10.7150/ijbs.82191'\nreferenced_works = read_references_from_openalex(doi)\nfor r in referenced_works:\n    e = load_paper_from_openalex(r)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CellXGene Datasets"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/ncats_natural_history_studies.html",
    "href": "cookbook/single_doc_extraction/ncats_natural_history_studies.html",
    "title": "NCATS Natural History Studies",
    "section": "",
    "text": "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "NCATS Natural History Studies"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/ncats_natural_history_studies.html#preliminaries",
    "href": "cookbook/single_doc_extraction/ncats_natural_history_studies.html#preliminaries",
    "title": "NCATS Natural History Studies",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\n\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'natural_history_studies'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\nbackup_ceifns_database('natural_history_studies', loc+'/natural_history_studies.db')\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \n#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n\ncb = AlhazenAgent(llm2, llm2)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\n\ndois = ['10.1007/s40123-019-00218-9', \n                    '10.1136/heartjnl-2013-304920',\n                    '10.21037/cdt.2018.09.18',\n                    '10.1038/sc.2013.170',\n                    '10.1016/j.jacc.2006.07.053',\n                    '10.1186/s12884-016-1076-8',\n                    '10.1200/PO.20.00218',\n                    '10.1056/NEJMoa021736',\n                    '10.1093/europace/euw067',\n                    '10.7150/jca.32579']\n\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nstep = 40\nfor start_i in range(0, len(dois), step):\n    query = ' OR '.join(['doi:\\\"'+dois[i]+'\\\"' for i in range(start_i, start_i+step) if i &lt; len(dois)])\n    addEMPCCollection_tool.run({'id': '0', 'name':'Basic Extraction Demo', 'query':query, 'full_text':True})\n\n\ncb.agent_executor.invoke({'input':'Download all available full text for papers in the collection with id=\"0\"'})\n\n\nldb.report_collection_composition()\n\n\nstudy_type = 'natural history studies'\n\n\n# Get the metadata extraction tool\nt2 = [t for t in cb.tk.get_tools() if isinstance(t, MetadataExtraction_EverythingEverywhere_Tool)][0]\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\ndf = pd.DataFrame()\nfor d in dois:\n    item_types = set()\n    d_id = 'doi:'+d\n    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, study_type))\n    df = pd.concat([df, df2])\n\n\n# Iterate over papers to run the metadata extraction tool\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nfor d in [d for d in dois]:\n    item_types = set()\n\n    d_id = 'doi:'+d\n\n    # Skip if the doi is already in the database\n    #if len(df)&gt;0 and d_id in df.doi.unique():\n    #    continue\n\n    # Run the metadata extraction tool on the doi\n    t2.run(tool_input={'paper_id': d_id, 'extraction_type': study_type})\n\n    # Add the results to the dataframe\n    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, study_type))\n    df = pd.concat([df, df2])\n\n\ndf = pd.DataFrame()\nfor d in [d for d in dois]:\n    d_id = 'doi:'+d\n    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, study_type))\n    df = pd.concat([df, df2]) \ndf.to_csv(loc+'/nhs_metadata_extraction.csv', index=False, sep='\\t')\n\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nq3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'MetadataExtractionNote') \nfor row in q3.all():\n    d_id = row[0]\n    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n    notes_to_delete = []\n    for n in ldb.read_notes_about_x(e):\n        notes_to_delete.append(n.id)\n    for n in notes_to_delete:\n        ldb.delete_note(n)\n\n\nbackup_ceifns_database(os.environ['ALHAZEN_DB_NAME'], loc+'/nhs_metadata_extraction.db.backup2')",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "NCATS Natural History Studies"
    ]
  },
  {
    "objectID": "cookbook/question_answering/virtualcell_corepapers.html",
    "href": "cookbook/question_answering/virtualcell_corepapers.html",
    "title": "Virtual Cell Landscaping Analysis",
    "section": "",
    "text": "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Question Answering",
      "Virtual Cell Landscaping Analysis"
    ]
  },
  {
    "objectID": "cookbook/question_answering/virtualcell_corepapers.html#preliminaries",
    "href": "cookbook/question_answering/virtualcell_corepapers.html#preliminaries",
    "title": "Virtual Cell Landscaping Analysis",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\n\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database\nfrom alhazen.utils.searchEngineUtils import *\n\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nimport local_resources.queries.vcm_key_papers as vcm_key_papers\nimport json\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'virtual_cell'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDatabase has been backed up to /users/gully.burns/alhazen/alhazen_workbooks/backup2024-02-14-09-25-50.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 310/310 [00:00&lt;00:00, 2540.15it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \n#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n\ncb = AlhazenAgent(llm, llm)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAGENT TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtraction_EverythingEverywhere_Tool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    ProcotolExtractionTool\n    CheckExpressionTool\n    TitleAbstractClassifier_OneDocAtATime_Tool\n\n\n\nwith open(files(vcm_key_papers).joinpath('kp.json')) as f:\n    kp = json.load(f)\n\nfor c  in kp['Single-cell transformers']:\n    print(c.split('\\t')[-1].strip())\n\n10.1101/2024.01.25.577152\n10.1101/2023.11.29.569320\n10.1038/s41586-023-06139-9\n10.1101/2023.11.28.568918\n10.1038/s41592-024-02201-0\n10.1038/s41467-023-35923-4\n10.1145/3583780.3615061\n10.1016/j.isci.2023.106536\n10.48550/arXiv.2302.03038\n10.1101/2023.05.29.542705\n10.1101/2023.03.24.534055\n10.48550/arXiv.2306.04371\nhttps://openreview.net/forum?id=KMtM5ZHxct\n10.1101/2023.09.26.559542\n10.1101/2023.10.03.560734\n10.1101/2024.02.13.580114\nhttps://openreview.net/forum?id=QFm186CbBp\n10.1101/2023.07.04.547619\n10.3390/biom13040611\n10.1093/bioinformatics/btad165\n10.1093/bib/bbad195\n10.1101/2022.11.20.517285\n10.48550/arXiv.2210.14330\n10.3389/fgene.2022.1038919\n10.1038/s42256-022-00534-z\n10.1093/bib/bbab573\n10.1101/2020.02.05.935239",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Question Answering",
      "Virtual Cell Landscaping Analysis"
    ]
  },
  {
    "objectID": "cookbook/question_answering/cellbiology.html",
    "href": "cookbook/question_answering/cellbiology.html",
    "title": "Cell Biology Database.",
    "section": "",
    "text": "Here we set up libraries and methods to create and query the local Postgres database we will be using to store our information from the Alhazen tools and agent\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.tools.basic import *\nfrom alhazen.tools.paperqa_emulation_tool import *\nfrom alhazen.toolkit import *\n\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom datetime import datetime\n\nfrom importlib_resources import files\nimport os\nimport pandas as pd\n\nfrom sqlalchemy import func, text\n\nfrom time import time\nfrom tqdm import tqdm\n\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nimport torch\nimport os\nimport torch\nfrom transformers import pipeline\n\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\nfrom operator import itemgetter\nfrom langchain.chat_models import ChatOllama\nfrom langchain.schema import get_buffer_string, OutputParserException, format_document\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain_core.output_parsers import StrOutputParser, JsonOutputParser\nfrom langchain.prompts import ChatPromptTemplate, PromptTemplate\nfrom alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n\nfrom langchain.schema import format_document\nfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\nfrom langchain_core.runnables import RunnableParallel\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the Postgres database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save files for your digital library, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'cell_biology'\n\nRun this command to destroy your current database\nUSE WITH CAUTION\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nRun this command to create a new, empty database.\n\ncreate_ceifns_database(db_name)\n\nThis command lists all the tools the Alhazen agent system has access to\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n\nllms = lookup_chat_models()\nllm_dbrx = llms.get('gpt4_1106')\n\ncb = AlhazenAgent(db_name=db_name, agent_llm=llm_dbrx, tool_llm=llm_dbrx)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Question Answering",
      "Cell Biology Database."
    ]
  },
  {
    "objectID": "cookbook/question_answering/cellbiology.html#preliminaries",
    "href": "cookbook/question_answering/cellbiology.html#preliminaries",
    "title": "Cell Biology Database.",
    "section": "",
    "text": "Here we set up libraries and methods to create and query the local Postgres database we will be using to store our information from the Alhazen tools and agent\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.tools.basic import *\nfrom alhazen.tools.paperqa_emulation_tool import *\nfrom alhazen.toolkit import *\n\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom datetime import datetime\n\nfrom importlib_resources import files\nimport os\nimport pandas as pd\n\nfrom sqlalchemy import func, text\n\nfrom time import time\nfrom tqdm import tqdm\n\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nimport torch\nimport os\nimport torch\nfrom transformers import pipeline\n\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\nfrom operator import itemgetter\nfrom langchain.chat_models import ChatOllama\nfrom langchain.schema import get_buffer_string, OutputParserException, format_document\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain_core.output_parsers import StrOutputParser, JsonOutputParser\nfrom langchain.prompts import ChatPromptTemplate, PromptTemplate\nfrom alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n\nfrom langchain.schema import format_document\nfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\nfrom langchain_core.runnables import RunnableParallel\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the Postgres database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save files for your digital library, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'cell_biology'\n\nRun this command to destroy your current database\nUSE WITH CAUTION\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nRun this command to create a new, empty database.\n\ncreate_ceifns_database(db_name)\n\nThis command lists all the tools the Alhazen agent system has access to\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n\nllms = lookup_chat_models()\nllm_dbrx = llms.get('gpt4_1106')\n\ncb = AlhazenAgent(db_name=db_name, agent_llm=llm_dbrx, tool_llm=llm_dbrx)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Question Answering",
      "Cell Biology Database."
    ]
  },
  {
    "objectID": "cookbook/question_answering/cellbiology.html#attempting-to-reconstruct-paper-qa-pipeline-in-our-system.",
    "href": "cookbook/question_answering/cellbiology.html#attempting-to-reconstruct-paper-qa-pipeline-in-our-system.",
    "title": "Cell Biology Database.",
    "section": "ATTEMPTING TO RECONSTRUCT PAPER-QA PIPELINE IN OUR SYSTEM.",
    "text": "ATTEMPTING TO RECONSTRUCT PAPER-QA PIPELINE IN OUR SYSTEM.\n\nEmbed paper sections + question\nGiven the question, summarize the retrieved paper sections relative to the question\nScore and select relevant passages\nPut summaries into prompt\nGenerate answer with prompt\n\n\nos.environ['PGVECTOR_CONNECTION_STRING'] = \"postgresql+psycopg2:///\"+ldb.name\nvectorstore = PGVector.from_existing_index(\n        embedding = ldb.embed_model, \n        collection_name = 'ScienceKnowledgeItem') \nretriever = vectorstore.as_retriever(search_kwargs={'k':15, 'filter': {'skc_ids': 81}})\n#retriever = vectorstore.as_retriever()\n\n\nretriever.invoke(question)\n\n\n#from paperqa.prompts import summary_prompt as paperqa_summary_prompt, qa_prompt as paperqa_qa_prompt, select_paper_prompt, citation_prompt, default_system_prompt\n\nhum_p = '''First, read through the following JSON encoding of {k} research articles: \n\nEach document has three attributes: (A) a digital object identifier ('DOI') code, (B) a CITATION string containing the authors, publication year, title and publication location, and the (C) CONTENT field with the title and abstract of the paper.  \n\n```json:{context}```\n\nThen, generate a JSON list of summaries of each article in order to help answer the following question:\n\nQuestion: {question}\n\nDo NOT directly answer the question, instead summarize to give evidence to help answer the question. \nFocus on specific details, including numbers, equations, or specific quotes. \nReply \"Not applicable\" if text is irrelevant. \nRestrict each summary to {summary_length} words. \nAlso, provide a score from 1-10 indicating relevance to question. Do not explain your score. \n\nWrite this answer as JSON formatted output. Provide a list of {k} dict objects with the following fields: DOI, SUMMARY, RELEVANCE SCORE. \n\nDo not provide additional explanation for the answer.\nDo not include any other response other than a JSON object.\n'''\nsys_p = '''Answer in a direct and concise tone. Your audience is an expert, so be highly specific. If there are ambiguous terms or acronyms, first define them.'''\n\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"'DOI': '{ske_id}', CITATION: '{citation}', CONTENT:'{page_content}'\")\ndef combine_documents(\n    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"},{\\n\"\n):\n    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n    return '[{'+document_separator.join(doc_strings)+'}]'\n\ntemplate = ChatPromptTemplate.from_messages([\n            (\"system\", sys_p),\n            (\"human\", hum_p)])\n\nqa_chain = (\n    RunnableParallel({\n        \"k\": itemgetter(\"k\"),\n        \"question\": itemgetter(\"question\"),\n        \"summary_length\": itemgetter(\"summary_length\"),\n        \"context\": itemgetter(\"question\") | retriever | combine_documents,\n    })\n    | {\n        \"summary\": template | ChatOllama(model='mixtral') | JsonEnclosedByTextOutputParser(),\n        \"context\": itemgetter(\"context\"),\n    }\n)\n\ninput = {'question': question, 'summary_length': 1000, 'k':5}    \nout = qa_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})\nprint(json.dumps(out, indent=4))",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Question Answering",
      "Cell Biology Database."
    ]
  },
  {
    "objectID": "utils/ceifns_db.html",
    "href": "utils/ceifns_db.html",
    "title": "Database for Scientific Knowledge",
    "section": "",
    "text": "We define a local database for use with the Alhazen schema conforming to the following schema:\n\n\n\nSchema\n\n\nThe key elements of the schema are:\nAbstract Parent Class\n\nInformationContentEntity - a named piece of information (all other entities inherit from this)\n\nClasses Denoting Extant Scientific Knowledge from External Sources\n\nScientificKnowledgeCollection - a collection of scientific knowledge expressions\nScientificKnowledgeExpression - an expression of scientific knowledge (e.g., a paper, a database record, a blog post, etc.)\nScientificKnowledgeItem - the data item that manifests the expression (e.g., a pdf, an html data file, a yaml file, etc.)\nScientificKnowledgeFragment - a relevant fragment of the item (e.g., a paragraph, a table, a figure, etc.)\n\nClasses Denoting Information Generated by Alhazen\n\nNote - an annotation placed on any InformationContentEntity (e.g., a comment, a question, a link to another fragment, etc.)\n\n\nOur definitions are inspired by FABIO, but only differentiate between ‘expression’ (as the reference to an scientific underlying work product, e.g., a paper or database record) and ‘item’ (as the actual data that delivers on that expression). The system is coded in LinkML and generated as a Postgresql database and a SQLAlchemy ORM.\n\n\nsource\n\nlist_databases\n\n list_databases ()\n\nList all CEIFNS databases in the postgres server\n\nsource\n\n\nrestore_ceifns_database\n\n restore_ceifns_database (db_name, backup_file, verbose=False)\n\nRestore postgres db from a file.\n\nsource\n\n\nbackup_ceifns_database\n\n backup_ceifns_database (db_name, dest_file, verbose=False)\n\nBackup postgres db to a local file. Note that this\n\nsource\n\n\ndrop_ceifns_database\n\n drop_ceifns_database (db_name, backupFirst=True)\n\nUse this function to delete a CEIFNS database from the local postgres server. Set the backupFirst flag to True to backup the database before deletion.\n\nsource\n\n\ncreate_ceifns_database\n\n create_ceifns_database (db_name)\n\nUse this function to create a CEIFNS database within the local postgres server.\n\nsource\n\n\nCeifns_LiteratureDb\n\n Ceifns_LiteratureDb (loc:str, name:str,\n                      engine:sqlalchemy.engine.base.Engine=None,\n                      session:sqlalchemy.orm.session.Session=None, sent_de\n                      tector:nltk.tokenize.punkt.PunktSentenceTokenizer=No\n                      ne, embed_model:langchain_core.embeddings.embeddings\n                      .Embeddings=None)\n\nThis class runs a set of queries on external literature databases to build a local database of linked corpora and papers.\nFunctionality includes:\n\nExecutes queries over European PMC\nCan run combinatorial sets of queries using a dataframe structure\n\nRequires a column of queries expressed in boolean logic\nOptional to define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n\nHas capability to run boolean logic over sources (currently only European PMC, but possibly others)\nBuilds a local Postgresql database with tables for collections, expressions, items, fragments, and notes.\nProvides an API for querying the database and returning results as sqlAlchemy objects.\nPermits user to download a local copy of full text papers in NXML(JATS), PDF, and HTML format.\n\n\nsource\n\n\nread_information_content_entity_iri\n\n read_information_content_entity_iri (ice, id_prefix)\n\nReads an identifier for a given prefix",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "Database for Scientific Knowledge"
    ]
  },
  {
    "objectID": "utils/web_robot.html",
    "href": "utils/web_robot.html",
    "title": "Web Robots",
    "section": "",
    "text": "These functions spawn a web-browser to search external websites and retrieve papers and files for collation into an underlying document store. Developers using Alhazen must abide by data licensing requirements and third party websites terms and conditions. Users of this code should ensure that they do not infringe upon third party privacy or intellectual property rights through the use of this code.\n\nsource\n\nretrieve_pdf_from_doidotorg\n\n retrieve_pdf_from_doidotorg (doi, base_dir, headless=False)\n\n\nsource\n\n\nretrieve_full_text_links_from_biorxiv\n\n retrieve_full_text_links_from_biorxiv (doi, base_dir)\n\n\nsource\n\n\nexecute_search_on_biorxiv\n\n execute_search_on_biorxiv (search_term)\n\n\nsource\n\n\nextract_reconstructed_nxml\n\n extract_reconstructed_nxml (html)\n\n\nsource\n\n\nclean_and_convert_tags\n\n clean_and_convert_tags (soup, tag)\n\n\nsource\n\n\nget_html_from_pmc_doi\n\n get_html_from_pmc_doi (doi, base_file_path)\n\nGiven a DOI, navigate to the PMC HTML page and reconstruct NXML from that",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "Web Robots"
    ]
  },
  {
    "objectID": "utils/html_text_extractor.html",
    "href": "utils/html_text_extractor.html",
    "title": "HTML Text Extractor Utility",
    "section": "",
    "text": "source\n\nTrafilaturaSectionLoader\n\n TrafilaturaSectionLoader (file_path:str)\n\nLoad HTML files and parse them with trafilatura into sections.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "HTML Text Extractor Utility"
    ]
  },
  {
    "objectID": "agents/index.html",
    "href": "agents/index.html",
    "title": "Agents",
    "section": "",
    "text": "Alhazen is an agent based system. We here experiment with different types of agent to perform different classes of task.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nBasic Alhazen Agent / Chatbot.\n\n\nBuilding a simple application to allow the user to chat to Alhazen.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Agents"
    ]
  },
  {
    "objectID": "sqla_fixes.html",
    "href": "sqla_fixes.html",
    "title": "Hack to fix errors in the SQL Alchemy python library",
    "section": "",
    "text": "Edits to SQL Alchemy Source Code\n\n\nEdits to Raw SQL Code"
  },
  {
    "objectID": "tools/protocol_extraction_tool.html",
    "href": "tools/protocol_extraction_tool.html",
    "title": "Protocol Workflow Extraction Tool",
    "section": "",
    "text": "source\n\nBaseProtocolExtractionTool\n\n BaseProtocolExtractionTool\n                             (db:alhazen.utils.ceifns_db.Ceifns_Literature\n                             Db, llm:Optional[langchain_core.language_mode\n                             ls.chat_models.BaseChatModel]=None, slm:Optio\n                             nal[langchain_core.language_models.chat_model\n                             s.BaseChatModel]=None,\n                             name:str='protocol_workflow_extraction',\n                             description:str='Runs a specified protocol\n                             extraction pipeline over a research paper\n                             that has been loaded in the local literature\n                             database.', args_schema:Optional[Type[pydanti\n                             c.v1.main.BaseModel]]=None,\n                             return_direct:bool=True, verbose:bool=False, \n                             callbacks:Union[List[langchain_core.callbacks\n                             .base.BaseCallbackHandler],langchain_core.cal\n                             lbacks.base.BaseCallbackManager,NoneType]=Non\n                             e, callback_manager:Optional[langchain_core.c\n                             allbacks.base.BaseCallbackManager]=None,\n                             tags:Optional[List[str]]=None,\n                             metadata:Optional[Dict[str,Any]]=None, handle\n                             _tool_error:Union[bool,str,Callable[[langchai\n                             n_core.tools.ToolException],str],NoneType]=Fa\n                             lse, handle_validation_error:Union[bool,str,C\n                             allable[[pydantic.v1.error_wrappers.Validatio\n                             nError],str],NoneType]=False)\n\nRuns a specified protocol extraction pipeline over a research paper that has been loaded in the local literature database.\n\nsource\n\n\nProtocolExtractionToolSchema\n\n ProtocolExtractionToolSchema (paper_id:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nProcotolDiagramExtractionTool\n\n ProcotolDiagramExtractionTool\n                                (db:alhazen.utils.ceifns_db.Ceifns_Literat\n                                ureDb, llm:Optional[langchain_core.languag\n                                e_models.chat_models.BaseChatModel]=None, \n                                slm:Optional[langchain_core.language_model\n                                s.chat_models.BaseChatModel]=None,\n                                name:str='protocol_diagram_extraction',\n                                description:str='Extracts a mermaid\n                                diagram of the protocol from a paper.', ar\n                                gs_schema:Optional[Type[pydantic.v1.main.B\n                                aseModel]]=None, return_direct:bool=True,\n                                verbose:bool=False, callbacks:Union[List[l\n                                angchain_core.callbacks.base.BaseCallbackH\n                                andler],langchain_core.callbacks.base.Base\n                                CallbackManager,NoneType]=None, callback_m\n                                anager:Optional[langchain_core.callbacks.b\n                                ase.BaseCallbackManager]=None,\n                                tags:Optional[List[str]]=None,\n                                metadata:Optional[Dict[str,Any]]=None, han\n                                dle_tool_error:Union[bool,str,Callable[[la\n                                ngchain_core.tools.ToolException],str],Non\n                                eType]=False, handle_validation_error:Unio\n                                n[bool,str,Callable[[pydantic.v1.error_wra\n                                ppers.ValidationError],str],NoneType]=Fals\n                                e)\n\nExtracts a mermaid diagram of the protocol from a paper.\n\nsource\n\n\nProcotolEntitiesExtractionTool\n\n ProcotolEntitiesExtractionTool\n                                 (db:alhazen.utils.ceifns_db.Ceifns_Litera\n                                 tureDb, llm:Optional[langchain_core.langu\n                                 age_models.chat_models.BaseChatModel]=Non\n                                 e, slm:Optional[langchain_core.language_m\n                                 odels.chat_models.BaseChatModel]=None,\n                                 name:str='protocol_entities_extraction',\n                                 description:str='Extracts all entities\n                                 used in a protocol.', args_schema:Optiona\n                                 l[Type[pydantic.v1.main.BaseModel]]=None,\n                                 return_direct:bool=True,\n                                 verbose:bool=False, callbacks:Union[List[\n                                 langchain_core.callbacks.base.BaseCallbac\n                                 kHandler],langchain_core.callbacks.base.B\n                                 aseCallbackManager,NoneType]=None, callba\n                                 ck_manager:Optional[langchain_core.callba\n                                 cks.base.BaseCallbackManager]=None,\n                                 tags:Optional[List[str]]=None,\n                                 metadata:Optional[Dict[str,Any]]=None, ha\n                                 ndle_tool_error:Union[bool,str,Callable[[\n                                 langchain_core.tools.ToolException],str],\n                                 NoneType]=False, handle_validation_error:\n                                 Union[bool,str,Callable[[pydantic.v1.erro\n                                 r_wrappers.ValidationError],str],NoneType\n                                 ]=False)\n\nExtracts all entities used in a protocol.\n\nsource\n\n\nProcotolProcessesExtractionTool\n\n ProcotolProcessesExtractionTool\n                                  (db:alhazen.utils.ceifns_db.Ceifns_Liter\n                                  atureDb, llm:Optional[langchain_core.lan\n                                  guage_models.chat_models.BaseChatModel]=\n                                  None, slm:Optional[langchain_core.langua\n                                  ge_models.chat_models.BaseChatModel]=Non\n                                  e, name:str='protocol_processes_extracti\n                                  on', description:str='Extracts all\n                                  processes used in a protocol.', args_sch\n                                  ema:Optional[Type[pydantic.v1.main.BaseM\n                                  odel]]=None, return_direct:bool=True,\n                                  verbose:bool=False, callbacks:Union[List\n                                  [langchain_core.callbacks.base.BaseCallb\n                                  ackHandler],langchain_core.callbacks.bas\n                                  e.BaseCallbackManager,NoneType]=None, ca\n                                  llback_manager:Optional[langchain_core.c\n                                  allbacks.base.BaseCallbackManager]=None,\n                                  tags:Optional[List[str]]=None,\n                                  metadata:Optional[Dict[str,Any]]=None, h\n                                  andle_tool_error:Union[bool,str,Callable\n                                  [[langchain_core.tools.ToolException],st\n                                  r],NoneType]=False, handle_validation_er\n                                  ror:Union[bool,str,Callable[[pydantic.v1\n                                  .error_wrappers.ValidationError],str],No\n                                  neType]=False)\n\nExtracts all processes used in a protocol.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Protocol Workflow Extraction Tool"
    ]
  },
  {
    "objectID": "tools/basic_tools.html",
    "href": "tools/basic_tools.html",
    "title": "Basic Tools for Alhazen",
    "section": "",
    "text": "AddCollectionFromEPMCTool: Execute a query against the European PMC, creating a collection, downloading available full-text papers and saving expressions, items, and fragments to the database to denote that publication.\nDescribeCollectionCompositionTool: Describe the composition of a collection in the database.\nDeleteCollectionTool: Delete a collection from the database.\n\n\n\n\n\n**List__Paged_Expressions_CollectionCTool**: Delete a collection from the database.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Basic Tools for Alhazen"
    ]
  },
  {
    "objectID": "tools/basic_tools.html#available-tools",
    "href": "tools/basic_tools.html#available-tools",
    "title": "Basic Tools for Alhazen",
    "section": "",
    "text": "AddCollectionFromEPMCTool: Execute a query against the European PMC, creating a collection, downloading available full-text papers and saving expressions, items, and fragments to the database to denote that publication.\nDescribeCollectionCompositionTool: Describe the composition of a collection in the database.\nDeleteCollectionTool: Delete a collection from the database.\n\n\n\n\n\n**List__Paged_Expressions_CollectionCTool**: Delete a collection from the database.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Basic Tools for Alhazen"
    ]
  },
  {
    "objectID": "tools/basic_tools.html#tools-under-development",
    "href": "tools/basic_tools.html#tools-under-development",
    "title": "Basic Tools for Alhazen",
    "section": "Tools under development:",
    "text": "Tools under development:\n\nDevelop queries across external data sources\nExtract information from a collection using an LLM-based analysis to create Notes\nFilter a collection by an LLM-based analysis by tagging fragments with Notes\nReport on the state of the database in terms of numbers of collections, expressions, items, and fragments\nPrepare a report over a core research question by collecting a number of notes and synthesizing them into a report\n\n\nsource\n\nIntrospectionToolMixin\n\n IntrospectionToolMixin\n                         (agent:Optional[langchain.agents.agent.RunnableAg\n                         ent]=None)\n\nBase class for providing information about Alhazen’s capabilities.\n\nsource\n\n\nAlhazenToolMixin\n\n AlhazenToolMixin (db:alhazen.utils.ceifns_db.Ceifns_LiteratureDb, llm:Opt\n                   ional[langchain_core.language_models.chat_models.BaseCh\n                   atModel]=None, slm:Optional[langchain_core.language_mod\n                   els.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nIntrospectionTool\n\n IntrospectionTool (name:str='introspect', description:str='This tool\n                    permits Alhazen to answer questions how its agent\n                    would plan to answer specific questions.', args_schema\n                    :Type[__main__.IntrospectionToolSchema]=&lt;class\n                    '__main__.IntrospectionToolSchema'&gt;,\n                    return_direct:bool=False, verbose:bool=False, callback\n                    s:Union[List[langchain_core.callbacks.base.BaseCallbac\n                    kHandler],langchain_core.callbacks.base.BaseCallbackMa\n                    nager,NoneType]=None, callback_manager:Optional[langch\n                    ain_core.callbacks.base.BaseCallbackManager]=None,\n                    tags:Optional[List[str]]=None,\n                    metadata:Optional[Dict[str,Any]]=None, handle_tool_err\n                    or:Union[bool,str,Callable[[langchain_core.tools.ToolE\n                    xception],str],NoneType]=False, handle_validation_erro\n                    r:Union[bool,str,Callable[[pydantic.v1.error_wrappers.\n                    ValidationError],str],NoneType]=False, agent:Optional[\n                    langchain.agents.agent.RunnableAgent]=None)\n\nBase class for providing information about Alhazen’s capabilities.\n\nsource\n\n\nIntrospectionToolSchema\n\n IntrospectionToolSchema (input:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nAddCollectionFromEPMCTool\n\n AddCollectionFromEPMCTool (name:str='add_collection_from_epmc_query',\n                            description:str='This tool constructs a\n                            collection in the database from an external\n                            source based on a search query .', args_schema\n                            :Type[__main__.AddCollectionFromEPMCToolSchema\n                            ]=&lt;class\n                            '__main__.AddCollectionFromEPMCToolSchema'&gt;,\n                            return_direct:bool=True, verbose:bool=False, c\n                            allbacks:Union[List[langchain_core.callbacks.b\n                            ase.BaseCallbackHandler],langchain_core.callba\n                            cks.base.BaseCallbackManager,NoneType]=None, c\n                            allback_manager:Optional[langchain_core.callba\n                            cks.base.BaseCallbackManager]=None,\n                            tags:Optional[List[str]]=None,\n                            metadata:Optional[Dict[str,Any]]=None, handle_\n                            tool_error:Union[bool,str,Callable[[langchain_\n                            core.tools.ToolException],str],NoneType]=False\n                            , handle_validation_error:Union[bool,str,Calla\n                            ble[[pydantic.v1.error_wrappers.ValidationErro\n                            r],str],NoneType]=False, db:alhazen.utils.ceif\n                            ns_db.Ceifns_LiteratureDb, llm:Optional[langch\n                            ain_core.language_models.chat_models.BaseChatM\n                            odel]=None, slm:Optional[langchain_core.langua\n                            ge_models.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nAddCollectionFromEPMCToolSchema\n\n AddCollectionFromEPMCToolSchema (id:str, name:str, query:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nListCollectionsTool\n\n ListCollectionsTool (name:str='list_collections', description:str='This\n                      tool lists available collections in the database.', \n                      args_schema:Type[__main__.ListCollectionsToolSchema]\n                      =&lt;class '__main__.ListCollectionsToolSchema'&gt;,\n                      return_direct:bool=True, verbose:bool=False, callbac\n                      ks:Union[List[langchain_core.callbacks.base.BaseCall\n                      backHandler],langchain_core.callbacks.base.BaseCallb\n                      ackManager,NoneType]=None, callback_manager:Optional\n                      [langchain_core.callbacks.base.BaseCallbackManager]=\n                      None, tags:Optional[List[str]]=None,\n                      metadata:Optional[Dict[str,Any]]=None, handle_tool_e\n                      rror:Union[bool,str,Callable[[langchain_core.tools.T\n                      oolException],str],NoneType]=False, handle_validatio\n                      n_error:Union[bool,str,Callable[[pydantic.v1.error_w\n                      rappers.ValidationError],str],NoneType]=False,\n                      db:alhazen.utils.ceifns_db.Ceifns_LiteratureDb, llm:\n                      Optional[langchain_core.language_models.chat_models.\n                      BaseChatModel]=None, slm:Optional[langchain_core.lan\n                      guage_models.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nListCollectionsToolSchema\n\n ListCollectionsToolSchema (name_filter:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nAddCollectionFromOpenAlexTool\n\n AddCollectionFromOpenAlexTool\n                                (name:str='add_collection_from_openalex_do\n                                is', description:str='This tool constructs\n                                a collection in the database from an\n                                external source based on a list of dois.',\n                                args_schema:Type[__main__.AddCollectionFro\n                                mOpenAlexToolSchema]=&lt;class '__main__.AddC\n                                ollectionFromOpenAlexToolSchema'&gt;,\n                                return_direct:bool=True,\n                                verbose:bool=False, callbacks:Union[List[l\n                                angchain_core.callbacks.base.BaseCallbackH\n                                andler],langchain_core.callbacks.base.Base\n                                CallbackManager,NoneType]=None, callback_m\n                                anager:Optional[langchain_core.callbacks.b\n                                ase.BaseCallbackManager]=None,\n                                tags:Optional[List[str]]=None,\n                                metadata:Optional[Dict[str,Any]]=None, han\n                                dle_tool_error:Union[bool,str,Callable[[la\n                                ngchain_core.tools.ToolException],str],Non\n                                eType]=False, handle_validation_error:Unio\n                                n[bool,str,Callable[[pydantic.v1.error_wra\n                                ppers.ValidationError],str],NoneType]=Fals\n                                e, db:alhazen.utils.ceifns_db.Ceifns_Liter\n                                atureDb, llm:Optional[langchain_core.langu\n                                age_models.chat_models.BaseChatModel]=None\n                                , slm:Optional[langchain_core.language_mod\n                                els.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nAddCollectionFromOpenAlexToolSchema\n\n AddCollectionFromOpenAlexToolSchema (id:str, name:str, dois:List[str])\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nAddAuthorsToCollectionTool\n\n AddAuthorsToCollectionTool (name:str='add_authors_to_collection_query',\n                             description:str='This tool adds authors to a\n                             collection.', args_schema:Type[__main__.AddAu\n                             thorsToCollectionToolSchema]=&lt;class\n                             '__main__.AddAuthorsToCollectionToolSchema'&gt;,\n                             return_direct:bool=True, verbose:bool=False, \n                             callbacks:Union[List[langchain_core.callbacks\n                             .base.BaseCallbackHandler],langchain_core.cal\n                             lbacks.base.BaseCallbackManager,NoneType]=Non\n                             e, callback_manager:Optional[langchain_core.c\n                             allbacks.base.BaseCallbackManager]=None,\n                             tags:Optional[List[str]]=None,\n                             metadata:Optional[Dict[str,Any]]=None, handle\n                             _tool_error:Union[bool,str,Callable[[langchai\n                             n_core.tools.ToolException],str],NoneType]=Fa\n                             lse, handle_validation_error:Union[bool,str,C\n                             allable[[pydantic.v1.error_wrappers.Validatio\n                             nError],str],NoneType]=False, db:alhazen.util\n                             s.ceifns_db.Ceifns_LiteratureDb, llm:Optional\n                             [langchain_core.language_models.chat_models.B\n                             aseChatModel]=None, slm:Optional[langchain_co\n                             re.language_models.chat_models.BaseChatModel]\n                             =None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nAddAuthorsToCollectionToolSchema\n\n AddAuthorsToCollectionToolSchema (id:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nCheckExpressionTool\n\n CheckExpressionTool (name:str='describe_expression_in_local_database',\n                      description:str='This tool searches for a paper in\n                      the database and reports if we have full text\n                      version of it.', args_schema:Type[__main__.CheckExpr\n                      essionToolSchema]=&lt;class\n                      '__main__.CheckExpressionToolSchema'&gt;,\n                      return_direct:bool=True, verbose:bool=False, callbac\n                      ks:Union[List[langchain_core.callbacks.base.BaseCall\n                      backHandler],langchain_core.callbacks.base.BaseCallb\n                      ackManager,NoneType]=None, callback_manager:Optional\n                      [langchain_core.callbacks.base.BaseCallbackManager]=\n                      None, tags:Optional[List[str]]=None,\n                      metadata:Optional[Dict[str,Any]]=None, handle_tool_e\n                      rror:Union[bool,str,Callable[[langchain_core.tools.T\n                      oolException],str],NoneType]=False, handle_validatio\n                      n_error:Union[bool,str,Callable[[pydantic.v1.error_w\n                      rappers.ValidationError],str],NoneType]=False,\n                      db:alhazen.utils.ceifns_db.Ceifns_LiteratureDb, llm:\n                      Optional[langchain_core.language_models.chat_models.\n                      BaseChatModel]=None, slm:Optional[langchain_core.lan\n                      guage_models.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nCheckExpressionToolSchema\n\n CheckExpressionToolSchema (query:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nDescribeCollectionCompositionTool\n\n DescribeCollectionCompositionTool\n                                    (name:str='describe_collection_in_loca\n                                    l_database', description:str='This\n                                    tool describes the contents of a\n                                    Collection, in terms of counts of\n                                    papers', args_schema:Type[__main__.Des\n                                    cribeCollectionCompositionToolSchema]=\n                                    &lt;class '__main__.DescribeCollectionCom\n                                    positionToolSchema'&gt;,\n                                    return_direct:bool=True,\n                                    verbose:bool=False, callbacks:Union[Li\n                                    st[langchain_core.callbacks.base.BaseC\n                                    allbackHandler],langchain_core.callbac\n                                    ks.base.BaseCallbackManager,NoneType]=\n                                    None, callback_manager:Optional[langch\n                                    ain_core.callbacks.base.BaseCallbackMa\n                                    nager]=None,\n                                    tags:Optional[List[str]]=None,\n                                    metadata:Optional[Dict[str,Any]]=None,\n                                    handle_tool_error:Union[bool,str,Calla\n                                    ble[[langchain_core.tools.ToolExceptio\n                                    n],str],NoneType]=False, handle_valida\n                                    tion_error:Union[bool,str,Callable[[py\n                                    dantic.v1.error_wrappers.ValidationErr\n                                    or],str],NoneType]=False, db:alhazen.u\n                                    tils.ceifns_db.Ceifns_LiteratureDb, ll\n                                    m:Optional[langchain_core.language_mod\n                                    els.chat_models.BaseChatModel]=None, s\n                                    lm:Optional[langchain_core.language_mo\n                                    dels.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nDescribeCollectionCompositionToolSchema\n\n DescribeCollectionCompositionToolSchema (id:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nDeleteCollectionTool\n\n DeleteCollectionTool (name:str='delete_collection', description:str='This\n                       deletes a collection from the database based on an\n                       collection_id value.', args_schema:Type[__main__.De\n                       leteCollectionToolSchema]=&lt;class\n                       '__main__.DeleteCollectionToolSchema'&gt;,\n                       return_direct:bool=True, verbose:bool=False, callba\n                       cks:Union[List[langchain_core.callbacks.base.BaseCa\n                       llbackHandler],langchain_core.callbacks.base.BaseCa\n                       llbackManager,NoneType]=None, callback_manager:Opti\n                       onal[langchain_core.callbacks.base.BaseCallbackMana\n                       ger]=None, tags:Optional[List[str]]=None,\n                       metadata:Optional[Dict[str,Any]]=None, handle_tool_\n                       error:Union[bool,str,Callable[[langchain_core.tools\n                       .ToolException],str],NoneType]=False, handle_valida\n                       tion_error:Union[bool,str,Callable[[pydantic.v1.err\n                       or_wrappers.ValidationError],str],NoneType]=False,\n                       db:alhazen.utils.ceifns_db.Ceifns_LiteratureDb, llm\n                       :Optional[langchain_core.language_models.chat_model\n                       s.BaseChatModel]=None, slm:Optional[langchain_core.\n                       language_models.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nDeleteCollectionToolSchema\n\n DeleteCollectionToolSchema (collection_id:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nRetrieveFullTextToolForACollection\n\n RetrieveFullTextToolForACollection\n                                     (name:str='retrieve_full_text_for_pap\n                                     ers_in_collection',\n                                     description:str='This retrieves all\n                                     full text papers in a given\n                                     collection.', args_schema:Type[__main\n                                     __.RetrieveFullTextToolForACollection\n                                     Schema]=&lt;class '__main__.RetrieveFull\n                                     TextToolForACollectionSchema'&gt;,\n                                     return_direct:bool=True,\n                                     verbose:bool=False, callbacks:Union[L\n                                     ist[langchain_core.callbacks.base.Bas\n                                     eCallbackHandler],langchain_core.call\n                                     backs.base.BaseCallbackManager,NoneTy\n                                     pe]=None, callback_manager:Optional[l\n                                     angchain_core.callbacks.base.BaseCall\n                                     backManager]=None,\n                                     tags:Optional[List[str]]=None, metada\n                                     ta:Optional[Dict[str,Any]]=None, hand\n                                     le_tool_error:Union[bool,str,Callable\n                                     [[langchain_core.tools.ToolException]\n                                     ,str],NoneType]=False, handle_validat\n                                     ion_error:Union[bool,str,Callable[[py\n                                     dantic.v1.error_wrappers.ValidationEr\n                                     ror],str],NoneType]=False, db:alhazen\n                                     .utils.ceifns_db.Ceifns_LiteratureDb,\n                                     llm:Optional[langchain_core.language_\n                                     models.chat_models.BaseChatModel]=Non\n                                     e, slm:Optional[langchain_core.langua\n                                     ge_models.chat_models.BaseChatModel]=\n                                     None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nRetrieveFullTextToolForACollectionSchema\n\n RetrieveFullTextToolForACollectionSchema (collection_id:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nRetrieveFullTextTool\n\n RetrieveFullTextTool (name:str='retrieve_full_text_for_paper_id',\n                       description:str='This retrieves a full text paper\n                       based on its doi, copies it to local disk, and adds\n                       it the database.', args_schema:Type[__main__.Retrie\n                       veFullTextToolSchema]=&lt;class\n                       '__main__.RetrieveFullTextToolSchema'&gt;,\n                       return_direct:bool=True, verbose:bool=False, callba\n                       cks:Union[List[langchain_core.callbacks.base.BaseCa\n                       llbackHandler],langchain_core.callbacks.base.BaseCa\n                       llbackManager,NoneType]=None, callback_manager:Opti\n                       onal[langchain_core.callbacks.base.BaseCallbackMana\n                       ger]=None, tags:Optional[List[str]]=None,\n                       metadata:Optional[Dict[str,Any]]=None, handle_tool_\n                       error:Union[bool,str,Callable[[langchain_core.tools\n                       .ToolException],str],NoneType]=False, handle_valida\n                       tion_error:Union[bool,str,Callable[[pydantic.v1.err\n                       or_wrappers.ValidationError],str],NoneType]=False,\n                       db:alhazen.utils.ceifns_db.Ceifns_LiteratureDb, llm\n                       :Optional[langchain_core.language_models.chat_model\n                       s.BaseChatModel]=None, slm:Optional[langchain_core.\n                       language_models.chat_models.BaseChatModel]=None)\n\nBase tool for interacting with an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\nsource\n\n\nRetrieveFullTextToolSchema\n\n RetrieveFullTextToolSchema (paper_id:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Basic Tools for Alhazen"
    ]
  },
  {
    "objectID": "tools/index.html",
    "href": "tools/index.html",
    "title": "Tools",
    "section": "",
    "text": "Alhazen has a library of tools, programs with simple call / return signatures that agents call to solve problems.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAlhazen Toolkit\n\n\nA set of Langchain tools that populate and query a CEIFNS database by (1) Build collections of expressions; (2) locate and load items that represent expressions; (3) segregate the parts of items as ‘fragments’; (4) analyze the fragments to generate notes that can then be summarized to provide summaries.\n\n\n\n\nBasic Tools for Alhazen\n\n\nSimple tools to demonstrate utility and ‘agentic’ functionality.\n\n\n\n\nDocument Classifier Tool\n\n\nLangchain tools that execute zero-shot classification tasks over a local database of title/abstracts from papers previously imported into our database.\n\n\n\n\nDocument Classifier Tool\n\n\nLangchain tools that execute zero-shot classification tasks over a local database of title/abstracts from papers previously imported into our database.\n\n\n\n\nDocument Classifier Tool\n\n\nLangchain tools that execute zero-shot classification tasks over a local database of title/abstracts from papers previously imported into our database.\n\n\n\n\nMethods Metadata Extraction Tool\n\n\nLangchain tools that execute zero-shot extraction over a local database of full text papers previously imported into our database.\n\n\n\n\nPaper QA Emulation Tool\n\n\nWe here emulate the workflow of the PaperQA system by Andrew White (https://thewhitelab.org/). This is a Retrieval Augmented Generation (RAG) application using a Map-Reduce model where we query our embedded index for based on a question, we then write summaries for each returned document based on relevance to the underlying question. Finally, we synthesize each summary into an essay presented as an answer to the original question.\n\n\n\n\nProtocol Workflow Extraction Tool\n\n\nLangchain tools that execute zero-shot extraction over a local database of full text papers previously imported into our database.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools"
    ]
  },
  {
    "objectID": "tools/toolkit.html",
    "href": "tools/toolkit.html",
    "title": "Alhazen Toolkit",
    "section": "",
    "text": "Base Toolkit for the\n\n\nPrimary Agent Toolkit\n\nsource\n\nAlhazenToolkit\n\n AlhazenToolkit (db:alhazen.utils.ceifns_db.Ceifns_LiteratureDb,\n                 llm:langchain_core.language_models.chat_models.BaseChatMo\n                 del, agent:Optional[langchain.agents.agent.RunnableAgent]\n                 =None)\n\nToolkit for building and querying an Alhazen CEIFNS (pron. ‘SAI-FiNS’) database (CEIFNS = Collection-Expression-Item-Fragment-Note-Summary).\n\n\n\nMetadata Extraction Toolkit for Experimentation\n\nsource\n\nMetadataExtractionToolkit\n\n MetadataExtractionToolkit\n                            (db:alhazen.utils.ceifns_db.Ceifns_LiteratureD\n                            b, llm:langchain_core.language_models.chat_mod\n                            els.BaseChatModel, agent:Optional[langchain.ag\n                            ents.agent.RunnableAgent]=None)\n\nToolkit for running and testing Alhazen Metadata Extraction Tools",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Alhazen Toolkit"
    ]
  },
  {
    "objectID": "tools/paperqa_emulation_tool.html",
    "href": "tools/paperqa_emulation_tool.html",
    "title": "Paper QA Emulation Tool",
    "section": "",
    "text": "source\n\nPaperQAEmulationTool\n\n PaperQAEmulationTool (db:alhazen.utils.ceifns_db.Ceifns_LiteratureDb, llm\n                       :Optional[langchain_core.language_models.chat_model\n                       s.BaseChatModel]=None, slm:Optional[langchain_core.\n                       language_models.chat_models.BaseChatModel]=None,\n                       name:str='simple_qa_over_papers',\n                       description:str='Runs a Map-Reduce model where we\n                       write a short essay to answer a scientific question\n                       based on a set of supporting documents.', args_sche\n                       ma:Optional[Type[pydantic.v1.main.BaseModel]]=None,\n                       return_direct:bool=True, verbose:bool=False, callba\n                       cks:Union[List[langchain_core.callbacks.base.BaseCa\n                       llbackHandler],langchain_core.callbacks.base.BaseCa\n                       llbackManager,NoneType]=None, callback_manager:Opti\n                       onal[langchain_core.callbacks.base.BaseCallbackMana\n                       ger]=None, tags:Optional[List[str]]=None,\n                       metadata:Optional[Dict[str,Any]]=None, handle_tool_\n                       error:Union[bool,str,Callable[[langchain_core.tools\n                       .ToolException],str],NoneType]=False, handle_valida\n                       tion_error:Union[bool,str,Callable[[pydantic.v1.err\n                       or_wrappers.ValidationError],str],NoneType]=False)\n\nWrite a short essay to answer a scientific question based documents from a preset collection.\n\nsource\n\n\nPaperQAEmulationToolSchema\n\n PaperQAEmulationToolSchema (question:str,\n                             n_sample_size:Optional[int]=None,\n                             n_summary_size:Optional[int]=None,\n                             collection_id:Optional[int]=None)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Paper QA Emulation Tool"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "source\n\nPromptTemplateRegistry\n\n PromptTemplateRegistry ()\n\nA class that stores and tracks PromptTemplates for use within a given function.\n\nsource\n\n\nPromptTemplateSpec\n\n PromptTemplateSpec (name:str, description:str, system:str,\n                     instruction:str, input_variables:List[str]=&lt;factory&gt;,\n                     output_variables:List[str]=&lt;factory&gt;)\n\nA class that provides structure for task instructions (to be converted to LangChain PromptTemplates).\n\nsource\n\n\nload_alhazen_tool_environment\n\n load_alhazen_tool_environment ()\n\nSet broad variables for Alhazen. Currently only set default local file path.\n\nsource\n\n\nlookup_embeddings\n\n lookup_embeddings ()\n\nUtility function to provide access to all available embedding models.\n\nsource\n\n\nlookup_chat_models\n\n lookup_chat_models ()\n\nUtility function to provide access to all available chat models.\n\nsource\n\n\nsuppress_stdout_stderr\n\n suppress_stdout_stderr ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nOllamaRunner\n\n OllamaRunner (model)\n\nClass to run Ollama in a subprocess and to\nrun LLMs or chains locally with a timeout to prevent long-running processes from hanging the server."
  },
  {
    "objectID": "tools/tiab_mapping_tool.html",
    "href": "tools/tiab_mapping_tool.html",
    "title": "Document Classifier Tool",
    "section": "",
    "text": "source\n\nBaseTitleAbstractMappingTool\n\n BaseTitleAbstractMappingTool\n                               (db:alhazen.utils.ceifns_db.Ceifns_Literatu\n                               reDb, llm:Optional[langchain_core.language_\n                               models.chat_models.BaseChatModel]=None, slm\n                               :Optional[langchain_core.language_models.ch\n                               at_models.BaseChatModel]=None,\n                               name:str='tiab_mapping',\n                               description:str='Runs a specified document\n                               mapping pipeline over papers in a\n                               collection.', args_schema:Optional[Type[pyd\n                               antic.v1.main.BaseModel]]=None,\n                               return_direct:bool=True,\n                               verbose:bool=False, callbacks:Union[List[la\n                               ngchain_core.callbacks.base.BaseCallbackHan\n                               dler],langchain_core.callbacks.base.BaseCal\n                               lbackManager,NoneType]=None, callback_manag\n                               er:Optional[langchain_core.callbacks.base.B\n                               aseCallbackManager]=None,\n                               tags:Optional[List[str]]=None,\n                               metadata:Optional[Dict[str,Any]]=None, hand\n                               le_tool_error:Union[bool,str,Callable[[lang\n                               chain_core.tools.ToolException],str],NoneTy\n                               pe]=False, handle_validation_error:Union[bo\n                               ol,str,Callable[[pydantic.v1.error_wrappers\n                               .ValidationError],str],NoneType]=False)\n\nRuns a specified document mapping pipeline over papers in a collection.\n\nsource\n\n\nTitleAbstractMappingToolSchema\n\n TitleAbstractMappingToolSchema (collection_id:str,\n                                 repeat_run:Optional[bool]=None,\n                                 run_label:Optional[str]=None)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nTitleAbstractDiscourseMappingTool\n\n TitleAbstractDiscourseMappingTool\n                                    (db:alhazen.utils.ceifns_db.Ceifns_Lit\n                                    eratureDb, llm:Optional[langchain_core\n                                    .language_models.chat_models.BaseChatM\n                                    odel]=None, slm:Optional[langchain_cor\n                                    e.language_models.chat_models.BaseChat\n                                    Model]=None, name:str='tiab_one_doc_cl\n                                    assification', description:str='Runs\n                                    through the text of each title +\n                                    abstract and split them based on\n                                    discourse.', args_schema:Optional[Type\n                                    [pydantic.v1.main.BaseModel]]=None,\n                                    return_direct:bool=True,\n                                    verbose:bool=False, callbacks:Union[Li\n                                    st[langchain_core.callbacks.base.BaseC\n                                    allbackHandler],langchain_core.callbac\n                                    ks.base.BaseCallbackManager,NoneType]=\n                                    None, callback_manager:Optional[langch\n                                    ain_core.callbacks.base.BaseCallbackMa\n                                    nager]=None,\n                                    tags:Optional[List[str]]=None,\n                                    metadata:Optional[Dict[str,Any]]=None,\n                                    handle_tool_error:Union[bool,str,Calla\n                                    ble[[langchain_core.tools.ToolExceptio\n                                    n],str],NoneType]=False, handle_valida\n                                    tion_error:Union[bool,str,Callable[[py\n                                    dantic.v1.error_wrappers.ValidationErr\n                                    or],str],NoneType]=False)\n\nRuns through the text of each title + abstract and split them based on discourse.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Document Classifier Tool"
    ]
  },
  {
    "objectID": "tools/tiab_clasifier_tool.html",
    "href": "tools/tiab_clasifier_tool.html",
    "title": "Document Classifier Tool",
    "section": "",
    "text": "source\n\nBaseTitleAbstractClassifierTool\n\n BaseTitleAbstractClassifierTool\n                                  (db:alhazen.utils.ceifns_db.Ceifns_Liter\n                                  atureDb, llm:Optional[langchain_core.lan\n                                  guage_models.chat_models.BaseChatModel]=\n                                  None, slm:Optional[langchain_core.langua\n                                  ge_models.chat_models.BaseChatModel]=Non\n                                  e, name:str='tiab_classification',\n                                  description:str='Runs a specified\n                                  document classification pipeline over\n                                  papers in a collection.', args_schema:Op\n                                  tional[Type[pydantic.v1.main.BaseModel]]\n                                  =None, return_direct:bool=True,\n                                  verbose:bool=False, callbacks:Union[List\n                                  [langchain_core.callbacks.base.BaseCallb\n                                  ackHandler],langchain_core.callbacks.bas\n                                  e.BaseCallbackManager,NoneType]=None, ca\n                                  llback_manager:Optional[langchain_core.c\n                                  allbacks.base.BaseCallbackManager]=None,\n                                  tags:Optional[List[str]]=None,\n                                  metadata:Optional[Dict[str,Any]]=None, h\n                                  andle_tool_error:Union[bool,str,Callable\n                                  [[langchain_core.tools.ToolException],st\n                                  r],NoneType]=False, handle_validation_er\n                                  ror:Union[bool,str,Callable[[pydantic.v1\n                                  .error_wrappers.ValidationError],str],No\n                                  neType]=False, prompt_name:str='binary\n                                  methods paper', examples:dict={})\n\nRuns a specified document classification pipeline over papers in a collection.\n\nsource\n\n\nTitleAbstractClassifierToolSchema\n\n TitleAbstractClassifierToolSchema (collection_id:str,\n                                    classification_type:str,\n                                    repeat_run:Optional[bool]=None)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nTitleAbstractClassifier_OneDocAtATime_Tool\n\n TitleAbstractClassifier_OneDocAtATime_Tool\n                                             (db:alhazen.utils.ceifns_db.C\n                                             eifns_LiteratureDb, llm:Optio\n                                             nal[langchain_core.language_m\n                                             odels.chat_models.BaseChatMod\n                                             el]=None, slm:Optional[langch\n                                             ain_core.language_models.chat\n                                             _models.BaseChatModel]=None, \n                                             name:str='tiab_one_doc_classi\n                                             fication',\n                                             description:str='Runs a\n                                             specified document\n                                             classification pipeline over\n                                             papers in a collection by\n                                             running a simple classifier\n                                             over the text of each title +\n                                             abstract.', args_schema:Optio\n                                             nal[Type[pydantic.v1.main.Bas\n                                             eModel]]=None,\n                                             return_direct:bool=True,\n                                             verbose:bool=False, callbacks\n                                             :Union[List[langchain_core.ca\n                                             llbacks.base.BaseCallbackHand\n                                             ler],langchain_core.callbacks\n                                             .base.BaseCallbackManager,Non\n                                             eType]=None, callback_manager\n                                             :Optional[langchain_core.call\n                                             backs.base.BaseCallbackManage\n                                             r]=None, tags:Optional[List[s\n                                             tr]]=None, metadata:Optional[\n                                             Dict[str,Any]]=None, handle_t\n                                             ool_error:Union[bool,str,Call\n                                             able[[langchain_core.tools.To\n                                             olException],str],NoneType]=F\n                                             alse, handle_validation_error\n                                             :Union[bool,str,Callable[[pyd\n                                             antic.v1.error_wrappers.Valid\n                                             ationError],str],NoneType]=Fa\n                                             lse, prompt_name:str='binary\n                                             methods paper',\n                                             examples:dict={})\n\nRuns a specified document classification pipeline over papers in a collection.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Document Classifier Tool"
    ]
  },
  {
    "objectID": "tools/metadata_extraction_tool.html",
    "href": "tools/metadata_extraction_tool.html",
    "title": "Methods Metadata Extraction Tool",
    "section": "",
    "text": "source\n\nBaseMetadataExtractionTool\n\n BaseMetadataExtractionTool\n                             (db:alhazen.utils.ceifns_db.Ceifns_Literature\n                             Db, llm:Optional[langchain_core.language_mode\n                             ls.chat_models.BaseChatModel]=None, slm:Optio\n                             nal[langchain_core.language_models.chat_model\n                             s.BaseChatModel]=None,\n                             name:str='metadata_extraction',\n                             description:str='Runs a specified metadata\n                             extraction pipeline over a research paper\n                             that has been loaded in the local literature\n                             database.', args_schema:Optional[Type[pydanti\n                             c.v1.main.BaseModel]]=None,\n                             return_direct:bool=True, verbose:bool=False, \n                             callbacks:Union[List[langchain_core.callbacks\n                             .base.BaseCallbackHandler],langchain_core.cal\n                             lbacks.base.BaseCallbackManager,NoneType]=Non\n                             e, callback_manager:Optional[langchain_core.c\n                             allbacks.base.BaseCallbackManager]=None,\n                             tags:Optional[List[str]]=None,\n                             metadata:Optional[Dict[str,Any]]=None, handle\n                             _tool_error:Union[bool,str,Callable[[langchai\n                             n_core.tools.ToolException],str],NoneType]=Fa\n                             lse, handle_validation_error:Union[bool,str,C\n                             allable[[pydantic.v1.error_wrappers.Validatio\n                             nError],str],NoneType]=False,\n                             examples:dict={})\n\nRuns a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.\n\nsource\n\n\nMetadataExtractionToolSchema\n\n MetadataExtractionToolSchema (paper_id:str, extraction_type:str,\n                               run_label:Optional[str]=None)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nMetadataExtraction_EverythingEverywhere_Tool\n\n MetadataExtraction_EverythingEverywhere_Tool\n                                               (db:alhazen.utils.ceifns_db\n                                               .Ceifns_LiteratureDb, llm:O\n                                               ptional[langchain_core.lang\n                                               uage_models.chat_models.Bas\n                                               eChatModel]=None, slm:Optio\n                                               nal[langchain_core.language\n                                               _models.chat_models.BaseCha\n                                               tModel]=None, name:str='met\n                                               adata_extraction',\n                                               description:str='Runs a\n                                               specified metadata\n                                               extraction pipeline over a\n                                               research paper that has\n                                               been loaded in the local\n                                               literature database.', args\n                                               _schema:Optional[Type[pydan\n                                               tic.v1.main.BaseModel]]=Non\n                                               e, return_direct:bool=True,\n                                               verbose:bool=False, callbac\n                                               ks:Union[List[langchain_cor\n                                               e.callbacks.base.BaseCallba\n                                               ckHandler],langchain_core.c\n                                               allbacks.base.BaseCallbackM\n                                               anager,NoneType]=None, call\n                                               back_manager:Optional[langc\n                                               hain_core.callbacks.base.Ba\n                                               seCallbackManager]=None, ta\n                                               gs:Optional[List[str]]=None\n                                               , metadata:Optional[Dict[st\n                                               r,Any]]=None, handle_tool_e\n                                               rror:Union[bool,str,Callabl\n                                               e[[langchain_core.tools.Too\n                                               lException],str],NoneType]=\n                                               False, handle_validation_er\n                                               ror:Union[bool,str,Callable\n                                               [[pydantic.v1.error_wrapper\n                                               s.ValidationError],str],Non\n                                               eType]=False,\n                                               examples:dict={})\n\nRuns a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.\n\nsource\n\n\nMetadataExtraction_MethodsSectionOnly_Tool\n\n MetadataExtraction_MethodsSectionOnly_Tool\n                                             (db:alhazen.utils.ceifns_db.C\n                                             eifns_LiteratureDb, llm:Optio\n                                             nal[langchain_core.language_m\n                                             odels.chat_models.BaseChatMod\n                                             el]=None, slm:Optional[langch\n                                             ain_core.language_models.chat\n                                             _models.BaseChatModel]=None, \n                                             name:str='metadata_extraction\n                                             ', description:str='Runs a\n                                             specified metadata extraction\n                                             pipeline over a research\n                                             paper that has been loaded in\n                                             the local literature\n                                             database.', args_schema:Optio\n                                             nal[Type[pydantic.v1.main.Bas\n                                             eModel]]=None,\n                                             return_direct:bool=True,\n                                             verbose:bool=False, callbacks\n                                             :Union[List[langchain_core.ca\n                                             llbacks.base.BaseCallbackHand\n                                             ler],langchain_core.callbacks\n                                             .base.BaseCallbackManager,Non\n                                             eType]=None, callback_manager\n                                             :Optional[langchain_core.call\n                                             backs.base.BaseCallbackManage\n                                             r]=None, tags:Optional[List[s\n                                             tr]]=None, metadata:Optional[\n                                             Dict[str,Any]]=None, handle_t\n                                             ool_error:Union[bool,str,Call\n                                             able[[langchain_core.tools.To\n                                             olException],str],NoneType]=F\n                                             alse, handle_validation_error\n                                             :Union[bool,str,Callable[[pyd\n                                             antic.v1.error_wrappers.Valid\n                                             ationError],str],NoneType]=Fa\n                                             lse, examples:dict={})\n\nRuns a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.\n\nsource\n\n\nMetadataExtraction_RAGOnSections_Tool\n\n MetadataExtraction_RAGOnSections_Tool\n                                        (db:alhazen.utils.ceifns_db.Ceifns\n                                        _LiteratureDb, llm:Optional[langch\n                                        ain_core.language_models.chat_mode\n                                        ls.BaseChatModel]=None, slm:Option\n                                        al[langchain_core.language_models.\n                                        chat_models.BaseChatModel]=None,\n                                        name:str='metadata_extraction',\n                                        description:str='Runs a specified\n                                        metadata extraction pipeline over\n                                        a research paper that has been\n                                        loaded in the local literature\n                                        database.', args_schema:Optional[T\n                                        ype[pydantic.v1.main.BaseModel]]=N\n                                        one, return_direct:bool=True,\n                                        verbose:bool=False, callbacks:Unio\n                                        n[List[langchain_core.callbacks.ba\n                                        se.BaseCallbackHandler],langchain_\n                                        core.callbacks.base.BaseCallbackMa\n                                        nager,NoneType]=None, callback_man\n                                        ager:Optional[langchain_core.callb\n                                        acks.base.BaseCallbackManager]=Non\n                                        e, tags:Optional[List[str]]=None, \n                                        metadata:Optional[Dict[str,Any]]=N\n                                        one, handle_tool_error:Union[bool,\n                                        str,Callable[[langchain_core.tools\n                                        .ToolException],str],NoneType]=Fal\n                                        se, handle_validation_error:Union[\n                                        bool,str,Callable[[pydantic.v1.err\n                                        or_wrappers.ValidationError],str],\n                                        NoneType]=False, examples:dict={})\n\nRuns a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.\n\nsource\n\n\nSimpleExtractionWithRAGTool\n\n SimpleExtractionWithRAGTool\n                              (db:alhazen.utils.ceifns_db.Ceifns_Literatur\n                              eDb, llm:Optional[langchain_core.language_mo\n                              dels.chat_models.BaseChatModel]=None, slm:Op\n                              tional[langchain_core.language_models.chat_m\n                              odels.BaseChatModel]=None,\n                              name:str='simple_extraction',\n                              description:str='Performs simple information\n                              extraction from a specified research paper\n                              from the database.', args_schema:Optional[Ty\n                              pe[pydantic.v1.main.BaseModel]]=None,\n                              return_direct:bool=False,\n                              verbose:bool=False, callbacks:Union[List[lan\n                              gchain_core.callbacks.base.BaseCallbackHandl\n                              er],langchain_core.callbacks.base.BaseCallba\n                              ckManager,NoneType]=None, callback_manager:O\n                              ptional[langchain_core.callbacks.base.BaseCa\n                              llbackManager]=None,\n                              tags:Optional[List[str]]=None,\n                              metadata:Optional[Dict[str,Any]]=None, handl\n                              e_tool_error:Union[bool,str,Callable[[langch\n                              ain_core.tools.ToolException],str],NoneType]\n                              =False, handle_validation_error:Union[bool,s\n                              tr,Callable[[pydantic.v1.error_wrappers.Vali\n                              dationError],str],NoneType]=False)\n\nPerforms simple information extraction from a specified research paper from the database.\n\nsource\n\n\nSimpleExtractionWithRAGToolSchema\n\n SimpleExtractionWithRAGToolSchema (paper_id:str, variable_name:str,\n                                    question:str)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Methods Metadata Extraction Tool"
    ]
  },
  {
    "objectID": "tools/tiab_extraction_tool.html",
    "href": "tools/tiab_extraction_tool.html",
    "title": "Document Classifier Tool",
    "section": "",
    "text": "source\n\nBaseTitleAbstractExtractionTool\n\n BaseTitleAbstractExtractionTool\n                                  (db:alhazen.utils.ceifns_db.Ceifns_Liter\n                                  atureDb, llm:Optional[langchain_core.lan\n                                  guage_models.chat_models.BaseChatModel]=\n                                  None, slm:Optional[langchain_core.langua\n                                  ge_models.chat_models.BaseChatModel]=Non\n                                  e, name:str='tiab_classification',\n                                  description:str='Runs a specified\n                                  document classification pipeline over\n                                  papers in a collection.', args_schema:Op\n                                  tional[Type[pydantic.v1.main.BaseModel]]\n                                  =None, return_direct:bool=True,\n                                  verbose:bool=False, callbacks:Union[List\n                                  [langchain_core.callbacks.base.BaseCallb\n                                  ackHandler],langchain_core.callbacks.bas\n                                  e.BaseCallbackManager,NoneType]=None, ca\n                                  llback_manager:Optional[langchain_core.c\n                                  allbacks.base.BaseCallbackManager]=None,\n                                  tags:Optional[List[str]]=None,\n                                  metadata:Optional[Dict[str,Any]]=None, h\n                                  andle_tool_error:Union[bool,str,Callable\n                                  [[langchain_core.tools.ToolException],st\n                                  r],NoneType]=False, handle_validation_er\n                                  ror:Union[bool,str,Callable[[pydantic.v1\n                                  .error_wrappers.ValidationError],str],No\n                                  neType]=False, prompt_name:str='binary\n                                  methods paper', examples:dict={})\n\nRuns a specified document classification pipeline over papers in a collection.\n\nsource\n\n\nTitleAbstractExtractionToolSchema\n\n TitleAbstractExtractionToolSchema (collection_id:str, domain:str,\n                                    extraction_type:str,\n                                    repeat_run:Optional[bool]=None)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nTitleAbstractExtraction_OneDocAtATime_Tool\n\n TitleAbstractExtraction_OneDocAtATime_Tool\n                                             (db:alhazen.utils.ceifns_db.C\n                                             eifns_LiteratureDb, llm:Optio\n                                             nal[langchain_core.language_m\n                                             odels.chat_models.BaseChatMod\n                                             el]=None, slm:Optional[langch\n                                             ain_core.language_models.chat\n                                             _models.BaseChatModel]=None, \n                                             name:str='tiab_one_doc_classi\n                                             fication',\n                                             description:str='Runs a\n                                             specified document\n                                             classification pipeline over\n                                             papers in a collection by\n                                             running a simple classifier\n                                             over the text of each title +\n                                             abstract.', args_schema:Optio\n                                             nal[Type[pydantic.v1.main.Bas\n                                             eModel]]=None,\n                                             return_direct:bool=True,\n                                             verbose:bool=False, callbacks\n                                             :Union[List[langchain_core.ca\n                                             llbacks.base.BaseCallbackHand\n                                             ler],langchain_core.callbacks\n                                             .base.BaseCallbackManager,Non\n                                             eType]=None, callback_manager\n                                             :Optional[langchain_core.call\n                                             backs.base.BaseCallbackManage\n                                             r]=None, tags:Optional[List[s\n                                             tr]]=None, metadata:Optional[\n                                             Dict[str,Any]]=None, handle_t\n                                             ool_error:Union[bool,str,Call\n                                             able[[langchain_core.tools.To\n                                             olException],str],NoneType]=F\n                                             alse, handle_validation_error\n                                             :Union[bool,str,Callable[[pyd\n                                             antic.v1.error_wrappers.Valid\n                                             ationError],str],NoneType]=Fa\n                                             lse, prompt_name:str='binary\n                                             methods paper',\n                                             examples:dict={})\n\nRuns a specified document classification pipeline over papers in a collection.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Tools",
      "Document Classifier Tool"
    ]
  },
  {
    "objectID": "docnb1_use_cases.html",
    "href": "docnb1_use_cases.html",
    "title": "Use Cases",
    "section": "",
    "text": "Broadly, our scientific mission at the Chan Zuckerberg Initiative (CZI) is expressed as:\n\nWe build open source software tools to accelerate science and generate more accurate and biologically important sources of data. We fund scientific research worldwide to advance the frontiers of knowledge. And we launched a family of institutes to do research that can’t be done in conventional environments. Each aspect is essential to our approach to building for the long term.\n\nWe describe this coordinated activity as ‘Build / Fund / Do’, so that our SciTech development work (the ‘Build’ compontent), is based on scientific challenges and use cases provided by the collaborative research networks that funding supports (‘Fund’) and direct research activities undertaken within the CZI research organizations such as the CZI BioHubs and the Chan Zuckerberg Imaging Institute (‘Do’). In particular, our scientific work generates complex research landscaping requirements that we seek to address using the latest available AI technology.",
    "crumbs": [
      "Get Started",
      "Use Cases"
    ]
  },
  {
    "objectID": "docnb1_use_cases.html#key-use-case-metadata-extraction-for-curation-of-data-into-a-repository",
    "href": "docnb1_use_cases.html#key-use-case-metadata-extraction-for-curation-of-data-into-a-repository",
    "title": "Use Cases",
    "section": "Key Use Case: Metadata Extraction for Curation of Data into a Repository",
    "text": "Key Use Case: Metadata Extraction for Curation of Data into a Repository\nA key use case for this effort is the development of tools that can assist curation of complex datasets to a central repository (such as for the Chan Zuckerberg Imaging Institute’s CryoET Data Portal.\nWe currently seek to improve this use case by including ontology search / matching capabilities to the extracted text and to generalize the extraction process to other protocol types.",
    "crumbs": [
      "Get Started",
      "Use Cases"
    ]
  },
  {
    "objectID": "agents/agent.html",
    "href": "agents/agent.html",
    "title": "Basic Alhazen Agent / Chatbot.",
    "section": "",
    "text": "source\n\nJSONAgentOutputParser_withFixes\n\n JSONAgentOutputParser_withFixes (name:Optional[str]=None)\n\nmodified from JSONAgentOutputParser to deal with escaped underscore characters.\n\nsource\n\n\ncreate_structured_chat_agent_withFixes\n\n create_structured_chat_agent_withFixes\n                                         (llm:langchain_core.language_mode\n                                         ls.base.BaseLanguageModel, tools:\n                                         Sequence[langchain_core.tools.Bas\n                                         eTool], prompt:langchain_core.pro\n                                         mpts.chat.ChatPromptTemplate)\n\nmodified from create_structured_chat_agent to deal with escaped underscore characters.\n\nsource\n\n\nAlhazenAgent\n\n AlhazenAgent (agent_llm, tool_llm, db_name, sml=None, verbose=True,\n               return_intermediate_steps=True)\n\nThe base Alhazen agent.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Agents",
      "Basic Alhazen Agent / Chatbot."
    ]
  },
  {
    "objectID": "utils/jats_text_extractor.html",
    "href": "utils/jats_text_extractor.html",
    "title": "JATS Text Extractor Utility",
    "section": "",
    "text": "source\n\nget_ft_url_from_doi\n\n get_ft_url_from_doi (doi, file_path)\n\n\nsource\n\n\nNxmlDoc\n\n NxmlDoc (ft_id, xml)\n\nA class that provides structure for full text papers specified under the JATS ‘nxml’ format.",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "JATS Text Extractor Utility"
    ]
  },
  {
    "objectID": "utils/index.html",
    "href": "utils/index.html",
    "title": "Utilities",
    "section": "",
    "text": "The substance of much of the functionality supplied by Alhazen is provided by utility programs. These are the functional elements that are brought together and executed by tools. The consist of inputs and outputs to the local database, downloading papers, querying external databases and other functions.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nDatabase for Scientific Knowledge\n\n\nA local Postgresql database of scientific content with associated information generated by the agent. The intent is to use this repository as ‘memory’ in a Langchain-enabled agent.\n\n\n\n\nHTML Text Extractor Utility\n\n\nExtracts unstructured text from scientific papers published as HTML files .\n\n\n\n\nJATS Text Extractor Utility\n\n\nExtracts structured text from JATS XML Files with offset annotations for sections, etc.\n\n\n\n\nPDF Text Extractor Utility\n\n\nExtracts unstructured text from scientific papers published as PDF files .\n\n\n\n\nSearch Engine Tools\n\n\n\n\n\n\n\nWeb Robots\n\n\nWeb robots that automates the process of obtaining full text papers (and other interactions with the web)\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities"
    ]
  },
  {
    "objectID": "utils/pdf_text_extractor.html",
    "href": "utils/pdf_text_extractor.html",
    "title": "PDF Text Extractor Utility",
    "section": "",
    "text": "source\n\nLAPDFBlockParser\n\n LAPDFBlockParser (text_kwargs:Optional[Mapping[str,Any]]=None)\n\nParse PDF using PyMuPDF.\n\nsource\n\n\nLAPDFBlockLoader\n\n LAPDFBlockLoader (file_path:str)\n\nLoad PDF files using PyMuPDF into representative .\n\nsource\n\n\nLAPDF_FeatureBlock\n\n LAPDF_FeatureBlock (p:int, x0:float, y0:float, x1:float, y1:float,\n                     text:str, nlines:int, sizes:dict, fonts:dict,\n                     pos_err:float=0.05)\n\nA block of text with spatial features occuring in a PDF full-text article.\n\nsource\n\n\nCumulativeTextFeature\n\n CumulativeTextFeature (name:str)\n\n\nsource\n\n\nHuridocsPDFParser\n\n HuridocsPDFParser (text_kwargs:Optional[Mapping[str,Any]]=None,\n                    host='localhost')\n\nParse PDF using Huridocs (https://github.com/huridocs/pdf_paragraphs_extraction).\n\nsource\n\n\nHuridocsPDFLoader\n\n HuridocsPDFLoader (file_path:str, host='localhost')\n\nLoad PDF files using Huridocs (https://github.com/huridocs/pdf_paragraphs_extraction).",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "PDF Text Extractor Utility"
    ]
  },
  {
    "objectID": "utils/search_engine_eutils.html",
    "href": "utils/search_engine_eutils.html",
    "title": "Search Engine Tools",
    "section": "",
    "text": "source\n\n\n\n ESearchQuery (api_key=None, oa=False, db='pubmed')\n\nClass to provide query interface for ESearch (i.e., query terms in elaborate ways, return a list of ids) Each instance of this class executes queries of a given type\n\nsource\n\n\n\n\n NCBI_Database_Type (value, names=None, module=None, qualname=None,\n                     type=None, start=1, boundary=None)\n\nSimple enumeration of the different NCBI databases supported by this tool\n\nsource\n\n\n\n\n EFetchQuery (api_key=None, db='pubmed')\n\nClass to provide query interface for EFetch (i.e., query based on a list of ids) Each instance of this class executes queries of a given type\n\nsource\n\n\n\n\n get_pdf_from_pubmed_doi (doi, base_file_path)\n\nExecutes a query on the target database and returns a count of papers\n\nsource\n\n\n\n\n get_nxml_from_pubmed_doi (doi, base_file_path)\n\nExecutes a query on the target database and returns a count of papers\n\nsource\n\n\n\n\n download_file (url, local_filename)\n\nDownloads a file from an URL to a local disk",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "Search Engine Tools"
    ]
  },
  {
    "objectID": "utils/search_engine_eutils.html#ncbi-tools",
    "href": "utils/search_engine_eutils.html#ncbi-tools",
    "title": "Search Engine Tools",
    "section": "",
    "text": "source\n\n\n\n ESearchQuery (api_key=None, oa=False, db='pubmed')\n\nClass to provide query interface for ESearch (i.e., query terms in elaborate ways, return a list of ids) Each instance of this class executes queries of a given type\n\nsource\n\n\n\n\n NCBI_Database_Type (value, names=None, module=None, qualname=None,\n                     type=None, start=1, boundary=None)\n\nSimple enumeration of the different NCBI databases supported by this tool\n\nsource\n\n\n\n\n EFetchQuery (api_key=None, db='pubmed')\n\nClass to provide query interface for EFetch (i.e., query based on a list of ids) Each instance of this class executes queries of a given type\n\nsource\n\n\n\n\n get_pdf_from_pubmed_doi (doi, base_file_path)\n\nExecutes a query on the target database and returns a count of papers\n\nsource\n\n\n\n\n get_nxml_from_pubmed_doi (doi, base_file_path)\n\nExecutes a query on the target database and returns a count of papers\n\nsource\n\n\n\n\n download_file (url, local_filename)\n\nDownloads a file from an URL to a local disk",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "Search Engine Tools"
    ]
  },
  {
    "objectID": "utils/search_engine_eutils.html#europmcquery",
    "href": "utils/search_engine_eutils.html#europmcquery",
    "title": "Search Engine Tools",
    "section": "EuroPMCQuery",
    "text": "EuroPMCQuery\n\nsource\n\nEuroPMCQuery\n\n EuroPMCQuery (oa=False, db='pubmed')\n\nA class that executes search queries on the European PMC API",
    "crumbs": [
      "Get Started",
      "Tech Elements",
      "Utilities",
      "Search Engine Tools"
    ]
  },
  {
    "objectID": "cookbook/question_answering/index.html",
    "href": "cookbook/question_answering/index.html",
    "title": "Question Answering",
    "section": "",
    "text": "Click through to any of these notebooks to run tools over digital libraries that permit question answering over corpora of documents.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nCell Biology Database.\n\n\nApplying AI to understand trends of research.\n\n\n\n\nVirtual Cell Landscaping Analysis\n\n\nUsing Alhazen to study and interrogate papers concerning the Virtual Cell data modeling work.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Question Answering"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html",
    "href": "cookbook/single_doc_extraction/cryoet.html",
    "title": "CryoET",
    "section": "",
    "text": "Cryo-electron Tomography (CryoET) involves rapidly freezing biological samples in their natural state to preserve their three-dimensional structure without the need for staining or crystallization. This methodology allows researchers to visualize proteins and other biomolecules at near-atomic resolution.\nThis digital library is based on capturing all papers that mention the technique in their titles, abstracts, or methods sections and then analyzing the various methods used and their applications. Our focus is on supporting the work of the Chan Zuckerberg Imaging Institute, CZII on developing the CryoET data portal, an open source repository for CryoET-based data.",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html#introduction-to-cryoet",
    "href": "cookbook/single_doc_extraction/cryoet.html#introduction-to-cryoet",
    "title": "CryoET",
    "section": "",
    "text": "Cryo-electron Tomography (CryoET) involves rapidly freezing biological samples in their natural state to preserve their three-dimensional structure without the need for staining or crystallization. This methodology allows researchers to visualize proteins and other biomolecules at near-atomic resolution.\nThis digital library is based on capturing all papers that mention the technique in their titles, abstracts, or methods sections and then analyzing the various methods used and their applications. Our focus is on supporting the work of the Chan Zuckerberg Imaging Institute, CZII on developing the CryoET data portal, an open source repository for CryoET-based data.",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html#basics",
    "href": "cookbook/single_doc_extraction/cryoet.html#basics",
    "title": "CryoET",
    "section": "Basics",
    "text": "Basics\n\nPython Imports\nSetting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.tools.tiab_classifier_tool import *\nfrom alhazen.tools.tiab_extraction_tool import *\nfrom alhazen.tools.tiab_mapping_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\n\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database\n\nfrom alhazen.utils.searchEngineUtils import *\n\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport uuid\nimport yaml\n\nimport local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\nfrom rapidfuzz import fuzz\n\n# Plot the distribution of the lengths of the methods sections \nimport seaborn as sns  \nimport matplotlib.pyplot as plt\nimport tiktoken\nimport transformers\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nimport torch\nimport local_resources.queries.em_tech as em_tech_queries\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\nimport json\nfrom jsonpath_ng import jsonpath, parse\nfrom langchain_community.chat_models.openai import ChatOpenAI\n\n\n\nEnvironment Variables\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'em_tech'\n\n\n\nSetup utils, agents, and tools\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllms_lookup = lookup_chat_models()\nprint(llms_lookup.keys())\n\n\nllm_gpt4_1106 = ChatOpenAI(model='gpt-4-1106-preview') \nllm_gpt35 = ChatOpenAI(model='gpt-3.5-turbo')\n\n\n#llm = llms_lookup.get('databricks_llama3')\n\ncb = AlhazenAgent(llm_gpt35, llm_gpt35, db_name=db_name)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\ntest_tk = MetadataExtractionToolkit(db=ldb, llm=llm_gpt35)\nprint('\\nTESTING TOOLS')\nfor t in test_tk.get_tools():\n    print('\\t'+type(t).__name__)\n\n\n\nSet Evaluation Dataset\nThese are cases directly taken from *.yaml files that\n\nIdentify cases from the CZI CryoET Portal.\n\ndois = {10000: ['10.1101/2022.04.12.488077'], \n        10001: ['10.1101/2022.04.12.488077'], \n        10003: ['10.1038/s41586-022-05255-2', '10.1038/s41592-020-01054-7'], \n        10004: ['10.1101/2023.04.28.538734'], \n        10005: ['10.1038/s41594-022-00861-0'], \n        10006: ['10.1038/s41586-020-2665-2'], \n        10007: [], \n        10008: ['10.1038/s41586-022-04971-z'], \n        10009: ['10.1126/science.abm6704'], \n        10010: ['10.1083/jcb.202204093', '10.1101/2022.01.23.477440']}\ndois_flattened = [doi for doi_list in dois.values() for doi in doi_list]\ndois_flattened = list(set(dois_flattened))\ndois_flattened\n\n\n\nRetrieve Gold Standard experimental metadata from EMPIAR database.\n\nDownload the entire database to a local file from: https://www.ebi.ac.uk/emdb/search/database:EMPIAR\nSave the location in a temporary variable: empiar_metadata_path\nProcess the downloaded file for (A) EMDB ids, (B) DOI values for publications.\n\n\n# local_path to the file downloaded from the EMPIAR search results: \n# https://www.ebi.ac.uk/emdb/api/empiar/search/database:EMPIAR?wt=json&download=true\n# download the file and save it to a local path\nurl = \"https://www.ebi.ac.uk/emdb/api/empiar/search/database:EMPIAR?wt=json&download=true\"\nempiar_metadata_path = loc+db_name+'/EMPIAR_search_results.json'\nresponse = requests.get(url, stream=True)\nwith open(empiar_metadata_path, \"wb\") as handle:\n    for data in response.iter_content():\n        handle.write(data)\n\nwith open(empiar_metadata_path, 'r') as f:\n    empiar_metadata = json.load(f)\nempiar_dataset_ids = list(empiar_metadata.keys())\nd = {}\nfor empiar_id in empiar_dataset_ids:\n    d[empiar_id] = {'dois':[], 'emd_ids': []}\n    for citation in empiar_metadata.get(empiar_id, {}).get('citation', []):\n        if citation.get('doi') is not None:\n            d[empiar_id]['dois'].append(citation.get('doi'))\n    for emd_id in empiar_metadata.get(empiar_id, {}).get('cross_references'):\n        d[empiar_id]['emd_ids'].append(emd_id.get('name'))    \n\ndef get_nested(data, *args):\n    if args and data:\n        element  = args[0]\n        if element:\n            value = data.get(element)\n            return value if len(args) == 1 else get_nested(value, *args[1:])\n\n\n# get metadata from the EMDB entries for each case\nmetadlist = []\n\n# jsonpath expressions to identify specific metadata from the EMDB entries\n# focus mainly on the specimen preparation (grids, buffers, vitrification, etc.)\nsd_jp = 'structure_determination_list.structure_determination[*]'\nsample_preparation_type_jp = parse(sd_jp + '.method')\nagg_state_jp = parse(sd_jp + '.aggregation_state')\nspecprep_list_jp = sd_jp + '.specimen_preparation_list.specimen_preparation[*]'\nbuffer_jp = parse(specprep_list_jp + '.buffer.ph') \ngrid_model_jp = parse(specprep_list_jp + '.grid.model')\ngrid_material_jp = parse(specprep_list_jp + '.grid.material') \ngrid_mesh_jp = parse(specprep_list_jp + '.grid.mesh')\ngrid_support_topology_jp = parse(specprep_list_jp + '.grid.support_film[*].film_topology')\ngrid_pretreatment_jp = parse(specprep_list_jp + '.grid.pretreatment.type_')\ngrid_vitrification_cryogen_jp = parse(specprep_list_jp + '.vitrification.cryogen_name')\ngrid_vit_ctemp_jp = specprep_list_jp + '.vitrification.chamber_temperature.'\ngrid_vit_chumid_jp = specprep_list_jp + '.vitrification.chamber_humidity'\n\njp_method = parse('structure_determination_list.structure_determination[*]')\ni = 0\nfor k,v in d.items():\n    #i += 1\n    #if i &gt; 10:\n    #    break\n    print(k,v)\n    for emd_id in v['emd_ids']:\n        emd_exp = requests.get('https://www.ebi.ac.uk/emdb/api/entry/experiment/'+emd_id)\n        if emd_exp.status_code == 200:\n            emd = emd_exp.json()\n            sample_preparation_type = ', '.join([m.value for m in sample_preparation_type_jp.find(emd)])\n            agg_state = ', '.join([m.value for m in agg_state_jp.find(emd)])\n            buffer = ', '.join([str(m.value) for m in buffer_jp.find(emd)])\n            grid_model = ', '.join([m.value for m in grid_model_jp.find(emd)])\n            grid_material = ', '.join([m.value for m in grid_material_jp.find(emd)])\n            grid_mesh = ', '.join([str(m.value) for m in grid_mesh_jp.find(emd)])\n            grid_support_topology = ', '.join([m.value for m in grid_support_topology_jp.find(emd)])\n            grid_pretreatment = ', '.join([m.value for m in grid_pretreatment_jp.find(emd)])\n            grid_vitrification_cryogen = ', '.join([m.value for m in grid_vitrification_cryogen_jp.find(emd)])\n            grid_support_topology = ', '.join([m.value for m in grid_support_topology_jp.find(emd)])\n\n            grid_vit_ctemp_units = [m.value for m in parse(grid_vit_ctemp_jp+'.units').find(emd)]\n            grid_vit_ctemp_values = [str(m.value) for m in parse(grid_vit_ctemp_jp+'.valueOf_').find(emd)]\n            grid_vit_ctemp = ','.join([t[0]+' '+t[1] for t in zip(grid_vit_ctemp_values, grid_vit_ctemp_units)])\n\n            grid_vit_chumid_units = [m.value for m in parse(grid_vit_chumid_jp+'.units').find(emd)]\n            grid_vit_chumid_values = [str(m.value) for m in parse(grid_vit_chumid_jp+'.valueOf_').find(emd)]\n            grid_vit_chumid = ', '.join([t[0]+' '+t[1] for t in zip(grid_vit_chumid_values, grid_vit_chumid_units)])\n\n            for doi in v['dois']:\n                metadlist.append({'doi':doi, \n                                  'emd_id': emd_id, \n                                  'sample_preparation_type': sample_preparation_type, \n                                  'agg_state': agg_state, \n                                  'sample_preparation_buffer_ph': buffer, \n                                  'grid_model': grid_model, \n                                  'grid_material': grid_material, \n                                  'grid_mesh': grid_mesh, \n                                  'grid_support_topology': grid_support_topology, \n                                  'grid_pretreatment': grid_pretreatment, \n                                  'grid_vitrification_cryogen': grid_vitrification_cryogen, \n                                  'grid_vit_ctemp': grid_vit_ctemp, \n                                  'grid_vit_chumid': grid_vit_chumid})\n        else:\n            print('ERROR: ', emd_exp.status_code)\n\nempiar_df = pd.DataFrame(metadlist)\nempiar_df.to_csv(loc+db_name+'/empiar_metadata.tsv', sep='\\t', index=False)\nempiar_dois = sorted(empiar_df['doi'].unique())\nempiar_df\n\n\n\nLoad the EMPIAR data from disk\nThis is from local directory that we just created\n\nempiar_df = pd.read_csv(loc+db_name+'/empiar/empiar_metadata.tsv', sep='\\t')\nempiar_dois = sorted(empiar_df['doi'].unique())\nempiar_df",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html#building-the-database",
    "href": "cookbook/single_doc_extraction/cryoet.html#building-the-database",
    "title": "CryoET",
    "section": "Building the database",
    "text": "Building the database\n\nScripts to Build / Delete the database\nIf you need to restore a deleted database from backup, use the following shell commands:\n$ createdb em_tech\n$ psql -d em_tech -f /local/file/path/em_tech/backup&lt;date_time&gt;.sql\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\nloc = os.environ['LOCAL_FILE_PATH']\ncurrent_date_time = datetime.now()\nformatted_date_time = f'{current_date_time:%Y-%m-%d-%H-%M-%S}'\nbackup_path = loc+'/'+db_name+'/backup'+formatted_date_time+'.sql'\nbackup_ceifns_database(db_name, backup_path)\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\n\nBuild CEIFNS database from queries\n\nAdd a collection based on EMPIAR papers\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nstep = 20\nfor start_i in range(0, len(empiar_dois), step):\n    query = ' OR '.join(['doi:\"'+empiar_dois[i]+'\"' for i in range(start_i, start_i+step)])\n    addEMPCCollection_tool.run({'id': '3', 'name':'EMPIAR Papers', 'query':query, 'full_text':True})\n\n\ndef join_set(x):\n    out = ''\n    try:\n        out = ' '.join(set(x))\n    except:\n        pass\n    return out\n\n# identify papers that we have full text for in EMPIAR\nq = ldb.session.query(SKE.id) \\\n        .distinct() \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \\\n        .filter(SKC.id == '3') \\\n        .filter(or_(SKI.type == 'JATSFullText', SKI.type == 'PDFFullText')) \ndois_to_include = [d[0][4:] for d in q.all()]    \n\nempiar_gold_standard = []\nfor i, row in empiar_df.iterrows():\n    if row.doi in dois_to_include:\n        empiar_gold_standard.append( row.to_dict() )\nempiar_gold_standard_df = pd.DataFrame(empiar_gold_standard)\n\nempiar_gs_df = empiar_gold_standard_df.groupby(['doi']).agg({'sample_preparation_type': join_set, \n                                                             'agg_state': join_set, \n                                                             'sample_preparation_buffer_ph': join_set, \n                                                             'grid_model': join_set, \n                                                             'grid_material': join_set, \n                                                             'grid_mesh': join_set, \n                                                             'grid_support_topology': join_set, \n                                                             'grid_pretreatment': join_set, \n                                                             'grid_vitrification_cryogen': join_set, \n                                                             'grid_vit_ctemp': join_set, \n                                                             'grid_vit_chumid': join_set}).reset_index()\nempiar_gs_df\n\n\n\nImport papers from DOIs pertaining to CryoET-Portal records 10000-10010\nThe CryoET Data portal system is based on submitted data to our curation team, accompanied by papers referenced by DOIs. Each dataset is assigned an ID value associated with DOIs.\n\n# use the EMPCSearchTool to run a query for the dois mentioned\nquery = ' OR '.join(['doi:\"'+d+'\"' for d_id in dois for d in dois[d_id] ])\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\naddEMPCCollection_tool.run(tool_input={'id': '0', 'name':'CryoET Portal (10000-10010)', 'query':query, 'full_text':True})\n\n\n\nExtend Database to include all CryoET papers\n\ncols_to_include = ['ID', 'CORPUS_NAME', 'QUERY']\ndf = pd.read_csv(files(em_tech_queries).joinpath('EM_Methods.tsv'), sep='\\t')\ndf = df.drop(columns=[c for c in df.columns if c not in cols_to_include])\ndf\n\n\nqt = QueryTranslator(df.sort_values('ID'), 'ID', 'QUERY', 'CORPUS_NAME')\n(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=['TITLE_ABS', 'METHODS'])\ncorpus_names = df['CORPUS_NAME']\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nfor (id, name, query) in zip(corpus_ids, corpus_names, epmc_queries):\n    if id != 2:\n        continue\n    addEMPCCollection_tool.run(tool_input={'id': id, 'name':name, 'query':query, 'full_text':False})\n\n\n\nCombine + Sample CryoET + EMPIAR Collections to provide a test set of papers.\n\nldb.create_new_collection_from_intersection('4', 'EMPIAR CryoET Papers', '2', '3')\n\n\n\nAdding Machine Learning\n\nml_query = '''\n(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet ) AND \n(\"Machine Learning\" OR \"Artificial Intelligence\" OR \"Deep Learning\" OR \"Neural Networks\")\n'''\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\naddEMPCCollection_tool.run(tool_input={'id': '6', \n                                       'name': 'Machine Learning in CryoET', \n                                       'query': ml_query, \n                                       'full_text': False})\n\n\ndelCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, DeleteCollectionTool)][0]\ndelCollection_tool.run(tool_input={'collection_id': '6'})\n\n\n\nBreak up TIAB of papers into sentences + classify by discourse\nNOTE - HUGGING FACE MODELS DO NOT WORK WELL ON THIS CORPUS. (NOT SURPRISINGLY - THEY WERE TRAINED ON MEDICAL PAPERS WHERE THE DIFFERENT SECTIONS OF THE PAPER WERE EXPLICITLY LABELED)\nUSE LLMS TO DO THE EXTRACTION - GPT3.5?\n\n# Get the metadata extraction tool\nt2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\nt2.run(tool_input={'collection_id': '5', 'run_label': 'dev'})\n\n\nj = '''{\n\"Background\": \"Eps15-homology domain containing proteins (EHDs) are eukaryotic, dynamin-related ATPases involved in cellular membrane trafficking. They oligomerize on membranes into filaments that induce membrane tubulation. While EHD crystal structures in open and closed conformations were previously reported, little structural information is available for the membrane-bound oligomeric form. Consequently, mechanistic insights into the membrane remodeling mechanism have remained sparse.\",\n\"Objectives_Methods\": \"Here, by using cryo-electron tomography and subtomogram averaging, we determined structures of nucleotide-bound EHD4 filaments on membrane tubes of various diameters at an average resolution of 7.6 Å.\",\n\"Results_Conclusions\": \"Assembly of EHD4 is mediated via interfaces in the G-domain and the helical domain. The oligomerized EHD4 structure resembles the closed conformation, where the tips of the helical domains protrude into the membrane. The variation in filament geometry and tube radius suggests a spontaneous filament curvature of approximately 1/70 nm&lt;sup&gt;-1&lt;/sup&gt;. Combining the available structural and functional data, we suggest a model for EHD-mediated membrane remodeling.\"\n}'''\njson.loads(j)\n\n\n# Get the metadata extraction tool\nmodels = ['databricks_dbrx']\nfor m in models:\n    llm = llms_lookup.get(m)\n    cb = AlhazenAgent(llm, llm, db_name=db_name)\n    t2 = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractDiscourseMappingTool)][0]\n    t2.run(tool_input={'collection_id': '2', 'run_label': m, 'repeat_run': False})\n\n\nto_remove = [\"doi:10.1101/2024.03.04.583254\", \"doi:10.1101/2023.11.21.567712\",\n            \"doi:10.3791/6515\", \"doi:10.1101/2023.07.28.550950\",\n            \"doi:10.1093/micmic/ozad067.483\", \"doi:10.1007/978-1-0716-2639-9_20\",\n            \"doi:10.1016/j.yjsbx.2022.100076\", \"doi:10.1016/j.xpro.2022.101658\",\n            \"doi:10.1016/j.cell.2022.06.034\", \"doi:10.1093/plphys/kiab449\",\n            \"doi:10.1073/pnas.2118020118\", \"doi:10.3791/62886\",\n            \"doi:10.20944/preprints202105.0098.v1\", \"doi:10.1016/bs.mcb.2020.12.009\",\n            \"doi:10.1007/978-1-0716-0966-8_1\", \"doi:10.1007/978-1-0716-0966-8_2\",\n            \"doi:10.21769/bioprotoc.3768\", \"doi:10.1371/journal.ppat.1008883\",\n            \"doi:10.1101/2020.05.19.104828\", \"doi:10.1073/pnas.1916331116\",\n            \"doi:10.1042/bst20170351_cor\", \"doi:10.1038/s41594-018-0043-7\",\n            \"doi:10.1007/978-1-4939-8585-2_4\", \"doi:10.1007/s41048-017-0040-0\",\n            \"doi:10.1007/978-1-4939-6927-2_20\", \"doi:10.1016/j.str.2015.03.008\",\n            \"doi:10.1007/978-1-62703-227-8_4\", \"doi:10.1016/b978-0-12-397945-2.00017-2\",\n            \"doi:10.1016/j.jmb.2010.10.021\", \"doi:10.1186/1757-5036-3-6\",\n            \"doi:10.1016/j.jmb.2008.03.014\", \"doi:10.1007/978-1-59745-294-6_20\"]\n\nfor d in to_remove:\n    q = \"\"\"\n    SELECT DISTINCT n.id FROM langchain_pg_embedding as emb, \"Note\" as n\n    WHERE emb.cmetadata-&gt;&gt;'n_type' = 'TiAbMappingNote__Discourse' AND\n        emb.cmetadata-&gt;&gt;'about_id' = '{}' AND \n        emb.cmetadata-&gt;&gt;'discourse_type' = 'ResultsConclusions' AND \n        emb.cmetadata-&gt;&gt;'n_id' = n.id;\"\"\".format(d)\n    for row in ldb.session.execute(text(q)).all():\n        ldb.delete_note(row[0], commit_this=True)\n\n\nldb.session.rollback()\nexp_q = ldb.session.query(SKE) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.type=='CitationRecord') \\\n        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n        .order_by(desc(SKE.publication_date))\n\ncount = 0\nfor e in tqdm(exp_q.all()):\n    q = ldb.session.query(N) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == e.id) \\\n        .filter(N.type =='TiAbMappingNote__Discourse')\n    for n in q.all():\n        dmap = json.loads(n.content) \n        if 'Objectives_Methods' in dmap.keys():\n            print('beep')\n            #new_dmap = {'Background': dmap.get('Background'), 'ObjectivesMethods': dmap.get('Objectives_Methods'), 'ResultsConclusions': dmap.get('Results_Conclusions')}\n            #n.content = json.dumps(new_dmap, indent=4)\n            #ldb.session.flush()\n#ldb.session.commit()\n\n\nldb.session.rollback()\nexp_q = ldb.session.query(SKE) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == str('2')) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.type=='CitationRecord') \\\n        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \\\n        .order_by(desc(SKE.publication_date))\n\ntexts = []\nmetadatas = []\n\ncount = 0\nfor e in tqdm(exp_q.all()):\n    q = ldb.session.query(N) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == e.id) \\\n        .filter(N.type =='TiAbMappingNote__Discourse')\n\n\n    n = q.first()\n    dmap = json.loads(n.content)\n    '''Runs through the list of expressions, generates embeddings, and stores them in the database'''\n\n    for dtype in ['Background', 'ObjectivesMethods', 'ResultsConclusions']:\n        t = dmap.get(dtype)\n        if t is None:\n            continue\n        texts.append(t)\n        metadatas.append({'about_id': e.id, \\\n                        'about_type': 'ScientificKnowledgeExpression', \\\n                        'n_id': n.id, \\\n                        'n_type': 'TiAbMappingNote__Discourse', \\\n                        'discourse_type': dtype})\n\ndocs = []\nfor t,m in zip(texts, metadatas):\n    docs.append(Document(page_content=t, metadata=m))\n    \ndb = PGVector.from_documents(\n    embedding=ldb.embed_model,\n    documents=docs,\n    collection_name=\"Note\"\n)\n\n\nmodel_path = '/Users/gully.burns/Documents/2024H1/models/discourse_tagger'\ntokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\", \n                                          truncation=True, \n                                          max_length=512)\nlabels = ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS', 'CONCLUSIONS']\nlookup = {'LABEL_%d'%(i):l for i, l in enumerate(labels)}\nmodel = AutoModel.from_pretrained(model_path)\nmodel.eval()\n\nclassifier = pipeline(\"text-classification\", \n                      model = model_path, \n                      tokenizer=tokenizer, \n                      truncation=True,\n                      batch_size=8,\n                      device='mps')\n\n\nself = ldb\ncollection_id = '2'\n\nq1 = self.session.query(SKE, SKI) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n        .filter(SKI_HP.has_part_id == SKF.id) \\\n        .filter(SKI.type == 'CitationRecord') \\\n        .filter(or_(SKE.type == 'ScientificPrimaryResearchArticle', SKE.type == 'ScientificPrimaryResearchPreprint')) \n\nfor ske, ski in tqdm(q1.all()):\n    b = ''\n    om = ''\n    rc = ''  \n\n    fragments = []\n    for f in ski.has_part:\n      if f.type in ['title', 'abstract']:\n        fragments.append(f)\n\n    # USE AN LLM HERE INSTEAD OF A DEEP LEARNING CLASSIFER\n\n\n    for skf in sorted(fragments, key=lambda f: f.offset):\n        for s in self.sent_detector.tokenize(skf.content):\n            m = classifier(skf.content)\n            l = lookup.get(m[0].get('label'))\n            if l == 'BACKGROUND':\n                if len(b) &gt; 0:\n                    b += '\\n'\n                b += s\n            elif l == 'OBJECTIVE' or l == 'METHODS':\n                if len(om) &gt; 0:\n                    om += '\\n'\n                om += s\n            else: \n                if len(rc) &gt; 0:\n                    rc += '\\n'\n                rc += s\n    skf_stem = ske.id+'.'+ski.type+'.'\n    if len(b) &gt; 0:\n        f_b = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n                type='background_sentences', offset=-1, length=len(b),\n                name=skf_stem+'background', content=b)\n        self.session.add(f_b)\n        ski.has_part.append(f_b)\n        f_b.part_of = ski.id    \n    if len(om) &gt; 0:\n        f_om = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n                type='objective_methods_sentences', offset=-1, length=len(om),\n                name=skf_stem+'objective_methods', content=om)\n        self.session.add(f_om)\n        ski.has_part.append(f_om)\n        f_om.part_of = ski.id\n    if len(rc) &gt; 0:\n        f_rc = ScientificKnowledgeFragment(id=str(uuid.uuid4().hex)[:10], \n                type='results_conclusions_sentences', offset=-1, length=len(rc),\n                name=skf_stem+'results_conclusions', content=rc)\n        self.session.add(f_rc)\n        ski.has_part.append(f_rc)\n        f_rc.part_of = ski.id\n    self.session.flush()\nself.session.commit()\n\n\nself = ldb\ncollection_id = '2'\n#self.session.rollback()\nq2 = self.session.query(SKF) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n        .filter(SKI_HP.has_part_id == SKF.id) \\\n        .filter(SKI.type == 'CitationRecord') \\\n        .filter(or_(SKF.type == 'results_conclusions_sentences', \\\n                SKF.type == 'objective_methods_sentences', \\\n                SKF.type == 'background_sentences'))\nfor skf in tqdm(q2.all()):\n    self.delete_fragment(skf.id)\n\n\nself = ldb\ncollection_id = '2'\n#self.session.rollback()\nq2 = self.session.query(SKE, SKF) \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == collection_id) \\\n        .filter(SKC_HM.has_members_id == SKE.id) \\\n        .filter(SKE.id == SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id == SKI.id) \\\n        .filter(SKI.id == SKI_HP.ScientificKnowledgeItem_id) \\\n        .filter(SKI_HP.has_part_id == SKF.id) \\\n        .filter(SKI.type == 'CitationRecord') \\\n        .filter(SKF.type == 'objective_methods_sentences') \\\n        .order_by(desc(SKE.publication_date)) \\\n        .order_by(SKF.name)\n\nfor ske, skf in tqdm(q2.all()):\n    print(skf)\n\n\n\nGet full text copies of all the papers about CryoET\n\ncb.agent_executor.invoke({'input':'Get full text copies of all papers in the collection with id=\"2\".'})\n\n\nldb.create_new_collection_from_sample('5', 'EMPIAR CryoET Papers Tests', '4', 20, ['ScientificPrimaryResearchArticle', 'ScientificPrimaryResearchPreprint'])",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html#analyze-collections",
    "href": "cookbook/single_doc_extraction/cryoet.html#analyze-collections",
    "title": "CryoET",
    "section": "Analyze Collections",
    "text": "Analyze Collections\n\nq = ldb.session.query(SKC.id, SKC.name, SKE.id, SKI.type) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \ndf = pd.DataFrame(q.all(), columns=['id', 'collection name', 'doi', 'item type'])\ndf.pivot_table(index=['id', 'collection name'], columns='item type', values='doi', aggfunc=lambda x: len(x.unique())).fillna(0)\n\n\nSurvey + Run Classifications over Papers\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nl = []\nq = ldb.session.query(N, SKE) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n\noutput = []        \nprint(len(q.all()))\nfor n, ske in q.all():\n    ldb.delete_note(n.id)    \nprint(len(q.all()))\n\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '5', 'classification_type':'cryoet_study_types', 'repeat_run':True})\n\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '2', 'classification_type':'cryoet_study_types'})\n\n\nl = []\nldb.session.rollback()\nq = ldb.session.query(N, SKE) \\\n        .join(NIA, NIA.Note_id == N.id) \\\n        .join(SKE, SKE.id == NIA.is_about_id) \\\n        .join(SKC_HM, SKE.id == SKC_HM.has_members_id) \\\n        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n        .filter(SKC_HM.ScientificKnowledgeCollection_id == '5') \\\n        .order_by(SKE.id, N.provenance)\n\noutput = []        \nfor n, ske in q.all():\n        tup = json.loads(n.content)\n        tup['doi'] = 'http://doi.org/'+re.sub('doi:', '', ske.id)\n        tup['year'] = ske.publication_date.year\n        tup['month'] = ske.publication_date.month\n        tup['ref'] = ske.content\n        output.append(tup)\ndf = pd.DataFrame(output).sort_values(['year', 'month'], ascending=[False, False])\ndf.to_csv(loc+'/'+db_name+'/cryoet_study_types.tsv', sep='\\t')\ndf\n\n\nstudy_type_lookup = {'A': 'Viral Pathogens', \n                     'B': \"Mutated protein structure\", \n                     'C': 'Bacterial pathogens', \n                     'D': 'Plant cells', \n                     'E': 'Material science', \n                     'F': 'Intracellular Transport Structure', \n                     'G': 'Synapses or Vesicle Release', \n                     'H': 'Other Intracellular Structure', \n                     'I': 'Cellular Processes',\n                     'J': 'Dynamics of molecular interactions',    \n                     'K': 'New CryoET imaging methods', \n                     'L': 'New data analysis methods'}\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nstep = 20\n\nfor k in study_type_lookup.keys():\n    df1 = df[df['cryoet_study_type_code'] == k]\n    dois_to_add = [re.sub('http://doi.org/', 'doi:', r.doi) for i, r in df1.iterrows()]\n\n    c_id = '2.'+k\n    c_name = 'CryoET - ' + study_type_lookup[k]\n\n    corpus = None\n    all_existing_query = ldb.session.query(SKC).filter(SKC.id==c_id)\n    for c in all_existing_query.all():\n      corpus = c\n    if corpus is None:      \n      corpus = ScientificKnowledgeCollection(id=c_id,\n                                           type='skem:ScientificKnowledgeCollection',\n                                           name=c_name,\n                                           has_members=[])\n    ldb.session.add(corpus)\n    ldb.session.flush()\n\n    for doi in tqdm(dois_to_add):\n        p = ldb.session.query(SKE) \\\n            .filter(SKE.id==doi).first()\n        if p is None:\n          continue\n        ldb.session.add(p)\n        corpus.has_members.append(p)\n        p.member_of.append(corpus)\n        ldb.session.flush()\nldb.session.commit()\n\n\ndelete_collection_tool = [t for t in cb.tk.get_tools() if isinstance(t, DeleteCollectionTool)][0]\n \nfor k in study_type_lookup.keys():\n    print(k)\n    delete_collection_tool.run({'collection_id': '2.'+k})\n\n\n\nSurvey + Run Extractions over Papers\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractExtraction_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '5', 'extraction_type':'cryoet'})",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html#tests-checks",
    "href": "cookbook/single_doc_extraction/cryoet.html#tests-checks",
    "title": "CryoET",
    "section": "Tests + Checks",
    "text": "Tests + Checks\n\nAgent tool selection + execution + interpretation\n\ncb.agent_executor.invoke({'input':'Hi who are you and what can you do?'})",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html#run-metadata-extraction-chain-over-listed-papers",
    "href": "cookbook/single_doc_extraction/cryoet.html#run-metadata-extraction-chain-over-listed-papers",
    "title": "CryoET",
    "section": "Run MetaData Extraction Chain over listed papers",
    "text": "Run MetaData Extraction Chain over listed papers\nHere, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset.\n\nq = ldb.session.query(SKE.id) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKC.id=='5')  \ndois = [e.id for e in q.all()]\ndois\n\n\n# need to count tokens submitted to the server as a way of tracking usage. \n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device='mps', token=os.environ['HF_API_KEY'])\nprompt = \"The methods section of the paper is as follows:\"\ntokenized = tokenizer(prompt, return_tensors=\"pt\")\nprint(len(tokenized[\"input_ids\"][0]))\n\n\n# How long are methods sections in the CryoET papers?\nldb.session.rollback()\nq = ldb.session.query(SKE.id) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKC.id=='2') \\\n        .filter(or_(SKE.type=='ScientificPrimaryResearchArticle', SKE.type=='ScientificPrimaryResearchPreprint'))\n\nencoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n\ntups = []\nfor e in tqdm(q.all()):\n    item_types = set()\n    item_type = None\n    for i in ldb.list_items_for_expression(e.id):\n        item_types.add(i.type)\n    for i_type in item_types:\n        if i_type == 'CitationRecord':\n            continue\n        item_type = i_type\n        break\n    if item_type is None:\n        continue\n\n    fragments = [f.content for f in ldb.list_fragments_for_paper(e.id, item_type, fragment_types=['section'])]\n    on_off = False\n    text = ''\n    all_text = ''\n    for t in fragments:\n        all_text += t\n        l1 = t.split('\\n')[0].lower()\n        if 'method' in l1:\n            on_off = True\n        elif 'results' in l1 or 'discussion' in l1 or 'conclusion' in l1 or 'acknowledgements' in l1 \\\n                or 'references' in l1 or 'supplementary' in l1 or 'appendix' in l1 or 'introduction' in l1 or 'abstract' in l1 or 'cited' in l1:\n            on_off = False\n        if on_off:\n            if len(text) &gt; 0:\n                text += '\\n\\n'\n            text += t\n\n    all_text_length = len(tokenizer(all_text, return_tensors=\"pt\")['input_ids'][0])\n    text_length = len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n    tups.append({'doi':e.id, 'doc_length': all_text_length, 'method_length': text_length})\ndf_length = pd.DataFrame(tups)\ndf_length\n\n\nprint(len(df_length[df_length['method_length']&gt;8000]))\nprint(len(df_length[df_length['method_length']&lt;8000]))\n\n\ndef plot_length_distribution(df_length):\n    plt.hist(df_length, bins=10)\n    plt.xlabel('Length')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Lengths')\n    plt.show()\n\nplot_length_distribution(df_length['method_length'])\n\n\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\n\nfor i, r in tqdm(df_length.iterrows()):\n    if len(df[df['doi']==r['doi']]) &gt; 0:\n        continue\n    # Run the metadata extraction tool on the doi\n    try: \n        t2.run(tool_input={'paper_id': r['doi'], 'extraction_type': 'cryoet', 'run_label': 'test_llama3'})\n    except Exception as e:\n        print(e)\n        continue\n\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\ndf2 = pd.DataFrame()\nfor i, r in tqdm(df_length.iterrows()):\n        item_types = set()\n        l = t2.read_metadata_extraction_notes(r['doi'], 'cryoet', 'test')\n        if(len(l) == 0):\n            continue\n        df2 = pd.concat([df2, pd.DataFrame(l)])\n\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\ndf = pd.DataFrame()\nfor i, r in tqdm(df_length.iterrows()):\n    if r['method_length'] &lt; 8000:\n        item_types = set()\n        l = t2.read_metadata_extraction_notes(r['doi'], 'cryoet', 'test_llama3')\n        if(len(l) == 0):\n            continue\n        df = pd.concat([df, pd.DataFrame(l)]) \ndf\n\n\ndf[df['doi']=='doi:10.1101/2022.04.12.488077']\n\n\ndf2[df2['doi']=='doi:10.1101/2022.04.12.488077']\n\n\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\nmetadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\nt2.compile_answers('cryoet', metadata_dir)\nt2.write_answers_as_notes('cryoet', metadata_dir)\n#sorted(list(set([doi for q in t2.examples for doi in t2.examples[q]])))\n\n\n# Get the metadata extraction tool\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\n# Hack to get the path to the metadata directory as a string\n#metadata_dir = str(files(cryoet_portal_metadata).joinpath('temp'))[0:-4]\nmetadata_dir = '/Users/gully.burns/alhazen/em_tech/empiar/'\n\n# Compile the answers from the metadata directory\nt2.compile_answers('cryoet', metadata_dir)\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\ndf = pd.DataFrame()\nfor d in [d for d in dois]:\n    item_types = set()\n    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df = pd.concat([df, pd.DataFrame(l)]) \n     \n# Iterate over papers to run the metadata extraction tool\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nfor d in [d for d in dois]:\n    item_types = set()\n\n    # Skip if the doi is already in the database\n    if len(df)&gt;0 and d in df.doi.unique():\n        continue\n\n    # Run the metadata extraction tool on the doi\n    t2.run(tool_input={'paper_id': d, 'extraction_type': 'cryoet', 'run_label': 'test_llama3'})\n\n    # Add the results to the dataframe    \n    l2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df = pd.concat([df, pd.DataFrame(l2)])\n\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\ndf_final = pd.DataFrame()\nfor d in [d for d in dois]:\n    item_types = set()\n    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df_final = pd.concat([df_final, pd.DataFrame(l)]) \ndf_final\n\n\n# Get the metadata extraction tool\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nl = []\nfor d in [d for d in dois]:\n    item_types = set()\n    pred1 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    pred2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test_dbrx')\n    gold = t2.read_metadata_extraction_notes(d, 'cryoet', 'gold') \n    if pred1 is None or pred2 is None or gold is None or \\\n            len(pred1)==0 or len(pred2)==0 or len(gold)!=1:\n        continue\n    for k in gold[0]:\n        g_case = gold[0][k]\n        if g_case=='' or g_case is None:\n            continue    \n        for j, p_case in enumerate(pred1):\n            sim = fuzz.ratio(str(g_case), str(p_case.get(k,''))) / 100.0\n            print(k, str(g_case), str(p_case.get(k,'')), sim)\n\n\n# Get the metadata extraction tool\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\ndf = t2.report_metadata_extraction_for_collection('5', 'cryoet', 'test').set_index('doi')\ndf.to_csv(loc+'/'+db_name+'/reports/cryoet_metadata_gpt4.tsv', sep='\\t')\n\n\nldb.create_zip_archive_of_full_text_files('5', loc+'/'+db_name+'/full_text_files.zip')\n\n\nq3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'MetadataExtractionNote') \nl = []\nfor row in q3.all():\n    paper = row[0]\n    name = row[1]\n#    provenance = json.loads(row[2])\n    result = json.loads(row[3])\n    kv = {k:result[k] for k in result}\n    kv['DOI'] = paper\n    kv['run'] = name\n    l.append(kv)\n# create a dataframe from the list of dictionaries with DOI as the index column\nif len(l)&gt;0:\n    df = pd.DataFrame(l).set_index(['DOI', 'run'])\nelse: \n    df = pd.DataFrame()\ndf\n\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nfor row in q3.all():\n    d_id = row[0]\n    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n    notes_to_delete = []\n    for n in ldb.read_notes_about_x(e):\n        notes_to_delete.append(n.id)\n    for n in notes_to_delete:\n        ldb.delete_note(n)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/cryoet.html#protocol-modeling-extraction",
    "href": "cookbook/single_doc_extraction/cryoet.html#protocol-modeling-extraction",
    "title": "CryoET",
    "section": "Protocol Modeling + Extraction",
    "text": "Protocol Modeling + Extraction\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nslm = ChatOllama(model='stablelm-zephyr') \nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm3 = ChatOpenAI(model='gpt-3.5-turbo') \nd = (\"This tool attempts to draw a protocol design from the description of a scientific paper.\")\n\n\nt1 = ProcotolEntitiesExtractionTool(db=ldb, llm=llm3, description=d)\nentities = t1.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\nentities\n\n\nt2 = ProcotolProcessesExtractionTool(db=ldb, llm=llm3, description=d)\nprocesses = t2.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077'})\nprocesses.get('data')",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "CryoET"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/rnaquarium.html",
    "href": "cookbook/single_doc_extraction/rnaquarium.html",
    "title": "RNAquarium",
    "section": "",
    "text": "Setting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.aliases import *\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\n\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\nfrom alhazen.utils.searchEngineUtils import *\n\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, text, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nimport pymde\nimport torch\nimport local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\n\nfrom alhazen.utils.searchEngineUtils import load_paper_from_openalex, read_references_from_openalex \nfrom pyalex import config, Works, Work\nconfig.email = \"gully.burns@chanzuckerberg.com\"\n\nimport requests\nimport os\nimport local_resources.data_files.rnaquarium as rnaquarium\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\n\n\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'rnaquarium'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n    \nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \n#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n\ncb = AlhazenAgent(llm2, llm2)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\ntest_tk = MetadataExtractionToolkit(db=ldb, llm=llm2)\nprint('\\nTESTING TOOLS')\nfor t in test_tk.get_tools():\n    print('\\t'+type(t).__name__)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "RNAquarium"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/rnaquarium.html#basics",
    "href": "cookbook/single_doc_extraction/rnaquarium.html#basics",
    "title": "RNAquarium",
    "section": "",
    "text": "Setting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.aliases import *\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\n\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\nfrom alhazen.utils.searchEngineUtils import *\n\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, text, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nimport pymde\nimport torch\nimport local_resources.data_files.cryoet_portal_metadata as cryoet_portal_metadata\n\nfrom alhazen.utils.searchEngineUtils import load_paper_from_openalex, read_references_from_openalex \nfrom pyalex import config, Works, Work\nconfig.email = \"gully.burns@chanzuckerberg.com\"\n\nimport requests\nimport os\nimport local_resources.data_files.rnaquarium as rnaquarium\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\n\n\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'rnaquarium'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n    \nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \n#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n\ncb = AlhazenAgent(llm2, llm2)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\ntest_tk = MetadataExtractionToolkit(db=ldb, llm=llm2)\nprint('\\nTESTING TOOLS')\nfor t in test_tk.get_tools():\n    print('\\t'+type(t).__name__)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "RNAquarium"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/rnaquarium.html#building-the-database",
    "href": "cookbook/single_doc_extraction/rnaquarium.html#building-the-database",
    "title": "RNAquarium",
    "section": "Building the database",
    "text": "Building the database\n\nScripts to Build / Delete the database\nIf you need to restore a deleted database from backup, use the following shell commands:\n$ createdb em_tech\n$ psql -d em_tech -f /local/file/path/em_tech/backup&lt;date_time&gt;.sql\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\n\nBuild CEIFNS database from 900 dois in database\nLoad data from the spreadsheet\n\ndf = pd.read_csv(files(rnaquarium).joinpath('RNAquarium_paper_list.tsv'), sep='\\t')\ndois = df['DOI'].to_list()\ndf\n\nRun this cell to execute paged queries (length 40) over the European PMC for each of the DOIs mentioned in the spreadsheet loaded above.\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nstep = 40\nfor start_i in range(0, len(dois), step):\n    query = ' OR '.join(['doi:\\\"'+dois[i]+'\\\"' for i in range(start_i, start_i+step) if i &lt; len(dois)])\n    addEMPCCollection_tool.run({'id': '0', 'name':'RNAquarium Papers', 'query':query, 'full_text':True})\n\nRun this cell to check how many papers from the list are loaded in our database.\n\n# Compare contents of database to the list of dois\nmissing_list = []\ntitles = []\nfor doi in dois:\n    row = df[df['DOI']==doi]\n    doi_in_db = ldb.session.query(SKE).filter(SKE.id=='doi:'+doi.lower()).all()\n    if len(doi_in_db) == 0:\n        print('DOI: '+doi)\n        print('\\t%s (%d) %s %s'%(row['Author'].iloc[0],row['Publication Year'].iloc[0],row['Title'].iloc[0],row['Journal Abbreviation'].iloc[0]))\n        missing_list.append(doi)\n        titles.append(row['Title'].iloc[0])\nprint('%d Missing DOIs'%(len(missing_list)))\n\nUse OpenAlex as filler to add papers that were missed on EPMC\n\nldb.session.rollback()\ncorpus = ldb.session.query(SKC).filter(SKC.id=='0').first()\ncount = 0\nprint(len(corpus.has_members))\n\npapers_to_index = []\nfor i, doi in enumerate(missing_list):\n    p = load_paper_from_openalex(doi)\n    ldb.session.add(p)\n    corpus.has_members.append(p)\n    p.member_of.append(corpus)\n    for item in p.has_representation:\n        for f in item.has_part:\n            #f.content = '\\n'.join(self.sent_detector.tokenize(f.content))\n            f.part_of = item.id\n            ldb.session.add(f)\n        item.represented_by = p.id\n        ldb.session.add(item)\n    papers_to_index.append(p)\n    ldb.session.flush()\n\nldb.embed_expression_list(papers_to_index)\n\nldb.session.commit()\n\n\nGet full text copies of all the papers about CryoET\nThis invokes the agent directly to make it easy to run the retrieval tool.\n\ncb.db.session.rollback()\ncb.agent_executor.invoke({'input':'Retrieve full text for the collection with id=\"0\".'})",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "RNAquarium"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/rnaquarium.html#analyze-collections",
    "href": "cookbook/single_doc_extraction/rnaquarium.html#analyze-collections",
    "title": "RNAquarium",
    "section": "Analyze Collections",
    "text": "Analyze Collections\nBuild a basic report of the composition over all collections in the database (listed by types of items).\n\ncb.db.report_collection_composition()\n\n\ncb.db.report_non_full_text_for_collection(0)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "RNAquarium"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/rnaquarium.html#tests-checks",
    "href": "cookbook/single_doc_extraction/rnaquarium.html#tests-checks",
    "title": "RNAquarium",
    "section": "Tests + Checks",
    "text": "Tests + Checks\n\nAgent tool selection + execution + interpretation\n\n# use this cell to test the agent's \ncb.agent_executor.invoke({'input':'Hi who are you and what can you do?'})",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "RNAquarium"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/rnaquarium.html#run-metadata-extraction-chain-over-listed-papers",
    "href": "cookbook/single_doc_extraction/rnaquarium.html#run-metadata-extraction-chain-over-listed-papers",
    "title": "RNAquarium",
    "section": "Run MetaData Extraction Chain over listed papers",
    "text": "Run MetaData Extraction Chain over listed papers\nHere, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset.\n\nstr(files(cryoet_portal_metadata).joinpath('temp'))[0:-4]\n\n\n# Get the metadata extraction tool\nt2 = [t for t in test_tk.get_tools() if isinstance(t, MetadataExtraction_EverythingEverywhere_Tool)][0]\n\n# Hack to get the path to the metadata directory as a string\nmetadata_dir = str(files(rnaquarium).joinpath('temp'))[0:-4]\n\n# Compile the answers from the metadata directory\n#t2.compile_answers('cryoet', metadata_dir)\n\n# Create a dataframe to store previously extracted metadata\ndf = pd.DataFrame()\nfor d_id in dois:\n    item_types = set()\n    #d_id = 'doi:'+d\n    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, 'rnaquarium')) \n    df = pd.concat([df, df2]) \n     \n# Iterate over papers to run the metadata extraction tool\nfor d_id in dois[0:10]:\n    item_types = set()\n    #d_id = 'doi:'+d\n\n    # Skip if the doi is already in the database\n    if len(df)&gt;0 and d_id in df.doi.unique():\n        continue\n\n    # Run the metadata extraction tool on the doi\n    t2.run(tool_input={'paper_id': d_id, 'extraction_type': 'rnaquarium'})\n\n    # Add the results to the dataframe\n    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, 'rnaquarium')) \n    df = pd.concat([df, df2])\n\n\nq = cb.db.session.query(N) \\\n    .filter(N.id == NIA.Note_id) \\\n    .filter(N.type == 'MetadataExtractionNote') \\\n    .filter(N.name.like('rnaquarium_%')) \nl = []\nfor n in q.all():\n    tup = json.loads(n.content)\n    t, doi, label = n.name.split('__')\n    tup['doi'] = 'doi:'+doi\n    tup['extraction_type'] = t\n    tup['run_label'] = label\n    l.append(tup)\nreport_df = pd.DataFrame(l).set_index('doi')\nreport_df\n\n\nreport_df.to_csv(loc+'/rnaquarium_metadata_extraction_report.tsv', sep='\\t')\n\n\n# Create a dataframe to store previously extracted metadata\ndf = pd.DataFrame()\nfor d_id in dois:\n    df2 = pd.DataFrame(t2.read_metadata_extraction_notes(d_id, 'rnaquarium')) \n    df = pd.concat([df, df2]) \ndf\n\n\nldb.session.rollback()\n\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nfor d in list(set(dois[0:10])):\n    d_id = 'doi:'+d\n    e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n    notes_to_delete = []\n    if e is None:\n        continue\n    for n in ldb.read_notes_about_x(e):\n        notes_to_delete.append(n.id)\n    for n in notes_to_delete:\n        ldb.delete_note(n)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "RNAquarium"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/rnaquarium.html#protocol-modeling-extraction",
    "href": "cookbook/single_doc_extraction/rnaquarium.html#protocol-modeling-extraction",
    "title": "RNAquarium",
    "section": "Protocol Modeling + Extraction",
    "text": "Protocol Modeling + Extraction\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nslm = ChatOllama(model='stablelm-zephyr') \nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nd = (\"This tool attempts to draw a protocol design from the description of a scientific paper.\")\nt = ProcotolExtractionTool(db=ldb, llm=llm2, description=d)\nt.run(tool_input={'paper_id': 'doi:10.1101/2022.04.12.488077', 'extraction_type': 'cryoet'})\n\n\nldb.session.rollback()\nrag_embeddings_list = [json.loads(e[0]) for e in ldb.session.execute(text(\"\"\"\n                        SELECT DISTINCT emb.embedding \n                         FROM langchain_pg_embedding as emb, \n                            \"ScientificKnowledgeExpression\" as ske,\n                            \"ScientificKnowledgeCollection_has_members\" as skc_hm\n                         WHERE cmetadata-&gt;&gt;'i_type' = 'CitationRecord' AND\n                            cmetadata-&gt;&gt;'e_id' = ske.id AND \n                            ske.id = skc_hm.has_members_id AND\n                            skc_hm.\"ScientificKnowledgeCollection_id\"='0';\n                        \"\"\")).fetchall()]\nrag_embeddings_tensor = torch.FloatTensor(rag_embeddings_list)\n\nproj_embeddings = pymde.preserve_neighbors(rag_embeddings_tensor, constraint=pymde.Standardized()).embed()\npymde.plot(proj_embeddings)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction",
      "RNAquarium"
    ]
  },
  {
    "objectID": "cookbook/single_doc_extraction/index.html",
    "href": "cookbook/single_doc_extraction/index.html",
    "title": "Metadata Extraction",
    "section": "",
    "text": "Click through to any of these notebooks to run tools over digital libraries that extract information from papers one at a time.\nThese use cases are typically focussed on extracting experimental metadata from papers to accurately identify how general parameters are set in an experimental protocol.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nCellXGene Datasets\n\n\nCollaboration work to enable curation of data into the CellXGene system\n\n\n\n\nCryoET\n\n\nMethods to extract metadata and study the structure of scientific protocols based on all available online data and knowledge.\n\n\n\n\nNCATS Natural History Studies\n\n\nUsing Alhazen to extract information from Natural History Studies.\n\n\n\n\nRNAquarium\n\n\nUsing LLMs to extract information from RNA studies in Zebrafish\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Metadata Extraction"
    ]
  },
  {
    "objectID": "cookbook/key_opinion_leaders/african_microscopy.html",
    "href": "cookbook/key_opinion_leaders/african_microscopy.html",
    "title": "African Microscopy",
    "section": "",
    "text": "from alhazen.aliases import *\nfrom alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'african_microscopy'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n    \nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nDatabase has been backed up to /users/gully.burns/alhazen/em_tech/backup2024-02-12-13-24-55.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 311/311 [00:00&lt;00:00, 4023.60it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAVAILABLE TOOLS\n    AddCollectionFromEPMCTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtractionTool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    CheckExpressionTool\n    IntrospectionTool",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Key Opinion Leaders",
      "African Microscopy"
    ]
  },
  {
    "objectID": "cookbook/key_opinion_leaders/african_microscopy.html#preliminaries",
    "href": "cookbook/key_opinion_leaders/african_microscopy.html#preliminaries",
    "title": "African Microscopy",
    "section": "",
    "text": "from alhazen.aliases import *\nfrom alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'african_microscopy'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n    \nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nDatabase has been backed up to /users/gully.burns/alhazen/em_tech/backup2024-02-12-13-24-55.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 311/311 [00:00&lt;00:00, 4023.60it/s]\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAVAILABLE TOOLS\n    AddCollectionFromEPMCTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtractionTool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    CheckExpressionTool\n    IntrospectionTool",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Key Opinion Leaders",
      "African Microscopy"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/rare_disease_literature.html",
    "href": "cookbook/trend_analysis/rare_disease_literature.html",
    "title": "Rare Disease Literature.",
    "section": "",
    "text": "Here we set up libraries and methods to create and query the local Postgres database we will be using to store our information from the Alhazen tools and agent\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool\nfrom alhazen.tools.paperqa_emulation_tool import *\nfrom alhazen.toolkit import *\n\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_openai import ChatOpenAI\nfrom langchain_google_vertexai import ChatVertexAI\n\nfrom datetime import datetime\n\nfrom importlib_resources import files\nimport os\nimport pandas as pd\n\nfrom sqlalchemy import func, text\n\nfrom time import time\nfrom tqdm import tqdm\n\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nimport torch\nimport os\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\nfrom operator import itemgetter\nfrom langchain.chat_models import ChatOllama\nfrom langchain.schema import get_buffer_string, OutputParserException, format_document\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain_core.output_parsers import StrOutputParser, JsonOutputParser\nfrom langchain.prompts import ChatPromptTemplate, PromptTemplate\nfrom alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n\n#from paperqa.prompts import summary_prompt as paperqa_summary_prompt, qa_prompt as paperqa_qa_prompt, select_paper_prompt, citation_prompt, default_system_prompt\nfrom langchain.schema import format_document\nfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\nfrom langchain_core.runnables import RunnableParallel\nimport local_resources.queries.rao_grantees as rao_files\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the Postgres database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save files for your digital library, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'rare_as_one_diseases'\n\nRun this command to destroy your current database\nUSE WITH CAUTION\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nRun this command to create a new, empty database.\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nThis command lists all the tools the Alhazen agent system has access to\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n\nllms = lookup_chat_models()\n\n\nllm_databricks_llama3 = ChatOpenAI(base_url='https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/serving-endpoints', \n                api_key=os.environ['DATABRICKS_API_KEY'], \n                model='databricks-meta-llama-3-70b-instruct')\n\n\nllm_dbrx = llms.get('gpt4_1106')\n\ncb = AlhazenAgent(db_name=db_name, agent_llm=llm_databricks_llama3, tool_llm=llm_databricks_llama3)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Rare Disease Literature."
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/rare_disease_literature.html#preliminaries",
    "href": "cookbook/trend_analysis/rare_disease_literature.html#preliminaries",
    "title": "Rare Disease Literature.",
    "section": "",
    "text": "Here we set up libraries and methods to create and query the local Postgres database we will be using to store our information from the Alhazen tools and agent\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool\nfrom alhazen.tools.paperqa_emulation_tool import *\nfrom alhazen.toolkit import *\n\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, list_databases\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_openai import ChatOpenAI\nfrom langchain_google_vertexai import ChatVertexAI\n\nfrom datetime import datetime\n\nfrom importlib_resources import files\nimport os\nimport pandas as pd\n\nfrom sqlalchemy import func, text\n\nfrom time import time\nfrom tqdm import tqdm\n\nfrom transformers import pipeline, AutoModel, AutoTokenizer\nimport torch\nimport os\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\nfrom operator import itemgetter\nfrom langchain.chat_models import ChatOllama\nfrom langchain.schema import get_buffer_string, OutputParserException, format_document\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain_core.output_parsers import StrOutputParser, JsonOutputParser\nfrom langchain.prompts import ChatPromptTemplate, PromptTemplate\nfrom alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n\n#from paperqa.prompts import summary_prompt as paperqa_summary_prompt, qa_prompt as paperqa_qa_prompt, select_paper_prompt, citation_prompt, default_system_prompt\nfrom langchain.schema import format_document\nfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\nfrom langchain_core.runnables import RunnableParallel\nimport local_resources.queries.rao_grantees as rao_files\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the Postgres database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save files for your digital library, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'rare_as_one_diseases'\n\nRun this command to destroy your current database\nUSE WITH CAUTION\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nRun this command to create a new, empty database.\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nThis command lists all the tools the Alhazen agent system has access to\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n\nllms = lookup_chat_models()\n\n\nllm_databricks_llama3 = ChatOpenAI(base_url='https://czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com/serving-endpoints', \n                api_key=os.environ['DATABRICKS_API_KEY'], \n                model='databricks-meta-llama-3-70b-instruct')\n\n\nllm_dbrx = llms.get('gpt4_1106')\n\ncb = AlhazenAgent(db_name=db_name, agent_llm=llm_databricks_llama3, tool_llm=llm_databricks_llama3)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Rare Disease Literature."
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/rare_disease_literature.html#attempting-to-reconstruct-paper-qa-pipeline-in-our-system.",
    "href": "cookbook/trend_analysis/rare_disease_literature.html#attempting-to-reconstruct-paper-qa-pipeline-in-our-system.",
    "title": "Rare Disease Literature.",
    "section": "ATTEMPTING TO RECONSTRUCT PAPER-QA PIPELINE IN OUR SYSTEM.",
    "text": "ATTEMPTING TO RECONSTRUCT PAPER-QA PIPELINE IN OUR SYSTEM.\n\nEmbed paper sections + question\nGiven the question, summarize the retrieved paper sections relative to the question\nScore and select relevant passages\nPut summaries into prompt\nGenerate answer with prompt\n\n\nos.environ['PGVECTOR_CONNECTION_STRING'] = \"postgresql+psycopg2:///\"+ldb.name\nvectorstore = PGVector.from_existing_index(\n        embedding = ldb.embed_model, \n        collection_name = 'ScienceKnowledgeItem') \nretriever = vectorstore.as_retriever(search_kwargs={'k':15, 'filter': {'skc_ids': 81}})\n#retriever = vectorstore.as_retriever()\n\n\nretriever.invoke(question)\n\n\nhum_p = '''First, read through the following JSON encoding of {k} research articles: \n\nEach document has three attributes: (A) a digital object identifier ('DOI') code, (B) a CITATION string containing the authors, publication year, title and publication location, and the (C) CONTENT field with the title and abstract of the paper.  \n\n```json:{context}```\n\nThen, generate a JSON list of summaries of each article in order to help answer the following question:\n\nQuestion: {question}\n\nDo NOT directly answer the question, instead summarize to give evidence to help answer the question. \nFocus on specific details, including numbers, equations, or specific quotes. \nReply \"Not applicable\" if text is irrelevant. \nRestrict each summary to {summary_length} words. \nAlso, provide a score from 1-10 indicating relevance to question. Do not explain your score. \n\nWrite this answer as JSON formatted output. Provide a list of {k} dict objects with the following fields: DOI, SUMMARY, RELEVANCE SCORE. \n\nDo not provide additional explanation for the answer.\nDo not include any other response other than a JSON object.\n'''\nsys_p = '''Answer in a direct and concise tone. Your audience is an expert, so be highly specific. If there are ambiguous terms or acronyms, first define them.'''\n\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"'DOI': '{ske_id}', CITATION: '{citation}', CONTENT:'{page_content}'\")\ndef combine_documents(\n    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"},{\\n\"\n):\n    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n    return '[{'+document_separator.join(doc_strings)+'}]'\n\ntemplate = ChatPromptTemplate.from_messages([\n            (\"system\", sys_p),\n            (\"human\", hum_p)])\n\nqa_chain = (\n    RunnableParallel({\n        \"k\": itemgetter(\"k\"),\n        \"question\": itemgetter(\"question\"),\n        \"summary_length\": itemgetter(\"summary_length\"),\n        \"context\": itemgetter(\"question\") | retriever | combine_documents,\n    })\n    | {\n        \"summary\": template | ChatOllama(model='mixtral') | JsonEnclosedByTextOutputParser(),\n        \"context\": itemgetter(\"context\"),\n    }\n)\n\ninput = {'question': question, 'summary_length': 1000, 'k':5}    \nout = qa_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})\nprint(json.dumps(out, indent=4))",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Rare Disease Literature."
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/index.html",
    "href": "cookbook/trend_analysis/index.html",
    "title": "Trend Analysis",
    "section": "",
    "text": "Notebooks that permit trend analyses over papers in terms of the evolution of subfields and subtypes of studies. These analyses typically act on titles, abstracts, keywords, and study taxonomies (e.g., MeSH etc).\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nCapacity Building\n\n\nHow do funders help scientists, patient groups and other stakeholders increase their capacity to influence, and accelerate scientific research?\n\n\n\n\nImage Tech Evolution\n\n\nMethods to extract and model how imaging technology evolves.\n\n\n\n\nPathogen_Landscaping\n\n\nBuilding a database indexing the literature across all bacteria, viruses, fungi, and the diseases that they cause.\n\n\n\n\nRare Disease Literature.\n\n\nApplying AI to understand trends of research in rare disease.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/imaging_technology_evolution.html",
    "href": "cookbook/trend_analysis/imaging_technology_evolution.html",
    "title": "Image Tech Evolution",
    "section": "",
    "text": "This notebook is concerned with building a digital library of publications derived from four subdisciplines of biomedical imaging:\n\nCryo-Electron Tomography\nVolume Electron Microscopy\nHiercharchy Phase Contrast Tomography\nPhotoacoustic Imaging\n\n\n\nSetting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.agent import AlhazenAgent\n\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.tools.tiab_classifier_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, restore_ceifns_database\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nfrom alhazen.utils.searchEngineUtils import load_paper_from_openalex, read_references_from_openalex \nfrom pyalex import config, Works, Work\nconfig.email = \"gully.burns@chanzuckerberg.com\"\n\nimport local_resources.queries.imaging_tech as imaging_tech\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\n\n\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'imaging_tech_innovation'\n\n\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm3 = ChatOpenAI(model='gpt-3.5-turbo') \n#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n\ncb = AlhazenAgent(llm2, llm2)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAGENT TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtraction_EverythingEverywhere_Tool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    ProcotolEntitiesExtractionTool\n    CheckExpressionTool\n    TitleAbstractClassifier_OneDocAtATime_Tool",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Image Tech Evolution"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/imaging_technology_evolution.html#modeling-technological-evolution-and-innovation",
    "href": "cookbook/trend_analysis/imaging_technology_evolution.html#modeling-technological-evolution-and-innovation",
    "title": "Image Tech Evolution",
    "section": "",
    "text": "This notebook is concerned with building a digital library of publications derived from four subdisciplines of biomedical imaging:\n\nCryo-Electron Tomography\nVolume Electron Microscopy\nHiercharchy Phase Contrast Tomography\nPhotoacoustic Imaging\n\n\n\nSetting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.agent import AlhazenAgent\n\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.tools.tiab_classifier_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, restore_ceifns_database\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\nfrom alhazen.utils.searchEngineUtils import load_paper_from_openalex, read_references_from_openalex \nfrom pyalex import config, Works, Work\nconfig.email = \"gully.burns@chanzuckerberg.com\"\n\nimport local_resources.queries.imaging_tech as imaging_tech\nfrom alhazen.utils.queryTranslator import QueryTranslator, QueryType\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\n\n\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'imaging_tech_innovation'\n\n\n\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = ChatOllama(model='mixtral:instruct') \nllm2 = ChatOpenAI(model='gpt-4-1106-preview') \nllm3 = ChatOpenAI(model='gpt-3.5-turbo') \n#llm3 = ChatVertexAI(model_name=\"gemini-pro\", convert_system_message_to_human=True)\n\ncb = AlhazenAgent(llm2, llm2)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)\n\nAGENT TOOLS\n    AddCollectionFromEPMCTool\n    AddAuthorsToCollectionTool\n    DescribeCollectionCompositionTool\n    DeleteCollectionTool\n    RetrieveFullTextTool\n    RetrieveFullTextToolForACollection\n    MetadataExtraction_EverythingEverywhere_Tool\n    SimpleExtractionWithRAGTool\n    PaperQAEmulationTool\n    ProcotolEntitiesExtractionTool\n    CheckExpressionTool\n    TitleAbstractClassifier_OneDocAtATime_Tool",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Image Tech Evolution"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/imaging_technology_evolution.html#building-the-database",
    "href": "cookbook/trend_analysis/imaging_technology_evolution.html#building-the-database",
    "title": "Image Tech Evolution",
    "section": "Building the database",
    "text": "Building the database\n\nScripts to Build / Delete the database\nIf you need to restore a deleted database from backup, use the following shell commands:\n$ createdb em_tech\n$ psql -d em_tech -f /local/file/path/em_tech/backup&lt;date_time&gt;.sql\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nDatabase has been backed up to /users/gully.burns/alhazen/imaging_tech_innovation/backup2024-03-04-23-34-21.sql\nDatabase has been dropped successfully !!\n\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n100%|██████████| 310/310 [00:00&lt;00:00, 3467.45it/s]\n\n\n\n\nBuild CEIFNS database from queries\n\nRun queries on European PMC based on innovation categories\nHere we build general corpora across the categories of interest.\n\nHierarchical phase-contrast tomography\nCryo-Electron Tomography\nVolume Electron Microscopy\nPhotoacoustic imaging\n\n\ncols_to_include = ['ID', 'CORPUS_NAME', 'QUERY']\ndf = pd.read_csv(files(imaging_tech).joinpath('imaging_tech.tsv'), sep='\\t', )\ndf = df.drop(columns=[c for c in df.columns if c not in cols_to_include])\ndf\n\n\n\n\n\n\n\n\nID\nCORPUS_NAME\nQUERY\n\n\n\n\n0\n1\nHierarchical phase-contrast tomography\nHierarchical phase-contrast tomography | HIP-C...\n\n\n1\n2\nCryo-Electron Tomography\nCryoelectron Tomography | Cryo Electron Tomogr...\n\n\n2\n3\nVolume Electron Microscopy\nVolume Electron Microscopy | Volume EM | (seri...\n\n\n3\n4\nPhotoacoustic imaging\nPhotoacoustic imaging | Photoacoustic microscopy\n\n\n\n\n\n\n\n\nqt = QueryTranslator(df.sort_values('ID'), 'ID', 'QUERY', 'CORPUS_NAME')\n(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=['TITLE_ABS', 'METHODS'])\ncorpus_names = df['CORPUS_NAME']\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nfor (id, name, query) in zip(corpus_ids, corpus_names, epmc_queries):\n    addEMPCCollection_tool.run(tool_input={'id': id, 'name':name, 'query':query, 'full_text':False})\n\n100%|██████████| 4/4 [00:00&lt;00:00, 7533.55it/s]\n100%|██████████| 4/4 [00:00&lt;00:00, 3442.19it/s]\n100%|██████████| 1/1 [00:03&lt;00:00,  3.54s/it]\n100%|██████████| 135/135 [00:00&lt;00:00, 507.06it/s]\n100%|██████████| 3/3 [00:55&lt;00:00, 18.45s/it]\n100%|██████████| 2558/2558 [00:06&lt;00:00, 375.25it/s]\n100%|██████████| 7/7 [02:28&lt;00:00, 21.18s/it]\n100%|██████████| 6820/6820 [00:41&lt;00:00, 164.39it/s]\n100%|██████████| 5/5 [00:59&lt;00:00, 11.84s/it]\n100%|██████████| 4478/4478 [00:17&lt;00:00, 257.83it/s]\n\n\nhttps://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize=1000&synonym=TRUE&resultType=core&query=((TITLE_ABS:\"Hierarchical phase-contrast tomography\" OR METHODS:\"Hierarchical phase-contrast tomography\") OR (TITLE_ABS:\"HIP-CT\" OR METHODS:\"HIP-CT\") OR (TITLE_ABS:\"Hierarchical phase contrast tomography\" OR METHODS:\"Hierarchical phase contrast tomography\")), 143 European PMC PAPERS FOUND\n Returning 135\nhttps://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize=1000&synonym=TRUE&resultType=core&query=((TITLE_ABS:\"Cryoelectron Tomography\" OR METHODS:\"Cryoelectron Tomography\") OR (TITLE_ABS:\"Cryo Electron Tomography\" OR METHODS:\"Cryo Electron Tomography\") OR (TITLE_ABS:\"Cryo-Electron Tomography\" OR METHODS:\"Cryo-Electron Tomography\") OR (TITLE_ABS:\"Cryo-ET\" OR METHODS:\"Cryo-ET\") OR (TITLE_ABS:\"CryoET\" OR METHODS:\"CryoET\")), 2581 European PMC PAPERS FOUND\n Returning 2558\nhttps://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize=1000&synonym=TRUE&resultType=core&query=((TITLE_ABS:\"Volume Electron Microscopy\" OR METHODS:\"Volume Electron Microscopy\") OR (TITLE_ABS:\"Volume EM\" OR METHODS:\"Volume EM\") OR (TITLE_ABS:\"multibeam SEM\" OR METHODS:\"multibeam SEM\") OR (TITLE_ABS:\"FAST-SEM\" OR METHODS:\"FAST-SEM\") OR ((TITLE_ABS:\"serial section\" OR METHODS:\"serial section\") AND ((TITLE_ABS:\"electron microscopy\" OR METHODS:\"electron microscopy\") OR (TITLE_ABS:\"EM\" OR METHODS:\"EM\") OR (TITLE_ABS:\"transmission electron microscopy\" OR METHODS:\"transmission electron microscopy\") OR (TITLE_ABS:\"TEM\" OR METHODS:\"TEM\") OR (TITLE_ABS:\"scanning electron microscopy\" OR METHODS:\"scanning electron microscopy\") OR (TITLE_ABS:\"SEM\" OR METHODS:\"SEM\") OR (TITLE_ABS:\"electron tomography\" OR METHODS:\"electron tomography\"))) OR ((TITLE_ABS:\"serial block-face\" OR METHODS:\"serial block-face\") AND ((TITLE_ABS:\"scanning electron microscopy\" OR METHODS:\"scanning electron microscopy\") OR (TITLE_ABS:\"SEM\" OR METHODS:\"SEM\"))) OR ((TITLE_ABS:\"focused ion beam\" OR METHODS:\"focused ion beam\") AND ((TITLE_ABS:\"scanning electron microscopy\" OR METHODS:\"scanning electron microscopy\") OR (TITLE_ABS:\"SEM\" OR METHODS:\"SEM\"))) OR ((TITLE_ABS:\"automated serial\" OR METHODS:\"automated serial\") AND ((TITLE_ABS:\"transmission electron microscopy\" OR METHODS:\"transmission electron microscopy\") OR (TITLE_ABS:\"TEM\" OR METHODS:\"TEM\"))) OR ((TITLE_ABS:\"massively parallel imaging\" OR METHODS:\"massively parallel imaging\") AND ((TITLE_ABS:\"scanning electron microscopy\" OR METHODS:\"scanning electron microscopy\") OR (TITLE_ABS:\"SEM\" OR METHODS:\"SEM\")))), 6891 European PMC PAPERS FOUND\n Returning 6820\nhttps://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize=1000&synonym=TRUE&resultType=core&query=((TITLE_ABS:\"Photoacoustic imaging\" OR METHODS:\"Photoacoustic imaging\") OR (TITLE_ABS:\"Photoacoustic microscopy\" OR METHODS:\"Photoacoustic microscopy\")), 4600 European PMC PAPERS FOUND\n Returning 4478\n\n\n\n\nRun queries on known lists of papers from CZI grantees on the four imaging innovation categories\nHere we seach pre-developed lists of papers from CZI grantee’s work, indexed in a local file: ./local_resources/queries/imaging_tech/grantee_dois.json\n\nwith open(files(imaging_tech).joinpath('grantee_dois.json'), 'r') as f:\n    dict_lists = json.load(f)\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nfor i, k in enumerate(dict_lists.keys()):\n    query = ' OR '.join(['doi:\"'+d_id+'\"' for d_id in dict_lists[k] ])\n    print('%s: Searching for %d'%(k, len(dict_lists[k])))\n    addEMPCCollection_tool.run(tool_input={'id': str(5+i), 'name': k + ' (grantees)', 'query':query})\n\nCryo-Electron Tomography: Searching for 23\nVolume Electron Microscopy: Searching for 12\nHierarchical phase-contrast tomography: Searching for 14\nPhotoacoustic imaging: Searching for 26",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Image Tech Evolution"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/imaging_technology_evolution.html#analyze-collections",
    "href": "cookbook/trend_analysis/imaging_technology_evolution.html#analyze-collections",
    "title": "Image Tech Evolution",
    "section": "Analyze Collections",
    "text": "Analyze Collections\n\nq = ldb.session.query(SKC.id, SKC.name, SKE.id, SKI.type) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \ndf = pd.DataFrame(q.all(), columns=['id', 'collection name', 'doi', 'item type'])    \ndf.pivot_table(index=['id', 'collection name'], columns='item type', values='doi', aggfunc=lambda x: len(x.unique()))\n\n\n\n\n\n\n\n\nitem type\nCitationRecord\n\n\nid\ncollection name\n\n\n\n\n\n0\nImaging Program\n752\n\n\n1\nHierarchical phase-contrast tomography\n135\n\n\n2\nCryo-Electron Tomography\n2556\n\n\n3\nVolume Electron Microscopy\n6817\n\n\n4\nPhotoacoustic imaging\n4477\n\n\n5\nCryo-Electron Tomography (grantees)\n20\n\n\n6\nVolume Electron Microscopy (grantees)\n11\n\n\n7\nHierarchical phase-contrast tomography (grantees)\n12\n\n\n8\nPhotoacoustic imaging (grantees)\n23\n\n\n\n\n\n\n\n\ncb.agent_executor.invoke({'input':'Drop collection with id=\"0\"'})\n\n\n\n&gt; Entering new AgentExecutor chain...\n{\n    \"action\": \"delete_collection\",\n    \"action_input\": {\n        \"collection_id\": \"0\"\n    }\n}{'response': 'Successfully deleted a collection with collection_id:`0`.'}\n\n\n&gt; Finished chain.\n\n\n{'input': 'Drop collection with id=\"0\"',\n 'output': {'response': 'Successfully deleted a collection with collection_id:`0`.'},\n 'intermediate_steps': [(AgentAction(tool='delete_collection', tool_input={'collection_id': '0'}, log='{\\n    \"action\": \"delete_collection\",\\n    \"action_input\": {\\n        \"collection_id\": \"0\"\\n    }\\n}'),\n   {'response': 'Successfully deleted a collection with collection_id:`0`.'})]}\n\n\n\nwith open(files(imaging_tech).joinpath('dois.txt'), 'r') as f:\n    dois = f.readlines()\ndois = [d.strip() for d in dois]\nprint(len(dois))\n\n806\n\n\n\nldb.add_collection_from_dois_using_openalex('0', 'Imaging Program', dois, commit_this=True)\n\n100%|██████████| 806/806 [04:04&lt;00:00,  3.30it/s]\n\n\n\n# Run local analysis on program data\n\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\nstep = 40\nfor start_i in range(0, len(dois), step):\n    query = ' OR '.join(['doi:\\\"'+dois[i].lower()+'\\\"' for i in range(start_i, start_i+step) if i &lt; len(dois)])\n    addEMPCCollection_tool.run({'id': '0', 'name':'Imaging Program', 'query':query, 'full_text':True})\n\n\nmissing_dois = []\nfor doi in dois:\n    c = ldb.session.query(func.count(SKE.id)).filter(SKE.id=='doi:'+doi.lower()).first()\n    if c[0] == 0:\n        missing_dois.append(doi)\nprint('Missing %d DOIs'%(len(missing_dois)))\nprint(missing_dois)\n\nMissing 34 DOIs\n['10.3389%2Ffmed.2022.849677', '10.48550/arXiv.2210.04033', '10.3389%2Ffnins.2023.1135494', '10.48550/arXiv.2308.00870', '10.48550/arXiv.2307.14572', '10.48550/arXiv.2107.09145', '10.1038/s41598-021-94852-4', '10.1172%2Fjci.insight.142945', '10.1016/j.biopha.2022', '10.6084/m9.figshare.12758957.v1', '10.6084/m9.figshare.12299915.v2', '10.1126/sciadv.aaz2598.', '10.5281/ZENODO.3901011', '10.1152/ajpendo.00501.2018.', '10.48550/arXiv.2008.00807', '10.1136/ jitc-2022-006133', '10.1101/2020.05.27/119750', '10.1038/s41592-021- 01156-w', '0.1021/acschembio.0c00988', '10.1126/sciimmunol.abm693', '10.1038/s41598-021-85036-w', '10.1016%2Fj.ijcha.2020.100672', '10.1017/S2633903X2300003X[Opens in a new window]', '10.1016%2Fj.pacs.2021.100276', '10.1117%2F1.JBO.29.S1.S11521', '10.22443/rms.mmc2023.274', '10.5281/zenodo.10200758', '10.48550/arXiv.2306.15898', '10.48550/arXiv.2311.13417', '10.5281/zenodo.10451511', '10.5281/zenodo.10685021', '10.5281/zenodo.10057023', '10.5281/zenodo.10591803', '10.5281/zenodo.10591588']\n\n\n\nstep = 40\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\n\nfor start_i in range(0, len(missing_dois), step):\n    query = ' OR '.join(['doi:\\\"'+missing_dois[i].lower()+'\\\"' for i in range(start_i, start_i+step) if i &lt; len(missing_dois)])\n    addEMPCCollection_tool.run({'id': '0', 'name':'Imaging Program', 'query':query, 'full_text':True})\n\nhttps://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize=1000&synonym=TRUE&resultType=core&query=doi:\"10.3389%2ffmed.2022.849677\" OR doi:\"10.48550/arxiv.2210.04033\" OR doi:\"10.3389%2ffnins.2023.1135494\" OR doi:\"10.48550/arxiv.2308.00870\" OR doi:\"10.48550/arxiv.2307.14572\" OR doi:\"10.48550/arxiv.2107.09145\" OR doi:\"10.1038/s41598-021-94852-4\" OR doi:\"10.1172%2fjci.insight.142945\" OR doi:\"10.1016/j.biopha.2022\" OR doi:\"10.6084/m9.figshare.12758957.v1\" OR doi:\"10.6084/m9.figshare.12299915.v2\" OR doi:\"10.1126/sciadv.aaz2598.\" OR doi:\"10.5281/zenodo.3901011\" OR doi:\"10.1152/ajpendo.00501.2018.\" OR doi:\"10.48550/arxiv.2008.00807\" OR doi:\"10.1136/ jitc-2022-006133\" OR doi:\"10.1101/2020.05.27/119750\" OR doi:\"10.1038/s41592-021- 01156-w\" OR doi:\"0.1021/acschembio.0c00988\" OR doi:\"10.1126/sciimmunol.abm693\" OR doi:\"10.1038/s41598-021-85036-w\" OR doi:\"10.1016%2fj.ijcha.2020.100672\" OR doi:\"10.1017/s2633903x2300003x[opens in a new window]\" OR doi:\"10.1016%2fj.pacs.2021.100276\" OR doi:\"10.1117%2f1.jbo.29.s1.s11521\" OR doi:\"10.22443/rms.mmc2023.274\" OR doi:\"10.5281/zenodo.10200758\" OR doi:\"10.48550/arxiv.2306.15898\" OR doi:\"10.48550/arxiv.2311.13417\" OR doi:\"10.5281/zenodo.10451511\" OR doi:\"10.5281/zenodo.10685021\" OR doi:\"10.5281/zenodo.10057023\" OR doi:\"10.5281/zenodo.10591803\" OR doi:\"10.5281/zenodo.10591588\", 6 European PMC PAPERS FOUND\n Returning 6\n\n\n100%|██████████| 1/1 [00:00&lt;00:00,  1.01it/s]\n100%|██████████| 6/6 [00:00&lt;00:00, 168.60it/s]",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Image Tech Evolution"
    ]
  },
  {
    "objectID": "cookbook/trend_analysis/imaging_technology_evolution.html#run-llm-over-each-paper-in-imaging-program-collection-to-determine-if-papers-are-methods-or-applications",
    "href": "cookbook/trend_analysis/imaging_technology_evolution.html#run-llm-over-each-paper-in-imaging-program-collection-to-determine-if-papers-are-methods-or-applications",
    "title": "Image Tech Evolution",
    "section": "Run LLM over each paper in Imaging Program collection to determine if papers are methods or applications",
    "text": "Run LLM over each paper in Imaging Program collection to determine if papers are methods or applications\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '0', 'classification_type':'is_method_paper'})\n\n\ncollection_id  = '0'\nclassification_type = 'is_methods_paper'\nq = ldb.session.query(SKE, N) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKE.id==SKE_HR.ScientificKnowledgeExpression_id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \\\n        .filter(SKE_HN.ScientificKnowledgeExpression_id==SKE.id) \\\n        .filter(SKE_HN.has_notes_id==N.id) \\\n        .filter(SKE_HR.has_representation_id==SKI.id) \\\n        .filter(SKC.id==collection_id) \\\n        .filter(N.type=='TiAbClassificationNote__'+classification_type) \nl = []\nfor e,n in q.all():\n    c = json.loads(n.content)\n    doi_link = 'https://doi.org/'+e.id[4:]\n    l.append((doi_link, e.type, e.content, c.get('is_method_paper'), c.get('explanation')))\ndf = pd.DataFrame(l, columns=['doi', 'paper_type', 'citation', 'is_methods_paper', 'explanation']) \n#df.to_csv(loc+db_name+'/imaging_cohort_methods.tsv', sep='\\t', index=False)  \ndf\n\n\n\n\n\n\n\n\ndoi\npaper_type\ncitation\nis_methods_paper\nexplanation\n\n\n\n\n0\nhttps://doi.org/10.1073/pnas.2301852120\nScientificPrimaryResearchArticle\nLucas BA, Grigorieff N. (2023) Quantification ...\nTrue\nThe main goal of the paper is to develop and t...\n\n\n1\nhttps://doi.org/10.1016/j.ultramic.2023.113730\nScientificPrimaryResearchArticle\nAxelrod JJ, Petrov PN, Zhang JT, Remis J, Buij...\nTrue\nThe main goal of the paper is to identify and ...\n\n\n2\nhttps://doi.org/10.1021/acs.jpcb.2c08995\nScientificPrimaryResearchArticle\nSartor AM, Dahlberg PD, Perez D, Moerner WE. (...\nTrue\nThe main goal of the paper is to characterize ...\n\n\n3\nhttps://doi.org/10.1101/2023.02.12.528160\nScientificPrimaryResearchPreprint\nAxelrod JJ, Petrov PN, Zhang JT, Remis J, Buij...\nTrue\nThe paper is concerned with developing new tec...\n\n\n4\nhttps://doi.org/10.1016/j.jsb.2023.107941\nScientificPrimaryResearchArticle\nDu DX, Simjanoska M, Fitzpatrick AWP. (2023) F...\nTrue\nThe main goal of the paper is to combine two e...\n\n\n...\n...\n...\n...\n...\n...\n\n\n763\nhttps://doi.org/10.1016/b978-0-12-420138-5.000...\nScientificReviewArticle\nLambert TJ, Waters JC. (2014) Assessing camera...\nTrue\nThe main goal of the paper is to assess and me...\n\n\n764\nhttps://doi.org/10.1016/b978-0-12-420138-5.000...\nScientificPrimaryResearchArticle\nPetrak LJ, Waters JC. (2014) A practical guide...\nTrue\nThe main goal of the paper is to provide a pra...\n\n\n765\nhttps://doi.org/10.1016/b978-0-12-420138-5.000...\nScientificPrimaryResearchArticle\nWaters JC, Wittmann T. (2014) Concepts in quan...\nTrue\nThe main goal of the paper is to discuss conce...\n\n\n766\nhttps://doi.org/10.1016/b978-0-12-407761-4.000...\nScientificPrimaryResearchArticle\nWaters JC. (2013) Live-cell fluorescence imaging.\nTrue\nThe main goal of the paper is to optimize live...\n\n\n767\nhttps://doi.org/10.1016/b978-0-12-407761-4.000...\nScientificPrimaryResearchArticle\nSalmon ED, Shaw SL, Waters JC, Waterman-Storer...\nTrue\nThe main goal of the paper is to describe the ...\n\n\n\n\n768 rows × 5 columns",
    "crumbs": [
      "Get Started",
      "Example Notebooks",
      "Trend Analysis",
      "Image Tech Evolution"
    ]
  },
  {
    "objectID": "cookbook/personal_use/machine_learning_in_biology copy.html",
    "href": "cookbook/personal_use/machine_learning_in_biology copy.html",
    "title": "General Workbook",
    "section": "",
    "text": "Note - this question is inherently driven by discussion and informal experience (as opposed to formal experimentation). So we would expect to"
  },
  {
    "objectID": "cookbook/personal_use/machine_learning_in_biology copy.html#preliminaries",
    "href": "cookbook/personal_use/machine_learning_in_biology copy.html#preliminaries",
    "title": "General Workbook",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nfrom alhazen.apps.chat import  AlhazenAgentChatBot\nfrom alhazen.core import get_langchain_chatmodel, MODEL_TYPE\nfrom alhazen.schema_sqla import *\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \nfrom alhazen.toolkit import AlhazenToolkit\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import create_engine, exists, func\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport yaml\n\n\n# Using Aliases like this massively simplifies the use of SQLAlchemy\nIR = aliased(InformationResource)\n\nSKC = aliased(ScientificKnowledgeCollection)\nSKC_HM = aliased(ScientificKnowledgeCollectionHasMembers)\nSKE = aliased(ScientificKnowledgeExpression)\nSKE_XREF = aliased(ScientificKnowledgeExpressionXref)\nSKE_IRI = aliased(ScientificKnowledgeExpressionIri)\nSKE_HR = aliased(ScientificKnowledgeExpressionHasRepresentation)\nSKE_MO = aliased(ScientificKnowledgeExpressionMemberOf)\nSKI = aliased(ScientificKnowledgeItem)\nSKI_HP = aliased(ScientificKnowledgeItemHasPart)\nSKF = aliased(ScientificKnowledgeFragment)\n\nN = aliased(Note)\nNIA = aliased(NoteIsAbout)\nSKC_HN = aliased(ScientificKnowledgeCollectionHasNotes)\nSKE_HN = aliased(ScientificKnowledgeExpressionHasNotes)\nSKI_HN = aliased(ScientificKnowledgeItemHasNotes)\nSKF_HN = aliased(ScientificKnowledgeFragmentHasNotes)\n\nRemember to set environmental variables for this code:\n\nALHAZEN_DB_NAME - the name of the PostGresQL database you are storing information into\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\n\nos.environ['ALHAZEN_DB_NAME'] = 'machine_learning_and_biology'\nos.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])\n\nif os.environ.get('ALHAZEN_DB_NAME') is None: \n    raise Exception('Which database do you want to use for this application?')\ndb_name = os.environ['ALHAZEN_DB_NAME']\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nloc = os.environ['LOCAL_FILE_PATH']\n\n\ndrop_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\ncreate_ceifns_database(os.environ['ALHAZEN_DB_NAME'])\n\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllm = get_langchain_chatmodel(model_type=MODEL_TYPE.Ollama, llm_name='mixtral:instruction')\ncb = AlhazenAgentChatBot()\n\nprint('AVAILABLE TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)"
  },
  {
    "objectID": "docnb2_architecture.html",
    "href": "docnb2_architecture.html",
    "title": "Platform Architecture",
    "section": "",
    "text": "The general architecture of the Alhazen toolset is based on the following high-level workflow:\n\n\n\nFig1: Overall system architecture of the Alhazen platform.\n\n\nThe system’s functionality is based on applying LLM-based technology to scientific knowledge gathered from available resources on the web. This uses a local database to store information downloaded from the web as Collections (named corpora), Expressions (references to scientific information entities); Items (copies of the information entities themselves); and Fragments(excerpts of the information entities indexed back into the original items) - see documentation on the CEIFiNS Database for additional detail.\nThis initial system provides function calls for running either a single logical query or a collection of queries on the European PMC system and building a local database on top of that.\nThis data can then be queried and analyzed with LLM-based tools and methods which in turn records the results of those analyses in the database as Notes which in turn can be analysed to generate Summaries.",
    "crumbs": [
      "Get Started",
      "Platform Architecture"
    ]
  },
  {
    "objectID": "utils_undocumented/query_translator.html",
    "href": "utils_undocumented/query_translator.html",
    "title": "Query Translation Tools",
    "section": "",
    "text": "source\n\nQueryTranslator\n\n QueryTranslator (df, id_col, query_col, name_col, notes_col=None)\n\nThis class allows a user to define a set of logical boolean queries in a Pandas dataframe and then convert them to a variety of formats for use on various online API systems.\nFunctionality includes:\n\nSpecify queries as a table using ‘|’ and ‘&’ symbols\ngenerate search strings to be used in API calls for PMID, SOLR, and European PMC\n\nAttributes:\n\ndf: The dataframe of queries to be processed (note: this dataframe must have a numerical ID column specified)\nquery_col: the column in the data frame where the query is specified\n\n\nsource\n\n\nQueryType\n\n QueryType (value, names=None, module=None, qualname=None, type=None,\n            start=1, boundary=None)\n\nAn enumeration that permits conversion of complex boolean queries to different formats"
  },
  {
    "objectID": "utils_undocumented/airtable_utils.html",
    "href": "utils_undocumented/airtable_utils.html",
    "title": "Airtable Utilities",
    "section": "",
    "text": "source\n\nAirtableUtils\n\n AirtableUtils (api_key)\n\nThis class permits simple input / output from airtable\nAttributes: * api_key: an API key obtained from Airtable to provide authentication"
  },
  {
    "objectID": "tutorials/cryoet_tutorial.html",
    "href": "tutorials/cryoet_tutorial.html",
    "title": "CryoET Tutorial",
    "section": "",
    "text": "Cryo-electron Tomography (CryoET) involves rapidly freezing biological samples in their natural state to preserve their three-dimensional structure without the need for staining or crystallization. This methodology allows researchers to visualize proteins and other biomolecules at near-atomic resolution.\nThis digital library is based on capturing all papers that mention the technique in their titles, abstracts, or methods sections and then analyzing the various methods used and their applications. Our focus is on supporting the work of the Chan Zuckerberg Imaging Institute, CZII on developing the CryoET data portal, an open source repository for CryoET-based data.",
    "crumbs": [
      "Get Started",
      "Tutorial Notebooks",
      "CryoET Tutorial"
    ]
  },
  {
    "objectID": "tutorials/cryoet_tutorial.html#introduction-to-cryoet",
    "href": "tutorials/cryoet_tutorial.html#introduction-to-cryoet",
    "title": "CryoET Tutorial",
    "section": "",
    "text": "Cryo-electron Tomography (CryoET) involves rapidly freezing biological samples in their natural state to preserve their three-dimensional structure without the need for staining or crystallization. This methodology allows researchers to visualize proteins and other biomolecules at near-atomic resolution.\nThis digital library is based on capturing all papers that mention the technique in their titles, abstracts, or methods sections and then analyzing the various methods used and their applications. Our focus is on supporting the work of the Chan Zuckerberg Imaging Institute, CZII on developing the CryoET data portal, an open source repository for CryoET-based data.",
    "crumbs": [
      "Get Started",
      "Tutorial Notebooks",
      "CryoET Tutorial"
    ]
  },
  {
    "objectID": "tutorials/cryoet_tutorial.html#basics",
    "href": "tutorials/cryoet_tutorial.html#basics",
    "title": "CryoET Tutorial",
    "section": "Basics",
    "text": "Basics\n\nPython Imports\nSetting python imports, environment variables, and other crucial set up parameters here.\n\nfrom alhazen.aliases import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.agent import AlhazenAgent\nfrom alhazen.schema_sqla import *\nfrom alhazen.core import lookup_chat_models\nfrom alhazen.tools.basic import AddCollectionFromEPMCTool, DeleteCollectionTool\nfrom alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\nfrom alhazen.tools.metadata_extraction_tool import * \nfrom alhazen.tools.protocol_extraction_tool import *\nfrom alhazen.tools.tiab_classifier_tool import *\nfrom alhazen.tools.tiab_extraction_tool import *\nfrom alhazen.tools.tiab_mapping_tool import *\nfrom alhazen.toolkit import *\nfrom alhazen.utils.jats_text_extractor import NxmlDoc\n\nfrom alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database, backup_ceifns_database, list_databases\n\nfrom alhazen.utils.searchEngineUtils import *\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_openai import ChatOpenAI\n\nimport nltk\nnltk.download('punkt')\n\nfrom bs4 import BeautifulSoup,Tag,Comment,NavigableString\nfrom databricks import sql\nfrom datetime import datetime\nfrom importlib_resources import files\nimport json\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport requests\n\nfrom sqlalchemy import text, create_engine, exists, func, or_, and_, not_, desc, asc\nfrom sqlalchemy.orm import sessionmaker, aliased\n\nfrom time import time,sleep\nfrom tqdm import tqdm\nfrom urllib.request import urlopen\nfrom urllib.parse import quote_plus, quote, unquote\nfrom urllib.error import URLError, HTTPError\nimport uuid\nimport yaml\n\n\nimport pymde\n\n\nengine = create_engine(\"postgresql+psycopg2://%s:%s@%s:5432/%s\"%(os.environ['POSTGRES_USER'], os.environ['POSTGRES_PASSWORD'], os.environ['POSTGRES_HOST'], 'postgres'))\nconnection = engine.connect()\nresult = connection.execute(text(\"SELECT datname FROM pg_database;\"))\ndbn = [row[0] for row in result if row[0] != 'postgres']\nconnection.close()\ndbn\n\n\n\nEnvironment Variables\nYou must set the following environmental variables for this code:\n\nLOCAL_FILE_PATH - the location on disk where you save temporary files, downloaded models or other data.\n\nNote that this notebook will build and use a database specified as cryoet_tutorial, specified below\n\nif os.environ.get('LOCAL_FILE_PATH') is None: \n    raise Exception('Where are you storing your local literature database?')\nif os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n    os.makedirs(os.environ['LOCAL_FILE_PATH'])    \n\nloc = os.environ['LOCAL_FILE_PATH']\ndb_name = 'cryoet'\n\n# Variable to prevent accidental deletion of the database or any records\nOK_TO_DELETE = False\n\n\n\nSetup utils, agents, and tools\nThis cell sets up a database engine (ldb) and lists the available large-language models you can use.\n\nldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\nllms_lookup = lookup_chat_models()\nprint(llms_lookup.keys())\n\nThis cell initiates an AlhazenAgent that you can use to run tools or execute commands over.\n\nllm = llms_lookup.get('databricks_llama3')\n\ncb = AlhazenAgent(llm, llm, db_name=db_name)\nprint('AGENT TOOLS')\nfor t in cb.tk.get_tools():\n    print('\\t'+type(t).__name__)",
    "crumbs": [
      "Get Started",
      "Tutorial Notebooks",
      "CryoET Tutorial"
    ]
  },
  {
    "objectID": "tutorials/cryoet_tutorial.html#building-the-database",
    "href": "tutorials/cryoet_tutorial.html#building-the-database",
    "title": "CryoET Tutorial",
    "section": "Building the database",
    "text": "Building the database\n\nScripts to Build / Delete the database\nIf you need to restore a deleted database from backup, use the following shell commands:\n$ createdb em_tech\n$ psql -d em_tech -f /local/file/path/em_tech/backup&lt;date_time&gt;.sql\nThis command will delete your existing database (but will also store a copy).\n\nif OK_TO_DELETE:\n    drop_ceifns_database(db_name, backupFirst=True)\n\nThis command will backup your current database\n\nif OK_TO_DELETE:\n    current_date_time = datetime.now()\n    formatted_date_time = f'{current_date_time:%Y-%m-%d-%H-%M-%S}'\n    backup_path = loc+'/'+db_name+'/backup'+formatted_date_time+'.sql'\n    backup_ceifns_database(db_name, backup_path)\n\nThis command will create a new, fresh, empty copy of your database.\n\ncreate_ceifns_database(db_name)\n\n\nos.environ['POSTGRES_HOST']\n\n\nlist_databases()\n\n\n\nBuild CEIFNS database from queries\n\nAdd a collection of all CryoET papers based on a query\nThis runs a query on European PMC for terms + synonyms related to Cryo Electron Tomography\n\ncryoet_query = '''\n(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet)\n'''\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\naddEMPCCollection_tool.run(tool_input={'id': '1', \n                                       'name': 'CryoET Papers', \n                                       'query': cryoet_query})\n\n\nl = []\nq = ldb.session.query(SKE) \noutput = []        \nfor ske in q.all():\n    l.append(ske)\nprint(len(l))\n\n\n\nAdding Machine Learning also from a query\n\nml_query = '''\n(\"Cryoelectron Tomography\" OR \"Cryo Electron Tomography\" OR \"Cryo-Electron Tomography\" OR\n    \"Cryo-ET\" OR \"CryoET\" OR \"Cryoelectron Tomography\" OR \"cryo electron tomography\" or \n    \"cryo-electron tomography\" OR \"cryo-et\" OR cryoet ) AND \n(\"Machine Learning\" OR \"Artificial Intelligence\" OR \"Deep Learning\" OR \"Neural Networks\")\n'''\naddEMPCCollection_tool = [t for t in cb.tk.get_tools() if isinstance(t, AddCollectionFromEPMCTool)][0]\naddEMPCCollection_tool.run(tool_input={'id': '2', \n                                       'name': 'Machine Learning in CryoET', \n                                       'query': ml_query, \n                                       'full_text': False})\n\n\n\nCreates a new collection of randomly sampled papers to showcase full-text download capability\n\nldb.create_new_collection_from_sample('3', 'CryoET Papers Tests', '1', 20, ['ScientificPrimaryResearchArticle', 'ScientificPrimaryResearchPreprint'])",
    "crumbs": [
      "Get Started",
      "Tutorial Notebooks",
      "CryoET Tutorial"
    ]
  },
  {
    "objectID": "tutorials/cryoet_tutorial.html#analyze-collections",
    "href": "tutorials/cryoet_tutorial.html#analyze-collections",
    "title": "CryoET Tutorial",
    "section": "Analyze Collections",
    "text": "Analyze Collections\n\nSurvey + Run Classifications over Papers\nThis invoke the following classification process on the paper (defined in the prompt definition in ./local_resources/prompts/tiab_prompts):\n\nA - Structural descriptions of Viral Pathogens (such as HIV, Influenza, SARS-CoV-2, etc.)\nB - Studies of mutated protein structures associated with disease (such as Alzheimer’s, Parkinson’s, etc.)\nC - Structural studies of bacterial pathogens (such as E. coli, Salmonella, etc.)\nD - Structural studies of plant cells\nE - Structural studies of material science of non-biological samples\nF - Structural studies of transporters or transport mechanisms within cells, studies involving the cytoskeleton or active transport processes.\nG - Structural studies of synapses or other mechansism of releasing vesicles over the plasma membrane\nH - Structural studies of any other organelle or structured component of a cell.\nI - Studies of dynamic biological processes at a cellular level (such as cell division, cell migration, etc.)\nJ - Studies of dynamics of molecular interactions within a cell.\n\nK - Development of new CryoET imaging methods (including grid preparation techniques, such as lift-out).\nL - Development of new data analysis methods (including machine learning, segmentation, point-picking, object recognition, or reconstruction).\n\n\nt = [t for t in cb.tk.get_tools() if isinstance(t, TitleAbstractClassifier_OneDocAtATime_Tool)][0]\nt.run({'collection_id': '3', 'classification_type':'cryoet_study_types', 'repeat_run':True})\n\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nif OK_TO_DELETE:        \n    l = []\n    q = ldb.session.query(N, SKE) \\\n            .filter(N.id == NIA.Note_id) \\\n            .filter(NIA.is_about_id == SKE.id) \\\n            .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n\n    output = []        \n    print(len(q.all()))\n    for n, ske in q.all():\n        ldb.delete_note(n.id)    \n    print(len(q.all()))\n\nRuns a query over the notes extracted and saved to the database to show the zero-shot document classifications based on the titles + abstracts\n\nl = []\nq = ldb.session.query(N, SKE) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'TiAbClassificationNote__cryoet_study_types') \\\n        .order_by(SKE.id)\n\noutput = []        \nfor n, ske in q.all():\n        tup = json.loads(n.content)\n        tup['prov'] = n.name\n        tup['doi'] = 'http://doi.org/'+re.sub('doi:', '', ske.id)\n        tup['year'] = ske.publication_date.year\n        tup['month'] = ske.publication_date.month\n        tup['ref'] = ske.content\n        output.append(tup)\ndf = pd.DataFrame(output).sort_values(['year', 'month'], ascending=[False, False])\ndf.to_csv(loc+'/'+db_name+'/cryoet_study_types.tsv', sep='\\t')\ndf",
    "crumbs": [
      "Get Started",
      "Tutorial Notebooks",
      "CryoET Tutorial"
    ]
  },
  {
    "objectID": "tutorials/cryoet_tutorial.html#run-metadata-extraction-chain-over-listed-papers",
    "href": "tutorials/cryoet_tutorial.html#run-metadata-extraction-chain-over-listed-papers",
    "title": "CryoET Tutorial",
    "section": "Run MetaData Extraction Chain over listed papers",
    "text": "Run MetaData Extraction Chain over listed papers\nHere, we run various versions of the metadata extraction tool to examine performance over the cryoet dataset.\n\nGet full text copies of all the papers about CryoET\n\ncb.agent_executor.invoke({'input':'Get full text copies of all papers in the collection with id=\"3\".'})\n\nIdentify which papers are in the sampled collection through their dois.\n\nq = ldb.session.query(SKE.id) \\\n        .filter(SKC.id==SKC_HM.ScientificKnowledgeCollection_id) \\\n        .filter(SKC_HM.has_members_id==SKE.id) \\\n        .filter(SKC.id=='2')  \ndois = [e.id for e in q.all()]\ndois\n\nIterate over those dois and extract 15 metadata variables based on the questions shown in ./local_resources/prompt_elements/metadata_extraction.yaml\n\n# Get the metadata extraction tool\nt2 = [t for t in cb.tk.get_tools() if isinstance(t, MetadataExtraction_MethodsSectionOnly_Tool)][0]\n\n# Create a dataframe to store previously extracted metadata\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\ndf = pd.DataFrame()\nfor d in [d for d in dois]:\n    item_types = set()\n    l = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df = pd.concat([df, pd.DataFrame(l)]) \n     \n# Iterate over papers to run the metadata extraction tool\n#for d in [d for d_id in dois_to_include for d in dois_to_include[d_id]]:\nfor d in [d for d in dois]:\n    item_types = set()\n\n    # Skip if the doi is already in the database\n    if len(df)&gt;0 and d in df.doi.unique():\n        continue\n\n    # Run the metadata extraction tool on the doi\n    t2.run(tool_input={'paper_id': d, 'extraction_type': 'cryoet', 'run_label': 'test'})\n\n    # Add the results to the dataframe    \n    l2 = t2.read_metadata_extraction_notes(d, 'cryoet', 'test')\n    df = pd.concat([df, pd.DataFrame(l2)]) \n\ndf\n\n\nldb.create_zip_archive_of_full_text_files('2', loc+'/'+db_name+'/full_text_files.zip')\n\n\nq3 = ldb.session.query(SKE.id, N.name, N.provenance, N.content) \\\n        .filter(N.id == NIA.Note_id) \\\n        .filter(NIA.is_about_id == SKE.id) \\\n        .filter(N.type == 'MetadataExtractionNote') \nl = []\nfor row in q3.all():\n    paper = row[0]\n    name = row[1]\n#    provenance = json.loads(row[2])\n    result = json.loads(row[3])\n    kv = {k:result[k] for k in result}\n    kv['DOI'] = paper\n    kv['run'] = name\n    l.append(kv)\n# create a dataframe from the list of dictionaries with DOI as the index column\nif len(l)&gt;0:\n    df = pd.DataFrame(l).set_index(['DOI', 'run'])\nelse: \n    df = pd.DataFrame()\ndf\n\n\n# USE WITH CAUTION - this will delete all extracted metadata notes in the database\n# clear all notes across papers listed in `dois` list\nif OK_TO_DELETE:\n    for row in q3.all():\n        d_id = row[0]\n        e = ldb.session.query(SKE).filter(SKE.id==d_id).first()\n        notes_to_delete = []\n        for n in ldb.read_notes_about_x(e):\n            notes_to_delete.append(n.id)\n        for n in notes_to_delete:\n            ldb.delete_note(n)",
    "crumbs": [
      "Get Started",
      "Tutorial Notebooks",
      "CryoET Tutorial"
    ]
  }
]