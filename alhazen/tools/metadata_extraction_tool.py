# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/22_metadata_extraction_tool.ipynb.

# %% auto 0
__all__ = ['skc', 'skc_hm', 'ske', 'ske_hr', 'ski', 'ski_hp', 'skf', 'n', 'skc_hn', 'ske_hn', 'ski_hn', 'skf_hn',
           'MetadataExtractionToolSchema', 'MetadataExtractionTool', 'MetadataExtractionWithRAGToolSchema',
           'MetadataExtractionWithRAGTool']

# %% ../../nbs/22_metadata_extraction_tool.ipynb 3
from ..core import OllamaRunner, PromptTemplateRegistry, get_langchain_llm, get_cached_gguf, \
    get_langchain_embeddings, GGUF_LOOKUP_URL, MODEL_TYPE
from .basic import AlhazenToolMixin
from ..utils.output_parsers import JsonEnclosedByTextOutputParser
from ..utils.ceifns_db import *
from ..schema_sqla import ScientificKnowledgeCollection, \
    ScientificKnowledgeExpression, ScientificKnowledgeCollectionHasMembers, \
    ScientificKnowledgeItem, ScientificKnowledgeExpressionHasRepresentation, \
    ScientificKnowledgeFragment, ScientificKnowledgeItemHasPart, \
    InformationResource, Note, \
    ScientificKnowledgeCollectionHasNotes, ScientificKnowledgeExpressionHasNotes, \
    ScientificKnowledgeItemHasNotes, ScientificKnowledgeFragmentHasNotes 
from datetime import datetime
from importlib_resources import files
import json

from langchain.callbacks.tracers import ConsoleCallbackHandler
from langchain.chat_models import ChatOllama
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.pydantic_v1 import BaseModel, Field, root_validator
from langchain.schema import get_buffer_string, StrOutputParser, OutputParserException, format_document
from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain.tools import BaseTool, StructuredTool
from langchain.vectorstores.pgvector import PGVector

import local_resources.prompt_elements as prompt_elements
import local_resources.linkml as linkml
from operator import itemgetter
import os
import re
import regex
from sqlalchemy import create_engine, exists
from sqlalchemy.orm import sessionmaker, aliased
from time import time,sleep
from urllib.request import urlopen
from urllib.parse import quote_plus, quote, unquote
import uuid
import yaml

# %% ../../nbs/22_metadata_extraction_tool.ipynb 4
skc = aliased(ScientificKnowledgeCollection)
skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)
ske = aliased(ScientificKnowledgeExpression)
ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)
ski = aliased(ScientificKnowledgeItem)
ski_hp = aliased(ScientificKnowledgeItemHasPart)
skf = aliased(ScientificKnowledgeFragment)
n = aliased(Note)
skc_hn = aliased(ScientificKnowledgeCollectionHasNotes)
ske_hn = aliased(ScientificKnowledgeExpressionHasNotes)
ski_hn = aliased(ScientificKnowledgeItemHasNotes)
skf_hn = aliased(ScientificKnowledgeFragmentHasNotes)

# %% ../../nbs/22_metadata_extraction_tool.ipynb 5
class MetadataExtractionToolSchema(BaseModel):
    paper_id: str = Field(description="the digitial objecty identifier (doi) of the paper being analyzed")
    section_name: str = Field(description="This is a search string for the section heading of the Items to be searched for in full text papers, typically set to 'methods'")
    extraction_type: str = Field(description="This is the name of the type of extraction and must be one of the following strings: ['cryoet']")

class MetadataExtractionTool(BaseTool, AlhazenToolMixin):
    '''Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'''
    name = 'metadata_extraction'
    description = 'Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'
    args_schema = MetadataExtractionToolSchema
    
    def _run(self, paper_id, section_name, extraction_type, item_type='JATSFullText'):
        '''Runs the metadata extraction pipeline over a specified paper.'''

        if self.db.session is None:
            session_class = sessionmaker(bind=self.db.engine)
            self.db.session = session_class()

        ske = self.db.session.query(ScientificKnowledgeExpression) \
                .filter(ScientificKnowledgeExpression.id.like('%'+paper_id+'%')).first()    

        # 1. Load the text of the paper from the local database
        text = '\n'.join([f.content for f in self.db.list_fragments_for_paper(paper_id, item_type) if section_name in f.name.lower()])

        # 2. Build LangChain elements
        pts = PromptTemplateRegistry()
        pts.load_prompts_from_yaml('metadata_extraction.yaml')
        prompt_elements_yaml = files(prompt_elements).joinpath('metadata_extraction.yaml').read_text()
        prompt_elements_dict = yaml.safe_load(prompt_elements_yaml).get(extraction_type)
        method_goal = prompt_elements_dict['method goal']
        methodology = prompt_elements_dict['methodology']
        metadata_specs = prompt_elements_dict.get('metadata specs',[])
        metadata_extraction_prompt_template = pts.get_prompt_template('metadata extraction (all questions, whole paper)').generate_llama2_prompt_template()
        run_name = 'metadata_extraction_' + re.sub(' ','_',extraction_type) + ':' + paper_id
        extract_lcel = metadata_extraction_prompt_template | self.llm | JsonEnclosedByTextOutputParser()

        # 3. Compile the extraction questions
        question_text_list = [("%d. %s Record this value in the '%s' field of the output."
                                "Record any supporting sentences from the section text in the"
                                " '%s_original_text' field of the output.")
                                %(i+1, spec.get('spec'), spec.get('name'), spec.get('name')) 
                                for i, spec in enumerate(metadata_specs)]
        questions_output_specification = '\n'.join(question_text_list)
        questions_output_specification += '\nGenerate only JSON formatted output with %d fields:\n'%(len(metadata_specs)*2)
        questions_output_specification += ", ".join(['%s, %s_original_text'%(spec.get('name'), spec.get('name')) for spec in metadata_specs])

        # 4. Assemble chain input
        s1 = {'section_text': text,
                'methodology': methodology,
                'method_goal': method_goal,
                'questions_output_specification': questions_output_specification}
        
        # 5. Run the chain using the Ollama runner with a JsonEnclosedByTextOutputParser failsafe loop 
        output = None
        attempts = 0
        while output is None and attempts < 5:
            try: 
                #with suppress_stdout_stderr():
                output = extract_lcel.invoke(s1, config={'callbacks': [ConsoleCallbackHandler()]})
                if output is None:
                    attempts += 1
                    continue

                for spec in metadata_specs:
                    if spec.get('name') in output is False or spec.get('name')+'_original_text' in output is False:
                        continue
                    vname = spec.get('name')
                    question = spec.get('spec')
                    answer = output.get(vname) 
                    original_text = output.get(vname+'_original_text')
                    note_content = json.dumps({
                        'question': question,
                        'answer': answer,
                        'original_text': original_text}, indent=4)
                    # add a note to the fragment
                    n = Note(
                        id=uuid.uuid4().hex[0:10],
                        type='NoteAboutExpression', 
                        name=run_name+':'+vname,
                        content=note_content, 
                        creation_date=datetime.now(), 
                        format='json')
                    n.is_about.append(ske)
                    self.db.session.add(n)
                    self.db.session.flush()
                else:                     
                    attempts += 1
            except OutputParserException as e:
                attempts += 1
                print(e) 
                print('Retrying...')
                    
        # commit the changes to the database
        self.db.session.commit()
        
        return "Final Answer: completed metadata extraction of an experiment of type '%s' from %s."%(methodology, paper_id)

# %% ../../nbs/22_metadata_extraction_tool.ipynb 8
class MetadataExtractionWithRAGToolSchema(BaseModel):
    paper_id: str = Field(description="the digitial objecty identifier (doi) of the paper being analyzed")
    extraction_type: str = Field(description="This is the name of the type of extraction and must be one of the following strings: ['cryoet']")

class MetadataExtractionWithRAGTool(BaseTool, AlhazenToolMixin):
    '''Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'''
    name = 'metadata_extraction'
    description = 'Runs a specified metadata extraction pipeline over a research paper that has been loaded in the local literature database.'
    args_schema = MetadataExtractionWithRAGToolSchema
    
    def _run(self, paper_id, extraction_type ):
        '''Runs the metadata extraction pipeline over the FullText of a specified paper.'''

        if self.db.session is None:
            session_class = sessionmaker(bind=self.db.engine)
            self.db.session = session_class()

        # 1. Load basic prompts and build vectorstore    
        os.environ['PGVECTOR_CONNECTION_STRING'] = "postgresql+psycopg2:///"+self.db.name
        vectorstore = PGVector.from_existing_index(
                embedding = self.db.embed_model, 
                collection_name = 'ScienceKnowledgeItem_FullText') 
        if paper_id[0:4] != 'doi:':
            paper_id = 'doi:'+paper_id
        retriever = vectorstore.as_retriever(search_kwargs={'filter': {'ske_id': paper_id}})

        pts = PromptTemplateRegistry()
        pts.load_prompts_from_yaml('metadata_extraction.yaml')
        prompt_elements_yaml = files(prompt_elements).joinpath('metadata_extraction.yaml').read_text()
        prompt_elements_dict = yaml.safe_load(prompt_elements_yaml).get(extraction_type)
        method_goal = prompt_elements_dict['method goal']
        methodology = prompt_elements_dict['methodology']
        metadata_specs = prompt_elements_dict.get('metadata specs',[])
        metadata_extraction_prompt_template = pts.get_prompt_template(
            'metadata extraction (RAG over full text snippets)').generate_chat_prompt_template()
        run_name = 'metadata_extraction_' + re.sub(' ','_',extraction_type) + ':' + paper_id

        # 2. Iterate over questions to run extraction pipeline
        for i, spec in enumerate(metadata_specs):
            question = spec.get('spec')
            answer_spec = "Record this value in the '%s' field of the output."%(spec.get('name')) 
            answer_spec += '\nGenerate only JSON formatted output with one field\n'
            answer_spec += spec.get('name')

            input = {'question': question,
                'answer_specification': answer_spec,
                'methodology': methodology,
                'method_goal': method_goal}    
            
            qa_chain = (
                RunnableParallel({
                    "question": itemgetter("question"),
                    "context": itemgetter("question") | retriever,
                    "answer_specification": itemgetter("answer_specification"),
                    "methodology": itemgetter("methodology"),
                    "method_goal": itemgetter("method_goal")
                })
                | {
                    "response": metadata_extraction_prompt_template | ChatOllama(model='mixtral') | JsonEnclosedByTextOutputParser(),
                    "context": itemgetter("context"),
                }
            )
            
            # 5. Run the chain using the Ollama runner with a JsonEnclosedByTextOutputParser failsafe loop 
            output = qa_chain.invoke(input, config={'callbacks': [ConsoleCallbackHandler()]})
            vname = spec.get('name')
            question = spec.get('spec')
            answer = output.get('response',{}).get(vname) 
            for skf_id in [d.metadata.get('skf_id') for d in output.get('context', [])]:
                f = self.db.session.query(skf) \
                    .filter(skf.id == skf_id).first()
                note_content = json.dumps({
                    'question': question,
                    'answer': answer,
                    'variable': vname}, indent=4)
                # add a note to the fragment
                n = Note(id=uuid.uuid4().hex[0:10],
                        type='NoteAboutFragment', 
                        name=run_name+':'+vname,
                        content=note_content, 
                        creation_date=datetime.now(), 
                        format='json')
                n.is_about.append(f)
                self.db.session.add(n)
                self.db.session.flush()
            self.db.session.commit()
        
        return "Final Answer: completed metadata extraction of an experiment of type '%s' from %s."%(methodology, paper_id)
