# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/35_localdb.ipynb.

# %% auto 0
__all__ = ['get_nxml_from_pubmed_doi', 'download_file', 'get_pdf_from_pubmed_doi', 'QuerySpec', 'LocalLiteratureDb']

# %% ../../nbs/35_localdb.ipynb 4
import local_resources.linkml as linkml

from .airtableUtils import AirtableUtils
from .searchEngineUtils import ESearchQuery, EuroPMCQuery
from .queryTranslator import QueryTranslator, QueryType
import alhazen.schema_sqla as linkml_sqla
import alhazen.schema_python as linkml_py
from .jats_text_extractor import NxmlDoc

from bs4 import BeautifulSoup,Tag,Comment,NavigableString
from dataclasses import dataclass, field
from datetime import datetime
from importlib_resources import files
import os
import pandas as pd
from pathlib import Path
import re
import requests
from sqlalchemy import create_engine, exists
from sqlalchemy.orm import sessionmaker
from time import time,sleep
from tqdm import tqdm
from urllib.request import urlopen
from urllib.parse import quote_plus, quote, unquote
from urllib.error import URLError, HTTPError

# %% ../../nbs/35_localdb.ipynb 6
def get_nxml_from_pubmed_doi(doi, base_file_path):    
    """
    Executes a query on the target database and returns a count of papers 
    """
    if os.environ.get('NCBI_API_KEY') is None:
        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')
    api_key = os.environ.get('NCBI_API_KEY')

    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'
    sleep(0.1)
    print(esearch_url)
    esearch_response = urlopen(esearch_url)
    esearch_data = esearch_response.read().decode('utf-8')
    esearch_soup = BeautifulSoup(esearch_data, "lxml-xml")
    id_tag = esearch_soup.find('Id')    
    if id_tag is None:
      print('No paper found with that DOI')
      return
      # raise Exception('Could not find "' + doi + '" in PMC')
    pmc_id = id_tag.string
    
    efetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?api_key='+api_key+'&db=pmc&id='+pmc_id+'&retmode=xml'
    sleep(0.1)
    efetch_response = urlopen(efetch_url)
    efetch_data = efetch_response.read().decode('utf-8')
    efetch_soup = BeautifulSoup(efetch_data, "lxml-xml")
    xml = efetch_soup.prettify()

    file_path = Path(base_file_path + '/' + doi + '.nxml')
    parent_dir = file_path.parent
    if os.path.exists(parent_dir) is False:
        os.makedirs(parent_dir)
    with open(file_path, 'w') as f:
        f.write(xml)

def download_file(url, local_filename):
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192): 
                f.write(chunk)
    return local_filename

def get_pdf_from_pubmed_doi(doi, base_file_path):    
    """
    Executes a query on the target database and returns a count of papers 
    """
    if os.environ.get('NCBI_API_KEY') is None:
        raise Exception('Error attempting to query NCBI for URL data, did you set the NCBI_API_KEY environment variable?')
    api_key = os.environ.get('NCBI_API_KEY')

    esearch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+api_key+'&db=pmc&term='+doi+'[doi]&retmode=xml'
    sleep(0.1)
    print(esearch_url)
    esearch_response = urlopen(esearch_url)
    esearch_data = esearch_response.read().decode('utf-8')
    esearch_soup = BeautifulSoup(esearch_data, "lxml-xml")
    id_tag = esearch_soup.find('Id')    
    if id_tag is None:
      print('No paper found with that DOI')
      return
      # raise Exception('Could not find "' + doi + '" in PMC')
    pmc_id = id_tag.string

    # OA Dataset 
    oapi_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id='+pmc_id+'&format=pdf'
    sleep(0.1)
    print(oapi_url)
    oapi_response = urlopen(oapi_url)
    oapi_data = oapi_response.read().decode('utf-8')    
    oapi_soup = BeautifulSoup(oapi_data, "lxml-xml")
    
    pdf_link_tag = oapi_soup.find('link')
    if(pdf_link_tag is None):
        print('No PDF found for that DOI')
        return
    
    pdf_url = pdf_link_tag['href']
    if pdf_url.startswith('ftp:'):
        pdf_url = pdf_url.replace('ftp:','https:') 
    file_path = Path(base_file_path + '/' + doi + '.pdf')
    parent_dir = file_path.parent
    if os.path.exists(parent_dir) is False:
        os.makedirs(parent_dir)
    download_file(pdf_url, file_path)   

# %% ../../nbs/35_localdb.ipynb 7
@dataclass
class QuerySpec:
    name: str
    id_col: str
    query_col: str
    col_map: dict[str, str] = field(default_factory=dict)
    sections: list[str] = field(default_factory=list)

    def process_cdf(self, cdf: pd.DataFrame):
        cdf = cdf.rename(columns={self.id_col:'ID', self.query_col:'QUERY'})
        cdf = cdf.rename(columns=self.col_map)
        cdf = cdf.fillna('').rename(
            columns={c:re.sub('[\s\(\)]','_', c.upper()) for c in cdf.columns}
            )
        cdf.QUERY = [re.sub('^http[s]*://', '', r.QUERY) if r.QUERY[:4]=='http' else r.QUERY 
                     for i,r in cdf.iterrows()] 
        cdf.QUERY = [re.sub('/$', '', r.QUERY.strip())  
                        for i,r in cdf.iterrows()]
        return cdf

class LocalLiteratureDb:

  """This class runs a set of queries on external literature databases to build a local database of linked corpora and papers.

  Functionality includes:

    * Define a spreadsheet with a column of queries expressed in boolean logic
    * Optional: Define a secondary spreadsheet with a column of subqueries expressed in boolean logic
    * Iterate over different sources (Pubmed + European Pubmed) to execute all combinations of queries and subqueries
    
  """

  def __init__(self, name, description, loc):
    self.name = name
    self.description = description
    
    if loc[-1] != '/':
      loc = loc + '/' 
    self.loc = loc

    self.engine = create_engine('sqlite:///'+loc+'sciknow.db')
    self.session = None # instantiate session when needed

    if os.path.exists(loc) is False:
      os.mkdir(loc)

    log_path = '%s/db_log.txt' % (loc)
    if os.path.exists(log_path) is False:
      Path(log_path).touch()

  def add_corpus_from_epmc(self, qt, qt2, sections=['paper_title', 'ABSTRACT']):
    
    if self.session is None:
      session_class = sessionmaker(bind=self.engine)
      self.session = session_class()

    if self.session.query(exists().where(linkml_sqla.InformationResource.id=='EPMC')).scalar():
      info_resource = self.session.query(linkml_sqla.InformationResource) \
          .filter(linkml_sqla.InformationResource.id=='EPMC').first()
    else:
      info_resource = linkml_sqla.InformationResource(id='EPMC', 
                                                      name='European Pubmed Central', 
                                                      xref=['https://europepmc.org/'])
    
    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)
    if qt2:
      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections)
    else: 
      (subset_ids, epmc_subset_queries) = ([0],[''])
    for (i, q) in zip(corpus_ids, epmc_queries):
      for (j, sq) in zip(subset_ids, epmc_subset_queries):
        query = q
        corpus_id = str(i)
        ith_row_df = qt.df[qt.df[qt.id_col]==i]
        corpus_name = ith_row_df.loc[i, qt.name_col]
        if qt.notes_col:
          corpus_notes = ith_row_df.loc[i, qt.notes_col]
        else:
          corpus_notes = ''
        if query is None or query=='nan' or len(query)==0: 
          continue
        if len(sq) > 0:
          query = '(%s) AND (%s)'%(q, sq)
          corpus_id = str(i+'.'+j)

        # does this collection already exist?  
        if self.session.query(exists().where(linkml_sqla.ScientificPublicationCollection.id==corpus_id)).scalar():
          cp = self.session.query(linkml_sqla.ScientificPublicationCollection) \
              .join(linkml_sqla.ScientificPublicationCollection.has_part) \
              .filter(linkml_sqla.ScientificPublicationCollection.id==corpus_id).first()
          for p in cp.has_part:
            cp.has_part.remove(p)
          self.session.flush()
          self.session.delete(cp)
          self.session.commit()
        #self.session.query(linkml_sqla.ScientificPublicationCollection).delete()
        #self.session.commit()

        corpus = linkml_sqla.ScientificPublicationCollection(id=corpus_id, logical_query=query, name=corpus_name)
        corpus.information_sources.append(info_resource)
        self.session.add(corpus)
        
        epmcq = EuroPMCQuery()
        numFound, epmc_publications = epmcq.run_empc_query(query)
        for p in tqdm(epmc_publications):
          if self.session.query(exists().where(linkml_sqla.ScientificPublication.id==p.id)).scalar():
            p = self.session.query(linkml_sqla.ScientificPublication) \
                .filter(linkml_sqla.ScientificPublication.id==p.id).first()
          self.session.add(p)
          corpus.has_part.append(p)       
        self.session.commit()

  def check_query_terms(self, qt, qt2=None, pubmed_api_key=''):
    pmq = ESearchQuery(api_key=pubmed_api_key)
    terms = set()
    for t in qt.terms2id.keys():
        terms.add(t)
    if qt2 is not None:
        for t2 in qt2.terms2id.keys():
            terms.add(t2)
    check_table = {} 
    for t in tqdm(terms):
        (is_ok, t2, c) = pmq._check_query_phrase(t)
        check_table[t] = (is_ok, c)
    return check_table
  
  def add_corpora(self, cdf, id_col, query_col, name_col, notes_col=None, col_map={}, sections=['TITLE','ABSTRACT']):  
    cdf = cdf.rename(columns={id_col:'ID', query_col:'QUERY'})
    cdf = cdf.rename(columns=col_map)
    cdf = cdf.fillna('').rename(
        columns={c:re.sub('[\s\(\)]','_', c.upper()) for c in cdf.columns}
        )
    cdf.QUERY = [re.sub('^http[s]*://', '', r.QUERY) if r.QUERY[:4]=='http' else r.QUERY 
                  for i,r in cdf.iterrows()] 
    cdf.QUERY = [re.sub('/$', '', r.QUERY.strip())  
                    for i,r in cdf.iterrows()] 
    qt = QueryTranslator(cdf, id_col, query_col, name_col, notes_col)
    self.add_corpus_from_epmc(qt, None, sections=sections)

  def list_corpora(self, corpus_id=None): 
    cs = self.session.query(linkml_sqla.ScientificPublicationCollection).all()
    for c in cs:
      yield(c)
  
  def list_corpus_publications(self, corpus_id): 
    cp = self.session.query(linkml_sqla.ScientificPublicationCollection) \
            .join(linkml_sqla.ScientificPublicationCollection.has_part) \
            .filter(linkml_sqla.ScientificPublicationCollection.id==corpus_id).first()
    for p in cp.has_part:
      yield(p)


