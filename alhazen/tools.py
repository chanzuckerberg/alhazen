# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_tools.ipynb.

# %% auto 0
__all__ = ['callback_manager', 'n_gpu_layers', 'n_batch', 'model', 'db']

# %% ../nbs/02_tools.ipynb 3
from langchain.llms import LlamaCpp
from langchain import PromptTemplate, LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.tools import BaseTool
from langchain.utilities import SQLDatabase
import os

# %% ../nbs/02_tools.ipynb 4
# Callbacks support token-wise streaming
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

n_gpu_layers = 5  # Change this value based on your model and your GPU VRAM pool.
n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.

# Make sure the model path is correct for your system!
model = LlamaCpp(
    model_path="/Users/gburns/Documents/Coding/ChatGPT_etc/LLMs/llama-2-70b-chat.Q5_K_M.gguf",
    n_ctx=4096,
    n_gpu_layers=n_gpu_layers,
    n_batch=n_batch,
    callback_manager=callback_manager,
    f16_kv=True,
    verbose=True, # Verbose is required to pass to the callback manager
)

# %% ../nbs/02_tools.ipynb 7
db = SQLDatabase.from_databricks('gburns', 'imaging_tech', 
                                 host='czi-shared-infra-czi-sci-general-prod-databricks.cloud.databricks.com',
                                 api_token=os.environ['DB_TOKEN'],
                                 warehouse_id='1c4df94f2f1a6305')

db.run('''
SELECT c.QUERY, p.TITLE, p.ABSTRACT, p.ID_PAPER
FROM corpus c
  INNER JOIN corpus_papers cp ON c.ID = cp.ID_CORPUS
  INNER JOIN papers p ON cp.ID_PAPER = p.ID_PAPER
WHERE 
       ''')

