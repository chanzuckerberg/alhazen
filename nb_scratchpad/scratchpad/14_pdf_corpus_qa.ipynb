{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA on a corpus of PDF files.\n",
    "\n",
    "> Building a simple chatbot to allow the user to ask questions about a collection of PDF files (e.g., grant proposals). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools.pdf_corpus_qa_llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index import Document, ServiceContext, set_global_service_context\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "    \n",
    "from alhazen.core import get_llamaindex_llm, get_langchain_llm\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "from llama_index import download_loader\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "\n",
    "class PdfFileCollectionLlamaIndexChatBot:\n",
    "\n",
    "    def __init__(self, \n",
    "                doc_dir, \n",
    "                llm_name, \n",
    "                embed_model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "                sentence_window_size=10):\n",
    "\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        \n",
    "        if doc_dir[-1:] != '/':\n",
    "            doc_dir += '/'\n",
    "        self.doc_dir = doc_dir\n",
    "\n",
    "        self.llm = get_llamaindex_llm(llm_name)     \n",
    "\n",
    "        # create the sentence window node parser w/ default settings\n",
    "        self.node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "            window_size=sentence_window_size,\n",
    "            window_metadata_key=\"window\",\n",
    "            original_text_metadata_key=\"original_text\",\n",
    "            )\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "        # create a service context\n",
    "        self.service_context = ServiceContext.from_defaults(\n",
    "            llm=self.llm,\n",
    "            embed_model=self.embed_model,\n",
    "            node_parser=self.node_parser)\n",
    "                \n",
    "        self.loader = PyMuPDFReader()\n",
    "\n",
    "    def load_all_documents(self):\n",
    "        \n",
    "        # List all pdf files in doc_dir by walking the whole directory tree\n",
    "        pdf_file_list = []\n",
    "        for root, dirs, files in os.walk(self.doc_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.pdf'):\n",
    "                    pdf_file_list.append(os.path.join(root, file))\n",
    "\n",
    "        # Load each PDF block as a document\n",
    "        documents = []\n",
    "        print('Loading Documents...')\n",
    "        for file in pdf_file_list:\n",
    "            pdf_documents = self.loader.load_data(file_path=file, metadata=True)\n",
    "            documents.extend(pdf_documents)\n",
    "        print('Loading Complete!')\n",
    "\n",
    "        self.index = VectorStoreIndex.from_documents(documents, service_context=self.service_context)\n",
    "        self.query_engine = self.index.as_query_engine()\n",
    "    \n",
    "    def load_one_document(self, file_path):\n",
    "        pdf_document = self.loader.load_data(file_path=file_path, metadata=True)\n",
    "        self.index = VectorStoreIndex.from_documents(pdf_document, service_context=self.service_context)\n",
    "        self.query_engine = self.index.as_query_engine()\n",
    "        \n",
    "    def run_batch_of_questions_over_each_file(self, questions):\n",
    "        l =[]\n",
    "        # List all pdf files in doc_dir by walking the whole directory tree\n",
    "        pdf_file_list = []\n",
    "        for root, dirs, files in os.walk(self.doc_dir):\n",
    "            for file in files:\n",
    "                print(file)\n",
    "                if file.endswith('.pdf'):\n",
    "                    self.load_one_document(os.path.join(root, file))\n",
    "                    for q in questions:\n",
    "                        print('\\t%s'%(q))\n",
    "                        a = self.query_engine.query(q)\n",
    "                        print('\\t\\t%s'%(a))\n",
    "                        l.append({'file': file, 'question': q, 'answer': a})\n",
    "        df = pd.DataFrame(l)\n",
    "        return df\n",
    "    \n",
    "    def run_gradio(self):\n",
    "        \n",
    "        def add_text(history, text):\n",
    "            #print('add_text: history: %s, text: %s'%(history, text))\n",
    "            history = history + [(text, None)]\n",
    "            return history, gr.Textbox(value=\"\", interactive=False)\n",
    "\n",
    "        def add_file(history, file):\n",
    "            #print('add_history: history: %s, file: %s'%(history, file))\n",
    "            history = history + [((file.name,), None)]\n",
    "            return history\n",
    "\n",
    "        def bot(history):\n",
    "            #print('bot: history: %s'%(history))\n",
    "            # prompt to send to the agent is the last message from the user\n",
    "            prompt = history[-1][0]\n",
    "            response = self.query_engine.query(prompt)\n",
    "            print('RESPONSE: %s'%(str(response)))\n",
    "            history[-1][1] = str(response)\n",
    "            print('WHOLE HISTORY: %s'%(history))\n",
    "            return history\n",
    "\n",
    "        with gr.Blocks() as demo:\n",
    "            chatbot = gr.Chatbot(\n",
    "                [],\n",
    "                elem_id=\"chatbot\",\n",
    "                bubble_full_width=False,\n",
    "                #avatar_images=(None, files(alhazen_resources).joinpath('alhazen.png'))\n",
    "            )\n",
    "            with gr.Row():\n",
    "                txt = gr.Textbox(\n",
    "                    scale=4,\n",
    "                    show_label=False,\n",
    "                    placeholder=\"Enter text and press enter, or upload files\",\n",
    "                    container=False,\n",
    "                )\n",
    "                btn = gr.UploadButton(\"üìÅ\", file_types=[\"image\", \"video\", \"audio\"])\n",
    "\n",
    "            txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(bot, chatbot, chatbot)\n",
    "            txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)\n",
    "            \n",
    "            file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "\n",
    "        demo.queue()\n",
    "        demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_corpus_chatbot(doc_dir, \n",
    "        llm_name, \n",
    "        embed_model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "        sentence_window_size=3):\n",
    "    \n",
    "    chatbot = PdfFileCollectionLlamaIndexChatBot(doc_dir, \n",
    "                llm_name,\n",
    "                embed_model_name=embed_model_name, \n",
    "                sentence_window_size=sentence_window_size)\n",
    "    chatbot.run_gradio()\n",
    "\n",
    "#os.environ['LLMS_TEMP_DIR'] = '/tmp/alhazen'\n",
    "#pdf_corpus_chatbot('/Users/gburns/Documents/2023H2/makeathon/grant_analysis', 'llama-2-70b-chat')\n",
    "\n",
    "#os.environ['OPENAI_API_KEY'] = '<add your key here>'\n",
    "#pdf_corpus_chatbot('/Users/gburns/Documents/2023H2/makeathon/grant_analysis', 'gpt-3.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did they ask for a no-cost extension?\n",
    "#Do they have any money left?\n",
    "#Missing information (are all the fields filled out?)\n",
    "#P0: summarizing research outputs\n",
    "#P1: Given a set of milestones, did the grantee report progress on any of them? \n",
    "#What progress did grantees make towards their milestones?\n",
    "#What outputs support these milestones?\n",
    "\n",
    "questions = [\n",
    "    'Did they ask for a no-cost extension?',\n",
    "    'Do they have any money left?',\n",
    "    'Summarize all direct and indirect research outputs',\n",
    "    'Summarize all direct and indirect research outputs, preferably by their identifiers (URL, DOI, etc)', ### <- in case the previous one fails\n",
    "    '''What progress did grantees make towards the following milestones? \n",
    "    1) Gather a diverse dataset of biological images containing different \n",
    "    types of cells and imaging conditions. Develop preprocessing pipelines\n",
    "    to standardize and enhance the quality of the data. 2) Design and \n",
    "    train deep learning models for cell segmentation. Explore various \n",
    "    architectures, including convolutional neural networks (CNNs) and \n",
    "    recurrent neural networks (RNNs), to adapt to different cell types \n",
    "    and imaging modalities. Investigate the use of transfer learning \n",
    "    from pre-trained models to boost segmentation performance. 3) \n",
    "    Assess the performance of developed models on benchmark datasets \n",
    "    and real-world biological images. Employ metrics such as intersection \n",
    "    over union (IoU), accuracy, and F1 score to measure the accuracy and robustness \n",
    "    of the segmentation results. 4) Implement the best-performing model into a user-\n",
    "    friendly software tool for cell biologists and researchers. Ensure compatibility \n",
    "    with various microscopy platforms and data formats. Create documentation and provide\n",
    "    user support. 5) Collaborate with domain experts and end-users to validate the tool's \n",
    "    effectiveness in real research scenarios. Gather feedback and fine-tune the model and software based on user input.'''\n",
    "]\n",
    "\n",
    "#os.environ['LLMS_TEMP_DIR'] = '/path/to/temp/dir'\n",
    "#cb1 = PdfFileCollectionLlamaIndexChatBot('/path/to/input/files/', 'llama-2-70b-chat')\n",
    "#df1 = cb1.run_batch_of_questions_over_each_file(questions)\n",
    "#df1.to_csv('/path/to/output/fileUsers/gburns/Documents/2023H2/makeathon/grant_analysis/outputs/llama-2-70b-chat.csv', index=False, header=True, sep='\\t')\n",
    "#os.environ['OPENAI_API_KEY'] = '<add your key here>'\n",
    "#pdf_corpus_chatbot('/Users/gburns/Documents/2023H2/makeathon/grant_analysis', 'gpt-3.5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
