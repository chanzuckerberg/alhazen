{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Local Huggingface Modules\n",
    "\n",
    "> Basic Experiments with HuggingFace using LangChain models, chains, and agents.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eac501fd7f843709eff9ca78fb7a868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1 with CUDA None (you have 2.2.0.dev20230912)\n",
      "    Python  3.11.4 (you have 3.11.4)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import langchain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Universal-NER/UniNER-7B-type-sup\", token='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')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Universal-NER/UniNER-7B-type-sup\", token='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').to('mps')\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, device='mps'\n",
    ")\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A virtual assistant answers questions from a user based on the provided text. USER: Text: {text} ASSISTANT: I've read this text.</s>USER: What describes {entity_type} in the text? ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "from alhazen.ms_nlp_utils import preprocess_instance\n",
    "\n",
    "example = {\n",
    "    \"conversations\": [\n",
    "        {\"from\": \"human\", \"value\": \"Text: {text}\"}, \n",
    "        {\"from\": \"gpt\", \"value\": \"I've read this text.\"}, \n",
    "        {\"from\": \"human\", \"value\": \"What describes {entity_type} in the text?\"}, \n",
    "        {\"from\": \"gpt\", \"value\": \"[]\"}]\n",
    "}\n",
    "template = preprocess_instance(example['conversations'])\n",
    "print(template)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gburns/miniconda3/envs/alhazen/lib/python3.11/site-packages/transformers-4.31.0-py3.11.egg/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"fluorescence lifetime imaging\"]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "text = \"\"\"\n",
    "Simultaneous dual-color fluorescence lifetime imaging with novel red-shifted fluorescent proteins.\n",
    "We describe a red-shifted fluorescence resonance energy transfer (FRET) pair optimized for dual-\n",
    "color fluorescence lifetime imaging (FLIM). This pair utilizes a newly developed FRET donor,\n",
    "monomeric cyan-excitable red fluorescent protein (mCyRFP1), which has a large Stokes shift and a\n",
    "monoexponential fluorescence lifetime decay. When used together with EGFP-based biosensors, the new\n",
    "pair enables simultaneous imaging of the activities of two signaling molecules in single dendritic\n",
    "spines undergoing structural plasticity.\"\"\"\n",
    "entity_type = \"method\"\n",
    "\n",
    "print(chain.invoke({\"text\": text, 'entity_type': entity_type}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n",
      "/Users/gburns/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\", token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, my dog is cute. I am happy to see you. I hope you are having a good day.\\nWhat are some common phrases that people use to greet each other in different cultures?\\nSome common phrases that'},\n",
       " {'generated_text': \"Hello, my dog is cute and I love her, but she is a bit of a handful sometimes. She likes to bark a lot and she's always trying to get into things she shouldn't. Do you have any\"},\n",
       " {'generated_text': 'Hello, my dog is cute! 🐶❤️ #dogsofinstagram #puppylove #cutedog #doglovers #animallovers #petlovers #dogphotography'},\n",
       " {'generated_text': \"Hello, my dog is cute, and I love to play fetch with him. Here are some funny dog memes that you might enjoy.\\nThe best part of being a dog owner is seeing your furry friend's excitement when you\"},\n",
       " {'generated_text': 'Hello, my dog is cute, and I love spending time with him. He is a golden retriever mix and he is 2 years old. He loves to play fetch and go on long walks. I also have a'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Hello, my dog is cute\", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "t3 = \"\"\"\n",
    "Your job is to read the title and abstract of a scientific paper and perform the task described below. \n",
    "The text of the title and the text of the abstract will be delimited with triple backticks. \n",
    "\n",
    "First, split the text of the title and abstract into separate sentences.\n",
    "\n",
    "Second, classify each sentence as describing \n",
    "- background information about the work, \n",
    "- the goals of the paper, \n",
    "- the methods used by the scientists, \n",
    "- the results of the work, and \n",
    "- the conclusions of the work. \n",
    "\n",
    "Third, using only the goal and method sentences, decide if the work is concerned with (A) developing new technology or methods or \n",
    "(B) investigating how biological systems work.  \n",
    "\n",
    "Be careful about this decision and only answer 'true' if you are confident that the main goal of the published work \n",
    "was to create or test a new technique or method.  \n",
    "\n",
    "If paper describes the development of a new method, technique or approach, record a true value in a variable called is_method_paper in the output. \n",
    "\n",
    "If the goal of the work understanding biological phenomena, record a false value in a variable called is_method_paper in the output.\n",
    "\n",
    "Format your response as a JSON object with \"is_method_paper\" as the key.  \n",
    "\n",
    "Briefly explain your reasoning for your decision in one or two sentences.\n",
    "\n",
    "Article\n",
    "title: '''{title}'''\n",
    "abstract: '''{abstract}'''\n",
    "\"\"\"\n",
    "p3 = PromptTemplate(\n",
    "    template=t3,\n",
    "    input_variables=[\"title\", \"abstract\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"bigscience/bloom-1b7\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0.05, \"max_length\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, we need to understand what is an electroencephalogram. An electroencephalogram is a recording of brain activity. It is a recording of brain activity that is made by placing electrodes on the scalp. The electrodes are placed in a way that they are connected to the brain. The electrodes are connected to the brain by wires. The wires are connected to the electrodes by a cable. The cable is connected\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '''Lung lipid deposition in pneumonias of viral and non-viral aetiology'''\n",
    "a = '''\n",
    "Pneumonia is an acute respiratory disease of varying aetiology, which drew much attention during the\n",
    "COVID-19 pandemic. Among many thoroughly studied aspects of pneumonia, lipid metabolism has been\n",
    "addressed insufficiently. Here, we report on abnormal lipid metabolism of both COVID-19- and non-\n",
    "COVID-19-associated pneumonias in human lungs. Morphometric analysis revealed extracellular and\n",
    "intracellular lipid depositions, most notably within vessels adjacent to inflamed regions, where\n",
    "they apparently interfere with the blood flow. Lipids were visualized on Sudan III- and Oil Red\n",
    "O-stained cryosections and on OsO 4 -contrasted semi-thin and ultrathin sections. Chromato-mass\n",
    "spectrometry revealed that unsaturated fatty acid content was elevated at inflammation sites\n",
    "compared with the intact sites of the same lung. The genes involved in lipid metabolism were\n",
    "downregulated in pneumonia, as shown by qPCR and in silico RNAseq analysis. Thus, pneumonias are\n",
    "associated with marked lipid abnormalities, and therefore lipid metabolism can be considered a\n",
    "target for new therapeutic strategies.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gburns/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"bigscience/bloom-1b7\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 64},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gburns/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/gburns/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 508, but `max_length` is set to 64. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'author'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(p3.format(title=t, abstract=a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain' has no attribute 'debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: question}))\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/langchain/schema/runnable/base.py:969\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Input, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m    967\u001b[0m     \u001b[39m# setup callbacks\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 969\u001b[0m     callback_manager \u001b[39m=\u001b[39m get_callback_manager_for_config(config)\n\u001b[1;32m    970\u001b[0m     \u001b[39m# start the root run\u001b[39;00m\n\u001b[1;32m    971\u001b[0m     run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    972\u001b[0m         dumpd(\u001b[39mself\u001b[39m), \u001b[39minput\u001b[39m, name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    973\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/langchain/schema/runnable/config.py:123\u001b[0m, in \u001b[0;36mget_callback_manager_for_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_callback_manager_for_config\u001b[39m(config: RunnableConfig) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CallbackManager:\n\u001b[1;32m    121\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanager\u001b[39;00m \u001b[39mimport\u001b[39;00m CallbackManager\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m CallbackManager\u001b[39m.\u001b[39;49mconfigure(\n\u001b[1;32m    124\u001b[0m         inheritable_callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    125\u001b[0m         inheritable_tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    126\u001b[0m         inheritable_metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/langchain/callbacks/manager.py:1334\u001b[0m, in \u001b[0;36mCallbackManager.configure\u001b[0;34m(cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   1303\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfigure\u001b[39m(\n\u001b[1;32m   1305\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     local_metadata: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1313\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CallbackManager:\n\u001b[1;32m   1314\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Configure the callback manager.\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \n\u001b[1;32m   1316\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39m        CallbackManager: The configured callback manager.\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1334\u001b[0m     \u001b[39mreturn\u001b[39;00m _configure(\n\u001b[1;32m   1335\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1336\u001b[0m         inheritable_callbacks,\n\u001b[1;32m   1337\u001b[0m         local_callbacks,\n\u001b[1;32m   1338\u001b[0m         verbose,\n\u001b[1;32m   1339\u001b[0m         inheritable_tags,\n\u001b[1;32m   1340\u001b[0m         local_tags,\n\u001b[1;32m   1341\u001b[0m         inheritable_metadata,\n\u001b[1;32m   1342\u001b[0m         local_metadata,\n\u001b[1;32m   1343\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/langchain/callbacks/manager.py:1740\u001b[0m, in \u001b[0;36m_configure\u001b[0;34m(callback_manager_cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   1736\u001b[0m tracer_project \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1737\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mLANGCHAIN_PROJECT\u001b[39m\u001b[39m\"\u001b[39m, os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mLANGCHAIN_SESSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1738\u001b[0m )\n\u001b[1;32m   1739\u001b[0m run_collector_ \u001b[39m=\u001b[39m run_collector_var\u001b[39m.\u001b[39mget()\n\u001b[0;32m-> 1740\u001b[0m debug \u001b[39m=\u001b[39m _get_debug()\n\u001b[1;32m   1741\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1742\u001b[0m     verbose\n\u001b[1;32m   1743\u001b[0m     \u001b[39mor\u001b[39;00m debug\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[39mor\u001b[39;00m open_ai \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m ):\n\u001b[1;32m   1749\u001b[0m     \u001b[39mif\u001b[39;00m verbose \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m   1750\u001b[0m         \u001b[39misinstance\u001b[39m(handler, StdOutCallbackHandler)\n\u001b[1;32m   1751\u001b[0m         \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m callback_manager\u001b[39m.\u001b[39mhandlers\n\u001b[1;32m   1752\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/langchain/callbacks/manager.py:87\u001b[0m, in \u001b[0;36m_get_debug\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_debug\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m langchain\u001b[39m.\u001b[39;49mdebug\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'langchain' has no attribute 'debug'"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": question}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
