{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langserve endpoint \n",
    "\n",
    "> A server-side delivery of langchain agents and chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp langserve.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "from langchain.prompts import (ChatPromptTemplate, MessagesPlaceholder,\n",
    "                               PromptTemplate)\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from langchain.schema.runnable import (Runnable, RunnableBranch,\n",
    "                                       RunnableLambda, RunnableMap)\n",
    "from langchain.vectorstores import Chroma\n",
    "from langserve import add_routes\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langserve import add_routes\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools.render import render_text_description, render_text_description_and_args\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain import hub\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser\n",
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
    "\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "from alhazen.utils.output_parsers import ReActJsonSingleInputOutputParser_llama2\n",
    "\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "from uuid import UUID\n",
    "\n",
    "# Hack to fix Incorrect formatting for Llama Chat models\n",
    "from alhazen.utils.langchain_utils import ChatPromptValue_to_string\n",
    "from langchain.prompts.chat import ChatPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Provides access to an agent interface via langserve.\n",
    "# Very useful for NextJS applications + demos\n",
    "\n",
    "import nest_asyncio\n",
    "from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain.tools.playwright.utils import (\n",
    "    create_async_playwright_browser,  # A synchronous browser is available, though it isn't compatible with jupyter.\n",
    ")\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from alhazen.utils.output_parsers import JsonEnclosedByTextOutputParser\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#async_browser = create_async_playwright_browser()\n",
    "#browser_toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "#tools = browser_toolkit.get_tools()\n",
    "tools = tools = load_tools([], )\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react-multi-input-json\")\n",
    "prompt = prompt.partial(\n",
    "    tools=render_text_description_and_args(tools),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "\n",
    "chat_model = ChatOllama(model='mixtral')\n",
    "chat_model_with_stop = chat_model.bind(stop=[\"\\nObservation\"])\n",
    "#ChatPromptValue.to_string = ChatPromptValue_to_string\n",
    "\n",
    "#llm = Ollama(model='llama2:70b')\n",
    "#llm_with_stop = llm.bind(stop=[\"\\nObservation\"])\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model_with_stop\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# We need to add these input/output schemas because the current AgentExecutor\n",
    "# is lacking in schemas.\n",
    "class ChatRequest(BaseModel):\n",
    "    question: str\n",
    "    chat_history: Optional[List[Dict[str, str]]]\n",
    "\n",
    "class Input(BaseModel):\n",
    "    input: str\n",
    "\n",
    "class Output(BaseModel):\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Alhazen Server\",\n",
    "    version=\"0.0.1\",\n",
    "    description=\"An api server using Langchain's Runnable interfaces for Alhazen\",\n",
    ")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    "    expose_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Adds routes to the app for using the chain under:\n",
    "# /invoke\n",
    "# /batch\n",
    "# /stream\n",
    "add_routes(app, agent_executor, input_type=Input, output_type=Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
