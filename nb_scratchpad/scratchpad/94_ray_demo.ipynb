{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 23:20:03,091\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.7.2 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text articles: 777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import psutil\n",
    "import ray\n",
    "\n",
    "# Load 20newsgroups news articles that that belong to “rec.motorcycles” and “rec.sport.baseball” classes of the test set only\n",
    "test_data = fetch_20newsgroups(subset='test', shuffle=False, categories=['rec.motorcycles', 'rec.sport.baseball'], remove=('headers', 'footers', 'quotes'))\n",
    "# Remove empty news article texts\n",
    "test_data = [text for text in test_data.data if text!='']\n",
    "print('Number of text articles:', len(test_data))\n",
    "\n",
    "\"\"\"\n",
    "HuggingFace pipelines are objects that abstract most of the complex code from the library, \n",
    "offering a simple API dedicated to several tasks, including text classification.\n",
    "All pipelines can use batching.\n",
    "However, this is not automatically a win for performance. \n",
    "It can be either a 10x speedup or 5x slowdown depending on hardware, data and the actual model being used.\n",
    "Batching is only recommended on GPU. \n",
    "If you are using CPU, don’t batch.\n",
    "\"\"\"\n",
    "# Init pipeline with batchsize 1 on CPU for our example\n",
    "pipe = pipeline(task = 'zero-shot-classification', \n",
    "                model='typeform/distilbert-base-uncased-mnli', \n",
    "                batch_size=8, \n",
    "                device='mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hey, the Lone Biker of the Apocalypse (see Raising Arizona) had flames coming\n",
      "out of both his exhaust pipes. I love to toggle the kill switch on my Sportster\n",
      "to produce flaming backfires, especially underneath overpasses at night (it's\n",
      "loud and lights up the whole underpass!!!\n",
      "Labels: ['motorcycle', 'baseball']\n",
      "Scores: [0.9970590472221375, 0.002940941136330366]\n"
     ]
    }
   ],
   "source": [
    "# Predict single text\n",
    "prediction = pipe(test_data[100], ['motorcycle', 'baseball'])\n",
    "print('Text:', prediction['sequence'])\n",
    "print('Labels:', prediction['labels'])\n",
    "print('Scores:', prediction['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 0:00:52.313775\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict multipe texts on single CPU and time the inference duration\n",
    "start = time.time()\n",
    "\n",
    "predictions = [pipe(text, ['motorcycle', 'baseball']) for text in test_data]\n",
    "end = time.time()\n",
    "print('Prediction time:', str(timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPUs: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 23:22:18,171\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 0:00:28.430376\n"
     ]
    }
   ],
   "source": [
    "num_cpus = psutil.cpu_count(logical=True)\n",
    "print('Number of available CPUs:', num_cpus)\n",
    "\n",
    "# Start Ray cluster\n",
    "num_cpus = num_cpus\n",
    "#ray.init(num_cpus=num_cpus, ignore_reinit_error=True)\n",
    "\n",
    "\"\"\"\n",
    "The command ray.put(x) would be run by a worker process or by the driver process (the driver process is the one running your script). \n",
    "It takes a Python object and copies it to the local object store (here local means on the same node). \n",
    "Once the object has been stored in the object store, its value cannot be changed.\n",
    "In addition, ray.put(x) returns an object ID, which is essentially an ID that can be used to refer to the newly created remote object. \n",
    "If we save the object ID in a variable with x_id = ray.put(x), \n",
    "then we can pass x_id into remote functions, \n",
    "and those remote functions will operate on the corresponding remote object.\n",
    "\"\"\"\n",
    "pipe_id = ray.put(pipe)\n",
    "\n",
    "# @ray.remote decorator enables to use this \n",
    "# function in distributed setting\n",
    "@ray.remote\n",
    "def predict(pipeline, text_data, label_names):\n",
    "    return pipeline(text_data, label_names)\n",
    "\n",
    "# Predict multipe texts on all available CPUs and time the inference duration\n",
    "start = time.time()\n",
    "\n",
    "# Run the function using multiple cores and gather the results\n",
    "predictions = ray.get([predict.remote(pipe_id, text, ['motorcycle', 'baseball']) for text in test_data])\n",
    "\n",
    "end = time.time()\n",
    "print('Prediction time:', str(timedelta(seconds=end-start)))\n",
    "\n",
    "# Stop running Ray cluster\n",
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alhazen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
