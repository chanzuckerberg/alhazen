{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:11<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from papermage.recipes import CoreRecipe\n",
    "from pathlib import Path\n",
    "\n",
    "recipe = CoreRecipe()\n",
    "\n",
    "doc = recipe.run(Path(\"/Users/gully.burns/Documents/Coding/ChatGPT_etc/papermage/tests/fixtures/papermage.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PaperMage: A Unified Toolkit for Processing, Representing, and\\nManipulating Visually-Rich Scientific Documents\\nKyle Lo α ∗\\nZejiang Shen α,τ ∗\\nBenjamin Newman α ∗\\nJoseph Chee Chang α ∗\\nRussell Authur α Erin Bransom α Stefan Candra α Yoganand Chandrasekhar α\\nRegan Huff α Bailey Kuehl α Amanpreet Singh α Chris Wilhelm α Angele Zamarron α\\nMarti A. Hearst β\\nDaniel S. Weld α,ω\\nDoug Downey α,η\\nLuca Soldaini α ∗\\nα Allen Institute for AI\\nτ Massachusetts Institute of Technology\\nβ University of California Berkeley\\nω University of Washington\\nη Northwestern University\\n{kylel, lucas}@allenai.org\\nAbstract\\nDespite growing interest in applying natural\\nlanguage processing (NLP) and computer vi-\\nsion (CV) models to the scholarly domain,\\nscientific documents remain challenging to\\nwork with. They’re often in difficult-to-use\\nPDF formats, and the ecosystem of models\\nto process them is fragmented and incom-\\nplete.\\nWe introduce papermage , an open-\\nsource Python toolkit for analyzing and pro-\\ncessing visually-rich, structured scientific doc-\\numents. papermage offers clean and intuitive\\nabstractions for seamlessly representing and\\nmanipulating both textual and visual document\\nelements. papermage achieves this by integrat-\\ning disparate state-of-the-art NLP and CV mod-\\nels into a unified framework, and provides turn-\\nkey recipes for common scientific document\\nprocessing use-cases. papermage has powered\\nmultiple research prototypes of AI applications\\nover scientific documents, along with Seman-\\ntic Scholar’s large-scale production system for\\nprocessing millions of PDFs.\\n§ github.com/allenai/papermage 1\\n1\\nIntroduction\\nResearch papers and textbooks are central to the\\nscientific enterprise, and there is increasing inter-\\nest in developing new tools for extracting knowl-\\nedge from these visually-rich documents. Recent\\nresearch has explored, for example, AI-powered\\nreading support for math symbol definitions (Head\\net al., 2021), in-situ passage explanations or sum-\\nmaries (August et al., 2023; Rachatasumrit et al.,\\n2022; Kim et al., 2023), automatic span highlight-\\ning (Chang et al., 2023; Fok et al., 2023b), interac-\\ntive clipping and synthesis (Kang et al., 2022, 2023)\\n∗ Core contributors; see author contributions for details. 1\\nWe use code snippets to illustrate our toolkit’s core de- signs and abstractions. Exact syntax in paper may differ from\\nthe actual code, as software will evolve beyond the paper and we opt to simplify syntax when needed for legibility and clarity.\\nWe refer readers to our public code for latest documentation.\\nFigure 1: papermage ’s document creation and represen-\\ntation. (A) Recipes are turn-key methods for processing\\na PDF. (B) They compose models operating across dif-\\nferent data modalities and machine learning frameworks\\nto extract document structure, which we conceptualize\\nas layers of annotation that store textual and visual in-\\nformation. (C) Users can access and manipulate layers.\\nand more.\\nFurther, extracting clean, properly-\\nstructured scientific text from PDF documents (Lo\\net al., 2020; Wang et al., 2020) forms a critical\\nfirst step in pretraining language models of sci-\\nence (Beltagy et al., 2019; Lee et al., 2019; Gu et al.,\\n2020; Luo et al., 2022; Taylor et al., 2022; Tre-\\nwartha et al., 2022; Hong et al., 2023), automatic\\ngeneration of more accessible paper formats (Wang\\net al., 2021), and developing datasets for scientific\\nnatural language processing (NLP) tasks over struc-\\ntured full text (Jain et al., 2020; Subramanian et al.,\\n2020; Dasigi et al., 2021; Lee et al., 2023).\\nHowever, this type of NLP research on scientific\\ncorpora is difficult because the documents come\\nin difficult-to-use formats like PDF, 2 and existing\\ntools for working with the documents are limited.\\nTypically, the first step in scientific document pro-\\ncessing is to invoke a parser on a document file to\\nconvert it into a sequence of tokens and bounding\\nboxes in inferred reading order. Parsers extract only\\nthe raw document content, and obtaining richer\\ndocument structure (e.g., titles, authors, figures) or\\nlinguistic structure and semantics (e.g., sentences,\\ndiscourse units, scientific claims) requires sending\\nthe token sequence through downstream models.\\nUnlike more mature parsers (§2.1), these down-\\nstream models are often research prototypes (§2.2)\\nthat are limited to extracting only a subset of the\\nstructures needed for one’s research (e.g., the same\\nmodel may not provide both sentence splits and fig-\\nure detection). As a result, users must write exten-\\nsive custom code that strings pipelines of multiple\\nmodels together. Research projects using models\\nof different modalities (e.g., combining an image-\\nbased formula detector with a text-based definition\\nextractor) can require hundreds of lines of code.\\nWe introduce papermage , an open-source\\nPython toolkit for processing scientific documents.\\nIts contributions include (1) magelib , a library of\\nprimitives and methods for representing and ma-\\nnipulating visually-rich documents as multimodal\\nconstructs, (2) Predictors , a set of implementa-\\ntions that integrate different state-of-the-art scien-\\ntific document analysis models into a unified inter-\\nface, even if individual models are written in differ-\\nent frameworks or operate on different modalities,\\nand (3) Recipes , which provide turn-key access\\nto well-tested combinations of individual (often\\nsingle-modality) modules to form sophisticated, ex-\\ntensible multimodal pipelines.\\n2\\nRelated Work\\n2.1\\nTurn-key software for scientific documents\\nProcessing visually-rich documents like scientific\\ndocuments requires a joint understanding of both\\nvisual and textual information. In practice, this\\noften requires combining different models into\\ncomplex processing pipelines. For example, GRO-\\nBID (Grobid, 2008–2023), a widely-adopted soft-\\nware tool for scientific document processing, uses\\n2 PDFs store text as character glyphs and their ( x, y ) posi-\\ntions on a page. Converting this data to usable text for NLP requires error-prone operations like inferring token boundaries,\\nwhitespacing, and reading order using visual positioning.\\ntwelve interdependent sequence labeling models 3\\nto perform its full text extraction. Other similar\\ntools inlude CERMINE (Tkaczyk et al., 2015) and\\nParsCit (Councill et al., 2008). While such software\\nis often an ideal choice for off-the-shelf processing,\\nthey are not necessarily designed for easy extension\\nand/or integration with newer research models. 4\\n2.2\\nModels for scientific document processing\\nWhile aforementioned software tools use CRF or\\nBiLSTM-based models, Transformer-based models\\nhave seen wide adoption among NLP researchers\\nfor their powerful processing capabilities. Recent\\nyears have seen the rise of layout-infused Trans-\\nformers (Xu et al., 2019; Shen et al., 2022; Xu\\net al., 2021; Huang et al., 2022b; Chen et al., 2023)\\nfor processing visually-rich documents, including\\nrecovering logical structure (e.g., titles, abstracts)\\nof scientific papers (Huang et al., 2022a). Similarly,\\ncomputer vision (CV) researchers have also shown\\nimpressive capabilities of CNN-based object de-\\ntection models (Ren et al., 2015; Tan et al., 2020)\\nfor segmenting visually-rich documents based on\\ntheir layout. While these research models are pow-\\nerful and extensible for research purposes, it often\\nrequires significant “glue” code and stitching soft-\\nware tools to create robust processing pipelines.\\nFor example, Lincker et al. (2023) bootstraps a so-\\nphisticated processing pipeline around a research\\nmodel for processing children’s textbooks.\\n2.3\\nCombining models and pipelines\\npapermage ’s use case lies between that of turn-\\nkey software and a framework for supporting re-\\nsearch. Similar to Transformers (Wolfe et al.,\\n2022)’s integration of different research mod-\\nels into standard interfaces, others have done\\nsimilarly for the visually-rich document domain.\\nLayoutParser (Shen et al., 2021) provides mod-\\nels for visually-rich documents and supports\\nthe creation of document processing pipelines.\\npapermage , in fact, depends on LayoutParser\\nfor access to vision models, but is designed to\\nalso integrate text models which are omitted from\\n3 https://grobid.readthedocs.io/en/latest/\\nTraining-the-models-of-Grobid/#models 4\\nMost research in NLP requires that a researcher be able to manipulate models within Python. Yet, Grobid requires users\\nto manage a separate service process and send PDFs through a client. In performing evaluation in §3.3, we also found it\\ndifficult to run only the model components isolated from PDF utilities, which makes comparison with other research models\\nchallenging without significant “glue” code.\\nFigure 2: Entities are multimodal content units. Here,\\nspans of a sentence are used to identify its text among\\nall symbols, while boxes map its visual coordinates on\\na page. spans and boxes can include non-contiguous\\nunits, allowing great flexibility in Entities to handle\\nlayout nuances. A sentence split across columns/pages\\nand interrupted by floating figures/footnotes would re-\\nquire multiple spans and bounding boxes to represent.\\nLayoutParser .\\nTo allow models of different\\nmodalities to work well together, we also devel-\\noped the magelib library (§3.1).\\n3\\nDesign of papermage\\npapermage is three parts: (1) magelib , a library for\\nintuitively representing and manipulating visually-\\nrich documents, (2) Predictors , implementations\\nof models for analyzing scientific papers that unify\\ndisparate machine learning frameworks under a\\ncommon interface, and (3) Recipes , combinations\\nof Predictors that form multimodal pipelines.\\n3.1\\nRepresenting and manipulating\\nvisually-rich documents with magelib\\nIn this section, we use code snippets to show how\\nour library’s abstractions and syntax are tailored\\nfor the visually-rich document problem domain.\\nData Classes.\\nmagelib provides three base data\\nclasses for representing fundamental elements of\\nvisually-rich, structured documents: Document ,\\nLayers and Entities . First, a Document might\\nminimally store text as a string of symbols:\\n1 >>> from papermage import Document\\n2 >>> doc.symbols\\n3 \"Revolt: Collaborative Crowdsourcing...\"\\nBut visually-rich documents are more than a lin-\\nearized string. For example, analyzing a scientific\\npaper requires access to its visuospatial layout (e.g.,\\npages, blocks, lines), logical structure (e.g., title,\\nabstract, figures, tables, footnotes, sections), se-\\nmantic units (e.g., paragraphs, sentences, tokens),\\nand more (e.g., citations, terms). In practice, this\\nmeans different parts of doc.symbols can corre-\\nspond to different paragraphs, sentences, tokens,\\netc. in the Document , each with its own set of\\ncorresponding coordinates representing its visual\\nposition on a page.\\nmagelib represents structure using Layers that\\ncan be accessed as attributes of a Document (e.g.,\\ndoc.sentences ,\\ndoc.figures ,\\ndoc.tokens )\\n(Figure 1). Each Layer is a sequence of content\\nunits, called Entities , which store both textual\\n(e.g., spans, strings) and visuospatial (e.g.,\\nbounding boxes, pixel arrays) information:\\n1 >>> sentences = Layer(entities=[\\n2\\nEntity(...), Entity(...), ...\\n3\\n])\\nSee Figure 2 for an example on how “sentences” in\\na scientific document are represented as Entities .\\nSection §3.2 explains in more detail how a user can\\ngenerate Entities .\\nMethods.\\nmagelib also provides a set of func-\\ntions for building and interacting with data: aug-\\nmenting a Document with additional Layers ,\\ntraversing and spatially searching for matching\\nEntities in one Layer , and cross-referencing be-\\ntween Layers (see Figure 3).\\nA Document that only contains doc.symbols\\ncan be augmented with additional Layers :\\n1 >>> paragraphs = Layer(...)\\n2 >>> sentences = Layer(...)\\n3 >>> tokens = Layer(...)\\n4 5 >>> doc.add(paragraphs , sentences , tokens)\\nAdding Layers automatically grants users the\\nability to iterate through Entities and cross-\\nreference intersecting Entities across Layers :\\n1 >>> for paragraph in doc.paragraphs:\\n2\\nfor sent in paragraph.sentences:\\n3\\nfor token in sentence.tokens:\\n4\\n...\\nmagelib also supports cross-modality opera-\\ntions. For example, searching for textual Entities\\nwithin a visual region on the PDF (See Figure 3 F):\\n1 >>> query = Box(l=423, t=71, w=159, h=87)\\n2 >>> selection = doc.find(query , \"tokens\")\\n3 >>> [t.text for t in selection]\\n4 [\"Techniques\", \"for\", \"collecting\", ...]\\n>>> doc.paragraphs[0]\\n>>> doc.paragraphs[0].sentences[2] or\\n>>> doc.sentences[2]\\n>>> doc.sentences[2].tokens[9:13] or\\n>>> doc.tokens[169:173]\\n>>> doc.figures[0]\\n>>> doc.captions[0]\\n>>> user_query = Box(l,t,w,h, page=0)\\n>>> selected_tokens = doc.find(user_query, layer=“tokens”)\\n>>> [token.text for token in selected_tokens]\\n[“Techniques”, “for”, “collecting”, “labeled”, “data”, “perts”, “for”, “manual”, “annotation”, ...]\\nCrowdsourcing provides a scalable and efficient way to con- struct labeled datasets for training machine learning systems. However, creating comprehensive label guidelines for crowd-\\nworkers is often prohibitive even for seemingly simple con- cepts. Incomplete or ambiguous label guidelines can then result in differing interpretations of concepts and inconsistent\\nlabels. Existing approaches for improving label quality, such as worker screening or detection of poor work, are ineffective for this problem and can lead to rejection of honest work and a\\nmissed opportunity to capture rich interpretations about data. We introduce Revolt , a collaborative approach that brings ideas from expert annotation workflows to crowd-based labeling.\\nRevolt eliminates the burden of creating detailed label guide- lines by harnessing crowd disagreements to identify ambigu- ous concepts and create rich structures (groups of semantically\\nrelated items) for post-hoc label decisions. Experiments com- paring Revolt to traditional crowdsourced labeling show that Revolt produces high quality labels without requiring label\\nguidelines in turn for an increase in monetary cost. This up front cost, however, is mitigated by Revolt\\'s ability to produce reusable structures that can accommodate a variety of label\\nboundaries without requiring new data to be collected. Further comparisons of Revolt\\'s collaborative and non-collaborative variants show that collabvoration reaches higher label accura-\\ncy with lower monetary cost.\\nlearned models that must be trained on representative datasets labeled according to target concepts (e.g., speech labeled by their intended commands, faces labeled in images, emails la-\\nbeled as spam or not spam).\\ncrowdsourcing; machine learning; collaboration; real-time\\nH.5.m. Information Interfaces and Presentation (e.g. HCI): Miscellaneous\\nFrom conversational assistants on mobile devices, to facial\\nTechniques for collecting labeled data include recruiting ex- perts for manual annotation [51], extracting relations from readily available sources (e.g., identifying bodies of text in\\nparallel online translations [46, 13]), and automatically gener- ating labels based on user behaviors (e.g., using dwell time to implicitly mark search result relevance [2]). Recently,\\nmany practitioners have also turned to crowdsourcing for cre- ating labeled datasets at low cost [49]. Successful crowd-\\nFigure 1. Revolt creates labels for unanimously labeled “certain” items (e.g., cats and not cats ), and surfaces categories of “uncertain” items enriched with crowd feedback (e.g., cats and dogs and cartoon cats in the dotted middle region are annotated with crowd explanations). Rich\\nstructures allow label requesters to better understand concepts in the data and make post-hoc decisions on label boundaries (e.g., assigning cats and dogs to the cats label and cartoon cats to the not cats label) rather than providing crowd-workers with a priori label guidelines.\\nABSTRACT\\nACM Classification Keywords\\nAuthor Keywords\\nINTRODUCTION\\nA\\nB\\nC\\nD\\nE\\nF\\nA\\nB\\nC\\nD\\nE\\nF\\nFigure 3: Illustrates how Entities can be accessed flexibly in different ways: (A) Accessing the Entity of the first\\nparagraph in the Document via its own Layer (B) Accessing a sentence via the paragraph Entity or directly via the\\nsentences Layer (C) Similarly, the same tokens can be accessed via the overlapping sentence Entity or directly\\nvia the tokens Layer of the Document (where the first tokens are the title of the paper.) (D, E) Figures, captions,\\ntables and keywords can be accessed in similar ways (F) Additionally, given a bounding box (e.g., of a user selected\\nregion), papermage can find the corresponding Entities for a given Layer , in this case finding the tokens under\\nthe region. Excerpt from Chang et al. (2017).\\nProtocols\\nand\\nUtilities.\\nTo\\ninstantiate\\na\\nDocument ,\\nmagelib provides protocols and\\nutilities like Parsers and Rasterizers , which\\nhook into off-the-shelf PDF processing tools: 5\\n1 >>> import papermage as pm\\n2 >>> parser = pm.PDF2TextParser()\\n3 >>> doc = parser.parse(\"...pdf\")\\n4 >>> [token.text for token in doc.tokens]\\n5 [\"Revolt\", \":\", \"Collaborative\", ...]\\n6 >>> doc.images\\n7 None\\n8 9 >>> rasterizer = pm.PDF2ImageRasterizer()\\n10 >>> doc2 = rasterizer.rasterize(\"...pdf\")\\n11 >>> doc.images = doc2.images\\n12 >>> doc.images\\n13 [Image(np.array(...)), ...]\\nIn this example, papermage runs PDF2TextParser\\n(using pdfplumber ) to extract the textual in-\\nformation from a PDF file.\\nThen it runs\\nPDF2ImageRasterizer (using pdf2image ) to up-\\ndate the first Document with images of pages.\\n5 PDFs are not the only way of representing visually-rich\\ndocuments. For example, many scientific documents are dis- tributed in XML format. As PDFs are the dominant distribu-\\ntion format of scientific documents, we focus our efforts on PDF-specific needs. Nevertheless, we also provide Parsers\\nin magelib that can instantiate a Document from XML input.\\nSee Appendix A.1.\\n3.2\\nInterfacing with models for scientific\\ndocument analysis through Predictors\\nIn §3.1, we described how users create Layers\\nby assembling collections of Entities . But how\\nwould they make Entities in the first place?\\nFor example, to identify multimodal structures\\nin visually-rich documents, researchers might want\\nto build complex pipelines that run and combine\\noutput from many different models (e.g., computer\\nvision models for extracting figures, NLP models\\nfor classifying body text). papermage provides\\na unified interface, called Predictors , to ensure\\nmodels produce Entities that are compatible with\\nthe Document .\\npapermage\\nincludes\\nseveral\\nready-to-use\\nPredictors that leverage state-of-the-art models\\nto extract specific document structures (Table 1).\\nWhile magelib ’s abstractions are general for\\nvisually-rich documents, Predictors are opti-\\nmized for parsing of scientific documents. They\\nare designed to (1) be compatible with models\\nfrom many different machine learning frameworks,\\n(2) support inference with text-only, vision-only,\\nand multimodal models, and (3) support both adap-\\ntation of off-the-shelf, pretrained models as well as\\nUse case\\nDescription\\nExamples\\nLinguistic/Semantic\\nSegments doc into text units often used for down-\\nstream models.\\nSentencePredictor wraps sciSpaCy (Neumann et al., 2019) and PySBD (Sadvilkar and Neumann, 2020) to segment sentences. WordPredictor is\\na custom scikit-learn model to identify broken words split across PDF lines or\\ncolumns. ParagraphPredictor is a set of heuristics on top of both layout and logical structure models to extract paragraphs.\\nLayoutStructure\\nSegments doc into visual block regions.\\nBoxPredictor wraps models from LayoutParser (Shen et al., 2021), which\\nprovides vision models like EfficientDet (Tan et al., 2020) pretrained on scientific layouts (Zhong et al., 2019).\\nLogicalStructure\\nSegments doc into orga- nizational units like title,\\nabstract, body, footnotes, caption, and more.\\nSpanPredictor wraps Token Classifiers from Transformers (Wolfe et al., 2022),\\nwhich provides both pretrained weights from VILA (Shen et al., 2022), as well as RoBERTa (Liu et al., 2019), SciBERT (Beltagy et al., 2019) weights that we’ve\\nfinetuned on similar data.\\nTask-specific\\nModels for a given sci- entific document process-\\ning task can be used with papermage if wrapped as\\na Predictor following\\ncommon patterns.\\nAs many practitioners depend on prompting a model through an API call, we implement APIPredictor which interfaces external APIs, such as GPT-3 (Brown\\net al., 2020), to perform tasks like question answering over a structured Document . We also implement SnippetRetrievalPredictor which wraps models like Con-\\ntriever (Izacard et al., 2022) to perform top- k within-document snippet retrieval. See §4 for how these two can be combined.\\nTable 1: Types of Predictors implemented in papermage .\\nModel\\nFull\\nGrobid Subset P\\nR\\nF1\\nP\\nR\\nF1\\nGrobid CRF\\n40.6\\n38.3\\n39.1\\n81.2\\n76.7\\n78.9 Grobid NN\\n42.0\\n36.5\\n37.6\\n84.1\\n73.0\\n78.2 RoBERTa\\n75.9\\n80.0\\n76.8\\n82.6\\n83.9\\n83.2 I-VILA\\n92.0\\n94.1\\n92.7\\n92.2\\n95.2\\n93.7\\nTable 2: Evaluating performance of CoreRecipe for\\nlogical structure recovery on S2-VL (Shen et al., 2022).\\nMetrics are computed for token-level classification,\\nmacro-averaged over categories. The “Grobid Subset”\\nlimits evaluation to only categories for which Grobid\\nreturns bounding box information, which was necessary\\nfor evaluation on S2-VL. See Appendix A.3 for details.\\ndevelopment of new ones from scratch. Similarly\\nto the Transformers library, a Predictor ’s\\nimplementation is typically independent from\\nits configuration, allowing users to customize\\neach Predictor by tweaking hyperparameters or\\nloading a different set of weights.\\nBelow, we showcase how a vision model and\\ntwo text models (both neural and symbolic) can be\\napplied in succession to a single Document . See\\nTable 1 for a summary of supported Predictors .\\n1 >>> import papermage as pm\\n2 >>> cv = pm.BoxPredictor(...)\\n3 >>> tables , figures = cv.predict(doc)\\n4 >>> doc.add(tables , figures)\\n5 6 >>> nlp_neu = pm.SpanPredictor(...)\\n7 >>> titles , authors = nlp_neu.predict(doc)\\n8 >>> doc.add(titles , authors)\\n9 10 >>> nlp_sym = pm.SentencePredictor(...)\\n11 >>> sentences = nlp_sym.predict(doc)\\n12 >>> doc.add(sentences)\\nPredictors return a list of Entities , which\\ncan be group_by() to organize them based on pre-\\ndicted label value (e.g., tokens classified as “title”\\nor “authors”). Finally, these predictions are passed\\nto doc.annotate() to be added to Document .\\n3.3\\nEnd-to-end processing with Recipes\\nFinally, papermage provides predefined combina-\\ntions of Predictors , called Recipes , for users\\nseeking high-quality options for turn-key process-\\ning of visually-rich documents:\\n1 from papermage import CoreRecipe\\n2 recipe = CoreRecipe()\\n3 doc = recipe.run(\"...pdf\")\\n4 doc.captions[0].text\\n5 >>> \"Figure 1. ...\"\\nRecipes can also be flexibly modified to sup-\\nport development. For example, our current de-\\nfault combines the pdfplumber PDF parsing utility\\nwith the I-VILA (Shen et al., 2022) research model.\\nWe show in Table 2 an evaluation comparing this\\nagainst the same recipe but configured to (1) swap\\nI-VILA for a RoBERTa model, as well as (2) swap\\nboth for Grobid API calls. We expect Recipes\\nto appeal to two groups of users—end-to-end con-\\nsumers, and developers of high-level applications.\\nThe former is comprised of developers and re-\\nsearchers who are looking for a one-step solution\\nto multimodal scientific document analysis. The\\nlatter are likely developers and researchers looking\\nto combine document structure primitives to build\\na complex application (see example in §4).\\n4\\nVignette: Building an Attributed QA\\nSystem for Scientific Papers\\nHow could researchers leverage papermage for\\ntheir research? Here, we walk through a user sce-\\nnario in which a researcher (Lucy) is prototyping\\nan attributed QA system for science.\\nSystem Design.\\nDrawing inspiration from Ko\\net al. (2020), Lee et al. (2023), Fok et al. (2023a),\\nand Newman et al. (2023), Lucy is studying how\\nlanguage models can be used to resolve questions\\nthat arise while reading a paper (e.g. What does\\nthis mean? or What does this refer to? ). In her\\nprototype interface, a user can highlight a passage\\nin a PDF and ask a question about it. A retrieval\\nmodel then finds relevant passages from the rest\\nof the paper. The prototype then uses the text of\\nthe retrieved passages along with the user question\\nto prompt a language model to generate an answer.\\nWhen presenting the answer to the user, the proto-\\ntype also visually highlights the retrieved passages\\nas supporting evidence to the generated answer.\\nGetting started quickly.\\nAs a researcher profi-\\ncient in Python, it only takes Lucy minutes to install\\npapermage using pip and successfully process a lo-\\ncal PDF file by following the example code snippet\\nfor CoreRecipe in §3.2. In an interactive session,\\nshe familiarizes herself with the provided Layers\\nby following the traversal, cross-referencing and\\nquerying examples in §3.1. She makes sure she can\\nserialize and re-instantiate her Document (§A.2).\\nFormatting input.\\nBefore using papermage ,\\nLucy has prior experience building QA pipelines,\\nbut has only dealt with documents as sentence-\\nsplit text data (e.g., <List[str]> ). Lucy realizes\\nthat she can reuse her prior text-only code with\\npapermage by implementing a couple of wrappers\\nto gain additional capabilities: First, she converts\\na user’s highlighted passage from a visual selec-\\ntion to text following the example in Figure 3F.\\nNext, she converts Document to her required text\\nformat by following the traversal examples in §3.1\\n(e.g., using [s.text for s in doc.sentences] ).\\nWithin a few lines of code, Lucy has everything\\nshe needs for text-only input to her QA pipeline.\\nFormatting output.\\nLucy runs her QA system\\non her newly acquired text data and now has (1) a\\nmodel-generated answer and (2) several retrieved\\nevidence passages. She realizes that she already\\nhas access to the evidences’ bounding boxes via a\\nsimilar call to how she defined the model input con-\\ntext (e.g., [s.boxes for s in doc.sentences] ).\\nShe can easily pass this to the user interface to en-\\nable linking to and highlighting of those passages.\\nDefining a Predictor .\\nThe pattern Lucy has\\nfollowed is used in our many Predictor imple-\\nmentations: (1) gain access to text by traversing\\nLayers (e.g., sentences), (2) perform all usual\\nNLP computation on that text, and (3) format\\nmodel output as Entities . This simple pattern\\nallows users to reuse familiar models in existing\\nframeworks and eschews lengthy onboarding to\\npapermage . Lucy wraps her prompting and re-\\ntrieval code in new classes: APIPredictor and\\nSnippetRetrievalPredictor (see Table 1).\\nFast iterations.\\nLeveraging the bounding box\\ndata from papermage to visually highlight the re-\\ntrieved passages, Lucy suspects the retrieval com-\\nponent is likely underperforming. She makes a sim-\\nple edit from doc.sentences to doc.paragraphs\\nand evaluates system performance under different\\ninput granularity. She also realizes the system of-\\nten retrieves content outside the main body text.\\nShe restricts her traversal to filter out paragraphs\\nthat overlap with footnotes— [p.text for p in\\ndoc.paragraphs if len(p.footnotes) == 0] —\\nmaking clever use of the cross-referencing function-\\nality to detect when a paragraph is actually coming\\nfrom a footnote. This example demonstrates the\\nversatility of the affordances provided by magelib .\\n5\\nConclusion\\nIn this work, we’ve introduced papermage , an\\nopen-source Python toolkit for processing scientific\\ndocuments. papermage was developed to supply\\nhigh-quality data and reduce friction for research\\nprototype development at Semantic Scholar. To-\\nday, it is being used in the production PDF process-\\ning pipeline to provide data for both the literature\\ngraph (Ammar et al., 2018; Kinney et al., 2023)\\nand the paper-reading interface (Lo et al., 2023). It\\nhas also been used in working research prototypes\\nwhich have since contributed to research publica-\\ntions (Fok et al., 2023b; Kim et al., 2023). 6 We\\nopen-source papermage in hopes it will simplify\\nresearch workflows that depend on scientific doc-\\numents and promote extensions to other visually-\\nrich documents like textbooks (Lincker et al., 2023)\\nand digitized print media (Lee et al., 2020).\\n6 See a demo of such a prototype papeo.app/demo .\\nEthical Considerations\\nAs a toolkit primarily designed to process scientific\\ndocuments, there are two areas where papermage\\ncould cause harms or have unintended effects.\\nExtraction\\nof\\nbibliographic\\ninformation\\npapermage could be used to parse author names,\\naffiliation, emails from scientific documents. Like\\nany software, this extraction can be noisy, leading\\nto incorrect parsing and thus mis-attribution of\\nmanuscripts.\\nFurther, since papermage relies\\non static PDF documents, rather than metadata\\ndynamically retrieved from publishers, users of\\npapermage need consider how and when extracted\\nnames should no longer be associated with authors,\\na harmful practice called deadnaming (Queer in AI\\net al., 2023). We recommend papermage users to\\nexercise caution when using our toolkit to extract\\nmetadata, to cross-reference extracted content with\\nother sources when possible, and to design systems\\nsuch that authors have the ability to manually edit\\nany data about themselves.\\nMisrepresentation or fabrication of informa-\\ntion in documents\\nIn §3, we discussed how\\npapermage can be easily extended to support high-\\nlevel applications. Such applications might include\\nquestion answering chatbots, or AI summarizers\\nthat perform information synthesis over one or\\nmore papermage documents. Such applications\\ntypically rely on generative models to produce their\\noutput, which might fabricate incorrect informa-\\ntion or misstate claims. Developers should be vig-\\nilant when integrating papermage output into any\\ndownstream application, especially in systems that\\npurport to represent information gathered from sci-\\nentific publications.\\nAcknowledgements\\nWe thank our teammates at Semantic Scholar for\\ntheir help on this project. In particular: Rodney\\nKinney provided insight during discussions about\\nhow best to represent data extracted from docu-\\nments; Paul Sayre provided feedback on initial\\ndesigns of the library; Chloe Anastasiades, Dany\\nHaddad and Egor Klevak tested earlier versions of\\nthe library; Tal August, Raymond Fok, and Andrew\\nHead motivated the need for such a toolkit dur-\\ning their internships building augmented reading\\ninterfaces; Jaron Lochner and Kelsey MacMillan\\nhelped us get additional engineering support; and\\nOren Etzioni provided enthusiasm and support for\\ncontinued investment in this toolkit.\\nThis project was supported in part by NSF Grant\\nOIA-2033558 and NSF Grant CNS-2213656.\\nAuthor Contributions\\nAll authors contributed to the implementation of\\npapermage and/or the writing of this paper.\\nCore contributors.\\nKyle Lo and Zejiang Shen\\ninitiated the project and co-wrote initial implemen-\\ntations of magelib and some Predictors . Later,\\nKyle Lo and Luca Soldaini refactored a majority of\\nmagelib , Predictors , and added Recipes . Ben-\\njamin Newman added new Predictors to support\\nuse-cases like those in the Vignette (§4). Joseph\\nChee Chang implemented an end-to-end web-based\\nvisual interface for papermage and helped iterate\\non papermage ’s designs. All core contributors\\nhelped with writing. Finally, Kyle Lo led all aspects\\nof the project, including design and implementa-\\ntion, as well as mentorship of other contributors to\\nthe toolkit (see below).\\nOther contributors.\\nRussell Authur, Stefan Can-\\ndra, Yoganand Chandrasekhar, Regan Huff, Aman-\\npreet Singh and Angele Zamarron each worked\\nclosely with Kyle Lo to contribute a Predictor\\nto papermage . Erin Bransom and Bailey Kuehl\\nhelped with data annotation for training and evalu-\\nating those Predictors . Chris Wilhelm provided\\nfeedback on papermage ’s design and implemented\\nfaster indexing of Entities when building Layers .\\nFinally, Marti Hearst, Daniel Weld, and Doug\\nDowney helped with writing and overall advising\\non the project.\\nReferences\\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\\nman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\\nKyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe-\\nters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang,\\nChris Wilhelm, Zheng Yuan, Madeleine van Zuylen,\\nand Oren Etzioni. 2018. Construction of the litera-\\nture graph in semantic scholar. In Proceedings of\\nthe 2018 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 3 (Indus-\\ntry Papers) , pages 84–91, New Orleans - Louisiana.\\nAssociation for Computational Linguistics.\\nTal August, Lucy Lu Wang, Jonathan Bragg, Marti A.\\nHearst, Andrew Head, and Kyle Lo. 2023. Paper\\nplain: Making medical research papers approachable\\nto healthcare consumers with natural language pro-\\ncessing. ACM Trans. Comput.-Hum. Interact. , 30(5).\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nERT: A pretrained language model for scientific text.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 3615–\\n3620, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nClark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners.\\nIn Ad-\\nvances in Neural Information Processing Systems ,\\nvolume 33, pages 1877–1901. Curran Associates,\\nInc.\\nJoseph Chee Chang, Saleema Amershi, and Ece Kamar.\\n2017. Revolt: Collaborative crowdsourcing for label-\\ning machine learning datasets. In Proceedings of the\\n2017 CHI Conference on Human Factors in Comput-\\ning Systems , CHI ’17, page 2334–2346, New York,\\nNY, USA. Association for Computing Machinery.\\nJoseph Chee Chang, Amy X. Zhang, Jonathan Bragg,\\nAndrew Head, Kyle Lo, Doug Downey, and Daniel S.\\nWeld. 2023. Citesee: Augmenting citations in scien-\\ntific papers with persistent and personalized historical\\ncontext. In Proceedings of the 2023 CHI Conference\\non Human Factors in Computing Systems , CHI ’23,\\nNew York, NY, USA. Association for Computing\\nMachinery.\\nCatherine Chen, Zejiang Shen, Dan Klein, Gabriel\\nStanovsky, Doug Downey, and Kyle Lo. 2023. Are\\nlayout-infused language models robust to layout dis-\\ntribution shifts? a case study with scientific docu-\\nments. In Findings of the Association for Computa-\\ntional Linguistics: ACL 2023 , pages 13345–13360,\\nToronto, Canada. Association for Computational Lin-\\nguistics.\\nIsaac Councill, C. Lee Giles, and Min-Yen Kan. 2008.\\nParsCit: an open-source CRF reference string pars-\\ning package. In Proceedings of the Sixth Interna-\\ntional Conference on Language Resources and Eval-\\nuation (LREC’08) , Marrakech, Morocco. European\\nLanguage Resources Association (ELRA).\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\\nNoah A. Smith, and Matt Gardner. 2021. A dataset\\nof information-seeking questions and answers an-\\nchored in research papers. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies , pages 4599–4610, On-\\nline. Association for Computational Linguistics.\\nRaymond Fok, Joseph Chee Chang, Tal August, Amy X.\\nZhang, and Daniel S. Weld. 2023a. Qlarify: Bridg-\\ning scholarly abstracts and papers with recursively\\nexpandable summaries. arXiv , abs/2310.07581.\\nRaymond Fok, Hita Kambhamettu, Luca Soldaini,\\nJonathan Bragg, Kyle Lo, Marti Hearst, Andrew\\nHead, and Daniel S Weld. 2023b. Scim: Intelligent\\nskimming support for scientific papers. In Proceed-\\nings of the 28th International Conference on Intelli-\\ngent User Interfaces , IUI ’23, page 476–490, New\\nYork, NY, USA. Association for Computing Machin-\\nery.\\nGrobid. 2008–2023. Grobid. https://github.com/\\nkermitt2/grobid .\\nYu Gu, Robert Tinn, Hao Cheng, Michael R. Lucas,\\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\\nspecific language model pretraining for biomedical\\nnatural language processing. ACM Transactions on\\nComputing for Healthcare (HEALTH) , 3:1 – 23.\\nAndrew Head, Kyle Lo, Dongyeop Kang, Raymond\\nFok, Sam Skjonsberg, Daniel S. Weld, and Marti A.\\nHearst. 2021. Augmenting scientific papers with just-\\nin-time, position-sensitive definitions of terms and\\nsymbols. In Proceedings of the 2021 CHI Conference\\non Human Factors in Computing Systems , CHI ’21,\\nNew York, NY, USA. Association for Computing\\nMachinery.\\nZhi Hong, Aswathy Ajith, James Pauloski, Eamon\\nDuede, Kyle Chard, and Ian Foster. 2023. The dimin-\\nishing returns of masked language models to science.\\nIn Findings of the Association for Computational\\nLinguistics: ACL 2023 , pages 1270–1283, Toronto,\\nCanada. Association for Computational Linguistics.\\nPo-Wei Huang, Abhinav Ramesh Kashyap, Yanxia Qin,\\nYajing Yang, and Min-Yen Kan. 2022a. Lightweight\\ncontextual logical structure recovery. In Proceedings\\nof the Third Workshop on Scholarly Document Pro-\\ncessing , pages 37–48, Gyeongju, Republic of Korea.\\nAssociation for Computational Linguistics.\\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and\\nFuru Wei. 2022b. Layoutlmv3: Pre-training for doc-\\nument ai with unified text and image masking. Pro-\\nceedings of the 30th ACM International Conference\\non Multimedia .\\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\\nEdouard Grave. 2022. Unsupervised dense informa-\\ntion retrieval with contrastive learning. Transactions\\non Machine Learning Research .\\nSarthak Jain, Madeleine van Zuylen, Hannaneh Ha-\\njishirzi, and Iz Beltagy. 2020. SciREX: A challenge\\ndataset for document-level information extraction. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 7506–\\n7516, Online. Association for Computational Lin-\\nguistics.\\nHyeonsu B. Kang, Joseph Chee Chang, Yongsung Kim,\\nand Aniket Kittur. 2022. Threddy: An interactive\\nsystem for personalized thread-based exploration and\\norganization of scientific literature. In Proceedings of\\nthe 35th Annual ACM Symposium on User Interface\\nSoftware and Technology , UIST ’22, New York, NY,\\nUSA. Association for Computing Machinery.\\nHyeonsu B. Kang, Sherry Tongshuang Wu, Joseph Chee\\nChang, and Aniket Kittur. 2023. Synergi: A mixed-\\ninitiative system for scholarly synthesis and sense-\\nmaking. In Proceedings of the 36th Annual ACM\\nSymposium on User Interface Software and Technol-\\nogy . Association for Computing Machinery.\\nTae Soo Kim, Matt Latzke, Jonathan Bragg, Amy X.\\nZhang, and Joseph Chee Chang. 2023. Papeos: Aug-\\nmenting research papers with talk videos. In Proceed-\\nings of the 36th Annual ACM Symposium on User\\nInterface Software and Technology .\\nRodney Kinney, Chloe Anastasiades, Russell Authur,\\nIz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\\nski, Isabel Cachola, Stefan Candra, Yoganand Chan-\\ndrasekhar, Arman Cohan, Miles Crawford, Doug\\nDowney, Jason Dunkelberger, Oren Etzioni, Rob\\nEvans, Sergey Feldman, Joseph Gorney, David Gra-\\nham, Fangzhou Hu, Regan Huff, Daniel King, Se-\\nbastian Kohlmeier, Bailey Kuehl, Michael Langan,\\nDaniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\\nKelsey MacMillan, Tyler Murray, Chris Newell,\\nSmita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\\nShen, Amanpreet Singh, Luca Soldaini, Shivashankar\\nSubramanian, Amber Tanaka, Alex D. Wade, Linda\\nWagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu,\\nJiangjiang Yang, Angele Zamarron, Madeleine Van\\nZuylen, and Daniel S. Weld. 2023. The Semantic\\nScholar Open Data Platform. ArXiv , abs/2301.10140.\\nWei-Jen Ko, Te-yuan Chen, Yiyan Huang, Greg Durrett,\\nand Junyi Jessy Li. 2020. Inquisitive question gener-\\nation for high level text comprehension. In Proceed-\\nings of the 2020 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP) , pages\\n6544–6555, Online. Association for Computational\\nLinguistics.\\nBenjamin Charles Germain Lee, Jaime Mears, Eileen\\nJakeway, Meghan Ferriter, Chris Adams, Nathan\\nYarasavage, Deborah Thomas, Kate Zwaard, and\\nDaniel S. Weld. 2020. The newspaper navigator\\ndataset: Extracting headlines and visual content from\\n16 million historic newspaper pages in chronicling\\namerica. In Proceedings of the 29th ACM Interna-\\ntional Conference on Information & Knowledge Man-\\nagement , CIKM ’20, page 3055–3062, New York,\\nNY, USA. Association for Computing Machinery.\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\\n2019. BioBERT: a pre-trained biomedical language\\nrepresentation model for biomedical text mining.\\nBioinformatics , 36(4):1234–1240.\\nYoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol\\nHwang, Jaehyeon Kim, Hong-In Lee, and Moontae\\nLee. 2023. QASA: Advanced question answering on\\nscientific articles. In Proceedings of the 40th Inter-\\nnational Conference on Machine Learning , volume\\n202 of Proceedings of Machine Learning Research ,\\npages 19036–19052. PMLR.\\nÉlise Lincker, Olivier Pons, Camille Guinaudeau, Is-\\nabelle Barbet, Jérôme Dupire, Céline Hudelot, Vin-\\ncent Mousseau, and Caroline Huron. 2023. Layout\\nand activity-based textbook modeling for automatic\\npdf textbook extraction. In iTextbooks@AIED .\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. ArXiv , abs/1907.11692.\\nKyle Lo, Joseph Chee Chang, Andrew Head, Jonathan\\nBragg, Amy X. Zhang, Cassidy Trier, Chloe Anas-\\ntasiades, Tal August, Russell Authur, Danielle Bragg,\\nErin Bransom, Isabel Cachola, Stefan Candra, Yo-\\nganand Chandrasekhar, Yen-Sung Chen, Evie Yu-\\nYen Cheng, Yvonne Chou, Doug Downey, Rob\\nEvans, Raymond Fok, Fangzhou Hu, Regan Huff,\\nDongyeop Kang, Tae Soo Kim, Rodney Kinney,\\nAniket Kittur, Hyeonsu Kang, Egor Klevak, Bai-\\nley Kuehl, Michael Langan, Matt Latzke, Jaron\\nLochner, Kelsey MacMillan, Eric Marsh, Tyler Mur-\\nray, Aakanksha Naik, Ngoc-Uyen Nguyen, Srishti\\nPalani, Soya Park, Caroline Paulic, Napol Rachata-\\nsumrit, Smita Rao, Paul Sayre, Zejiang Shen, Pao\\nSiangliulue, Luca Soldaini, Huy Tran, Madeleine van\\nZuylen, Lucy Lu Wang, Christopher Wilhelm, Caro-\\nline Wu, Jiangjiang Yang, Angele Zamarron, Marti A.\\nHearst, and Daniel S. Weld. 2023. The Semantic\\nReader Project: Augmenting Scholarly Documents\\nthrough AI-Powered Interactive Reading Interfaces.\\nArXiv , abs/2303.14334.\\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\\nney, and Daniel Weld. 2020. S2ORC: The semantic\\nscholar open research corpus. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 4969–4983, Online. Asso-\\nciation for Computational Linguistics.\\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\\nBiogpt:\\nGenerative pre-trained transformer for\\nbiomedical text generation and mining. Briefings\\nin bioinformatics .\\nMark Neumann, Daniel King, Iz Beltagy, and Waleed\\nAmmar. 2019. ScispaCy: Fast and robust models\\nfor biomedical natural language processing. In Pro-\\nceedings of the 18th BioNLP Workshop and Shared\\nTask , pages 319–327, Florence, Italy. Association for\\nComputational Linguistics.\\nBenjamin Newman, Luca Soldaini, Raymond Fok, Ar-\\nman Cohan, and Kyle Lo. 2023. A question answer-\\ning framework for decontextualizing user-facing snip-\\npets from scientific documents. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) .\\nOrganizers of Queer in AI, Anaelia Ovalle, Arjun Sub-\\nramonian, Ashwin Singh, Claas Voelcker, Danica J.\\nSutherland, Davide Locatelli, Eva Breznik, Filip Klu-\\nbicka, Hang Yuan, Hetvi J, Huan Zhang, Jaidev\\nShriram, Kruno Lehman, Luca Soldaini, Maarten\\nSap, Marc Peter Deisenroth, Maria Leonor Pacheco,\\nMaria Ryskina, Martin Mundt, Milind Agarwal, Nyx\\nMclean, Pan Xu, A Pranav, Raj Korpan, Ruchira\\nRay, Sarah Mathew, Sarthak Arora, St John, Tanvi\\nAnand, Vishakha Agrawal, William Agnew, Yanan\\nLong, Zijie J. Wang, Zeerak Talat, Avijit Ghosh,\\nNathaniel Dennler, Michael Noseworthy, Sharvani\\nJha, Emi Baylor, Aditya Joshi, Natalia Y. Bilenko,\\nAndrew Mcnamara, Raphael Gontijo-Lopes, Alex\\nMarkham, Evyn Dong, Jackie Kay, Manu Saraswat,\\nNikhil Vytla, and Luke Stark. 2023. Queer In AI: A\\nCase Study in Community-Led Participatory AI. In\\nProceedings of the 2023 ACM Conference on Fair-\\nness, Accountability, and Transparency , FAccT ’23,\\npage 1882–1895, New York, NY, USA. Association\\nfor Computing Machinery.\\nNapol Rachatasumrit, Jonathan Bragg, Amy X. Zhang,\\nand Daniel S Weld. 2022. Citeread: Integrating lo-\\ncalized citation contexts into scientific paper reading.\\nIn 27th International Conference on Intelligent User\\nInterfaces , IUI ’22, page 707–719, New York, NY,\\nUSA. Association for Computing Machinery.\\nShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object de-\\ntection with region proposal networks. IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence ,\\n39:1137–1149.\\nNipun Sadvilkar and Mark Neumann. 2020. PySBD:\\nPragmatic sentence boundary disambiguation. In\\nProceedings of Second Workshop for NLP Open\\nSource Software (NLP-OSS) , pages 110–114, Online.\\nAssociation for Computational Linguistics.\\nZejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl,\\nDaniel S. Weld, and Doug Downey. 2022. VILA: Im-\\nproving structured content extraction from scientific\\nPDFs using visual layout groups. Transactions of the\\nAssociation for Computational Linguistics , 10:376–\\n392.\\nZejiang Shen, Ruochen Zhang, Melissa Dell, B. Lee,\\nJacob Carlson, and Weining Li. 2021. Layoutparser:\\nA unified toolkit for deep learning based document\\nimage analysis. In IEEE International Conference\\non Document Analysis and Recognition .\\nSanjay Subramanian, Lucy Lu Wang, Ben Bogin,\\nSachin Mehta, Madeleine van Zuylen, Sravanthi\\nParasa, Sameer Singh, Matt Gardner, and Hannaneh\\nHajishirzi. 2020. MedICaT: A dataset of medical\\nimages, captions, and textual references. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2020 , pages 2112–2120, Online. Association\\nfor Computational Linguistics.\\nM. Tan, R. Pang, and Q. V. Le. 2020. Efficientdet:\\nScalable and efficient object detection. In 2020\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR) , pages 10778–10787, Los\\nAlamitos, CA, USA. IEEE Computer Society.\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\\nScialom, Anthony S. Hartshorn, Elvis Saravia, An-\\ndrew Poulton, Viktor Kerkez, and Robert Stojnic.\\n2022. Galactica: A large language model for science.\\nArXiv , abs/2211.09085.\\npdf2image . 2023.\\npdf2image .\\nhttps://github.\\ncom/Belval/pdf2image .\\npdfplumber . 2023. pdfplumber . https://github.\\ncom/jsvine/pdfplumber .\\nDominika Tkaczyk, Paweł Szostek, Mateusz Fedo-\\nryszak, Piotr Jan Dendek, and Lukasz Bolikowski.\\n2015. Cermine: Automatic extraction of structured\\nmetadata from scientific literature. Int. J. Doc. Anal.\\nRecognit. , 18(4):317–335.\\nAmalie Trewartha, Nicholas Walker, Haoyan Huo,\\nSanghoon Lee, Kevin Cruse, John Dagdelen, Alex\\nDunn, Kristin Aslaug Persson, Gerbrand Ceder, and\\nAnubhav Jain. 2022. Quantifying the advantage of\\ndomain-specific pre-training on named entity recog-\\nnition tasks in materials science. Patterns , 3.\\nLucy Lu Wang, Isabel Cachola, Jonathan Bragg, Evie\\n(Yu-Yen) Cheng, Chelsea Hess Haupt, Matt Latzke,\\nBailey Kuehl, Madeleine van Zuylen, Linda M. Wag-\\nner, and Daniel S. Weld. 2021. Improving the acces-\\nsibility of scientific documents: Current state, user\\nneeds, and a system solution to enhance scientific pdf\\naccessibility for blind and low vision users. ArXiv ,\\nabs/2105.00076.\\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\\nRussell Reas, Jiangjiang Yang, Doug Burdick, Darrin\\nEide, Kathryn Funk, Yannis Katsis, Rodney Michael\\nKinney, Yunyao Li, Ziyang Liu, William Merrill,\\nPaul Mooney, Dewey A. Murdick, Devvret Rishi,\\nJerry Sheehan, Zhihong Shen, Brandon Stilson,\\nAlex D. Wade, Kuansan Wang, Nancy Xin Ru Wang,\\nChristopher Wilhelm, Boya Xie, Douglas M. Ray-\\nmond, Daniel S. Weld, Oren Etzioni, and Sebastian\\nKohlmeier. 2020. CORD-19: The COVID-19 open\\nresearch dataset. In Proceedings of the 1st Work-\\nshop on NLP for COVID-19 at ACL 2020 , Online.\\nAssociation for Computational Linguistics.\\nRosalee Wolfe, John McDonald, Ronan Johnson, Ben\\nSturr, Syd Klinghoffer, Anthony Bonzani, Andrew\\nAlexander, and Nicole Barnekow. 2022. Supporting\\nmouthing in signed languages: New innovations and\\na proposal for future corpus building. In Proceedings\\nof the 7th International Workshop on Sign Language\\nTranslation and Avatar Technology: The Junction of\\nthe Visual and the Textual: Challenges and Perspec-\\ntives , pages 125–130, Marseille, France. European\\nLanguage Resources Association.\\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu\\nWei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\\nZhang, Wanxiang Che, Min Zhang, and Lidong Zhou.\\n2021. LayoutLMv2: Multi-modal pre-training for\\nvisually-rich document understanding. In Proceed-\\nings of the 59th Annual Meeting of the Association for\\nComputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing\\n(Volume 1: Long Papers) , pages 2579–2591, Online.\\nAssociation for Computational Linguistics.\\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu\\nWei, and Ming Zhou. 2019. Layoutlm: Pre-training\\nof text and layout for document image understanding.\\nProceedings of the 26th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining .\\nXu Zhong, Jianbin Tang, and Antonio Jimeno Yepes.\\n2019. Publaynet: largest dataset ever for document\\nlayout analysis. In 2019 International Conference on\\nDocument Analysis and Recognition (ICDAR) , pages\\n1015–1022. IEEE.\\nA\\nAppendix\\nA.1\\nComparison and Compatibility with\\nXML\\nOne can view Layers as capturing content hier-\\narchy (e.g., tokens vs sentences) similar to that of\\nother structured document representations, like TEI\\nXML trees. We note that Layers are stored as un-\\nordered attributes and don’t require nesting. This\\nallows for specific cross-layer referencing opera-\\ntions that don’t adhere to strict nesting relationships.\\nFor example:\\n1 for sentence in doc.sentences:\\n2\\nfor line in sentence.lines:\\n3\\n...\\nRecall that a sentence can begin or end midway\\nthrough a line and cross multiple lines (§3.1). Sim-\\nilarly, not all lines are exactly contained within\\nthe boundaries of a sentence. As such, sentences\\nand lines are not strictly nested within each other.\\nThis relationship would be difficult to encode in an\\nXML format adhering to document tree structure.\\nRegardless, the way we represent structure\\nin documents is highly versatile.\\nWe demon-\\nstrate this by also implementing GrobidParser\\nas an alternative to the PDF2TextParser in §3.1.\\nGrobidParser invokes Grobid to process PDFs,\\nand reads the resulting TEI XML file generated by\\nGrobid by converting each XML tag of a common\\nlevel into an Entity of its own Layer . We use this\\nto perform the evaluation in Table 2.\\nA.2\\nAdditional magelib Protocols and\\nUtilities\\nSerialization.\\nAny Document and all of its\\nLayers can be exported to a JSON format, and\\nperfectly reconstructed:\\n1 import json\\n2 with open(\"....json\", \"w\") as f_out:\\n3\\njson.dump(doc.to_json(), f_out)\\n4 5 with open(\"...json\", \"r\") as f_in:\\n6\\ndoc = json.load(f_in)\\nA.3\\nEvaluating papermage ’s CoreRecipe\\nagainst Grobid\\nHere, we detail how we performed the evaluation\\nreported in §3.3 (Table 2). We also provide a full\\nbreakdown by category in Table 3.\\nAs described earlier in the paper, Grobid is quite\\ndifficult to evaluate as it is developed with tight\\ncoupling between the PDF parser ( pdfalto ) and\\nthe models it employs to perform logical struc-\\nture recovery over the resulting token stream. As\\nsuch, there is no straightforward way to run just\\nthe model components of Grobid on an alternative\\ntoken stream like that provided in the S2-VL (Shen\\net al., 2022) dataset.\\nTo perform this baseline evaluation, we ran\\nthe original PDFs that were annotated for S2-VL\\nthrough our GrobidParser using v0.7.3. Grobid\\nalso returns bounding boxes of some predicted cat-\\negories (e.g., authors, abstract, paragraphs). We\\nuse these bounding boxes to create Entities that\\nwe annotate on a Document constructed manually\\nfrom from S2-VL data. Using magelib cross-layer\\nreferencing, we were able to match Grobid predic-\\ntions to S2-VL data to perform this evaluation.\\nThough we found there are certain categories\\nfor which bounding box information was either not\\navailable (e.g., Titles) or Grobid simply did not re-\\nturn that output (e.g., Figure text extraction). These\\nare represented by zeros in Table 3, which con-\\ntributes to the lower scores in Table 2 after macro\\naveraging. For a more apples-to-apples compari-\\nson, we also included a “Grobid Subset” evaluation\\nwhich restricted to just categories in S2-VL for\\nwhich Grobid produced bounding box information.\\nIn addition to Grobid, we evaluate two of our pro-\\nvided Transformer-based models. The RoBERTa-\\nlarge (Liu et al., 2019) model is a Transformers\\ntoken classification model that we finetuned on the\\nS2-VL training set. The I-VILA model is a layout-\\ninfused Transformer model pretrained by Shen et al.\\n(2022) on the S2-VL training set. Like we did with\\nGrobid, we ran our CoreRecipe using these two\\nmodels on the original PDFs in S2-VL, and per-\\nformed a similar token mapping operation since our\\nPDF2TextParser also produces a different token\\nstream than that provided in S2-VL.\\nAt the end of the day, the Transformer-based\\nmodels performed better at this task than Grobid.\\nThis is unsurprising given expected improvements\\nusing a Transformer model over a CRF or BiL-\\nSTM. The Transformer models were also trained\\non S2-VL data, which gave them an advantage over\\nGrobid. Overall, this evaluation intended to show\\nhow papermage enables cross-system comparisons,\\neven eschewing token stream incompatibility, and\\nto illustrate an upper bound of the performance left\\non the table by existing software systems that don’t\\nuse of state-of-the-art models.\\nStructureCategory\\nGROBID CRF\\nGROBID NN\\nRoBERTa\\nI-VILA\\nP\\nR\\nF1\\nP\\nR\\nF1\\nP\\nR\\nF1\\nP\\nR\\nF1\\nAbstract\\n81.9\\n89.1\\n85.3\\n85.3\\n89.8\\n87.5\\n89.2\\n93.7\\n91.4\\n97.4\\n98.3\\n97.8\\nAuthor\\n55.2\\n42.6\\n48.1\\n75.1\\n14.0\\n23.6\\n87.5\\n73.5\\n79.9\\n65.5\\n96.9\\n78.2\\nBibliography\\n96.5\\n98.6\\n97.5\\n95.5\\n97.6\\n96.5\\n93.6\\n93.3\\n93.5\\n99.7\\n98.2\\n99.0\\nCaption\\n70.3\\n70.0\\n70.2\\n70.2\\n69.7\\n70.0\\n80.0\\n77.3\\n78.6\\n93.1\\n89.6\\n91.3\\nEquation\\n71.1\\n85.3\\n77.6\\n71.1\\n85.3\\n77.6\\n55.0\\n85.7\\n67.0\\n90.7\\n94.2\\n92.4\\nFigure\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n88.9\\n82.3\\n85.4\\n99.8\\n96.8\\n98.3\\nFooter\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n56.1\\n59.9\\n57.9\\n96.8\\n78.1\\n86.5\\nFootnote\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n59.8\\n44.3\\n50.9\\n80.2\\n93.5\\n86.3\\nHeader\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n40.5\\n84.3\\n54.7\\n92.9\\n99.1\\n95.9\\nKeywords\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n93.8\\n97.1\\n95.4\\n96.9\\n99.4\\n98.1\\nList\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n61.9\\n63.8\\n62.9\\n76.7\\n82.4\\n79.4\\nParagraph\\n94.5\\n89.8\\n92.1\\n94.4\\n89.9\\n92.1\\n93.5\\n93.0\\n93.3\\n98.7\\n97.9\\n98.3\\nSection\\n83.0\\n79.4\\n81.1\\n83.0\\n79.4\\n81.1\\n67.7\\n82.7\\n74.4\\n96.2\\n91.6\\n93.9\\nTable\\n97.3\\n58.6\\n73.2\\n97.9\\n58.6\\n73.3\\n94.7\\n71.8\\n81.7\\n96.1\\n94.9\\n95.5\\nTitle\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n76.3\\n96.7\\n85.3\\n98.7\\n99.9\\n99.3\\nMacro Avg\\n(Full S2-VL)\\n40.6\\n38.3\\n39.1\\n42.0\\n36.5\\n37.6\\n75.9\\n80.0\\n76.8\\n92.0\\n94.1\\n92.7\\nMacro Avg\\n(Grobid Subset)\\n81.2\\n76.7\\n78.9\\n84.1\\n73.0\\n78.2\\n82.6\\n83.9\\n83.2\\n92.2\\n95.2\\n93.7\\nTable 3: Evaluating CoreRecipe for logical structure recovery on S2-VL (Shen et al., 2022). These are per-category\\nmetrics for Table 2. Metrics are computed for token-level classification, macro-averaged over categories. The\\n“Grobid Subset” limits evaluation to only categories for which Grobid returns bounding box information, which was\\nnecessary for evaluation on S2-VL.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PAGE: 0 ===\n",
      "\n",
      "\n",
      "PaperMage: A Unified Toolkit for Processing, Representing, and\n",
      "Manipulating Visually-Rich Scientific Documents\n",
      "Kyle Lo α ∗\n",
      "Zejiang Shen α,τ ∗\n",
      "Benjamin Newman α ∗\n",
      "Joseph Chee Chang α ∗\n",
      "Russell Authur α Erin Bransom α Stefan Candra α Yoganand Chandrasekhar α\n",
      "Regan Huff α Bailey Kuehl α Amanpreet Singh α Chris Wilhelm α Angele Zamarron α\n",
      "Marti A. Hearst β\n",
      "Daniel S. Weld α,ω\n",
      "Doug Downey α,η\n",
      "Luca Soldaini α ∗\n",
      "α Allen Institute for AI\n",
      "τ Massachusetts Institute of Technology\n",
      "β University of California Berkeley\n",
      "ω University of Washington\n",
      "η Northwestern University\n",
      "{kylel, lucas}@allenai.org\n",
      "Abstract\n",
      "Despite growing interest in applying natural\n",
      "language processing (NLP) and computer vi-\n",
      "sion (CV) models to the scholarly domain,\n",
      "scientific documents remain challenging to\n",
      "work with. They’re often in difficult-to-use\n",
      "PDF formats, and the ecosystem of models\n",
      "to process them is fragmented and incom-\n",
      "plete.\n",
      "We introduce papermage , an open-\n",
      "source Python toolkit for analyzing and pro-\n",
      "cessing visually-rich, structured scientific doc-\n",
      "uments. papermage offers clean and intuitive\n",
      "abstractions for seamlessly representing and\n",
      "manipulating both textual and visual document\n",
      "elements. papermage achieves this by integrat-\n",
      "ing disparate state-of-the-art NLP and CV mod-\n",
      "els into a unified framework, and provides turn-\n",
      "key recipes for common scientific document\n",
      "processing use-cases. papermage has powered\n",
      "multiple research prototypes of AI applications\n",
      "over scientific documents, along with Seman-\n",
      "tic Scholar’s large-scale production system for\n",
      "processing millions of PDFs.\n",
      "§ github.com/allenai/papermage 1\n",
      "1\n",
      "Introduction\n",
      "Research papers and textbooks are central to the\n",
      "scientific enterprise, and there is increasing inter-\n",
      "est in developing new tools for extracting knowl-\n",
      "edge from these visually-rich documents. Recent\n",
      "research has explored, for example, AI-powered\n",
      "reading support for math symbol definitions (Head\n",
      "et al., 2021), in-situ passage explanations or sum-\n",
      "maries (August et al., 2023; Rachatasumrit et al.,\n",
      "2022; Kim et al., 2023), automatic span highlight-\n",
      "ing (Chang et al., 2023; Fok et al., 2023b), interac-\n",
      "tive clipping and synthesis (Kang et al., 2022, 2023)\n",
      "∗ Core contributors; see author contributions for details. 1\n",
      "We use code snippets to illustrate our toolkit’s core de- signs and abstractions. Exact syntax in paper may differ from\n",
      "the actual code, as software will evolve beyond the paper and we opt to simplify syntax when needed for legibility and clarity.\n",
      "We refer readers to our public code for latest documentation.\n",
      "Figure 1: papermage ’s document creation and represen-\n",
      "tation. (A) Recipes are turn-key methods for processing\n",
      "a PDF. (B) They compose models operating across dif-\n",
      "ferent data modalities and machine learning frameworks\n",
      "to extract document structure, which we conceptualize\n",
      "as layers of annotation that store textual and visual in-\n",
      "formation. (C) Users can access and manipulate layers.\n",
      "and more.\n",
      "Further, extracting clean, properly-\n",
      "structured scientific text from PDF documents (Lo\n",
      "et al., 2020; Wang et al., 2020) forms a critical\n",
      "first step in pretraining language models of sci-\n",
      "ence (Beltagy et al., 2019; Lee et al., 2019; Gu et al.,\n",
      "2020; Luo et al., 2022; Taylor et al., 2022; Tre-\n",
      "wartha et al., 2022; Hong et al., 2023), automatic\n",
      "generation of more accessible paper formats (Wang\n",
      "et al., 2021), and developing datasets for scientific\n",
      "natural language processing (NLP) tasks over struc-\n",
      "tured full text (Jain et al., 2020; Subramanian et al.,\n",
      "2020; Dasigi et al., 2021; Lee et al., 2023).\n",
      "However, this type of NLP research on scientific\n",
      "\n",
      "=== PAGE: 1 ===\n",
      "\n",
      "\n",
      "corpora is difficult because the documents come\n",
      "in difficult-to-use formats like PDF, 2 and existing\n",
      "tools for working with the documents are limited.\n",
      "Typically, the first step in scientific document pro-\n",
      "cessing is to invoke a parser on a document file to\n",
      "convert it into a sequence of tokens and bounding\n",
      "boxes in inferred reading order. Parsers extract only\n",
      "the raw document content, and obtaining richer\n",
      "document structure (e.g., titles, authors, figures) or\n",
      "linguistic structure and semantics (e.g., sentences,\n",
      "discourse units, scientific claims) requires sending\n",
      "the token sequence through downstream models.\n",
      "Unlike more mature parsers (§2.1), these down-\n",
      "stream models are often research prototypes (§2.2)\n",
      "that are limited to extracting only a subset of the\n",
      "structures needed for one’s research (e.g., the same\n",
      "model may not provide both sentence splits and fig-\n",
      "ure detection). As a result, users must write exten-\n",
      "sive custom code that strings pipelines of multiple\n",
      "models together. Research projects using models\n",
      "of different modalities (e.g., combining an image-\n",
      "based formula detector with a text-based definition\n",
      "extractor) can require hundreds of lines of code.\n",
      "We introduce papermage , an open-source\n",
      "Python toolkit for processing scientific documents.\n",
      "Its contributions include (1) magelib , a library of\n",
      "primitives and methods for representing and ma-\n",
      "nipulating visually-rich documents as multimodal\n",
      "constructs, (2) Predictors , a set of implementa-\n",
      "tions that integrate different state-of-the-art scien-\n",
      "tific document analysis models into a unified inter-\n",
      "face, even if individual models are written in differ-\n",
      "ent frameworks or operate on different modalities,\n",
      "and (3) Recipes , which provide turn-key access\n",
      "to well-tested combinations of individual (often\n",
      "single-modality) modules to form sophisticated, ex-\n",
      "tensible multimodal pipelines.\n",
      "2\n",
      "Related Work\n",
      "2.1\n",
      "Turn-key software for scientific documents\n",
      "Processing visually-rich documents like scientific\n",
      "documents requires a joint understanding of both\n",
      "visual and textual information. In practice, this\n",
      "often requires combining different models into\n",
      "complex processing pipelines. For example, GRO-\n",
      "BID (Grobid, 2008–2023), a widely-adopted soft-\n",
      "ware tool for scientific document processing, uses\n",
      "2 PDFs store text as character glyphs and their ( x, y ) posi-\n",
      "tions on a page. Converting this data to usable text for NLP requires error-prone operations like inferring token boundaries,\n",
      "whitespacing, and reading order using visual positioning.\n",
      "twelve interdependent sequence labeling models 3\n",
      "to perform its full text extraction. Other similar\n",
      "tools inlude CERMINE (Tkaczyk et al., 2015) and\n",
      "ParsCit (Councill et al., 2008). While such software\n",
      "is often an ideal choice for off-the-shelf processing,\n",
      "they are not necessarily designed for easy extension\n",
      "and/or integration with newer research models. 4\n",
      "2.2\n",
      "Models for scientific document processing\n",
      "While aforementioned software tools use CRF or\n",
      "BiLSTM-based models, Transformer-based models\n",
      "have seen wide adoption among NLP researchers\n",
      "for their powerful processing capabilities. Recent\n",
      "years have seen the rise of layout-infused Trans-\n",
      "formers (Xu et al., 2019; Shen et al., 2022; Xu\n",
      "et al., 2021; Huang et al., 2022b; Chen et al., 2023)\n",
      "for processing visually-rich documents, including\n",
      "recovering logical structure (e.g., titles, abstracts)\n",
      "of scientific papers (Huang et al., 2022a). Similarly,\n",
      "computer vision (CV) researchers have also shown\n",
      "impressive capabilities of CNN-based object de-\n",
      "tection models (Ren et al., 2015; Tan et al., 2020)\n",
      "for segmenting visually-rich documents based on\n",
      "their layout. While these research models are pow-\n",
      "erful and extensible for research purposes, it often\n",
      "requires significant “glue” code and stitching soft-\n",
      "ware tools to create robust processing pipelines.\n",
      "For example, Lincker et al. (2023) bootstraps a so-\n",
      "phisticated processing pipeline around a research\n",
      "model for processing children’s textbooks.\n",
      "2.3\n",
      "Combining models and pipelines\n",
      "papermage ’s use case lies between that of turn-\n",
      "key software and a framework for supporting re-\n",
      "search. Similar to Transformers (Wolfe et al.,\n",
      "2022)’s integration of different research mod-\n",
      "els into standard interfaces, others have done\n",
      "similarly for the visually-rich document domain.\n",
      "LayoutParser (Shen et al., 2021) provides mod-\n",
      "els for visually-rich documents and supports\n",
      "the creation of document processing pipelines.\n",
      "papermage , in fact, depends on LayoutParser\n",
      "for access to vision models, but is designed to\n",
      "also integrate text models which are omitted from\n",
      "3 https://grobid.readthedocs.io/en/latest/\n",
      "Training-the-models-of-Grobid/#models 4\n",
      "Most research in NLP requires that a researcher be able to manipulate models within Python. Yet, Grobid requires users\n",
      "to manage a separate service process and send PDFs through a client. In performing evaluation in §3.3, we also found it\n",
      "difficult to run only the model components isolated from PDF utilities, which makes comparison with other research models\n",
      "challenging without significant “glue” code.\n",
      "\n",
      "=== PAGE: 2 ===\n",
      "\n",
      "\n",
      "Figure 2: Entities are multimodal content units. Here,\n",
      "spans of a sentence are used to identify its text among\n",
      "all symbols, while boxes map its visual coordinates on\n",
      "a page. spans and boxes can include non-contiguous\n",
      "units, allowing great flexibility in Entities to handle\n",
      "layout nuances. A sentence split across columns/pages\n",
      "and interrupted by floating figures/footnotes would re-\n",
      "quire multiple spans and bounding boxes to represent.\n",
      "LayoutParser .\n",
      "To allow models of different\n",
      "modalities to work well together, we also devel-\n",
      "oped the magelib library (§3.1).\n",
      "3\n",
      "Design of papermage\n",
      "papermage is three parts: (1) magelib , a library for\n",
      "intuitively representing and manipulating visually-\n",
      "rich documents, (2) Predictors , implementations\n",
      "of models for analyzing scientific papers that unify\n",
      "disparate machine learning frameworks under a\n",
      "common interface, and (3) Recipes , combinations\n",
      "of Predictors that form multimodal pipelines.\n",
      "3.1\n",
      "Representing and manipulating\n",
      "visually-rich documents with magelib\n",
      "In this section, we use code snippets to show how\n",
      "our library’s abstractions and syntax are tailored\n",
      "for the visually-rich document problem domain.\n",
      "Data Classes.\n",
      "magelib provides three base data\n",
      "classes for representing fundamental elements of\n",
      "visually-rich, structured documents: Document ,\n",
      "Layers and Entities . First, a Document might\n",
      "minimally store text as a string of symbols:\n",
      "1 >>> from papermage import Document\n",
      "2 >>> doc.symbols\n",
      "3 \"Revolt: Collaborative Crowdsourcing...\"\n",
      "But visually-rich documents are more than a lin-\n",
      "earized string. For example, analyzing a scientific\n",
      "paper requires access to its visuospatial layout (e.g.,\n",
      "pages, blocks, lines), logical structure (e.g., title,\n",
      "abstract, figures, tables, footnotes, sections), se-\n",
      "mantic units (e.g., paragraphs, sentences, tokens),\n",
      "and more (e.g., citations, terms). In practice, this\n",
      "means different parts of doc.symbols can corre-\n",
      "spond to different paragraphs, sentences, tokens,\n",
      "etc. in the Document , each with its own set of\n",
      "corresponding coordinates representing its visual\n",
      "position on a page.\n",
      "magelib represents structure using Layers that\n",
      "can be accessed as attributes of a Document (e.g.,\n",
      "doc.sentences ,\n",
      "doc.figures ,\n",
      "doc.tokens )\n",
      "(Figure 1). Each Layer is a sequence of content\n",
      "units, called Entities , which store both textual\n",
      "(e.g., spans, strings) and visuospatial (e.g.,\n",
      "bounding boxes, pixel arrays) information:\n",
      "1 >>> sentences = Layer(entities=[\n",
      "2\n",
      "Entity(...), Entity(...), ...\n",
      "3\n",
      "])\n",
      "See Figure 2 for an example on how “sentences” in\n",
      "a scientific document are represented as Entities .\n",
      "Section §3.2 explains in more detail how a user can\n",
      "generate Entities .\n",
      "Methods.\n",
      "magelib also provides a set of func-\n",
      "tions for building and interacting with data: aug-\n",
      "menting a Document with additional Layers ,\n",
      "traversing and spatially searching for matching\n",
      "Entities in one Layer , and cross-referencing be-\n",
      "tween Layers (see Figure 3).\n",
      "A Document that only contains doc.symbols\n",
      "can be augmented with additional Layers :\n",
      "1 >>> paragraphs = Layer(...)\n",
      "2 >>> sentences = Layer(...)\n",
      "3 >>> tokens = Layer(...)\n",
      "4 5 >>> doc.add(paragraphs , sentences , tokens)\n",
      "Adding Layers automatically grants users the\n",
      "ability to iterate through Entities and cross-\n",
      "reference intersecting Entities across Layers :\n",
      "1 >>> for paragraph in doc.paragraphs:\n",
      "2\n",
      "for sent in paragraph.sentences:\n",
      "3\n",
      "for token in sentence.tokens:\n",
      "4\n",
      "...\n",
      "magelib also supports cross-modality opera-\n",
      "tions. For example, searching for textual Entities\n",
      "within a visual region on the PDF (See Figure 3 F):\n",
      "1 >>> query = Box(l=423, t=71, w=159, h=87)\n",
      "2 >>> selection = doc.find(query , \"tokens\")\n",
      "3 >>> [t.text for t in selection]\n",
      "4 [\"Techniques\", \"for\", \"collecting\", ...]\n",
      "\n",
      "=== PAGE: 3 ===\n",
      "\n",
      "\n",
      ">>> doc.paragraphs[0]\n",
      ">>> doc.paragraphs[0].sentences[2] or\n",
      ">>> doc.sentences[2]\n",
      ">>> doc.sentences[2].tokens[9:13] or\n",
      ">>> doc.tokens[169:173]\n",
      ">>> doc.figures[0]\n",
      ">>> doc.captions[0]\n",
      ">>> user_query = Box(l,t,w,h, page=0)\n",
      ">>> selected_tokens = doc.find(user_query, layer=“tokens”)\n",
      ">>> [token.text for token in selected_tokens]\n",
      "[“Techniques”, “for”, “collecting”, “labeled”, “data”, “perts”, “for”, “manual”, “annotation”, ...]\n",
      "Crowdsourcing provides a scalable and efficient way to con- struct labeled datasets for training machine learning systems. However, creating comprehensive label guidelines for crowd-\n",
      "workers is often prohibitive even for seemingly simple con- cepts. Incomplete or ambiguous label guidelines can then result in differing interpretations of concepts and inconsistent\n",
      "labels. Existing approaches for improving label quality, such as worker screening or detection of poor work, are ineffective for this problem and can lead to rejection of honest work and a\n",
      "missed opportunity to capture rich interpretations about data. We introduce Revolt , a collaborative approach that brings ideas from expert annotation workflows to crowd-based labeling.\n",
      "Revolt eliminates the burden of creating detailed label guide- lines by harnessing crowd disagreements to identify ambigu- ous concepts and create rich structures (groups of semantically\n",
      "related items) for post-hoc label decisions. Experiments com- paring Revolt to traditional crowdsourced labeling show that Revolt produces high quality labels without requiring label\n",
      "guidelines in turn for an increase in monetary cost. This up front cost, however, is mitigated by Revolt's ability to produce reusable structures that can accommodate a variety of label\n",
      "boundaries without requiring new data to be collected. Further comparisons of Revolt's collaborative and non-collaborative variants show that collabvoration reaches higher label accura-\n",
      "cy with lower monetary cost.\n",
      "learned models that must be trained on representative datasets labeled according to target concepts (e.g., speech labeled by their intended commands, faces labeled in images, emails la-\n",
      "beled as spam or not spam).\n",
      "crowdsourcing; machine learning; collaboration; real-time\n",
      "H.5.m. Information Interfaces and Presentation (e.g. HCI): Miscellaneous\n",
      "From conversational assistants on mobile devices, to facial\n",
      "Techniques for collecting labeled data include recruiting ex- perts for manual annotation [51], extracting relations from readily available sources (e.g., identifying bodies of text in\n",
      "parallel online translations [46, 13]), and automatically gener- ating labels based on user behaviors (e.g., using dwell time to implicitly mark search result relevance [2]). Recently,\n",
      "many practitioners have also turned to crowdsourcing for cre- ating labeled datasets at low cost [49]. Successful crowd-\n",
      "Figure 1. Revolt creates labels for unanimously labeled “certain” items (e.g., cats and not cats ), and surfaces categories of “uncertain” items enriched with crowd feedback (e.g., cats and dogs and cartoon cats in the dotted middle region are annotated with crowd explanations). Rich\n",
      "structures allow label requesters to better understand concepts in the data and make post-hoc decisions on label boundaries (e.g., assigning cats and dogs to the cats label and cartoon cats to the not cats label) rather than providing crowd-workers with a priori label guidelines.\n",
      "ABSTRACT\n",
      "ACM Classification Keywords\n",
      "Author Keywords\n",
      "INTRODUCTION\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "Figure 3: Illustrates how Entities can be accessed flexibly in different ways: (A) Accessing the Entity of the first\n",
      "paragraph in the Document via its own Layer (B) Accessing a sentence via the paragraph Entity or directly via the\n",
      "sentences Layer (C) Similarly, the same tokens can be accessed via the overlapping sentence Entity or directly\n",
      "via the tokens Layer of the Document (where the first tokens are the title of the paper.) (D, E) Figures, captions,\n",
      "tables and keywords can be accessed in similar ways (F) Additionally, given a bounding box (e.g., of a user selected\n",
      "region), papermage can find the corresponding Entities for a given Layer , in this case finding the tokens under\n",
      "the region. Excerpt from Chang et al. (2017).\n",
      "Protocols\n",
      "and\n",
      "Utilities.\n",
      "To\n",
      "instantiate\n",
      "a\n",
      "Document ,\n",
      "magelib provides protocols and\n",
      "utilities like Parsers and Rasterizers , which\n",
      "hook into off-the-shelf PDF processing tools: 5\n",
      "1 >>> import papermage as pm\n",
      "2 >>> parser = pm.PDF2TextParser()\n",
      "3 >>> doc = parser.parse(\"...pdf\")\n",
      "4 >>> [token.text for token in doc.tokens]\n",
      "5 [\"Revolt\", \":\", \"Collaborative\", ...]\n",
      "6 >>> doc.images\n",
      "7 None\n",
      "8 9 >>> rasterizer = pm.PDF2ImageRasterizer()\n",
      "10 >>> doc2 = rasterizer.rasterize(\"...pdf\")\n",
      "11 >>> doc.images = doc2.images\n",
      "12 >>> doc.images\n",
      "13 [Image(np.array(...)), ...]\n",
      "In this example, papermage runs PDF2TextParser\n",
      "(using pdfplumber ) to extract the textual in-\n",
      "formation from a PDF file.\n",
      "Then it runs\n",
      "PDF2ImageRasterizer (using pdf2image ) to up-\n",
      "date the first Document with images of pages.\n",
      "5 PDFs are not the only way of representing visually-rich\n",
      "documents. For example, many scientific documents are dis- tributed in XML format. As PDFs are the dominant distribu-\n",
      "tion format of scientific documents, we focus our efforts on PDF-specific needs. Nevertheless, we also provide Parsers\n",
      "in magelib that can instantiate a Document from XML input.\n",
      "See Appendix A.1.\n",
      "3.2\n",
      "Interfacing with models for scientific\n",
      "document analysis through Predictors\n",
      "In §3.1, we described how users create Layers\n",
      "by assembling collections of Entities . But how\n",
      "would they make Entities in the first place?\n",
      "For example, to identify multimodal structures\n",
      "in visually-rich documents, researchers might want\n",
      "to build complex pipelines that run and combine\n",
      "output from many different models (e.g., computer\n",
      "vision models for extracting figures, NLP models\n",
      "for classifying body text). papermage provides\n",
      "a unified interface, called Predictors , to ensure\n",
      "models produce Entities that are compatible with\n",
      "the Document .\n",
      "papermage\n",
      "includes\n",
      "several\n",
      "ready-to-use\n",
      "Predictors that leverage state-of-the-art models\n",
      "to extract specific document structures (Table 1).\n",
      "While magelib ’s abstractions are general for\n",
      "visually-rich documents, Predictors are opti-\n",
      "mized for parsing of scientific documents. They\n",
      "are designed to (1) be compatible with models\n",
      "from many different machine learning frameworks,\n",
      "(2) support inference with text-only, vision-only,\n",
      "and multimodal models, and (3) support both adap-\n",
      "tation of off-the-shelf, pretrained models as well as\n",
      "\n",
      "=== PAGE: 4 ===\n",
      "\n",
      "\n",
      "Use case\n",
      "Description\n",
      "Examples\n",
      "Linguistic/Semantic\n",
      "Segments doc into text units often used for down-\n",
      "stream models.\n",
      "SentencePredictor wraps sciSpaCy (Neumann et al., 2019) and PySBD (Sadvilkar and Neumann, 2020) to segment sentences. WordPredictor is\n",
      "a custom scikit-learn model to identify broken words split across PDF lines or\n",
      "columns. ParagraphPredictor is a set of heuristics on top of both layout and logical structure models to extract paragraphs.\n",
      "LayoutStructure\n",
      "Segments doc into visual block regions.\n",
      "BoxPredictor wraps models from LayoutParser (Shen et al., 2021), which\n",
      "provides vision models like EfficientDet (Tan et al., 2020) pretrained on scientific layouts (Zhong et al., 2019).\n",
      "LogicalStructure\n",
      "Segments doc into orga- nizational units like title,\n",
      "abstract, body, footnotes, caption, and more.\n",
      "SpanPredictor wraps Token Classifiers from Transformers (Wolfe et al., 2022),\n",
      "which provides both pretrained weights from VILA (Shen et al., 2022), as well as RoBERTa (Liu et al., 2019), SciBERT (Beltagy et al., 2019) weights that we’ve\n",
      "finetuned on similar data.\n",
      "Task-specific\n",
      "Models for a given sci- entific document process-\n",
      "ing task can be used with papermage if wrapped as\n",
      "a Predictor following\n",
      "common patterns.\n",
      "As many practitioners depend on prompting a model through an API call, we implement APIPredictor which interfaces external APIs, such as GPT-3 (Brown\n",
      "et al., 2020), to perform tasks like question answering over a structured Document . We also implement SnippetRetrievalPredictor which wraps models like Con-\n",
      "triever (Izacard et al., 2022) to perform top- k within-document snippet retrieval. See §4 for how these two can be combined.\n",
      "Table 1: Types of Predictors implemented in papermage .\n",
      "Model\n",
      "Full\n",
      "Grobid Subset P\n",
      "R\n",
      "F1\n",
      "P\n",
      "R\n",
      "F1\n",
      "Grobid CRF\n",
      "40.6\n",
      "38.3\n",
      "39.1\n",
      "81.2\n",
      "76.7\n",
      "78.9 Grobid NN\n",
      "42.0\n",
      "36.5\n",
      "37.6\n",
      "84.1\n",
      "73.0\n",
      "78.2 RoBERTa\n",
      "75.9\n",
      "80.0\n",
      "76.8\n",
      "82.6\n",
      "83.9\n",
      "83.2 I-VILA\n",
      "92.0\n",
      "94.1\n",
      "92.7\n",
      "92.2\n",
      "95.2\n",
      "93.7\n",
      "Table 2: Evaluating performance of CoreRecipe for\n",
      "logical structure recovery on S2-VL (Shen et al., 2022).\n",
      "Metrics are computed for token-level classification,\n",
      "macro-averaged over categories. The “Grobid Subset”\n",
      "limits evaluation to only categories for which Grobid\n",
      "returns bounding box information, which was necessary\n",
      "for evaluation on S2-VL. See Appendix A.3 for details.\n",
      "development of new ones from scratch. Similarly\n",
      "to the Transformers library, a Predictor ’s\n",
      "implementation is typically independent from\n",
      "its configuration, allowing users to customize\n",
      "each Predictor by tweaking hyperparameters or\n",
      "loading a different set of weights.\n",
      "Below, we showcase how a vision model and\n",
      "two text models (both neural and symbolic) can be\n",
      "applied in succession to a single Document . See\n",
      "Table 1 for a summary of supported Predictors .\n",
      "1 >>> import papermage as pm\n",
      "2 >>> cv = pm.BoxPredictor(...)\n",
      "3 >>> tables , figures = cv.predict(doc)\n",
      "4 >>> doc.add(tables , figures)\n",
      "5 6 >>> nlp_neu = pm.SpanPredictor(...)\n",
      "7 >>> titles , authors = nlp_neu.predict(doc)\n",
      "8 >>> doc.add(titles , authors)\n",
      "9 10 >>> nlp_sym = pm.SentencePredictor(...)\n",
      "11 >>> sentences = nlp_sym.predict(doc)\n",
      "12 >>> doc.add(sentences)\n",
      "Predictors return a list of Entities , which\n",
      "can be group_by() to organize them based on pre-\n",
      "dicted label value (e.g., tokens classified as “title”\n",
      "or “authors”). Finally, these predictions are passed\n",
      "to doc.annotate() to be added to Document .\n",
      "3.3\n",
      "End-to-end processing with Recipes\n",
      "Finally, papermage provides predefined combina-\n",
      "tions of Predictors , called Recipes , for users\n",
      "seeking high-quality options for turn-key process-\n",
      "ing of visually-rich documents:\n",
      "1 from papermage import CoreRecipe\n",
      "2 recipe = CoreRecipe()\n",
      "3 doc = recipe.run(\"...pdf\")\n",
      "4 doc.captions[0].text\n",
      "5 >>> \"Figure 1. ...\"\n",
      "Recipes can also be flexibly modified to sup-\n",
      "port development. For example, our current de-\n",
      "fault combines the pdfplumber PDF parsing utility\n",
      "with the I-VILA (Shen et al., 2022) research model.\n",
      "We show in Table 2 an evaluation comparing this\n",
      "against the same recipe but configured to (1) swap\n",
      "I-VILA for a RoBERTa model, as well as (2) swap\n",
      "both for Grobid API calls. We expect Recipes\n",
      "to appeal to two groups of users—end-to-end con-\n",
      "sumers, and developers of high-level applications.\n",
      "The former is comprised of developers and re-\n",
      "searchers who are looking for a one-step solution\n",
      "to multimodal scientific document analysis. The\n",
      "latter are likely developers and researchers looking\n",
      "to combine document structure primitives to build\n",
      "a complex application (see example in §4).\n",
      "\n",
      "=== PAGE: 5 ===\n",
      "\n",
      "\n",
      "4\n",
      "Vignette: Building an Attributed QA\n",
      "System for Scientific Papers\n",
      "How could researchers leverage papermage for\n",
      "their research? Here, we walk through a user sce-\n",
      "nario in which a researcher (Lucy) is prototyping\n",
      "an attributed QA system for science.\n",
      "System Design.\n",
      "Drawing inspiration from Ko\n",
      "et al. (2020), Lee et al. (2023), Fok et al. (2023a),\n",
      "and Newman et al. (2023), Lucy is studying how\n",
      "language models can be used to resolve questions\n",
      "that arise while reading a paper (e.g. What does\n",
      "this mean? or What does this refer to? ). In her\n",
      "prototype interface, a user can highlight a passage\n",
      "in a PDF and ask a question about it. A retrieval\n",
      "model then finds relevant passages from the rest\n",
      "of the paper. The prototype then uses the text of\n",
      "the retrieved passages along with the user question\n",
      "to prompt a language model to generate an answer.\n",
      "When presenting the answer to the user, the proto-\n",
      "type also visually highlights the retrieved passages\n",
      "as supporting evidence to the generated answer.\n",
      "Getting started quickly.\n",
      "As a researcher profi-\n",
      "cient in Python, it only takes Lucy minutes to install\n",
      "papermage using pip and successfully process a lo-\n",
      "cal PDF file by following the example code snippet\n",
      "for CoreRecipe in §3.2. In an interactive session,\n",
      "she familiarizes herself with the provided Layers\n",
      "by following the traversal, cross-referencing and\n",
      "querying examples in §3.1. She makes sure she can\n",
      "serialize and re-instantiate her Document (§A.2).\n",
      "Formatting input.\n",
      "Before using papermage ,\n",
      "Lucy has prior experience building QA pipelines,\n",
      "but has only dealt with documents as sentence-\n",
      "split text data (e.g., <List[str]> ). Lucy realizes\n",
      "that she can reuse her prior text-only code with\n",
      "papermage by implementing a couple of wrappers\n",
      "to gain additional capabilities: First, she converts\n",
      "a user’s highlighted passage from a visual selec-\n",
      "tion to text following the example in Figure 3F.\n",
      "Next, she converts Document to her required text\n",
      "format by following the traversal examples in §3.1\n",
      "(e.g., using [s.text for s in doc.sentences] ).\n",
      "Within a few lines of code, Lucy has everything\n",
      "she needs for text-only input to her QA pipeline.\n",
      "Formatting output.\n",
      "Lucy runs her QA system\n",
      "on her newly acquired text data and now has (1) a\n",
      "model-generated answer and (2) several retrieved\n",
      "evidence passages. She realizes that she already\n",
      "has access to the evidences’ bounding boxes via a\n",
      "similar call to how she defined the model input con-\n",
      "text (e.g., [s.boxes for s in doc.sentences] ).\n",
      "She can easily pass this to the user interface to en-\n",
      "able linking to and highlighting of those passages.\n",
      "Defining a Predictor .\n",
      "The pattern Lucy has\n",
      "followed is used in our many Predictor imple-\n",
      "mentations: (1) gain access to text by traversing\n",
      "Layers (e.g., sentences), (2) perform all usual\n",
      "NLP computation on that text, and (3) format\n",
      "model output as Entities . This simple pattern\n",
      "allows users to reuse familiar models in existing\n",
      "frameworks and eschews lengthy onboarding to\n",
      "papermage . Lucy wraps her prompting and re-\n",
      "trieval code in new classes: APIPredictor and\n",
      "SnippetRetrievalPredictor (see Table 1).\n",
      "Fast iterations.\n",
      "Leveraging the bounding box\n",
      "data from papermage to visually highlight the re-\n",
      "trieved passages, Lucy suspects the retrieval com-\n",
      "ponent is likely underperforming. She makes a sim-\n",
      "ple edit from doc.sentences to doc.paragraphs\n",
      "and evaluates system performance under different\n",
      "input granularity. She also realizes the system of-\n",
      "ten retrieves content outside the main body text.\n",
      "She restricts her traversal to filter out paragraphs\n",
      "that overlap with footnotes— [p.text for p in\n",
      "doc.paragraphs if len(p.footnotes) == 0] —\n",
      "making clever use of the cross-referencing function-\n",
      "ality to detect when a paragraph is actually coming\n",
      "from a footnote. This example demonstrates the\n",
      "versatility of the affordances provided by magelib .\n",
      "5\n",
      "Conclusion\n",
      "In this work, we’ve introduced papermage , an\n",
      "open-source Python toolkit for processing scientific\n",
      "documents. papermage was developed to supply\n",
      "high-quality data and reduce friction for research\n",
      "prototype development at Semantic Scholar. To-\n",
      "day, it is being used in the production PDF process-\n",
      "ing pipeline to provide data for both the literature\n",
      "graph (Ammar et al., 2018; Kinney et al., 2023)\n",
      "and the paper-reading interface (Lo et al., 2023). It\n",
      "has also been used in working research prototypes\n",
      "which have since contributed to research publica-\n",
      "tions (Fok et al., 2023b; Kim et al., 2023). 6 We\n",
      "open-source papermage in hopes it will simplify\n",
      "research workflows that depend on scientific doc-\n",
      "uments and promote extensions to other visually-\n",
      "rich documents like textbooks (Lincker et al., 2023)\n",
      "and digitized print media (Lee et al., 2020).\n",
      "6 See a demo of such a prototype papeo.app/demo .\n",
      "\n",
      "=== PAGE: 6 ===\n",
      "\n",
      "\n",
      "Ethical Considerations\n",
      "As a toolkit primarily designed to process scientific\n",
      "documents, there are two areas where papermage\n",
      "could cause harms or have unintended effects.\n",
      "Extraction\n",
      "of\n",
      "bibliographic\n",
      "information\n",
      "papermage could be used to parse author names,\n",
      "affiliation, emails from scientific documents. Like\n",
      "any software, this extraction can be noisy, leading\n",
      "to incorrect parsing and thus mis-attribution of\n",
      "manuscripts.\n",
      "Further, since papermage relies\n",
      "on static PDF documents, rather than metadata\n",
      "dynamically retrieved from publishers, users of\n",
      "papermage need consider how and when extracted\n",
      "names should no longer be associated with authors,\n",
      "a harmful practice called deadnaming (Queer in AI\n",
      "et al., 2023). We recommend papermage users to\n",
      "exercise caution when using our toolkit to extract\n",
      "metadata, to cross-reference extracted content with\n",
      "other sources when possible, and to design systems\n",
      "such that authors have the ability to manually edit\n",
      "any data about themselves.\n",
      "Misrepresentation or fabrication of informa-\n",
      "tion in documents\n",
      "In §3, we discussed how\n",
      "papermage can be easily extended to support high-\n",
      "level applications. Such applications might include\n",
      "question answering chatbots, or AI summarizers\n",
      "that perform information synthesis over one or\n",
      "more papermage documents. Such applications\n",
      "typically rely on generative models to produce their\n",
      "output, which might fabricate incorrect informa-\n",
      "tion or misstate claims. Developers should be vig-\n",
      "ilant when integrating papermage output into any\n",
      "downstream application, especially in systems that\n",
      "purport to represent information gathered from sci-\n",
      "entific publications.\n",
      "Acknowledgements\n",
      "We thank our teammates at Semantic Scholar for\n",
      "their help on this project. In particular: Rodney\n",
      "Kinney provided insight during discussions about\n",
      "how best to represent data extracted from docu-\n",
      "ments; Paul Sayre provided feedback on initial\n",
      "designs of the library; Chloe Anastasiades, Dany\n",
      "Haddad and Egor Klevak tested earlier versions of\n",
      "the library; Tal August, Raymond Fok, and Andrew\n",
      "Head motivated the need for such a toolkit dur-\n",
      "ing their internships building augmented reading\n",
      "interfaces; Jaron Lochner and Kelsey MacMillan\n",
      "helped us get additional engineering support; and\n",
      "Oren Etzioni provided enthusiasm and support for\n",
      "continued investment in this toolkit.\n",
      "This project was supported in part by NSF Grant\n",
      "OIA-2033558 and NSF Grant CNS-2213656.\n",
      "Author Contributions\n",
      "All authors contributed to the implementation of\n",
      "papermage and/or the writing of this paper.\n",
      "Core contributors.\n",
      "Kyle Lo and Zejiang Shen\n",
      "initiated the project and co-wrote initial implemen-\n",
      "tations of magelib and some Predictors . Later,\n",
      "Kyle Lo and Luca Soldaini refactored a majority of\n",
      "magelib , Predictors , and added Recipes . Ben-\n",
      "jamin Newman added new Predictors to support\n",
      "use-cases like those in the Vignette (§4). Joseph\n",
      "Chee Chang implemented an end-to-end web-based\n",
      "visual interface for papermage and helped iterate\n",
      "on papermage ’s designs. All core contributors\n",
      "helped with writing. Finally, Kyle Lo led all aspects\n",
      "of the project, including design and implementa-\n",
      "tion, as well as mentorship of other contributors to\n",
      "the toolkit (see below).\n",
      "Other contributors.\n",
      "Russell Authur, Stefan Can-\n",
      "dra, Yoganand Chandrasekhar, Regan Huff, Aman-\n",
      "preet Singh and Angele Zamarron each worked\n",
      "closely with Kyle Lo to contribute a Predictor\n",
      "to papermage . Erin Bransom and Bailey Kuehl\n",
      "helped with data annotation for training and evalu-\n",
      "ating those Predictors . Chris Wilhelm provided\n",
      "feedback on papermage ’s design and implemented\n",
      "faster indexing of Entities when building Layers .\n",
      "Finally, Marti Hearst, Daniel Weld, and Doug\n",
      "Downey helped with writing and overall advising\n",
      "on the project.\n",
      "References\n",
      "Waleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\n",
      "ula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\n",
      "son Dunkelberger, Ahmed Elgohary, Sergey Feld-\n",
      "man, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\n",
      "Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe-\n",
      "ters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang,\n",
      "Chris Wilhelm, Zheng Yuan, Madeleine van Zuylen,\n",
      "and Oren Etzioni. 2018. Construction of the litera-\n",
      "ture graph in semantic scholar. In Proceedings of\n",
      "the 2018 Conference of the North American Chap-\n",
      "ter of the Association for Computational Linguistics:\n",
      "Human Language Technologies, Volume 3 (Indus-\n",
      "try Papers) , pages 84–91, New Orleans - Louisiana.\n",
      "Association for Computational Linguistics.\n",
      "Tal August, Lucy Lu Wang, Jonathan Bragg, Marti A.\n",
      "Hearst, Andrew Head, and Kyle Lo. 2023. Paper\n",
      "\n",
      "=== PAGE: 7 ===\n",
      "\n",
      "\n",
      "plain: Making medical research papers approachable\n",
      "to healthcare consumers with natural language pro-\n",
      "cessing. ACM Trans. Comput.-Hum. Interact. , 30(5).\n",
      "Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\n",
      "ERT: A pretrained language model for scientific text.\n",
      "In Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing and the\n",
      "9th International Joint Conference on Natural Lan-\n",
      "guage Processing (EMNLP-IJCNLP) , pages 3615–\n",
      "3620, Hong Kong, China. Association for Computa-\n",
      "tional Linguistics.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-Voss,\n",
      "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
      "Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\n",
      "Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\n",
      "teusz Litwin, Scott Gray, Benjamin Chess, Jack\n",
      "Clark, Christopher Berner, Sam McCandlish, Alec\n",
      "Radford, Ilya Sutskever, and Dario Amodei. 2020.\n",
      "Language models are few-shot learners.\n",
      "In Ad-\n",
      "vances in Neural Information Processing Systems ,\n",
      "volume 33, pages 1877–1901. Curran Associates,\n",
      "Inc.\n",
      "Joseph Chee Chang, Saleema Amershi, and Ece Kamar.\n",
      "2017. Revolt: Collaborative crowdsourcing for label-\n",
      "ing machine learning datasets. In Proceedings of the\n",
      "2017 CHI Conference on Human Factors in Comput-\n",
      "ing Systems , CHI ’17, page 2334–2346, New York,\n",
      "NY, USA. Association for Computing Machinery.\n",
      "Joseph Chee Chang, Amy X. Zhang, Jonathan Bragg,\n",
      "Andrew Head, Kyle Lo, Doug Downey, and Daniel S.\n",
      "Weld. 2023. Citesee: Augmenting citations in scien-\n",
      "tific papers with persistent and personalized historical\n",
      "context. In Proceedings of the 2023 CHI Conference\n",
      "on Human Factors in Computing Systems , CHI ’23,\n",
      "New York, NY, USA. Association for Computing\n",
      "Machinery.\n",
      "Catherine Chen, Zejiang Shen, Dan Klein, Gabriel\n",
      "Stanovsky, Doug Downey, and Kyle Lo. 2023. Are\n",
      "layout-infused language models robust to layout dis-\n",
      "tribution shifts? a case study with scientific docu-\n",
      "ments. In Findings of the Association for Computa-\n",
      "tional Linguistics: ACL 2023 , pages 13345–13360,\n",
      "Toronto, Canada. Association for Computational Lin-\n",
      "guistics.\n",
      "Isaac Councill, C. Lee Giles, and Min-Yen Kan. 2008.\n",
      "ParsCit: an open-source CRF reference string pars-\n",
      "ing package. In Proceedings of the Sixth Interna-\n",
      "tional Conference on Language Resources and Eval-\n",
      "uation (LREC’08) , Marrakech, Morocco. European\n",
      "Language Resources Association (ELRA).\n",
      "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\n",
      "Noah A. Smith, and Matt Gardner. 2021. A dataset\n",
      "of information-seeking questions and answers an-\n",
      "chored in research papers. In Proceedings of the\n",
      "2021 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies , pages 4599–4610, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Raymond Fok, Joseph Chee Chang, Tal August, Amy X.\n",
      "Zhang, and Daniel S. Weld. 2023a. Qlarify: Bridg-\n",
      "ing scholarly abstracts and papers with recursively\n",
      "expandable summaries. arXiv , abs/2310.07581.\n",
      "Raymond Fok, Hita Kambhamettu, Luca Soldaini,\n",
      "Jonathan Bragg, Kyle Lo, Marti Hearst, Andrew\n",
      "Head, and Daniel S Weld. 2023b. Scim: Intelligent\n",
      "skimming support for scientific papers. In Proceed-\n",
      "ings of the 28th International Conference on Intelli-\n",
      "gent User Interfaces , IUI ’23, page 476–490, New\n",
      "York, NY, USA. Association for Computing Machin-\n",
      "ery.\n",
      "Grobid. 2008–2023. Grobid. https://github.com/\n",
      "kermitt2/grobid .\n",
      "Yu Gu, Robert Tinn, Hao Cheng, Michael R. Lucas,\n",
      "Naoto Usuyama, Xiaodong Liu, Tristan Naumann,\n",
      "Jianfeng Gao, and Hoifung Poon. 2020. Domain-\n",
      "specific language model pretraining for biomedical\n",
      "natural language processing. ACM Transactions on\n",
      "Computing for Healthcare (HEALTH) , 3:1 – 23.\n",
      "Andrew Head, Kyle Lo, Dongyeop Kang, Raymond\n",
      "Fok, Sam Skjonsberg, Daniel S. Weld, and Marti A.\n",
      "Hearst. 2021. Augmenting scientific papers with just-\n",
      "in-time, position-sensitive definitions of terms and\n",
      "symbols. In Proceedings of the 2021 CHI Conference\n",
      "on Human Factors in Computing Systems , CHI ’21,\n",
      "New York, NY, USA. Association for Computing\n",
      "Machinery.\n",
      "Zhi Hong, Aswathy Ajith, James Pauloski, Eamon\n",
      "Duede, Kyle Chard, and Ian Foster. 2023. The dimin-\n",
      "ishing returns of masked language models to science.\n",
      "In Findings of the Association for Computational\n",
      "Linguistics: ACL 2023 , pages 1270–1283, Toronto,\n",
      "Canada. Association for Computational Linguistics.\n",
      "Po-Wei Huang, Abhinav Ramesh Kashyap, Yanxia Qin,\n",
      "Yajing Yang, and Min-Yen Kan. 2022a. Lightweight\n",
      "contextual logical structure recovery. In Proceedings\n",
      "of the Third Workshop on Scholarly Document Pro-\n",
      "cessing , pages 37–48, Gyeongju, Republic of Korea.\n",
      "Association for Computational Linguistics.\n",
      "Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and\n",
      "Furu Wei. 2022b. Layoutlmv3: Pre-training for doc-\n",
      "ument ai with unified text and image masking. Pro-\n",
      "ceedings of the 30th ACM International Conference\n",
      "on Multimedia .\n",
      "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\n",
      "tian Riedel, Piotr Bojanowski, Armand Joulin, and\n",
      "Edouard Grave. 2022. Unsupervised dense informa-\n",
      "tion retrieval with contrastive learning. Transactions\n",
      "on Machine Learning Research .\n",
      "Sarthak Jain, Madeleine van Zuylen, Hannaneh Ha-\n",
      "jishirzi, and Iz Beltagy. 2020. SciREX: A challenge\n",
      "dataset for document-level information extraction. In\n",
      "\n",
      "=== PAGE: 8 ===\n",
      "\n",
      "\n",
      "Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 7506–\n",
      "7516, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Hyeonsu B. Kang, Joseph Chee Chang, Yongsung Kim,\n",
      "and Aniket Kittur. 2022. Threddy: An interactive\n",
      "system for personalized thread-based exploration and\n",
      "organization of scientific literature. In Proceedings of\n",
      "the 35th Annual ACM Symposium on User Interface\n",
      "Software and Technology , UIST ’22, New York, NY,\n",
      "USA. Association for Computing Machinery.\n",
      "Hyeonsu B. Kang, Sherry Tongshuang Wu, Joseph Chee\n",
      "Chang, and Aniket Kittur. 2023. Synergi: A mixed-\n",
      "initiative system for scholarly synthesis and sense-\n",
      "making. In Proceedings of the 36th Annual ACM\n",
      "Symposium on User Interface Software and Technol-\n",
      "ogy . Association for Computing Machinery.\n",
      "Tae Soo Kim, Matt Latzke, Jonathan Bragg, Amy X.\n",
      "Zhang, and Joseph Chee Chang. 2023. Papeos: Aug-\n",
      "menting research papers with talk videos. In Proceed-\n",
      "ings of the 36th Annual ACM Symposium on User\n",
      "Interface Software and Technology .\n",
      "Rodney Kinney, Chloe Anastasiades, Russell Authur,\n",
      "Iz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\n",
      "ski, Isabel Cachola, Stefan Candra, Yoganand Chan-\n",
      "drasekhar, Arman Cohan, Miles Crawford, Doug\n",
      "Downey, Jason Dunkelberger, Oren Etzioni, Rob\n",
      "Evans, Sergey Feldman, Joseph Gorney, David Gra-\n",
      "ham, Fangzhou Hu, Regan Huff, Daniel King, Se-\n",
      "bastian Kohlmeier, Bailey Kuehl, Michael Langan,\n",
      "Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\n",
      "Kelsey MacMillan, Tyler Murray, Chris Newell,\n",
      "Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\n",
      "Shen, Amanpreet Singh, Luca Soldaini, Shivashankar\n",
      "Subramanian, Amber Tanaka, Alex D. Wade, Linda\n",
      "Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu,\n",
      "Jiangjiang Yang, Angele Zamarron, Madeleine Van\n",
      "Zuylen, and Daniel S. Weld. 2023. The Semantic\n",
      "Scholar Open Data Platform. ArXiv , abs/2301.10140.\n",
      "Wei-Jen Ko, Te-yuan Chen, Yiyan Huang, Greg Durrett,\n",
      "and Junyi Jessy Li. 2020. Inquisitive question gener-\n",
      "ation for high level text comprehension. In Proceed-\n",
      "ings of the 2020 Conference on Empirical Methods\n",
      "in Natural Language Processing (EMNLP) , pages\n",
      "6544–6555, Online. Association for Computational\n",
      "Linguistics.\n",
      "Benjamin Charles Germain Lee, Jaime Mears, Eileen\n",
      "Jakeway, Meghan Ferriter, Chris Adams, Nathan\n",
      "Yarasavage, Deborah Thomas, Kate Zwaard, and\n",
      "Daniel S. Weld. 2020. The newspaper navigator\n",
      "dataset: Extracting headlines and visual content from\n",
      "16 million historic newspaper pages in chronicling\n",
      "america. In Proceedings of the 29th ACM Interna-\n",
      "tional Conference on Information & Knowledge Man-\n",
      "agement , CIKM ’20, page 3055–3062, New York,\n",
      "NY, USA. Association for Computing Machinery.\n",
      "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\n",
      "Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n",
      "2019. BioBERT: a pre-trained biomedical language\n",
      "representation model for biomedical text mining.\n",
      "Bioinformatics , 36(4):1234–1240.\n",
      "Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol\n",
      "Hwang, Jaehyeon Kim, Hong-In Lee, and Moontae\n",
      "Lee. 2023. QASA: Advanced question answering on\n",
      "scientific articles. In Proceedings of the 40th Inter-\n",
      "national Conference on Machine Learning , volume\n",
      "202 of Proceedings of Machine Learning Research ,\n",
      "pages 19036–19052. PMLR.\n",
      "Élise Lincker, Olivier Pons, Camille Guinaudeau, Is-\n",
      "abelle Barbet, Jérôme Dupire, Céline Hudelot, Vin-\n",
      "cent Mousseau, and Caroline Huron. 2023. Layout\n",
      "and activity-based textbook modeling for automatic\n",
      "pdf textbook extraction. In iTextbooks@AIED .\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\n",
      "dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n",
      "Luke Zettlemoyer, and Veselin Stoyanov. 2019.\n",
      "RoBERTa: A Robustly Optimized BERT Pretrain-\n",
      "ing Approach. ArXiv , abs/1907.11692.\n",
      "Kyle Lo, Joseph Chee Chang, Andrew Head, Jonathan\n",
      "Bragg, Amy X. Zhang, Cassidy Trier, Chloe Anas-\n",
      "tasiades, Tal August, Russell Authur, Danielle Bragg,\n",
      "Erin Bransom, Isabel Cachola, Stefan Candra, Yo-\n",
      "ganand Chandrasekhar, Yen-Sung Chen, Evie Yu-\n",
      "Yen Cheng, Yvonne Chou, Doug Downey, Rob\n",
      "Evans, Raymond Fok, Fangzhou Hu, Regan Huff,\n",
      "Dongyeop Kang, Tae Soo Kim, Rodney Kinney,\n",
      "Aniket Kittur, Hyeonsu Kang, Egor Klevak, Bai-\n",
      "ley Kuehl, Michael Langan, Matt Latzke, Jaron\n",
      "Lochner, Kelsey MacMillan, Eric Marsh, Tyler Mur-\n",
      "ray, Aakanksha Naik, Ngoc-Uyen Nguyen, Srishti\n",
      "Palani, Soya Park, Caroline Paulic, Napol Rachata-\n",
      "sumrit, Smita Rao, Paul Sayre, Zejiang Shen, Pao\n",
      "Siangliulue, Luca Soldaini, Huy Tran, Madeleine van\n",
      "Zuylen, Lucy Lu Wang, Christopher Wilhelm, Caro-\n",
      "line Wu, Jiangjiang Yang, Angele Zamarron, Marti A.\n",
      "Hearst, and Daniel S. Weld. 2023. The Semantic\n",
      "Reader Project: Augmenting Scholarly Documents\n",
      "through AI-Powered Interactive Reading Interfaces.\n",
      "ArXiv , abs/2303.14334.\n",
      "Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\n",
      "ney, and Daniel Weld. 2020. S2ORC: The semantic\n",
      "scholar open research corpus. In Proceedings of the\n",
      "58th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 4969–4983, Online. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\n",
      "Zhang, Hoifung Poon, and Tie-Yan Liu. 2022.\n",
      "Biogpt:\n",
      "Generative pre-trained transformer for\n",
      "biomedical text generation and mining. Briefings\n",
      "in bioinformatics .\n",
      "Mark Neumann, Daniel King, Iz Beltagy, and Waleed\n",
      "Ammar. 2019. ScispaCy: Fast and robust models\n",
      "for biomedical natural language processing. In Pro-\n",
      "ceedings of the 18th BioNLP Workshop and Shared\n",
      "Task , pages 319–327, Florence, Italy. Association for\n",
      "Computational Linguistics.\n",
      "\n",
      "=== PAGE: 9 ===\n",
      "\n",
      "\n",
      "Benjamin Newman, Luca Soldaini, Raymond Fok, Ar-\n",
      "man Cohan, and Kyle Lo. 2023. A question answer-\n",
      "ing framework for decontextualizing user-facing snip-\n",
      "pets from scientific documents. In Proceedings of the\n",
      "2023 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP) .\n",
      "Organizers of Queer in AI, Anaelia Ovalle, Arjun Sub-\n",
      "ramonian, Ashwin Singh, Claas Voelcker, Danica J.\n",
      "Sutherland, Davide Locatelli, Eva Breznik, Filip Klu-\n",
      "bicka, Hang Yuan, Hetvi J, Huan Zhang, Jaidev\n",
      "Shriram, Kruno Lehman, Luca Soldaini, Maarten\n",
      "Sap, Marc Peter Deisenroth, Maria Leonor Pacheco,\n",
      "Maria Ryskina, Martin Mundt, Milind Agarwal, Nyx\n",
      "Mclean, Pan Xu, A Pranav, Raj Korpan, Ruchira\n",
      "Ray, Sarah Mathew, Sarthak Arora, St John, Tanvi\n",
      "Anand, Vishakha Agrawal, William Agnew, Yanan\n",
      "Long, Zijie J. Wang, Zeerak Talat, Avijit Ghosh,\n",
      "Nathaniel Dennler, Michael Noseworthy, Sharvani\n",
      "Jha, Emi Baylor, Aditya Joshi, Natalia Y. Bilenko,\n",
      "Andrew Mcnamara, Raphael Gontijo-Lopes, Alex\n",
      "Markham, Evyn Dong, Jackie Kay, Manu Saraswat,\n",
      "Nikhil Vytla, and Luke Stark. 2023. Queer In AI: A\n",
      "Case Study in Community-Led Participatory AI. In\n",
      "Proceedings of the 2023 ACM Conference on Fair-\n",
      "ness, Accountability, and Transparency , FAccT ’23,\n",
      "page 1882–1895, New York, NY, USA. Association\n",
      "for Computing Machinery.\n",
      "Napol Rachatasumrit, Jonathan Bragg, Amy X. Zhang,\n",
      "and Daniel S Weld. 2022. Citeread: Integrating lo-\n",
      "calized citation contexts into scientific paper reading.\n",
      "In 27th International Conference on Intelligent User\n",
      "Interfaces , IUI ’22, page 707–719, New York, NY,\n",
      "USA. Association for Computing Machinery.\n",
      "Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\n",
      "Sun. 2015. Faster r-cnn: Towards real-time object de-\n",
      "tection with region proposal networks. IEEE Trans-\n",
      "actions on Pattern Analysis and Machine Intelligence ,\n",
      "39:1137–1149.\n",
      "Nipun Sadvilkar and Mark Neumann. 2020. PySBD:\n",
      "Pragmatic sentence boundary disambiguation. In\n",
      "Proceedings of Second Workshop for NLP Open\n",
      "Source Software (NLP-OSS) , pages 110–114, Online.\n",
      "Association for Computational Linguistics.\n",
      "Zejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl,\n",
      "Daniel S. Weld, and Doug Downey. 2022. VILA: Im-\n",
      "proving structured content extraction from scientific\n",
      "PDFs using visual layout groups. Transactions of the\n",
      "Association for Computational Linguistics , 10:376–\n",
      "392.\n",
      "Zejiang Shen, Ruochen Zhang, Melissa Dell, B. Lee,\n",
      "Jacob Carlson, and Weining Li. 2021. Layoutparser:\n",
      "A unified toolkit for deep learning based document\n",
      "image analysis. In IEEE International Conference\n",
      "on Document Analysis and Recognition .\n",
      "Sanjay Subramanian, Lucy Lu Wang, Ben Bogin,\n",
      "Sachin Mehta, Madeleine van Zuylen, Sravanthi\n",
      "Parasa, Sameer Singh, Matt Gardner, and Hannaneh\n",
      "Hajishirzi. 2020. MedICaT: A dataset of medical\n",
      "images, captions, and textual references. In Find-\n",
      "ings of the Association for Computational Linguistics:\n",
      "EMNLP 2020 , pages 2112–2120, Online. Association\n",
      "for Computational Linguistics.\n",
      "M. Tan, R. Pang, and Q. V. Le. 2020. Efficientdet:\n",
      "Scalable and efficient object detection. In 2020\n",
      "IEEE/CVF Conference on Computer Vision and Pat-\n",
      "tern Recognition (CVPR) , pages 10778–10787, Los\n",
      "Alamitos, CA, USA. IEEE Computer Society.\n",
      "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas\n",
      "Scialom, Anthony S. Hartshorn, Elvis Saravia, An-\n",
      "drew Poulton, Viktor Kerkez, and Robert Stojnic.\n",
      "2022. Galactica: A large language model for science.\n",
      "ArXiv , abs/2211.09085.\n",
      "pdf2image . 2023.\n",
      "pdf2image .\n",
      "https://github.\n",
      "com/Belval/pdf2image .\n",
      "pdfplumber . 2023. pdfplumber . https://github.\n",
      "com/jsvine/pdfplumber .\n",
      "Dominika Tkaczyk, Paweł Szostek, Mateusz Fedo-\n",
      "ryszak, Piotr Jan Dendek, and Lukasz Bolikowski.\n",
      "2015. Cermine: Automatic extraction of structured\n",
      "metadata from scientific literature. Int. J. Doc. Anal.\n",
      "Recognit. , 18(4):317–335.\n",
      "Amalie Trewartha, Nicholas Walker, Haoyan Huo,\n",
      "Sanghoon Lee, Kevin Cruse, John Dagdelen, Alex\n",
      "Dunn, Kristin Aslaug Persson, Gerbrand Ceder, and\n",
      "Anubhav Jain. 2022. Quantifying the advantage of\n",
      "domain-specific pre-training on named entity recog-\n",
      "nition tasks in materials science. Patterns , 3.\n",
      "Lucy Lu Wang, Isabel Cachola, Jonathan Bragg, Evie\n",
      "(Yu-Yen) Cheng, Chelsea Hess Haupt, Matt Latzke,\n",
      "Bailey Kuehl, Madeleine van Zuylen, Linda M. Wag-\n",
      "ner, and Daniel S. Weld. 2021. Improving the acces-\n",
      "sibility of scientific documents: Current state, user\n",
      "needs, and a system solution to enhance scientific pdf\n",
      "accessibility for blind and low vision users. ArXiv ,\n",
      "abs/2105.00076.\n",
      "Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\n",
      "Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin\n",
      "Eide, Kathryn Funk, Yannis Katsis, Rodney Michael\n",
      "Kinney, Yunyao Li, Ziyang Liu, William Merrill,\n",
      "Paul Mooney, Dewey A. Murdick, Devvret Rishi,\n",
      "Jerry Sheehan, Zhihong Shen, Brandon Stilson,\n",
      "Alex D. Wade, Kuansan Wang, Nancy Xin Ru Wang,\n",
      "Christopher Wilhelm, Boya Xie, Douglas M. Ray-\n",
      "mond, Daniel S. Weld, Oren Etzioni, and Sebastian\n",
      "Kohlmeier. 2020. CORD-19: The COVID-19 open\n",
      "research dataset. In Proceedings of the 1st Work-\n",
      "shop on NLP for COVID-19 at ACL 2020 , Online.\n",
      "Association for Computational Linguistics.\n",
      "Rosalee Wolfe, John McDonald, Ronan Johnson, Ben\n",
      "Sturr, Syd Klinghoffer, Anthony Bonzani, Andrew\n",
      "Alexander, and Nicole Barnekow. 2022. Supporting\n",
      "mouthing in signed languages: New innovations and\n",
      "a proposal for future corpus building. In Proceedings\n",
      "of the 7th International Workshop on Sign Language\n",
      "\n",
      "=== PAGE: 10 ===\n",
      "\n",
      "\n",
      "Translation and Avatar Technology: The Junction of\n",
      "the Visual and the Textual: Challenges and Perspec-\n",
      "tives , pages 125–130, Marseille, France. European\n",
      "Language Resources Association.\n",
      "Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu\n",
      "Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\n",
      "Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou.\n",
      "2021. LayoutLMv2: Multi-modal pre-training for\n",
      "visually-rich document understanding. In Proceed-\n",
      "ings of the 59th Annual Meeting of the Association for\n",
      "Computational Linguistics and the 11th International\n",
      "Joint Conference on Natural Language Processing\n",
      "(Volume 1: Long Papers) , pages 2579–2591, Online.\n",
      "Association for Computational Linguistics.\n",
      "Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu\n",
      "Wei, and Ming Zhou. 2019. Layoutlm: Pre-training\n",
      "of text and layout for document image understanding.\n",
      "Proceedings of the 26th ACM SIGKDD International\n",
      "Conference on Knowledge Discovery & Data Mining .\n",
      "Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes.\n",
      "2019. Publaynet: largest dataset ever for document\n",
      "layout analysis. In 2019 International Conference on\n",
      "Document Analysis and Recognition (ICDAR) , pages\n",
      "1015–1022. IEEE.\n",
      "\n",
      "=== PAGE: 11 ===\n",
      "\n",
      "\n",
      "A\n",
      "Appendix\n",
      "A.1\n",
      "Comparison and Compatibility with\n",
      "XML\n",
      "One can view Layers as capturing content hier-\n",
      "archy (e.g., tokens vs sentences) similar to that of\n",
      "other structured document representations, like TEI\n",
      "XML trees. We note that Layers are stored as un-\n",
      "ordered attributes and don’t require nesting. This\n",
      "allows for specific cross-layer referencing opera-\n",
      "tions that don’t adhere to strict nesting relationships.\n",
      "For example:\n",
      "1 for sentence in doc.sentences:\n",
      "2\n",
      "for line in sentence.lines:\n",
      "3\n",
      "...\n",
      "Recall that a sentence can begin or end midway\n",
      "through a line and cross multiple lines (§3.1). Sim-\n",
      "ilarly, not all lines are exactly contained within\n",
      "the boundaries of a sentence. As such, sentences\n",
      "and lines are not strictly nested within each other.\n",
      "This relationship would be difficult to encode in an\n",
      "XML format adhering to document tree structure.\n",
      "Regardless, the way we represent structure\n",
      "in documents is highly versatile.\n",
      "We demon-\n",
      "strate this by also implementing GrobidParser\n",
      "as an alternative to the PDF2TextParser in §3.1.\n",
      "GrobidParser invokes Grobid to process PDFs,\n",
      "and reads the resulting TEI XML file generated by\n",
      "Grobid by converting each XML tag of a common\n",
      "level into an Entity of its own Layer . We use this\n",
      "to perform the evaluation in Table 2.\n",
      "A.2\n",
      "Additional magelib Protocols and\n",
      "Utilities\n",
      "Serialization.\n",
      "Any Document and all of its\n",
      "Layers can be exported to a JSON format, and\n",
      "perfectly reconstructed:\n",
      "1 import json\n",
      "2 with open(\"....json\", \"w\") as f_out:\n",
      "3\n",
      "json.dump(doc.to_json(), f_out)\n",
      "4 5 with open(\"...json\", \"r\") as f_in:\n",
      "6\n",
      "doc = json.load(f_in)\n",
      "A.3\n",
      "Evaluating papermage ’s CoreRecipe\n",
      "against Grobid\n",
      "Here, we detail how we performed the evaluation\n",
      "reported in §3.3 (Table 2). We also provide a full\n",
      "breakdown by category in Table 3.\n",
      "As described earlier in the paper, Grobid is quite\n",
      "difficult to evaluate as it is developed with tight\n",
      "coupling between the PDF parser ( pdfalto ) and\n",
      "the models it employs to perform logical struc-\n",
      "ture recovery over the resulting token stream. As\n",
      "such, there is no straightforward way to run just\n",
      "the model components of Grobid on an alternative\n",
      "token stream like that provided in the S2-VL (Shen\n",
      "et al., 2022) dataset.\n",
      "To perform this baseline evaluation, we ran\n",
      "the original PDFs that were annotated for S2-VL\n",
      "through our GrobidParser using v0.7.3. Grobid\n",
      "also returns bounding boxes of some predicted cat-\n",
      "egories (e.g., authors, abstract, paragraphs). We\n",
      "use these bounding boxes to create Entities that\n",
      "we annotate on a Document constructed manually\n",
      "from from S2-VL data. Using magelib cross-layer\n",
      "referencing, we were able to match Grobid predic-\n",
      "tions to S2-VL data to perform this evaluation.\n",
      "Though we found there are certain categories\n",
      "for which bounding box information was either not\n",
      "available (e.g., Titles) or Grobid simply did not re-\n",
      "turn that output (e.g., Figure text extraction). These\n",
      "are represented by zeros in Table 3, which con-\n",
      "tributes to the lower scores in Table 2 after macro\n",
      "averaging. For a more apples-to-apples compari-\n",
      "son, we also included a “Grobid Subset” evaluation\n",
      "which restricted to just categories in S2-VL for\n",
      "which Grobid produced bounding box information.\n",
      "In addition to Grobid, we evaluate two of our pro-\n",
      "vided Transformer-based models. The RoBERTa-\n",
      "large (Liu et al., 2019) model is a Transformers\n",
      "token classification model that we finetuned on the\n",
      "S2-VL training set. The I-VILA model is a layout-\n",
      "infused Transformer model pretrained by Shen et al.\n",
      "(2022) on the S2-VL training set. Like we did with\n",
      "Grobid, we ran our CoreRecipe using these two\n",
      "models on the original PDFs in S2-VL, and per-\n",
      "formed a similar token mapping operation since our\n",
      "PDF2TextParser also produces a different token\n",
      "stream than that provided in S2-VL.\n",
      "At the end of the day, the Transformer-based\n",
      "models performed better at this task than Grobid.\n",
      "This is unsurprising given expected improvements\n",
      "using a Transformer model over a CRF or BiL-\n",
      "STM. The Transformer models were also trained\n",
      "on S2-VL data, which gave them an advantage over\n",
      "Grobid. Overall, this evaluation intended to show\n",
      "how papermage enables cross-system comparisons,\n",
      "even eschewing token stream incompatibility, and\n",
      "to illustrate an upper bound of the performance left\n",
      "on the table by existing software systems that don’t\n",
      "use of state-of-the-art models.\n",
      "\n",
      "=== PAGE: 12 ===\n",
      "\n",
      "\n",
      "StructureCategory\n",
      "GROBID CRF\n",
      "GROBID NN\n",
      "RoBERTa\n",
      "I-VILA\n",
      "P\n",
      "R\n",
      "F1\n",
      "P\n",
      "R\n",
      "F1\n",
      "P\n",
      "R\n",
      "F1\n",
      "P\n",
      "R\n",
      "F1\n",
      "Abstract\n",
      "81.9\n",
      "89.1\n",
      "85.3\n",
      "85.3\n",
      "89.8\n",
      "87.5\n",
      "89.2\n",
      "93.7\n",
      "91.4\n",
      "97.4\n",
      "98.3\n",
      "97.8\n",
      "Author\n",
      "55.2\n",
      "42.6\n",
      "48.1\n",
      "75.1\n",
      "14.0\n",
      "23.6\n",
      "87.5\n",
      "73.5\n",
      "79.9\n",
      "65.5\n",
      "96.9\n",
      "78.2\n",
      "Bibliography\n",
      "96.5\n",
      "98.6\n",
      "97.5\n",
      "95.5\n",
      "97.6\n",
      "96.5\n",
      "93.6\n",
      "93.3\n",
      "93.5\n",
      "99.7\n",
      "98.2\n",
      "99.0\n",
      "Caption\n",
      "70.3\n",
      "70.0\n",
      "70.2\n",
      "70.2\n",
      "69.7\n",
      "70.0\n",
      "80.0\n",
      "77.3\n",
      "78.6\n",
      "93.1\n",
      "89.6\n",
      "91.3\n",
      "Equation\n",
      "71.1\n",
      "85.3\n",
      "77.6\n",
      "71.1\n",
      "85.3\n",
      "77.6\n",
      "55.0\n",
      "85.7\n",
      "67.0\n",
      "90.7\n",
      "94.2\n",
      "92.4\n",
      "Figure\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "88.9\n",
      "82.3\n",
      "85.4\n",
      "99.8\n",
      "96.8\n",
      "98.3\n",
      "Footer\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "56.1\n",
      "59.9\n",
      "57.9\n",
      "96.8\n",
      "78.1\n",
      "86.5\n",
      "Footnote\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "59.8\n",
      "44.3\n",
      "50.9\n",
      "80.2\n",
      "93.5\n",
      "86.3\n",
      "Header\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "40.5\n",
      "84.3\n",
      "54.7\n",
      "92.9\n",
      "99.1\n",
      "95.9\n",
      "Keywords\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "93.8\n",
      "97.1\n",
      "95.4\n",
      "96.9\n",
      "99.4\n",
      "98.1\n",
      "List\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "61.9\n",
      "63.8\n",
      "62.9\n",
      "76.7\n",
      "82.4\n",
      "79.4\n",
      "Paragraph\n",
      "94.5\n",
      "89.8\n",
      "92.1\n",
      "94.4\n",
      "89.9\n",
      "92.1\n",
      "93.5\n",
      "93.0\n",
      "93.3\n",
      "98.7\n",
      "97.9\n",
      "98.3\n",
      "Section\n",
      "83.0\n",
      "79.4\n",
      "81.1\n",
      "83.0\n",
      "79.4\n",
      "81.1\n",
      "67.7\n",
      "82.7\n",
      "74.4\n",
      "96.2\n",
      "91.6\n",
      "93.9\n",
      "Table\n",
      "97.3\n",
      "58.6\n",
      "73.2\n",
      "97.9\n",
      "58.6\n",
      "73.3\n",
      "94.7\n",
      "71.8\n",
      "81.7\n",
      "96.1\n",
      "94.9\n",
      "95.5\n",
      "Title\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "76.3\n",
      "96.7\n",
      "85.3\n",
      "98.7\n",
      "99.9\n",
      "99.3\n",
      "Macro Avg\n",
      "(Full S2-VL)\n",
      "40.6\n",
      "38.3\n",
      "39.1\n",
      "42.0\n",
      "36.5\n",
      "37.6\n",
      "75.9\n",
      "80.0\n",
      "76.8\n",
      "92.0\n",
      "94.1\n",
      "92.7\n",
      "Macro Avg\n",
      "(Grobid Subset)\n",
      "81.2\n",
      "76.7\n",
      "78.9\n",
      "84.1\n",
      "73.0\n",
      "78.2\n",
      "82.6\n",
      "83.9\n",
      "83.2\n",
      "92.2\n",
      "95.2\n",
      "93.7\n",
      "Table 3: Evaluating CoreRecipe for logical structure recovery on S2-VL (Shen et al., 2022). These are per-category\n",
      "metrics for Table 2. Metrics are computed for token-level classification, macro-averaged over categories. The\n",
      "“Grobid Subset” limits evaluation to only categories for which Grobid returns bounding box information, which was\n",
      "necessary for evaluation on S2-VL.\n"
     ]
    }
   ],
   "source": [
    "for page in doc.pages:\n",
    "    print(f'\\n=== PAGE: {page.id} ===\\n\\n')\n",
    "    for row in page.rows:\n",
    "        print(row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: papermage ’s document creation and representation. (A) Recipes are turn-key methods for processing\n",
      "a PDF. (B) They compose models operating across different data modalities and machine learning frameworks\n",
      "to extract document structure, which we conceptualize\n",
      "as layers of annotation that store textual and visual information. (C) Users can access and manipulate layers.\n",
      "Figure 2: Entities are multimodal content units. Here,\n",
      "spans of a sentence are used to identify its text among\n",
      "all symbols, while boxes map its visual coordinates on\n",
      "a page. spans and boxes can include non-contiguous\n",
      "units, allowing great flexibility in Entities to handle\n",
      "layout nuances. A sentence split across columns/pages\n",
      "and interrupted by floating figures/footnotes would require multiple spans and bounding boxes to represent.\n",
      "Figure 3: Illustrates how Entities can be accessed flexibly in different ways: (A) Accessing the Entity of the first\n",
      "paragraph in the Document via its own Layer (B) Accessing a sentence via the paragraph Entity or directly via the\n",
      "sentences Layer (C) Similarly, the same tokens can be accessed via the overlapping sentence Entity or directly\n",
      "via the tokens Layer of the Document (where the first tokens are the title of the paper.) (D, E) Figures, captions,\n",
      "tables and keywords can be accessed in similar ways (F) Additionally, given a bounding box (e\n",
      "g., of a user selected\n",
      "region), papermage can find the corresponding Entities for a given Layer , in this case finding the tokens under\n",
      "the region. Excerpt from\n",
      "Table 1: Types of Predictors implemented in papermage .\n",
      "Table 2: Evaluating performance of CoreRecipe for\n",
      "logical structure recovery on S2-VL (Shen et al., 2022).\n",
      "Metrics are computed for token-level classification,\n",
      "macro-averaged over categories. The “Grobid Subset”\n",
      "limits evaluation to only categories for which Grobid\n",
      "returns bounding box information, which was necessary\n",
      "for evaluation on S2-VL. See Appendix A.3 for details.\n",
      "\"Figure 1. ...\"\n",
      "Vignette: Building an Attributed QA\n",
      "for\n",
      "Table 3: Evaluating CoreRecipe for logical structure recovery on S2-VL (Shen et al., 2022). These are per-category\n",
      "metrics for Table 2. Metrics are computed for token-level classification, macro-averaged over categories. The\n",
      "“Grobid Subset” limits evaluation to only categories for which Grobid returns bounding box information, which was\n",
      "necessary for evaluation on S2-VL.\n"
     ]
    }
   ],
   "source": [
    "for t in doc.captions:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PaperMage', ':', 'A', 'Unified', 'Toolkit', 'for', 'Processing', ',', 'Representing', ',', 'and']\n",
      "['Manipulating', 'Visually', '-', 'Rich', 'Scientific', 'Documents']\n",
      "['Kyle', 'Lo', 'α', '∗']\n",
      "['Zejiang', 'Shen', 'α', ',', 'τ', '∗']\n",
      "['Benjamin', 'Newman', 'α', '∗']\n",
      "['Joseph', 'Chee', 'Chang', 'α', '∗']\n",
      "['Russell', 'Authur', 'α', 'Erin', 'Bransom', 'α', 'Stefan', 'Candra', 'α', 'Yoganand', 'Chandrasekhar', 'α']\n",
      "['Regan', 'Huff', 'α', 'Bailey', 'Kuehl', 'α', 'Amanpreet', 'Singh', 'α', 'Chris', 'Wilhelm', 'α', 'Angele', 'Zamarron', 'α']\n",
      "['Marti', 'A', '.', 'Hearst', 'β']\n",
      "['Marti', 'A', '.', 'Hearst', 'β']\n",
      "['Daniel', 'S', '.', 'Weld', 'α', ',', 'ω']\n",
      "['Daniel', 'S', '.', 'Weld', 'α', ',', 'ω']\n",
      "['Doug', 'Downey', 'α', ',', 'η']\n",
      "['Luca', 'Soldaini', 'α', '∗']\n",
      "['α', 'Allen', 'Institute', 'for', 'AI']\n",
      "['τ', 'Massachusetts', 'Institute', 'of', 'Technology']\n",
      "['β', 'University', 'of', 'California', 'Berkeley']\n",
      "['ω', 'University', 'of', 'Washington']\n",
      "['η', 'Northwestern', 'University']\n",
      "['{', 'kylel', ',', 'lucas', '}', '@', 'allenai', '.', 'org']\n",
      "['{', 'kylel', ',', 'lucas', '}', '@', 'allenai', '.', 'org']\n",
      "['Abstract']\n",
      "['Despite', 'growing', 'interest', 'in', 'applying', 'natural']\n",
      "['language', 'processing', '(', 'NLP', ')', 'and', 'computer', 'vi', '-']\n",
      "['sion', '(', 'CV', ')', 'models', 'to', 'the', 'scholarly', 'domain', ',']\n",
      "['scientific', 'documents', 'remain', 'challenging', 'to']\n",
      "['work', 'with', '.', 'They’re', 'often', 'in', 'difficult', '-', 'to', '-', 'use']\n",
      "['work', 'with', '.', 'They’re', 'often', 'in', 'difficult', '-', 'to', '-', 'use']\n",
      "['PDF', 'formats', ',', 'and', 'the', 'ecosystem', 'of', 'models']\n",
      "['to', 'process', 'them', 'is', 'fragmented', 'and', 'incom', '-']\n",
      "['plete', '.']\n",
      "['We', 'introduce', 'papermage', ',', 'an', 'open', '-']\n",
      "['source', 'Python', 'toolkit', 'for', 'analyzing', 'and', 'pro', '-']\n",
      "['cessing', 'visually', '-', 'rich', ',', 'structured', 'scientific', 'doc', '-']\n",
      "['uments', '.', 'papermage', 'offers', 'clean', 'and', 'intuitive']\n",
      "['uments', '.', 'papermage', 'offers', 'clean', 'and', 'intuitive']\n",
      "['abstractions', 'for', 'seamlessly', 'representing', 'and']\n",
      "['manipulating', 'both', 'textual', 'and', 'visual', 'document']\n",
      "['elements', '.', 'papermage', 'achieves', 'this', 'by', 'integrat', '-']\n",
      "['elements', '.', 'papermage', 'achieves', 'this', 'by', 'integrat', '-']\n",
      "['ing', 'disparate', 'state', '-', 'of', '-', 'the', '-', 'art', 'NLP', 'and', 'CV', 'mod', '-']\n",
      "['els', 'into', 'a', 'unified', 'framework', ',', 'and', 'provides', 'turn', '-']\n",
      "['key', 'recipes', 'for', 'common', 'scientific', 'document']\n",
      "['processing', 'use', '-', 'cases', '.', 'papermage', 'has', 'powered']\n",
      "['processing', 'use', '-', 'cases', '.', 'papermage', 'has', 'powered']\n",
      "['multiple', 'research', 'prototypes', 'of', 'AI', 'applications']\n",
      "['over', 'scientific', 'documents', ',', 'along', 'with', 'Seman', '-']\n",
      "['tic', 'Scholar’s', 'large', '-', 'scale', 'production', 'system', 'for']\n",
      "['processing', 'millions', 'of', 'PDFs', '.']\n",
      "['§', 'github', '.', 'com', '/', 'allenai', '/', 'papermage', '1']\n",
      "['§', 'github', '.', 'com', '/', 'allenai', '/', 'papermage', '1']\n",
      "['1']\n",
      "['Introduction']\n",
      "['Research', 'papers', 'and', 'textbooks', 'are', 'central', 'to', 'the']\n",
      "['scientific', 'enterprise', ',', 'and', 'there', 'is', 'increasing', 'inter', '-']\n",
      "['est', 'in', 'developing', 'new', 'tools', 'for', 'extracting', 'knowl', '-']\n",
      "['edge', 'from', 'these', 'visually', '-', 'rich', 'documents', '.', 'Recent']\n",
      "['edge', 'from', 'these', 'visually', '-', 'rich', 'documents', '.', 'Recent']\n",
      "['research', 'has', 'explored', ',', 'for', 'example', ',', 'AI', '-', 'powered']\n",
      "['reading', 'support', 'for', 'math', 'symbol', 'definitions', '(', 'Head']\n",
      "['et', 'al', '.', ',', '2021', ')', ',', 'in', '-', 'situ', 'passage', 'explanations', 'or', 'sum', '-']\n",
      "['maries', '(', 'August', 'et', 'al', '.', ',', '2023', ';', 'Rachatasumrit', 'et', 'al', '.', ',']\n",
      "['2022', ';', 'Kim', 'et', 'al', '.', ',', '2023', ')', ',', 'automatic', 'span', 'highlight', '-']\n",
      "['ing', '(', 'Chang', 'et', 'al', '.', ',', '2023', ';', 'Fok', 'et', 'al', '.', ',', '2023b', ')', ',', 'interac', '-']\n",
      "['tive', 'clipping', 'and', 'synthesis', '(', 'Kang', 'et', 'al', '.', ',', '2022', ',', '2023', ')']\n",
      "['∗', 'Core', 'contributors', ';', 'see', 'author', 'contributions', 'for', 'details', '.', '1']\n",
      "['∗', 'Core', 'contributors', ';', 'see', 'author', 'contributions', 'for', 'details', '.', '1']\n",
      "['We', 'use', 'code', 'snippets', 'to', 'illustrate', 'our', 'toolkit’s', 'core', 'de', '-', 'signs', 'and', 'abstractions', '.', 'Exact', 'syntax', 'in', 'paper', 'may', 'differ', 'from']\n",
      "['We', 'use', 'code', 'snippets', 'to', 'illustrate', 'our', 'toolkit’s', 'core', 'de', '-', 'signs', 'and', 'abstractions', '.', 'Exact', 'syntax', 'in', 'paper', 'may', 'differ', 'from']\n",
      "['the', 'actual', 'code', ',', 'as', 'software', 'will', 'evolve', 'beyond', 'the', 'paper', 'and', 'we', 'opt', 'to', 'simplify', 'syntax', 'when', 'needed', 'for', 'legibility', 'and', 'clarity', '.']\n",
      "['We', 'refer', 'readers', 'to', 'our', 'public', 'code', 'for', 'latest', 'documentation', '.']\n",
      "['Figure', '1', ':', 'papermage', '’s', 'document', 'creation', 'and', 'represen', '-']\n",
      "['tation', '.', '(', 'A', ')', 'Recipes', 'are', 'turn', '-', 'key', 'methods', 'for', 'processing']\n",
      "['tation', '.', '(', 'A', ')', 'Recipes', 'are', 'turn', '-', 'key', 'methods', 'for', 'processing']\n",
      "['tation', '.', '(', 'A', ')', 'Recipes', 'are', 'turn', '-', 'key', 'methods', 'for', 'processing']\n",
      "['a', 'PDF', '.', '(', 'B', ')', 'They', 'compose', 'models', 'operating', 'across', 'dif', '-']\n",
      "['a', 'PDF', '.', '(', 'B', ')', 'They', 'compose', 'models', 'operating', 'across', 'dif', '-']\n",
      "['a', 'PDF', '.', '(', 'B', ')', 'They', 'compose', 'models', 'operating', 'across', 'dif', '-']\n",
      "['ferent', 'data', 'modalities', 'and', 'machine', 'learning', 'frameworks']\n",
      "['to', 'extract', 'document', 'structure', ',', 'which', 'we', 'conceptualize']\n",
      "['as', 'layers', 'of', 'annotation', 'that', 'store', 'textual', 'and', 'visual', 'in', '-']\n",
      "['formation', '.', '(', 'C', ')', 'Users', 'can', 'access', 'and', 'manipulate', 'layers', '.']\n",
      "['formation', '.', '(', 'C', ')', 'Users', 'can', 'access', 'and', 'manipulate', 'layers', '.']\n",
      "['formation', '.', '(', 'C', ')', 'Users', 'can', 'access', 'and', 'manipulate', 'layers', '.']\n",
      "['and', 'more', '.']\n",
      "['Further', ',', 'extracting', 'clean', ',', 'properly', '-']\n",
      "['structured', 'scientific', 'text', 'from', 'PDF', 'documents', '(', 'Lo']\n",
      "['et', 'al', '.', ',', '2020', ';', 'Wang', 'et', 'al', '.', ',', '2020', ')', 'forms', 'a', 'critical']\n",
      "['first', 'step', 'in', 'pretraining', 'language', 'models', 'of', 'sci', '-']\n",
      "['ence', '(', 'Beltagy', 'et', 'al', '.', ',', '2019', ';', 'Lee', 'et', 'al', '.', ',', '2019', ';', 'Gu', 'et', 'al', '.', ',']\n",
      "['2020', ';', 'Luo', 'et', 'al', '.', ',', '2022', ';', 'Taylor', 'et', 'al', '.', ',', '2022', ';', 'Tre', '-']\n",
      "['wartha', 'et', 'al', '.', ',', '2022', ';', 'Hong', 'et', 'al', '.', ',', '2023', ')', ',', 'automatic']\n",
      "['generation', 'of', 'more', 'accessible', 'paper', 'formats', '(', 'Wang']\n",
      "['et', 'al', '.', ',', '2021', ')', ',', 'and', 'developing', 'datasets', 'for', 'scientific']\n",
      "['natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'over', 'struc', '-']\n",
      "['tured', 'full', 'text', '(', 'Jain', 'et', 'al', '.', ',', '2020', ';', 'Subramanian', 'et', 'al', '.', ',']\n",
      "['2020', ';', 'Dasigi', 'et', 'al', '.', ',', '2021', ';', 'Lee', 'et', 'al', '.', ',', '2023', ')', '.']\n",
      "['However', ',', 'this', 'type', 'of', 'NLP', 'research', 'on', 'scientific']\n",
      "['corpora', 'is', 'difficult', 'because', 'the', 'documents', 'come']\n",
      "['in', 'difficult', '-', 'to', '-', 'use', 'formats', 'like', 'PDF', ',', '2', 'and', 'existing']\n",
      "['tools', 'for', 'working', 'with', 'the', 'documents', 'are', 'limited', '.']\n",
      "['Typically', ',', 'the', 'first', 'step', 'in', 'scientific', 'document', 'pro', '-']\n",
      "['cessing', 'is', 'to', 'invoke', 'a', 'parser', 'on', 'a', 'document', 'file', 'to']\n",
      "['convert', 'it', 'into', 'a', 'sequence', 'of', 'tokens', 'and', 'bounding']\n",
      "['boxes', 'in', 'inferred', 'reading', 'order', '.', 'Parsers', 'extract', 'only']\n",
      "['boxes', 'in', 'inferred', 'reading', 'order', '.', 'Parsers', 'extract', 'only']\n",
      "['the', 'raw', 'document', 'content', ',', 'and', 'obtaining', 'richer']\n",
      "['document', 'structure', '(', 'e', '.', 'g', '.', ',', 'titles', ',', 'authors', ',', 'figures', ')', 'or']\n",
      "['linguistic', 'structure', 'and', 'semantics', '(', 'e', '.', 'g', '.', ',', 'sentences', ',']\n",
      "['discourse', 'units', ',', 'scientific', 'claims', ')', 'requires', 'sending']\n",
      "['the', 'token', 'sequence', 'through', 'downstream', 'models', '.']\n",
      "['Unlike', 'more', 'mature', 'parsers', '(', '§', '2', '.', '1', ')', ',', 'these', 'down', '-']\n",
      "['stream', 'models', 'are', 'often', 'research', 'prototypes', '(', '§', '2', '.', '2', ')']\n",
      "['that', 'are', 'limited', 'to', 'extracting', 'only', 'a', 'subset', 'of', 'the']\n",
      "['structures', 'needed', 'for', 'one’s', 'research', '(', 'e', '.', 'g', '.', ',', 'the', 'same']\n",
      "['model', 'may', 'not', 'provide', 'both', 'sentence', 'splits', 'and', 'fig', '-']\n",
      "['ure', 'detection', ')', '.', 'As', 'a', 'result', ',', 'users', 'must', 'write', 'exten', '-']\n",
      "['ure', 'detection', ')', '.', 'As', 'a', 'result', ',', 'users', 'must', 'write', 'exten', '-']\n",
      "['sive', 'custom', 'code', 'that', 'strings', 'pipelines', 'of', 'multiple']\n",
      "['models', 'together', '.', 'Research', 'projects', 'using', 'models']\n",
      "['models', 'together', '.', 'Research', 'projects', 'using', 'models']\n",
      "['of', 'different', 'modalities', '(', 'e', '.', 'g', '.', ',', 'combining', 'an', 'image', '-']\n",
      "['based', 'formula', 'detector', 'with', 'a', 'text', '-', 'based', 'definition']\n",
      "['extractor', ')', 'can', 'require', 'hundreds', 'of', 'lines', 'of', 'code', '.']\n",
      "['We', 'introduce', 'papermage', ',', 'an', 'open', '-', 'source']\n",
      "['Python', 'toolkit', 'for', 'processing', 'scientific', 'documents', '.']\n",
      "['Its', 'contributions', 'include', '(', '1', ')', 'magelib', ',', 'a', 'library', 'of']\n",
      "['primitives', 'and', 'methods', 'for', 'representing', 'and', 'ma', '-']\n",
      "['nipulating', 'visually', '-', 'rich', 'documents', 'as', 'multimodal']\n",
      "['constructs', ',', '(', '2', ')', 'Predictors', ',', 'a', 'set', 'of', 'implementa', '-']\n",
      "['tions', 'that', 'integrate', 'different', 'state', '-', 'of', '-', 'the', '-', 'art', 'scien', '-']\n",
      "['tific', 'document', 'analysis', 'models', 'into', 'a', 'unified', 'inter', '-']\n",
      "['face', ',', 'even', 'if', 'individual', 'models', 'are', 'written', 'in', 'differ', '-']\n",
      "['ent', 'frameworks', 'or', 'operate', 'on', 'different', 'modalities', ',']\n",
      "['and', '(', '3', ')', 'Recipes', ',', 'which', 'provide', 'turn', '-', 'key', 'access']\n",
      "['to', 'well', '-', 'tested', 'combinations', 'of', 'individual', '(', 'often']\n",
      "['single', '-', 'modality', ')', 'modules', 'to', 'form', 'sophisticated', ',', 'ex', '-']\n",
      "['tensible', 'multimodal', 'pipelines', '.']\n",
      "['2']\n",
      "['Related', 'Work']\n",
      "['2', '.', '1']\n",
      "['2', '.', '1']\n",
      "['Turn', '-', 'key', 'software', 'for', 'scientific', 'documents']\n",
      "['Processing', 'visually', '-', 'rich', 'documents', 'like', 'scientific']\n",
      "['documents', 'requires', 'a', 'joint', 'understanding', 'of', 'both']\n",
      "['visual', 'and', 'textual', 'information', '.', 'In', 'practice', ',', 'this']\n",
      "['visual', 'and', 'textual', 'information', '.', 'In', 'practice', ',', 'this']\n",
      "['often', 'requires', 'combining', 'different', 'models', 'into']\n",
      "['complex', 'processing', 'pipelines', '.', 'For', 'example', ',', 'GRO', '-']\n",
      "['complex', 'processing', 'pipelines', '.', 'For', 'example', ',', 'GRO', '-']\n",
      "['BID', '(', 'Grobid', ',', '2008', '–', '2023', ')', ',', 'a', 'widely', '-', 'adopted', 'soft', '-']\n",
      "['ware', 'tool', 'for', 'scientific', 'document', 'processing', ',', 'uses']\n",
      "['2', 'PDFs', 'store', 'text', 'as', 'character', 'glyphs', 'and', 'their', '(', 'x', ',', 'y', ')', 'posi', '-']\n",
      "['tions', 'on', 'a', 'page', '.', 'Converting', 'this', 'data', 'to', 'usable', 'text', 'for', 'NLP', 'requires', 'error', '-', 'prone', 'operations', 'like', 'inferring', 'token', 'boundaries', ',']\n",
      "['tions', 'on', 'a', 'page', '.', 'Converting', 'this', 'data', 'to', 'usable', 'text', 'for', 'NLP', 'requires', 'error', '-', 'prone', 'operations', 'like', 'inferring', 'token', 'boundaries', ',']\n",
      "['whitespacing', ',', 'and', 'reading', 'order', 'using', 'visual', 'positioning', '.']\n",
      "['twelve', 'interdependent', 'sequence', 'labeling', 'models', '3']\n",
      "['to', 'perform', 'its', 'full', 'text', 'extraction', '.', 'Other', 'similar']\n",
      "['to', 'perform', 'its', 'full', 'text', 'extraction', '.', 'Other', 'similar']\n",
      "['tools', 'inlude', 'CERMINE', '(', 'Tkaczyk', 'et', 'al', '.', ',', '2015', ')', 'and']\n",
      "['ParsCit', '(', 'Councill', 'et', 'al', '.', ',', '2008', ')', '.', 'While', 'such', 'software']\n",
      "['ParsCit', '(', 'Councill', 'et', 'al', '.', ',', '2008', ')', '.', 'While', 'such', 'software']\n",
      "['is', 'often', 'an', 'ideal', 'choice', 'for', 'off', '-', 'the', '-', 'shelf', 'processing', ',']\n",
      "['they', 'are', 'not', 'necessarily', 'designed', 'for', 'easy', 'extension']\n",
      "['and', '/', 'or', 'integration', 'with', 'newer', 'research', 'models', '.', '4']\n",
      "['and', '/', 'or', 'integration', 'with', 'newer', 'research', 'models', '.', '4']\n",
      "['2', '.', '2']\n",
      "['2', '.', '2']\n",
      "['Models', 'for', 'scientific', 'document', 'processing']\n",
      "['While', 'aforementioned', 'software', 'tools', 'use', 'CRF', 'or']\n",
      "['BiLSTM', '-', 'based', 'models', ',', 'Transformer', '-', 'based', 'models']\n",
      "['have', 'seen', 'wide', 'adoption', 'among', 'NLP', 'researchers']\n",
      "['for', 'their', 'powerful', 'processing', 'capabilities', '.', 'Recent']\n",
      "['for', 'their', 'powerful', 'processing', 'capabilities', '.', 'Recent']\n",
      "['years', 'have', 'seen', 'the', 'rise', 'of', 'layout', '-', 'infused', 'Trans', '-']\n",
      "['formers', '(', 'Xu', 'et', 'al', '.', ',', '2019', ';', 'Shen', 'et', 'al', '.', ',', '2022', ';', 'Xu']\n",
      "['et', 'al', '.', ',', '2021', ';', 'Huang', 'et', 'al', '.', ',', '2022b', ';', 'Chen', 'et', 'al', '.', ',', '2023', ')']\n",
      "['for', 'processing', 'visually', '-', 'rich', 'documents', ',', 'including']\n",
      "['recovering', 'logical', 'structure', '(', 'e', '.', 'g', '.', ',', 'titles', ',', 'abstracts', ')']\n",
      "['of', 'scientific', 'papers', '(', 'Huang', 'et', 'al', '.', ',', '2022a', ')', '.', 'Similarly', ',']\n",
      "['of', 'scientific', 'papers', '(', 'Huang', 'et', 'al', '.', ',', '2022a', ')', '.', 'Similarly', ',']\n",
      "['computer', 'vision', '(', 'CV', ')', 'researchers', 'have', 'also', 'shown']\n",
      "['impressive', 'capabilities', 'of', 'CNN', '-', 'based', 'object', 'de', '-']\n",
      "['tection', 'models', '(', 'Ren', 'et', 'al', '.', ',', '2015', ';', 'Tan', 'et', 'al', '.', ',', '2020', ')']\n",
      "['for', 'segmenting', 'visually', '-', 'rich', 'documents', 'based', 'on']\n",
      "['their', 'layout', '.', 'While', 'these', 'research', 'models', 'are', 'pow', '-']\n",
      "['their', 'layout', '.', 'While', 'these', 'research', 'models', 'are', 'pow', '-']\n",
      "['erful', 'and', 'extensible', 'for', 'research', 'purposes', ',', 'it', 'often']\n",
      "['requires', 'significant', '“glue”', 'code', 'and', 'stitching', 'soft', '-']\n",
      "['ware', 'tools', 'to', 'create', 'robust', 'processing', 'pipelines', '.']\n",
      "['For', 'example', ',', 'Lincker', 'et', 'al', '.', '(', '2023', ')', 'bootstraps', 'a', 'so', '-']\n",
      "['For', 'example', ',', 'Lincker', 'et', 'al', '.', '(', '2023', ')', 'bootstraps', 'a', 'so', '-']\n",
      "['phisticated', 'processing', 'pipeline', 'around', 'a', 'research']\n",
      "['model', 'for', 'processing', 'children’s', 'textbooks', '.']\n",
      "['2', '.', '3']\n",
      "['2', '.', '3']\n",
      "['Combining', 'models', 'and', 'pipelines']\n",
      "['papermage', '’s', 'use', 'case', 'lies', 'between', 'that', 'of', 'turn', '-']\n",
      "['key', 'software', 'and', 'a', 'framework', 'for', 'supporting', 're', '-']\n",
      "['search', '.', 'Similar', 'to', 'Transformers', '(', 'Wolfe', 'et', 'al', '.', ',']\n",
      "['search', '.', 'Similar', 'to', 'Transformers', '(', 'Wolfe', 'et', 'al', '.', ',']\n",
      "['2022', ')', '’s', 'integration', 'of', 'different', 'research', 'mod', '-']\n",
      "['els', 'into', 'standard', 'interfaces', ',', 'others', 'have', 'done']\n",
      "['similarly', 'for', 'the', 'visually', '-', 'rich', 'document', 'domain', '.']\n",
      "['LayoutParser', '(', 'Shen', 'et', 'al', '.', ',', '2021', ')', 'provides', 'mod', '-']\n",
      "['els', 'for', 'visually', '-', 'rich', 'documents', 'and', 'supports']\n",
      "['the', 'creation', 'of', 'document', 'processing', 'pipelines', '.']\n",
      "['papermage', ',', 'in', 'fact', ',', 'depends', 'on', 'LayoutParser']\n",
      "['for', 'access', 'to', 'vision', 'models', ',', 'but', 'is', 'designed', 'to']\n",
      "['also', 'integrate', 'text', 'models', 'which', 'are', 'omitted', 'from']\n",
      "['3', 'https', ':', '/', '/', 'grobid', '.', 'readthedocs', '.', 'io', '/', 'en', '/', 'latest', '/']\n",
      "['3', 'https', ':', '/', '/', 'grobid', '.', 'readthedocs', '.', 'io', '/', 'en', '/', 'latest', '/']\n",
      "['3', 'https', ':', '/', '/', 'grobid', '.', 'readthedocs', '.', 'io', '/', 'en', '/', 'latest', '/']\n",
      "['Training', '-', 'the', '-', 'models', '-', 'of', '-', 'Grobid', '/', '#', 'models', '4']\n",
      "['Most', 'research', 'in', 'NLP', 'requires', 'that', 'a', 'researcher', 'be', 'able', 'to', 'manipulate', 'models', 'within', 'Python', '.', 'Yet', ',', 'Grobid', 'requires', 'users']\n",
      "['Most', 'research', 'in', 'NLP', 'requires', 'that', 'a', 'researcher', 'be', 'able', 'to', 'manipulate', 'models', 'within', 'Python', '.', 'Yet', ',', 'Grobid', 'requires', 'users']\n",
      "['to', 'manage', 'a', 'separate', 'service', 'process', 'and', 'send', 'PDFs', 'through', 'a', 'client', '.', 'In', 'performing', 'evaluation', 'in', '§', '3', '.', '3', ',', 'we', 'also', 'found', 'it']\n",
      "['to', 'manage', 'a', 'separate', 'service', 'process', 'and', 'send', 'PDFs', 'through', 'a', 'client', '.', 'In', 'performing', 'evaluation', 'in', '§', '3', '.', '3', ',', 'we', 'also', 'found', 'it']\n",
      "['to', 'manage', 'a', 'separate', 'service', 'process', 'and', 'send', 'PDFs', 'through', 'a', 'client', '.', 'In', 'performing', 'evaluation', 'in', '§', '3', '.', '3', ',', 'we', 'also', 'found', 'it']\n",
      "['difficult', 'to', 'run', 'only', 'the', 'model', 'components', 'isolated', 'from', 'PDF', 'utilities', ',', 'which', 'makes', 'comparison', 'with', 'other', 'research', 'models']\n",
      "['challenging', 'without', 'significant', '“glue”', 'code', '.']\n",
      "['Figure', '2', ':', 'Entities', 'are', 'multimodal', 'content', 'units', '.', 'Here', ',']\n",
      "['Figure', '2', ':', 'Entities', 'are', 'multimodal', 'content', 'units', '.', 'Here', ',']\n",
      "['spans', 'of', 'a', 'sentence', 'are', 'used', 'to', 'identify', 'its', 'text', 'among']\n",
      "['all', 'symbols', ',', 'while', 'boxes', 'map', 'its', 'visual', 'coordinates', 'on']\n",
      "['a', 'page', '.', 'spans', 'and', 'boxes', 'can', 'include', 'non', '-', 'contiguous']\n",
      "['a', 'page', '.', 'spans', 'and', 'boxes', 'can', 'include', 'non', '-', 'contiguous']\n",
      "['units', ',', 'allowing', 'great', 'flexibility', 'in', 'Entities', 'to', 'handle']\n",
      "['layout', 'nuances', '.', 'A', 'sentence', 'split', 'across', 'columns', '/', 'pages']\n",
      "['layout', 'nuances', '.', 'A', 'sentence', 'split', 'across', 'columns', '/', 'pages']\n",
      "['and', 'interrupted', 'by', 'floating', 'figures', '/', 'footnotes', 'would', 're', '-']\n",
      "['quire', 'multiple', 'spans', 'and', 'bounding', 'boxes', 'to', 'represent', '.']\n",
      "['LayoutParser', '.']\n",
      "['To', 'allow', 'models', 'of', 'different']\n",
      "['modalities', 'to', 'work', 'well', 'together', ',', 'we', 'also', 'devel', '-']\n",
      "['oped', 'the', 'magelib', 'library', '(', '§', '3', '.', '1', ')', '.']\n",
      "['3']\n",
      "['Design', 'of', 'papermage']\n",
      "['papermage', 'is', 'three', 'parts', ':', '(', '1', ')', 'magelib', ',', 'a', 'library', 'for']\n",
      "['intuitively', 'representing', 'and', 'manipulating', 'visually', '-']\n",
      "['rich', 'documents', ',', '(', '2', ')', 'Predictors', ',', 'implementations']\n",
      "['of', 'models', 'for', 'analyzing', 'scientific', 'papers', 'that', 'unify']\n",
      "['disparate', 'machine', 'learning', 'frameworks', 'under', 'a']\n",
      "['common', 'interface', ',', 'and', '(', '3', ')', 'Recipes', ',', 'combinations']\n",
      "['of', 'Predictors', 'that', 'form', 'multimodal', 'pipelines', '.']\n",
      "['3', '.', '1']\n",
      "['3', '.', '1']\n",
      "['Representing', 'and', 'manipulating']\n",
      "['visually', '-', 'rich', 'documents', 'with', 'magelib']\n",
      "['In', 'this', 'section', ',', 'we', 'use', 'code', 'snippets', 'to', 'show', 'how']\n",
      "['our', 'library’s', 'abstractions', 'and', 'syntax', 'are', 'tailored']\n",
      "['for', 'the', 'visually', '-', 'rich', 'document', 'problem', 'domain', '.']\n",
      "['Data', 'Classes', '.']\n",
      "['magelib', 'provides', 'three', 'base', 'data']\n",
      "['classes', 'for', 'representing', 'fundamental', 'elements', 'of']\n",
      "['visually', '-', 'rich', ',', 'structured', 'documents', ':', 'Document', ',']\n",
      "['Layers', 'and', 'Entities', '.', 'First', ',', 'a', 'Document', 'might']\n",
      "['Layers', 'and', 'Entities', '.', 'First', ',', 'a', 'Document', 'might']\n",
      "['minimally', 'store', 'text', 'as', 'a', 'string', 'of', 'symbols', ':']\n",
      "['1', '>', '>', '>', 'from', 'papermage', 'import', 'Document']\n",
      "['2', '>', '>', '>', 'doc', '.', 'symbols']\n",
      "['2', '>', '>', '>', 'doc', '.', 'symbols']\n",
      "['3', '\"', 'Revolt', ':', 'Collaborative', 'Crowdsourcing', '.', '.', '.', '\"']\n",
      "['But', 'visually', '-', 'rich', 'documents', 'are', 'more', 'than', 'a', 'lin', '-']\n",
      "['earized', 'string', '.', 'For', 'example', ',', 'analyzing', 'a', 'scientific']\n",
      "['earized', 'string', '.', 'For', 'example', ',', 'analyzing', 'a', 'scientific']\n",
      "['paper', 'requires', 'access', 'to', 'its', 'visuospatial', 'layout', '(', 'e', '.', 'g', '.', ',']\n",
      "['pages', ',', 'blocks', ',', 'lines', ')', ',', 'logical', 'structure', '(', 'e', '.', 'g', '.', ',', 'title', ',']\n",
      "['abstract', ',', 'figures', ',', 'tables', ',', 'footnotes', ',', 'sections', ')', ',', 'se', '-']\n",
      "['mantic', 'units', '(', 'e', '.', 'g', '.', ',', 'paragraphs', ',', 'sentences', ',', 'tokens', ')', ',']\n",
      "['and', 'more', '(', 'e', '.', 'g', '.', ',', 'citations', ',', 'terms', ')', '.', 'In', 'practice', ',', 'this']\n",
      "['and', 'more', '(', 'e', '.', 'g', '.', ',', 'citations', ',', 'terms', ')', '.', 'In', 'practice', ',', 'this']\n",
      "['means', 'different', 'parts', 'of', 'doc', '.', 'symbols', 'can', 'corre', '-']\n",
      "['means', 'different', 'parts', 'of', 'doc', '.', 'symbols', 'can', 'corre', '-']\n",
      "['spond', 'to', 'different', 'paragraphs', ',', 'sentences', ',', 'tokens', ',']\n",
      "['etc', '.', 'in', 'the', 'Document', ',', 'each', 'with', 'its', 'own', 'set', 'of']\n",
      "['etc', '.', 'in', 'the', 'Document', ',', 'each', 'with', 'its', 'own', 'set', 'of']\n",
      "['corresponding', 'coordinates', 'representing', 'its', 'visual']\n",
      "['position', 'on', 'a', 'page', '.']\n",
      "['magelib', 'represents', 'structure', 'using', 'Layers', 'that']\n",
      "['can', 'be', 'accessed', 'as', 'attributes', 'of', 'a', 'Document', '(', 'e', '.', 'g', '.', ',']\n",
      "['doc', '.', 'sentences', ',']\n",
      "['doc', '.', 'figures', ',']\n",
      "['doc', '.', 'tokens', ')']\n",
      "['(', 'Figure', '1', ')', '.', 'Each', 'Layer', 'is', 'a', 'sequence', 'of', 'content']\n",
      "['(', 'Figure', '1', ')', '.', 'Each', 'Layer', 'is', 'a', 'sequence', 'of', 'content']\n",
      "['units', ',', 'called', 'Entities', ',', 'which', 'store', 'both', 'textual']\n",
      "['(', 'e', '.', 'g', '.', ',', 'spans', ',', 'strings', ')', 'and', 'visuospatial', '(', 'e', '.', 'g', '.', ',']\n",
      "['bounding', 'boxes', ',', 'pixel', 'arrays', ')', 'information', ':']\n",
      "['1', '>', '>', '>', 'sentences', '=', 'Layer', '(', 'entities', '=', '[']\n",
      "['2']\n",
      "['Entity', '(', '.', '.', '.', ')', ',', 'Entity', '(', '.', '.', '.', ')', ',', '.', '.', '.']\n",
      "['3']\n",
      "[']', ')']\n",
      "['See', 'Figure', '2', 'for', 'an', 'example', 'on', 'how', '“sentences”', 'in']\n",
      "['a', 'scientific', 'document', 'are', 'represented', 'as', 'Entities', '.']\n",
      "['Section', '§', '3', '.', '2', 'explains', 'in', 'more', 'detail', 'how', 'a', 'user', 'can']\n",
      "['Section', '§', '3', '.', '2', 'explains', 'in', 'more', 'detail', 'how', 'a', 'user', 'can']\n",
      "['generate', 'Entities', '.']\n",
      "['Methods', '.']\n",
      "['magelib', 'also', 'provides', 'a', 'set', 'of', 'func', '-']\n",
      "['tions', 'for', 'building', 'and', 'interacting', 'with', 'data', ':', 'aug', '-']\n",
      "['menting', 'a', 'Document', 'with', 'additional', 'Layers', ',']\n",
      "['traversing', 'and', 'spatially', 'searching', 'for', 'matching']\n",
      "['Entities', 'in', 'one', 'Layer', ',', 'and', 'cross', '-', 'referencing', 'be', '-']\n",
      "['tween', 'Layers', '(', 'see', 'Figure', '3', ')', '.']\n",
      "['A', 'Document', 'that', 'only', 'contains', 'doc', '.', 'symbols']\n",
      "['A', 'Document', 'that', 'only', 'contains', 'doc', '.', 'symbols']\n",
      "['can', 'be', 'augmented', 'with', 'additional', 'Layers', ':']\n",
      "['1', '>', '>', '>', 'paragraphs', '=', 'Layer', '(', '.', '.', '.', ')']\n",
      "['2', '>', '>', '>', 'sentences', '=', 'Layer', '(', '.', '.', '.', ')']\n",
      "['3', '>', '>', '>', 'tokens', '=', 'Layer', '(', '.', '.', '.', ')']\n",
      "['4', '5', '>', '>', '>', 'doc', '.', 'add', '(', 'paragraphs', ',', 'sentences', ',', 'tokens', ')']\n",
      "['4', '5', '>', '>', '>', 'doc', '.', 'add', '(', 'paragraphs', ',', 'sentences', ',', 'tokens', ')']\n",
      "['Adding', 'Layers', 'automatically', 'grants', 'users', 'the']\n",
      "['ability', 'to', 'iterate', 'through', 'Entities', 'and', 'cross', '-']\n",
      "['reference', 'intersecting', 'Entities', 'across', 'Layers', ':']\n",
      "['1', '>', '>', '>', 'for', 'paragraph', 'in', 'doc', '.', 'paragraphs', ':']\n",
      "['1', '>', '>', '>', 'for', 'paragraph', 'in', 'doc', '.', 'paragraphs', ':']\n",
      "['2']\n",
      "['for', 'sent', 'in', 'paragraph', '.', 'sentences', ':']\n",
      "['for', 'sent', 'in', 'paragraph', '.', 'sentences', ':']\n",
      "['3']\n",
      "['for', 'token', 'in', 'sentence', '.', 'tokens', ':']\n",
      "['for', 'token', 'in', 'sentence', '.', 'tokens', ':']\n",
      "['4']\n",
      "['.', '.', '.']\n",
      "['magelib', 'also', 'supports', 'cross', '-', 'modality', 'opera', '-']\n",
      "['tions', '.', 'For', 'example', ',', 'searching', 'for', 'textual', 'Entities']\n",
      "['tions', '.', 'For', 'example', ',', 'searching', 'for', 'textual', 'Entities']\n",
      "['within', 'a', 'visual', 'region', 'on', 'the', 'PDF', '(', 'See', 'Figure', '3', 'F', ')', ':']\n",
      "['1', '>', '>', '>', 'query', '=', 'Box', '(', 'l', '=', '423', ',', 't', '=', '71', ',', 'w', '=', '159', ',', 'h', '=', '87', ')']\n",
      "['2', '>', '>', '>', 'selection', '=', 'doc', '.', 'find', '(', 'query', ',', '\"', 'tokens', '\"', ')']\n",
      "['2', '>', '>', '>', 'selection', '=', 'doc', '.', 'find', '(', 'query', ',', '\"', 'tokens', '\"', ')']\n",
      "['3', '>', '>', '>', '[', 't', '.', 'text', 'for', 't', 'in', 'selection', ']']\n",
      "['4', '[', '\"', 'Techniques', '\"', ',', '\"', 'for', '\"', ',', '\"', 'collecting', '\"', ',', '.', '.', '.', ']']\n",
      "['>', '>', '>', 'doc', '.', 'paragraphs', '[', '0', ']']\n",
      "['>', '>', '>', 'doc', '.', 'paragraphs', '[', '0', ']']\n",
      "['>', '>', '>', 'doc', '.', 'paragraphs', '[', '0', ']', '.', 'sentences', '[', '2', ']', 'or']\n",
      "['>', '>', '>', 'doc', '.', 'paragraphs', '[', '0', ']', '.', 'sentences', '[', '2', ']', 'or']\n",
      "['>', '>', '>', 'doc', '.', 'paragraphs', '[', '0', ']', '.', 'sentences', '[', '2', ']', 'or']\n",
      "['>', '>', '>', 'doc', '.', 'sentences', '[', '2', ']']\n",
      "['>', '>', '>', 'doc', '.', 'sentences', '[', '2', ']']\n",
      "['>', '>', '>', 'doc', '.', 'sentences', '[', '2', ']', '.', 'tokens', '[', '9', ':', '13', ']', 'or']\n",
      "['>', '>', '>', 'doc', '.', 'sentences', '[', '2', ']', '.', 'tokens', '[', '9', ':', '13', ']', 'or']\n",
      "['>', '>', '>', 'doc', '.', 'sentences', '[', '2', ']', '.', 'tokens', '[', '9', ':', '13', ']', 'or']\n",
      "['>', '>', '>', 'doc', '.', 'tokens', '[', '169', ':', '173', ']']\n",
      "['>', '>', '>', 'doc', '.', 'tokens', '[', '169', ':', '173', ']']\n",
      "['>', '>', '>', 'doc', '.', 'figures', '[', '0', ']']\n",
      "['>', '>', '>', 'doc', '.', 'figures', '[', '0', ']']\n",
      "['>', '>', '>', 'doc', '.', 'captions', '[', '0', ']']\n",
      "['>', '>', '>', 'doc', '.', 'captions', '[', '0', ']']\n",
      "['>', '>', '>', 'user', '_', 'query', '=', 'Box', '(', 'l', ',', 't', ',', 'w', ',', 'h', ',', 'page', '=', '0', ')']\n",
      "['>', '>', '>', 'selected', '_', 'tokens', '=', 'doc', '.', 'find', '(', 'user', '_', 'query', ',', 'layer', '=', '“tokens”', ')']\n",
      "['>', '>', '>', 'selected', '_', 'tokens', '=', 'doc', '.', 'find', '(', 'user', '_', 'query', ',', 'layer', '=', '“tokens”', ')']\n",
      "['>', '>', '>', '[', 'token', '.', 'text', 'for', 'token', 'in', 'selected', '_', 'tokens', ']']\n",
      "['[', '“Techniques”', ',', '“for”', ',', '“collecting”', ',', '“labeled”', ',', '“data”', ',', '“perts”', ',', '“for”', ',', '“manual”', ',', '“annotation”', ',', '.', '.', '.', ']']\n",
      "['Crowdsourcing', 'provides', 'a', 'scalable', 'and', 'efficient', 'way', 'to', 'con', '-', 'struct', 'labeled', 'datasets', 'for', 'training', 'machine', 'learning', 'systems', '.', 'However', ',', 'creating', 'comprehensive', 'label', 'guidelines', 'for', 'crowd', '-']\n",
      "['Crowdsourcing', 'provides', 'a', 'scalable', 'and', 'efficient', 'way', 'to', 'con', '-', 'struct', 'labeled', 'datasets', 'for', 'training', 'machine', 'learning', 'systems', '.', 'However', ',', 'creating', 'comprehensive', 'label', 'guidelines', 'for', 'crowd', '-']\n",
      "['workers', 'is', 'often', 'prohibitive', 'even', 'for', 'seemingly', 'simple', 'con', '-', 'cepts', '.', 'Incomplete', 'or', 'ambiguous', 'label', 'guidelines', 'can', 'then', 'result', 'in', 'differing', 'interpretations', 'of', 'concepts', 'and', 'inconsistent']\n",
      "['workers', 'is', 'often', 'prohibitive', 'even', 'for', 'seemingly', 'simple', 'con', '-', 'cepts', '.', 'Incomplete', 'or', 'ambiguous', 'label', 'guidelines', 'can', 'then', 'result', 'in', 'differing', 'interpretations', 'of', 'concepts', 'and', 'inconsistent']\n",
      "['labels', '.', 'Existing', 'approaches', 'for', 'improving', 'label', 'quality', ',', 'such', 'as', 'worker', 'screening', 'or', 'detection', 'of', 'poor', 'work', ',', 'are', 'ineffective', 'for', 'this', 'problem', 'and', 'can', 'lead', 'to', 'rejection', 'of', 'honest', 'work', 'and', 'a']\n",
      "['labels', '.', 'Existing', 'approaches', 'for', 'improving', 'label', 'quality', ',', 'such', 'as', 'worker', 'screening', 'or', 'detection', 'of', 'poor', 'work', ',', 'are', 'ineffective', 'for', 'this', 'problem', 'and', 'can', 'lead', 'to', 'rejection', 'of', 'honest', 'work', 'and', 'a']\n",
      "['missed', 'opportunity', 'to', 'capture', 'rich', 'interpretations', 'about', 'data', '.', 'We', 'introduce', 'Revolt', ',', 'a', 'collaborative', 'approach', 'that', 'brings', 'ideas', 'from', 'expert', 'annotation', 'workflows', 'to', 'crowd', '-', 'based', 'labeling', '.']\n",
      "['missed', 'opportunity', 'to', 'capture', 'rich', 'interpretations', 'about', 'data', '.', 'We', 'introduce', 'Revolt', ',', 'a', 'collaborative', 'approach', 'that', 'brings', 'ideas', 'from', 'expert', 'annotation', 'workflows', 'to', 'crowd', '-', 'based', 'labeling', '.']\n",
      "['Revolt', 'eliminates', 'the', 'burden', 'of', 'creating', 'detailed', 'label', 'guide', '-', 'lines', 'by', 'harnessing', 'crowd', 'disagreements', 'to', 'identify', 'ambigu', '-', 'ous', 'concepts', 'and', 'create', 'rich', 'structures', '(', 'groups', 'of', 'semantically']\n",
      "['related', 'items', ')', 'for', 'post', '-', 'hoc', 'label', 'decisions', '.', 'Experiments', 'com', '-', 'paring', 'Revolt', 'to', 'traditional', 'crowdsourced', 'labeling', 'show', 'that', 'Revolt', 'produces', 'high', 'quality', 'labels', 'without', 'requiring', 'label']\n",
      "['related', 'items', ')', 'for', 'post', '-', 'hoc', 'label', 'decisions', '.', 'Experiments', 'com', '-', 'paring', 'Revolt', 'to', 'traditional', 'crowdsourced', 'labeling', 'show', 'that', 'Revolt', 'produces', 'high', 'quality', 'labels', 'without', 'requiring', 'label']\n",
      "['guidelines', 'in', 'turn', 'for', 'an', 'increase', 'in', 'monetary', 'cost', '.', 'This', 'up', 'front', 'cost', ',', 'however', ',', 'is', 'mitigated', 'by', 'Revolt', \"'\", 's', 'ability', 'to', 'produce', 'reusable', 'structures', 'that', 'can', 'accommodate', 'a', 'variety', 'of', 'label']\n",
      "['guidelines', 'in', 'turn', 'for', 'an', 'increase', 'in', 'monetary', 'cost', '.', 'This', 'up', 'front', 'cost', ',', 'however', ',', 'is', 'mitigated', 'by', 'Revolt', \"'\", 's', 'ability', 'to', 'produce', 'reusable', 'structures', 'that', 'can', 'accommodate', 'a', 'variety', 'of', 'label']\n",
      "['boundaries', 'without', 'requiring', 'new', 'data', 'to', 'be', 'collected', '.', 'Further', 'comparisons', 'of', 'Revolt', \"'\", 's', 'collaborative', 'and', 'non', '-', 'collaborative', 'variants', 'show', 'that', 'collabvoration', 'reaches', 'higher', 'label', 'accura', '-']\n",
      "['cy', 'with', 'lower', 'monetary', 'cost', '.']\n",
      "['learned', 'models', 'that', 'must', 'be', 'trained', 'on', 'representative', 'datasets', 'labeled', 'according', 'to', 'target', 'concepts', '(', 'e', '.', 'g', '.', ',', 'speech', 'labeled', 'by', 'their', 'intended', 'commands', ',', 'faces', 'labeled', 'in', 'images', ',', 'emails', 'la', '-']\n",
      "['beled', 'as', 'spam', 'or', 'not', 'spam', ')', '.']\n",
      "['crowdsourcing', ';', 'machine', 'learning', ';', 'collaboration', ';', 'real', '-', 'time']\n",
      "['H', '.', '5', '.', 'm', '.', 'Information', 'Interfaces', 'and', 'Presentation', '(', 'e', '.', 'g', '.', 'HCI', ')', ':', 'Miscellaneous']\n",
      "['H', '.', '5', '.', 'm', '.', 'Information', 'Interfaces', 'and', 'Presentation', '(', 'e', '.', 'g', '.', 'HCI', ')', ':', 'Miscellaneous']\n",
      "['H', '.', '5', '.', 'm', '.', 'Information', 'Interfaces', 'and', 'Presentation', '(', 'e', '.', 'g', '.', 'HCI', ')', ':', 'Miscellaneous']\n",
      "['H', '.', '5', '.', 'm', '.', 'Information', 'Interfaces', 'and', 'Presentation', '(', 'e', '.', 'g', '.', 'HCI', ')', ':', 'Miscellaneous']\n",
      "['From', 'conversational', 'assistants', 'on', 'mobile', 'devices', ',', 'to', 'facial']\n",
      "['Techniques', 'for', 'collecting', 'labeled', 'data', 'include', 'recruiting', 'ex', '-', 'perts', 'for', 'manual', 'annotation', '[', '51', ']', ',', 'extracting', 'relations', 'from', 'readily', 'available', 'sources', '(', 'e', '.', 'g', '.', ',', 'identifying', 'bodies', 'of', 'text', 'in']\n",
      "['parallel', 'online', 'translations', '[', '46', ',', '13', ']', ')', ',', 'and', 'automatically', 'gener', '-', 'ating', 'labels', 'based', 'on', 'user', 'behaviors', '(', 'e', '.', 'g', '.', ',', 'using', 'dwell', 'time', 'to', 'implicitly', 'mark', 'search', 'result', 'relevance', '[', '2', ']', ')', '.', 'Recently', ',']\n",
      "['parallel', 'online', 'translations', '[', '46', ',', '13', ']', ')', ',', 'and', 'automatically', 'gener', '-', 'ating', 'labels', 'based', 'on', 'user', 'behaviors', '(', 'e', '.', 'g', '.', ',', 'using', 'dwell', 'time', 'to', 'implicitly', 'mark', 'search', 'result', 'relevance', '[', '2', ']', ')', '.', 'Recently', ',']\n",
      "['many', 'practitioners', 'have', 'also', 'turned', 'to', 'crowdsourcing', 'for', 'cre', '-', 'ating', 'labeled', 'datasets', 'at', 'low', 'cost', '[', '49', ']', '.', 'Successful', 'crowd', '-']\n",
      "['many', 'practitioners', 'have', 'also', 'turned', 'to', 'crowdsourcing', 'for', 'cre', '-', 'ating', 'labeled', 'datasets', 'at', 'low', 'cost', '[', '49', ']', '.', 'Successful', 'crowd', '-']\n",
      "['Figure', '1', '.', 'Revolt', 'creates', 'labels', 'for', 'unanimously', 'labeled', '“certain”', 'items', '(', 'e', '.', 'g', '.', ',', 'cats', 'and', 'not', 'cats', ')', ',', 'and', 'surfaces', 'categories', 'of', '“uncertain”', 'items', 'enriched', 'with', 'crowd', 'feedback', '(', 'e', '.', 'g', '.', ',', 'cats', 'and', 'dogs', 'and', 'cartoon', 'cats', 'in', 'the', 'dotted', 'middle', 'region', 'are', 'annotated', 'with', 'crowd', 'explanations', ')', '.', 'Rich']\n",
      "['Figure', '1', '.', 'Revolt', 'creates', 'labels', 'for', 'unanimously', 'labeled', '“certain”', 'items', '(', 'e', '.', 'g', '.', ',', 'cats', 'and', 'not', 'cats', ')', ',', 'and', 'surfaces', 'categories', 'of', '“uncertain”', 'items', 'enriched', 'with', 'crowd', 'feedback', '(', 'e', '.', 'g', '.', ',', 'cats', 'and', 'dogs', 'and', 'cartoon', 'cats', 'in', 'the', 'dotted', 'middle', 'region', 'are', 'annotated', 'with', 'crowd', 'explanations', ')', '.', 'Rich']\n",
      "['Figure', '1', '.', 'Revolt', 'creates', 'labels', 'for', 'unanimously', 'labeled', '“certain”', 'items', '(', 'e', '.', 'g', '.', ',', 'cats', 'and', 'not', 'cats', ')', ',', 'and', 'surfaces', 'categories', 'of', '“uncertain”', 'items', 'enriched', 'with', 'crowd', 'feedback', '(', 'e', '.', 'g', '.', ',', 'cats', 'and', 'dogs', 'and', 'cartoon', 'cats', 'in', 'the', 'dotted', 'middle', 'region', 'are', 'annotated', 'with', 'crowd', 'explanations', ')', '.', 'Rich']\n",
      "['structures', 'allow', 'label', 'requesters', 'to', 'better', 'understand', 'concepts', 'in', 'the', 'data', 'and', 'make', 'post', '-', 'hoc', 'decisions', 'on', 'label', 'boundaries', '(', 'e', '.', 'g', '.', ',', 'assigning', 'cats', 'and', 'dogs', 'to', 'the', 'cats', 'label', 'and', 'cartoon', 'cats', 'to', 'the', 'not', 'cats', 'label', ')', 'rather', 'than', 'providing', 'crowd', '-', 'workers', 'with', 'a', 'priori', 'label', 'guidelines', '.']\n",
      "['ABSTRACT']\n",
      "['ACM', 'Classification', 'Keywords']\n",
      "['Author', 'Keywords']\n",
      "['INTRODUCTION']\n",
      "['A']\n",
      "['B']\n",
      "['C']\n",
      "['D']\n",
      "['E']\n",
      "['F']\n",
      "['A']\n",
      "['B']\n",
      "['C']\n",
      "['D']\n",
      "['E']\n",
      "['F']\n",
      "['Figure', '3', ':', 'Illustrates', 'how', 'Entities', 'can', 'be', 'accessed', 'flexibly', 'in', 'different', 'ways', ':', '(', 'A', ')', 'Accessing', 'the', 'Entity', 'of', 'the', 'first']\n",
      "['paragraph', 'in', 'the', 'Document', 'via', 'its', 'own', 'Layer', '(', 'B', ')', 'Accessing', 'a', 'sentence', 'via', 'the', 'paragraph', 'Entity', 'or', 'directly', 'via', 'the']\n",
      "['sentences', 'Layer', '(', 'C', ')', 'Similarly', ',', 'the', 'same', 'tokens', 'can', 'be', 'accessed', 'via', 'the', 'overlapping', 'sentence', 'Entity', 'or', 'directly']\n",
      "['via', 'the', 'tokens', 'Layer', 'of', 'the', 'Document', '(', 'where', 'the', 'first', 'tokens', 'are', 'the', 'title', 'of', 'the', 'paper', '.', ')', '(', 'D', ',', 'E', ')', 'Figures', ',', 'captions', ',']\n",
      "['tables', 'and', 'keywords', 'can', 'be', 'accessed', 'in', 'similar', 'ways', '(', 'F', ')', 'Additionally', ',', 'given', 'a', 'bounding', 'box', '(', 'e', '.', 'g', '.', ',', 'of', 'a', 'user', 'selected']\n",
      "['region', ')', ',', 'papermage', 'can', 'find', 'the', 'corresponding', 'Entities', 'for', 'a', 'given', 'Layer', ',', 'in', 'this', 'case', 'finding', 'the', 'tokens', 'under']\n",
      "['the', 'region', '.', 'Excerpt', 'from', 'Chang', 'et', 'al', '.', '(', '2017', ')', '.']\n",
      "['the', 'region', '.', 'Excerpt', 'from', 'Chang', 'et', 'al', '.', '(', '2017', ')', '.']\n",
      "['the', 'region', '.', 'Excerpt', 'from', 'Chang', 'et', 'al', '.', '(', '2017', ')', '.']\n",
      "['Protocols']\n",
      "['and']\n",
      "['Utilities', '.']\n",
      "['To']\n",
      "['instantiate']\n",
      "['a']\n",
      "['Document', ',']\n",
      "['magelib', 'provides', 'protocols', 'and']\n",
      "['utilities', 'like', 'Parsers', 'and', 'Rasterizers', ',', 'which']\n",
      "['hook', 'into', 'off', '-', 'the', '-', 'shelf', 'PDF', 'processing', 'tools', ':', '5']\n",
      "['1', '>', '>', '>', 'import', 'papermage', 'as', 'pm']\n",
      "['2', '>', '>', '>', 'parser', '=', 'pm', '.', 'PDF2TextParser', '(', ')']\n",
      "['2', '>', '>', '>', 'parser', '=', 'pm', '.', 'PDF2TextParser', '(', ')']\n",
      "['3', '>', '>', '>', 'doc', '=', 'parser', '.', 'parse', '(', '\"', '.', '.', '.', 'pdf', '\"', ')']\n",
      "['3', '>', '>', '>', 'doc', '=', 'parser', '.', 'parse', '(', '\"', '.', '.', '.', 'pdf', '\"', ')']\n",
      "['4', '>', '>', '>', '[', 'token', '.', 'text', 'for', 'token', 'in', 'doc', '.', 'tokens', ']']\n",
      "['5', '[', '\"', 'Revolt', '\"', ',', '\"', ':', '\"', ',', '\"', 'Collaborative', '\"', ',', '.', '.', '.', ']']\n",
      "['6', '>', '>', '>', 'doc', '.', 'images']\n",
      "['6', '>', '>', '>', 'doc', '.', 'images']\n",
      "['7', 'None']\n",
      "['8', '9', '>', '>', '>', 'rasterizer', '=', 'pm', '.', 'PDF2ImageRasterizer', '(', ')']\n",
      "['8', '9', '>', '>', '>', 'rasterizer', '=', 'pm', '.', 'PDF2ImageRasterizer', '(', ')']\n",
      "['10', '>', '>', '>', 'doc2', '=', 'rasterizer', '.', 'rasterize', '(', '\"', '.', '.', '.', 'pdf', '\"', ')']\n",
      "['10', '>', '>', '>', 'doc2', '=', 'rasterizer', '.', 'rasterize', '(', '\"', '.', '.', '.', 'pdf', '\"', ')']\n",
      "['11', '>', '>', '>', 'doc', '.', 'images', '=', 'doc2', '.', 'images']\n",
      "['11', '>', '>', '>', 'doc', '.', 'images', '=', 'doc2', '.', 'images']\n",
      "['11', '>', '>', '>', 'doc', '.', 'images', '=', 'doc2', '.', 'images']\n",
      "['12', '>', '>', '>', 'doc', '.', 'images']\n",
      "['12', '>', '>', '>', 'doc', '.', 'images']\n",
      "['13', '[', 'Image', '(', 'np', '.', 'array', '(', '.', '.', '.', ')', ')', ',', '.', '.', '.', ']']\n",
      "['In', 'this', 'example', ',', 'papermage', 'runs', 'PDF2TextParser']\n",
      "['(', 'using', 'pdfplumber', ')', 'to', 'extract', 'the', 'textual', 'in', '-']\n",
      "['formation', 'from', 'a', 'PDF', 'file', '.']\n",
      "['Then', 'it', 'runs']\n",
      "['PDF2ImageRasterizer', '(', 'using', 'pdf2image', ')', 'to', 'up', '-']\n",
      "['date', 'the', 'first', 'Document', 'with', 'images', 'of', 'pages', '.']\n",
      "['5', 'PDFs', 'are', 'not', 'the', 'only', 'way', 'of', 'representing', 'visually', '-', 'rich']\n",
      "['documents', '.', 'For', 'example', ',', 'many', 'scientific', 'documents', 'are', 'dis', '-', 'tributed', 'in', 'XML', 'format', '.', 'As', 'PDFs', 'are', 'the', 'dominant', 'distribu', '-']\n",
      "['documents', '.', 'For', 'example', ',', 'many', 'scientific', 'documents', 'are', 'dis', '-', 'tributed', 'in', 'XML', 'format', '.', 'As', 'PDFs', 'are', 'the', 'dominant', 'distribu', '-']\n",
      "['documents', '.', 'For', 'example', ',', 'many', 'scientific', 'documents', 'are', 'dis', '-', 'tributed', 'in', 'XML', 'format', '.', 'As', 'PDFs', 'are', 'the', 'dominant', 'distribu', '-']\n",
      "['tion', 'format', 'of', 'scientific', 'documents', ',', 'we', 'focus', 'our', 'efforts', 'on', 'PDF', '-', 'specific', 'needs', '.', 'Nevertheless', ',', 'we', 'also', 'provide', 'Parsers']\n",
      "['tion', 'format', 'of', 'scientific', 'documents', ',', 'we', 'focus', 'our', 'efforts', 'on', 'PDF', '-', 'specific', 'needs', '.', 'Nevertheless', ',', 'we', 'also', 'provide', 'Parsers']\n",
      "['in', 'magelib', 'that', 'can', 'instantiate', 'a', 'Document', 'from', 'XML', 'input', '.']\n",
      "['See', 'Appendix', 'A', '.', '1', '.']\n",
      "['See', 'Appendix', 'A', '.', '1', '.']\n",
      "['3', '.', '2']\n",
      "['3', '.', '2']\n",
      "['Interfacing', 'with', 'models', 'for', 'scientific']\n",
      "['document', 'analysis', 'through', 'Predictors']\n",
      "['In', '§', '3', '.', '1', ',', 'we', 'described', 'how', 'users', 'create', 'Layers']\n",
      "['In', '§', '3', '.', '1', ',', 'we', 'described', 'how', 'users', 'create', 'Layers']\n",
      "['by', 'assembling', 'collections', 'of', 'Entities', '.', 'But', 'how']\n",
      "['by', 'assembling', 'collections', 'of', 'Entities', '.', 'But', 'how']\n",
      "['would', 'they', 'make', 'Entities', 'in', 'the', 'first', 'place', '?']\n",
      "['For', 'example', ',', 'to', 'identify', 'multimodal', 'structures']\n",
      "['in', 'visually', '-', 'rich', 'documents', ',', 'researchers', 'might', 'want']\n",
      "['to', 'build', 'complex', 'pipelines', 'that', 'run', 'and', 'combine']\n",
      "['output', 'from', 'many', 'different', 'models', '(', 'e', '.', 'g', '.', ',', 'computer']\n",
      "['vision', 'models', 'for', 'extracting', 'figures', ',', 'NLP', 'models']\n",
      "['for', 'classifying', 'body', 'text', ')', '.', 'papermage', 'provides']\n",
      "['for', 'classifying', 'body', 'text', ')', '.', 'papermage', 'provides']\n",
      "['a', 'unified', 'interface', ',', 'called', 'Predictors', ',', 'to', 'ensure']\n",
      "['models', 'produce', 'Entities', 'that', 'are', 'compatible', 'with']\n",
      "['the', 'Document', '.']\n",
      "['papermage']\n",
      "['includes']\n",
      "['several']\n",
      "['ready', '-', 'to', '-', 'use']\n",
      "['Predictors', 'that', 'leverage', 'state', '-', 'of', '-', 'the', '-', 'art', 'models']\n",
      "['to', 'extract', 'specific', 'document', 'structures', '(', 'Table', '1', ')', '.']\n",
      "['While', 'magelib', '’s', 'abstractions', 'are', 'general', 'for']\n",
      "['visually', '-', 'rich', 'documents', ',', 'Predictors', 'are', 'opti', '-']\n",
      "['mized', 'for', 'parsing', 'of', 'scientific', 'documents', '.', 'They']\n",
      "['mized', 'for', 'parsing', 'of', 'scientific', 'documents', '.', 'They']\n",
      "['are', 'designed', 'to', '(', '1', ')', 'be', 'compatible', 'with', 'models']\n",
      "['from', 'many', 'different', 'machine', 'learning', 'frameworks', ',']\n",
      "['(', '2', ')', 'support', 'inference', 'with', 'text', '-', 'only', ',', 'vision', '-', 'only', ',']\n",
      "['and', 'multimodal', 'models', ',', 'and', '(', '3', ')', 'support', 'both', 'adap', '-']\n",
      "['tation', 'of', 'off', '-', 'the', '-', 'shelf', ',', 'pretrained', 'models', 'as', 'well', 'as']\n",
      "['Use', 'case']\n",
      "['Description']\n",
      "['Examples']\n",
      "['Linguistic', '/', 'Semantic']\n",
      "['Segments', 'doc', 'into', 'text', 'units', 'often', 'used', 'for', 'down', '-']\n",
      "['stream', 'models', '.']\n",
      "['SentencePredictor', 'wraps', 'sciSpaCy', '(', 'Neumann', 'et', 'al', '.', ',', '2019', ')', 'and', 'PySBD', '(', 'Sadvilkar', 'and', 'Neumann', ',', '2020', ')', 'to', 'segment', 'sentences', '.', 'WordPredictor', 'is']\n",
      "['SentencePredictor', 'wraps', 'sciSpaCy', '(', 'Neumann', 'et', 'al', '.', ',', '2019', ')', 'and', 'PySBD', '(', 'Sadvilkar', 'and', 'Neumann', ',', '2020', ')', 'to', 'segment', 'sentences', '.', 'WordPredictor', 'is']\n",
      "['a', 'custom', 'scikit', '-', 'learn', 'model', 'to', 'identify', 'broken', 'words', 'split', 'across', 'PDF', 'lines', 'or']\n",
      "['columns', '.', 'ParagraphPredictor', 'is', 'a', 'set', 'of', 'heuristics', 'on', 'top', 'of', 'both', 'layout', 'and', 'logical', 'structure', 'models', 'to', 'extract', 'paragraphs', '.']\n",
      "['columns', '.', 'ParagraphPredictor', 'is', 'a', 'set', 'of', 'heuristics', 'on', 'top', 'of', 'both', 'layout', 'and', 'logical', 'structure', 'models', 'to', 'extract', 'paragraphs', '.']\n",
      "['LayoutStructure']\n",
      "['Segments', 'doc', 'into', 'visual', 'block', 'regions', '.']\n",
      "['BoxPredictor', 'wraps', 'models', 'from', 'LayoutParser', '(', 'Shen', 'et', 'al', '.', ',', '2021', ')', ',', 'which']\n",
      "['provides', 'vision', 'models', 'like', 'EfficientDet', '(', 'Tan', 'et', 'al', '.', ',', '2020', ')', 'pretrained', 'on', 'scientific', 'layouts', '(', 'Zhong', 'et', 'al', '.', ',', '2019', ')', '.']\n",
      "['LogicalStructure']\n",
      "['Segments', 'doc', 'into', 'orga', '-', 'nizational', 'units', 'like', 'title', ',']\n",
      "['abstract', ',', 'body', ',', 'footnotes', ',', 'caption', ',', 'and', 'more', '.']\n",
      "['SpanPredictor', 'wraps', 'Token', 'Classifiers', 'from', 'Transformers', '(', 'Wolfe', 'et', 'al', '.', ',', '2022', ')', ',']\n",
      "['which', 'provides', 'both', 'pretrained', 'weights', 'from', 'VILA', '(', 'Shen', 'et', 'al', '.', ',', '2022', ')', ',', 'as', 'well', 'as', 'RoBERTa', '(', 'Liu', 'et', 'al', '.', ',', '2019', ')', ',', 'SciBERT', '(', 'Beltagy', 'et', 'al', '.', ',', '2019', ')', 'weights', 'that', 'we’ve']\n",
      "['finetuned', 'on', 'similar', 'data', '.']\n",
      "['Task', '-', 'specific']\n",
      "['Models', 'for', 'a', 'given', 'sci', '-', 'entific', 'document', 'process', '-']\n",
      "['ing', 'task', 'can', 'be', 'used', 'with', 'papermage', 'if', 'wrapped', 'as']\n",
      "['a', 'Predictor', 'following']\n",
      "['common', 'patterns', '.']\n",
      "['As', 'many', 'practitioners', 'depend', 'on', 'prompting', 'a', 'model', 'through', 'an', 'API', 'call', ',', 'we', 'implement', 'APIPredictor', 'which', 'interfaces', 'external', 'APIs', ',', 'such', 'as', 'GPT', '-', '3', '(', 'Brown']\n",
      "['et', 'al', '.', ',', '2020', ')', ',', 'to', 'perform', 'tasks', 'like', 'question', 'answering', 'over', 'a', 'structured', 'Document', '.', 'We', 'also', 'implement', 'SnippetRetrievalPredictor', 'which', 'wraps', 'models', 'like', 'Con', '-']\n",
      "['et', 'al', '.', ',', '2020', ')', ',', 'to', 'perform', 'tasks', 'like', 'question', 'answering', 'over', 'a', 'structured', 'Document', '.', 'We', 'also', 'implement', 'SnippetRetrievalPredictor', 'which', 'wraps', 'models', 'like', 'Con', '-']\n",
      "['triever', '(', 'Izacard', 'et', 'al', '.', ',', '2022', ')', 'to', 'perform', 'top', '-', 'k', 'within', '-', 'document', 'snippet', 'retrieval', '.', 'See', '§', '4', 'for', 'how', 'these', 'two', 'can', 'be', 'combined', '.']\n",
      "['triever', '(', 'Izacard', 'et', 'al', '.', ',', '2022', ')', 'to', 'perform', 'top', '-', 'k', 'within', '-', 'document', 'snippet', 'retrieval', '.', 'See', '§', '4', 'for', 'how', 'these', 'two', 'can', 'be', 'combined', '.']\n",
      "['Table', '1', ':', 'Types', 'of', 'Predictors', 'implemented', 'in', 'papermage', '.']\n",
      "['Model']\n",
      "['Full']\n",
      "['Grobid', 'Subset', 'P']\n",
      "['R']\n",
      "['F1']\n",
      "['P']\n",
      "['R']\n",
      "['F1']\n",
      "['Grobid', 'CRF']\n",
      "['40', '.', '6']\n",
      "['40', '.', '6']\n",
      "['38', '.', '3']\n",
      "['38', '.', '3']\n",
      "['39', '.', '1']\n",
      "['39', '.', '1']\n",
      "['81', '.', '2']\n",
      "['81', '.', '2']\n",
      "['76', '.', '7']\n",
      "['76', '.', '7']\n",
      "['78', '.', '9', 'Grobid', 'NN']\n",
      "['78', '.', '9', 'Grobid', 'NN']\n",
      "['42', '.', '0']\n",
      "['42', '.', '0']\n",
      "['36', '.', '5']\n",
      "['36', '.', '5']\n",
      "['37', '.', '6']\n",
      "['37', '.', '6']\n",
      "['84', '.', '1']\n",
      "['84', '.', '1']\n",
      "['73', '.', '0']\n",
      "['73', '.', '0']\n",
      "['78', '.', '2', 'RoBERTa']\n",
      "['78', '.', '2', 'RoBERTa']\n",
      "['75', '.', '9']\n",
      "['75', '.', '9']\n",
      "['80', '.', '0']\n",
      "['80', '.', '0']\n",
      "['76', '.', '8']\n",
      "['76', '.', '8']\n",
      "['82', '.', '6']\n",
      "['82', '.', '6']\n",
      "['83', '.', '9']\n",
      "['83', '.', '9']\n",
      "['83', '.', '2', 'I', '-', 'VILA']\n",
      "['83', '.', '2', 'I', '-', 'VILA']\n",
      "['92', '.', '0']\n",
      "['92', '.', '0']\n",
      "['94', '.', '1']\n",
      "['94', '.', '1']\n",
      "['92', '.', '7']\n",
      "['92', '.', '7']\n",
      "['92', '.', '2']\n",
      "['92', '.', '2']\n",
      "['95', '.', '2']\n",
      "['95', '.', '2']\n",
      "['93', '.', '7']\n",
      "['93', '.', '7']\n",
      "['Table', '2', ':', 'Evaluating', 'performance', 'of', 'CoreRecipe', 'for']\n",
      "['logical', 'structure', 'recovery', 'on', 'S2', '-', 'VL', '(', 'Shen', 'et', 'al', '.', ',', '2022', ')', '.']\n",
      "['Metrics', 'are', 'computed', 'for', 'token', '-', 'level', 'classification', ',']\n",
      "['macro', '-', 'averaged', 'over', 'categories', '.', 'The', '“Grobid', 'Subset”']\n",
      "['macro', '-', 'averaged', 'over', 'categories', '.', 'The', '“Grobid', 'Subset”']\n",
      "['limits', 'evaluation', 'to', 'only', 'categories', 'for', 'which', 'Grobid']\n",
      "['returns', 'bounding', 'box', 'information', ',', 'which', 'was', 'necessary']\n",
      "['for', 'evaluation', 'on', 'S2', '-', 'VL', '.', 'See', 'Appendix', 'A', '.', '3', 'for', 'details', '.']\n",
      "['for', 'evaluation', 'on', 'S2', '-', 'VL', '.', 'See', 'Appendix', 'A', '.', '3', 'for', 'details', '.']\n",
      "['for', 'evaluation', 'on', 'S2', '-', 'VL', '.', 'See', 'Appendix', 'A', '.', '3', 'for', 'details', '.']\n",
      "['development', 'of', 'new', 'ones', 'from', 'scratch', '.', 'Similarly']\n",
      "['development', 'of', 'new', 'ones', 'from', 'scratch', '.', 'Similarly']\n",
      "['to', 'the', 'Transformers', 'library', ',', 'a', 'Predictor', '’s']\n",
      "['implementation', 'is', 'typically', 'independent', 'from']\n",
      "['its', 'configuration', ',', 'allowing', 'users', 'to', 'customize']\n",
      "['each', 'Predictor', 'by', 'tweaking', 'hyperparameters', 'or']\n",
      "['loading', 'a', 'different', 'set', 'of', 'weights', '.']\n",
      "['Below', ',', 'we', 'showcase', 'how', 'a', 'vision', 'model', 'and']\n",
      "['two', 'text', 'models', '(', 'both', 'neural', 'and', 'symbolic', ')', 'can', 'be']\n",
      "['applied', 'in', 'succession', 'to', 'a', 'single', 'Document', '.', 'See']\n",
      "['applied', 'in', 'succession', 'to', 'a', 'single', 'Document', '.', 'See']\n",
      "['Table', '1', 'for', 'a', 'summary', 'of', 'supported', 'Predictors', '.']\n",
      "['1', '>', '>', '>', 'import', 'papermage', 'as', 'pm']\n",
      "['2', '>', '>', '>', 'cv', '=', 'pm', '.', 'BoxPredictor', '(', '.', '.', '.', ')']\n",
      "['2', '>', '>', '>', 'cv', '=', 'pm', '.', 'BoxPredictor', '(', '.', '.', '.', ')']\n",
      "['3', '>', '>', '>', 'tables', ',', 'figures', '=', 'cv', '.', 'predict', '(', 'doc', ')']\n",
      "['3', '>', '>', '>', 'tables', ',', 'figures', '=', 'cv', '.', 'predict', '(', 'doc', ')']\n",
      "['4', '>', '>', '>', 'doc', '.', 'add', '(', 'tables', ',', 'figures', ')']\n",
      "['4', '>', '>', '>', 'doc', '.', 'add', '(', 'tables', ',', 'figures', ')']\n",
      "['5', '6', '>', '>', '>', 'nlp', '_', 'neu', '=', 'pm', '.', 'SpanPredictor', '(', '.', '.', '.', ')']\n",
      "['5', '6', '>', '>', '>', 'nlp', '_', 'neu', '=', 'pm', '.', 'SpanPredictor', '(', '.', '.', '.', ')']\n",
      "['7', '>', '>', '>', 'titles', ',', 'authors', '=', 'nlp', '_', 'neu', '.', 'predict', '(', 'doc', ')']\n",
      "['7', '>', '>', '>', 'titles', ',', 'authors', '=', 'nlp', '_', 'neu', '.', 'predict', '(', 'doc', ')']\n",
      "['8', '>', '>', '>', 'doc', '.', 'add', '(', 'titles', ',', 'authors', ')']\n",
      "['8', '>', '>', '>', 'doc', '.', 'add', '(', 'titles', ',', 'authors', ')']\n",
      "['9', '10', '>', '>', '>', 'nlp', '_', 'sym', '=', 'pm', '.', 'SentencePredictor', '(', '.', '.', '.', ')']\n",
      "['9', '10', '>', '>', '>', 'nlp', '_', 'sym', '=', 'pm', '.', 'SentencePredictor', '(', '.', '.', '.', ')']\n",
      "['11', '>', '>', '>', 'sentences', '=', 'nlp', '_', 'sym', '.', 'predict', '(', 'doc', ')']\n",
      "['11', '>', '>', '>', 'sentences', '=', 'nlp', '_', 'sym', '.', 'predict', '(', 'doc', ')']\n",
      "['12', '>', '>', '>', 'doc', '.', 'add', '(', 'sentences', ')']\n",
      "['12', '>', '>', '>', 'doc', '.', 'add', '(', 'sentences', ')']\n",
      "['Predictors', 'return', 'a', 'list', 'of', 'Entities', ',', 'which']\n",
      "['can', 'be', 'group', '_', 'by', '(', ')', 'to', 'organize', 'them', 'based', 'on', 'pre', '-']\n",
      "['dicted', 'label', 'value', '(', 'e', '.', 'g', '.', ',', 'tokens', 'classified', 'as', '“title”']\n",
      "['or', '“authors”', ')', '.', 'Finally', ',', 'these', 'predictions', 'are', 'passed']\n",
      "['or', '“authors”', ')', '.', 'Finally', ',', 'these', 'predictions', 'are', 'passed']\n",
      "['to', 'doc', '.', 'annotate', '(', ')', 'to', 'be', 'added', 'to', 'Document', '.']\n",
      "['to', 'doc', '.', 'annotate', '(', ')', 'to', 'be', 'added', 'to', 'Document', '.']\n",
      "['3', '.', '3']\n",
      "['3', '.', '3']\n",
      "['End', '-', 'to', '-', 'end', 'processing', 'with', 'Recipes']\n",
      "['Finally', ',', 'papermage', 'provides', 'predefined', 'combina', '-']\n",
      "['tions', 'of', 'Predictors', ',', 'called', 'Recipes', ',', 'for', 'users']\n",
      "['seeking', 'high', '-', 'quality', 'options', 'for', 'turn', '-', 'key', 'process', '-']\n",
      "['ing', 'of', 'visually', '-', 'rich', 'documents', ':']\n",
      "['1', 'from', 'papermage', 'import', 'CoreRecipe']\n",
      "['2', 'recipe', '=', 'CoreRecipe', '(', ')']\n",
      "['3', 'doc', '=', 'recipe', '.', 'run', '(', '\"', '.', '.', '.', 'pdf', '\"', ')']\n",
      "['3', 'doc', '=', 'recipe', '.', 'run', '(', '\"', '.', '.', '.', 'pdf', '\"', ')']\n",
      "['4', 'doc', '.', 'captions', '[', '0', ']', '.', 'text']\n",
      "['4', 'doc', '.', 'captions', '[', '0', ']', '.', 'text']\n",
      "['4', 'doc', '.', 'captions', '[', '0', ']', '.', 'text']\n",
      "['5', '>', '>', '>', '\"', 'Figure', '1', '.', '.', '.', '.', '\"']\n",
      "['Recipes', 'can', 'also', 'be', 'flexibly', 'modified', 'to', 'sup', '-']\n",
      "['port', 'development', '.', 'For', 'example', ',', 'our', 'current', 'de', '-']\n",
      "['port', 'development', '.', 'For', 'example', ',', 'our', 'current', 'de', '-']\n",
      "['fault', 'combines', 'the', 'pdfplumber', 'PDF', 'parsing', 'utility']\n",
      "['with', 'the', 'I', '-', 'VILA', '(', 'Shen', 'et', 'al', '.', ',', '2022', ')', 'research', 'model', '.']\n",
      "['We', 'show', 'in', 'Table', '2', 'an', 'evaluation', 'comparing', 'this']\n",
      "['against', 'the', 'same', 'recipe', 'but', 'configured', 'to', '(', '1', ')', 'swap']\n",
      "['I', '-', 'VILA', 'for', 'a', 'RoBERTa', 'model', ',', 'as', 'well', 'as', '(', '2', ')', 'swap']\n",
      "['both', 'for', 'Grobid', 'API', 'calls', '.', 'We', 'expect', 'Recipes']\n",
      "['both', 'for', 'Grobid', 'API', 'calls', '.', 'We', 'expect', 'Recipes']\n",
      "['to', 'appeal', 'to', 'two', 'groups', 'of', 'users—end', '-', 'to', '-', 'end', 'con', '-']\n",
      "['sumers', ',', 'and', 'developers', 'of', 'high', '-', 'level', 'applications', '.']\n",
      "['The', 'former', 'is', 'comprised', 'of', 'developers', 'and', 're', '-']\n",
      "['searchers', 'who', 'are', 'looking', 'for', 'a', 'one', '-', 'step', 'solution']\n",
      "['to', 'multimodal', 'scientific', 'document', 'analysis', '.', 'The']\n",
      "['to', 'multimodal', 'scientific', 'document', 'analysis', '.', 'The']\n",
      "['latter', 'are', 'likely', 'developers', 'and', 'researchers', 'looking']\n",
      "['to', 'combine', 'document', 'structure', 'primitives', 'to', 'build']\n",
      "['a', 'complex', 'application', '(', 'see', 'example', 'in', '§', '4', ')', '.']\n",
      "['4']\n",
      "['Vignette', ':', 'Building', 'an', 'Attributed', 'QA']\n",
      "['System', 'for', 'Scientific', 'Papers']\n",
      "['How', 'could', 'researchers', 'leverage', 'papermage', 'for']\n",
      "['their', 'research', '?', 'Here', ',', 'we', 'walk', 'through', 'a', 'user', 'sce', '-']\n",
      "['their', 'research', '?', 'Here', ',', 'we', 'walk', 'through', 'a', 'user', 'sce', '-']\n",
      "['nario', 'in', 'which', 'a', 'researcher', '(', 'Lucy', ')', 'is', 'prototyping']\n",
      "['an', 'attributed', 'QA', 'system', 'for', 'science', '.']\n",
      "['System', 'Design', '.']\n",
      "['Drawing', 'inspiration', 'from', 'Ko']\n",
      "['et', 'al', '.', '(', '2020', ')', ',', 'Lee', 'et', 'al', '.', '(', '2023', ')', ',', 'Fok', 'et', 'al', '.', '(', '2023a', ')', ',']\n",
      "['et', 'al', '.', '(', '2020', ')', ',', 'Lee', 'et', 'al', '.', '(', '2023', ')', ',', 'Fok', 'et', 'al', '.', '(', '2023a', ')', ',']\n",
      "['et', 'al', '.', '(', '2020', ')', ',', 'Lee', 'et', 'al', '.', '(', '2023', ')', ',', 'Fok', 'et', 'al', '.', '(', '2023a', ')', ',']\n",
      "['et', 'al', '.', '(', '2020', ')', ',', 'Lee', 'et', 'al', '.', '(', '2023', ')', ',', 'Fok', 'et', 'al', '.', '(', '2023a', ')', ',']\n",
      "['and', 'Newman', 'et', 'al', '.', '(', '2023', ')', ',', 'Lucy', 'is', 'studying', 'how']\n",
      "['and', 'Newman', 'et', 'al', '.', '(', '2023', ')', ',', 'Lucy', 'is', 'studying', 'how']\n",
      "['language', 'models', 'can', 'be', 'used', 'to', 'resolve', 'questions']\n",
      "['that', 'arise', 'while', 'reading', 'a', 'paper', '(', 'e', '.', 'g', '.', 'What', 'does']\n",
      "['this', 'mean', '?', 'or', 'What', 'does', 'this', 'refer', 'to', '?', ')', '.', 'In', 'her']\n",
      "['this', 'mean', '?', 'or', 'What', 'does', 'this', 'refer', 'to', '?', ')', '.', 'In', 'her']\n",
      "['prototype', 'interface', ',', 'a', 'user', 'can', 'highlight', 'a', 'passage']\n",
      "['in', 'a', 'PDF', 'and', 'ask', 'a', 'question', 'about', 'it', '.', 'A', 'retrieval']\n",
      "['in', 'a', 'PDF', 'and', 'ask', 'a', 'question', 'about', 'it', '.', 'A', 'retrieval']\n",
      "['model', 'then', 'finds', 'relevant', 'passages', 'from', 'the', 'rest']\n",
      "['of', 'the', 'paper', '.', 'The', 'prototype', 'then', 'uses', 'the', 'text', 'of']\n",
      "['of', 'the', 'paper', '.', 'The', 'prototype', 'then', 'uses', 'the', 'text', 'of']\n",
      "['the', 'retrieved', 'passages', 'along', 'with', 'the', 'user', 'question']\n",
      "['to', 'prompt', 'a', 'language', 'model', 'to', 'generate', 'an', 'answer', '.']\n",
      "['When', 'presenting', 'the', 'answer', 'to', 'the', 'user', ',', 'the', 'proto', '-']\n",
      "['type', 'also', 'visually', 'highlights', 'the', 'retrieved', 'passages']\n",
      "['as', 'supporting', 'evidence', 'to', 'the', 'generated', 'answer', '.']\n",
      "['Getting', 'started', 'quickly', '.']\n",
      "['As', 'a', 'researcher', 'profi', '-']\n",
      "['cient', 'in', 'Python', ',', 'it', 'only', 'takes', 'Lucy', 'minutes', 'to', 'install']\n",
      "['papermage', 'using', 'pip', 'and', 'successfully', 'process', 'a', 'lo', '-']\n",
      "['cal', 'PDF', 'file', 'by', 'following', 'the', 'example', 'code', 'snippet']\n",
      "['for', 'CoreRecipe', 'in', '§', '3', '.', '2', '.', 'In', 'an', 'interactive', 'session', ',']\n",
      "['for', 'CoreRecipe', 'in', '§', '3', '.', '2', '.', 'In', 'an', 'interactive', 'session', ',']\n",
      "['for', 'CoreRecipe', 'in', '§', '3', '.', '2', '.', 'In', 'an', 'interactive', 'session', ',']\n",
      "['she', 'familiarizes', 'herself', 'with', 'the', 'provided', 'Layers']\n",
      "['by', 'following', 'the', 'traversal', ',', 'cross', '-', 'referencing', 'and']\n",
      "['querying', 'examples', 'in', '§', '3', '.', '1', '.', 'She', 'makes', 'sure', 'she', 'can']\n",
      "['querying', 'examples', 'in', '§', '3', '.', '1', '.', 'She', 'makes', 'sure', 'she', 'can']\n",
      "['querying', 'examples', 'in', '§', '3', '.', '1', '.', 'She', 'makes', 'sure', 'she', 'can']\n",
      "['serialize', 'and', 're', '-', 'instantiate', 'her', 'Document', '(', '§', 'A', '.', '2', ')', '.']\n",
      "['Formatting', 'input', '.']\n",
      "['Before', 'using', 'papermage', ',']\n",
      "['Lucy', 'has', 'prior', 'experience', 'building', 'QA', 'pipelines', ',']\n",
      "['but', 'has', 'only', 'dealt', 'with', 'documents', 'as', 'sentence', '-']\n",
      "['split', 'text', 'data', '(', 'e', '.', 'g', '.', ',', '<', 'List', '[', 'str', ']', '>', ')', '.', 'Lucy', 'realizes']\n",
      "['split', 'text', 'data', '(', 'e', '.', 'g', '.', ',', '<', 'List', '[', 'str', ']', '>', ')', '.', 'Lucy', 'realizes']\n",
      "['that', 'she', 'can', 'reuse', 'her', 'prior', 'text', '-', 'only', 'code', 'with']\n",
      "['papermage', 'by', 'implementing', 'a', 'couple', 'of', 'wrappers']\n",
      "['to', 'gain', 'additional', 'capabilities', ':', 'First', ',', 'she', 'converts']\n",
      "['a', 'user’s', 'highlighted', 'passage', 'from', 'a', 'visual', 'selec', '-']\n",
      "['tion', 'to', 'text', 'following', 'the', 'example', 'in', 'Figure', '3F', '.']\n",
      "['Next', ',', 'she', 'converts', 'Document', 'to', 'her', 'required', 'text']\n",
      "['format', 'by', 'following', 'the', 'traversal', 'examples', 'in', '§', '3', '.', '1']\n",
      "['format', 'by', 'following', 'the', 'traversal', 'examples', 'in', '§', '3', '.', '1']\n",
      "['(', 'e', '.', 'g', '.', ',', 'using', '[', 's', '.', 'text', 'for', 's', 'in', 'doc', '.', 'sentences', ']', ')', '.']\n",
      "['Within', 'a', 'few', 'lines', 'of', 'code', ',', 'Lucy', 'has', 'everything']\n",
      "['she', 'needs', 'for', 'text', '-', 'only', 'input', 'to', 'her', 'QA', 'pipeline', '.']\n",
      "['Formatting', 'output', '.']\n",
      "['Lucy', 'runs', 'her', 'QA', 'system']\n",
      "['on', 'her', 'newly', 'acquired', 'text', 'data', 'and', 'now', 'has', '(', '1', ')', 'a']\n",
      "['model', '-', 'generated', 'answer', 'and', '(', '2', ')', 'several', 'retrieved']\n",
      "['evidence', 'passages', '.', 'She', 'realizes', 'that', 'she', 'already']\n",
      "['evidence', 'passages', '.', 'She', 'realizes', 'that', 'she', 'already']\n",
      "['has', 'access', 'to', 'the', 'evidences’', 'bounding', 'boxes', 'via', 'a']\n",
      "['similar', 'call', 'to', 'how', 'she', 'defined', 'the', 'model', 'input', 'con', '-']\n",
      "['text', '(', 'e', '.', 'g', '.', ',', '[', 's', '.', 'boxes', 'for', 's', 'in', 'doc', '.', 'sentences', ']', ')', '.']\n",
      "['She', 'can', 'easily', 'pass', 'this', 'to', 'the', 'user', 'interface', 'to', 'en', '-']\n",
      "['able', 'linking', 'to', 'and', 'highlighting', 'of', 'those', 'passages', '.']\n",
      "['Defining', 'a', 'Predictor', '.']\n",
      "['The', 'pattern', 'Lucy', 'has']\n",
      "['followed', 'is', 'used', 'in', 'our', 'many', 'Predictor', 'imple', '-']\n",
      "['mentations', ':', '(', '1', ')', 'gain', 'access', 'to', 'text', 'by', 'traversing']\n",
      "['Layers', '(', 'e', '.', 'g', '.', ',', 'sentences', ')', ',', '(', '2', ')', 'perform', 'all', 'usual']\n",
      "['NLP', 'computation', 'on', 'that', 'text', ',', 'and', '(', '3', ')', 'format']\n",
      "['model', 'output', 'as', 'Entities', '.', 'This', 'simple', 'pattern']\n",
      "['model', 'output', 'as', 'Entities', '.', 'This', 'simple', 'pattern']\n",
      "['allows', 'users', 'to', 'reuse', 'familiar', 'models', 'in', 'existing']\n",
      "['frameworks', 'and', 'eschews', 'lengthy', 'onboarding', 'to']\n",
      "['papermage', '.', 'Lucy', 'wraps', 'her', 'prompting', 'and', 're', '-']\n",
      "['papermage', '.', 'Lucy', 'wraps', 'her', 'prompting', 'and', 're', '-']\n",
      "['trieval', 'code', 'in', 'new', 'classes', ':', 'APIPredictor', 'and']\n",
      "['SnippetRetrievalPredictor', '(', 'see', 'Table', '1', ')', '.']\n",
      "['Fast', 'iterations', '.']\n",
      "['Leveraging', 'the', 'bounding', 'box']\n",
      "['data', 'from', 'papermage', 'to', 'visually', 'highlight', 'the', 're', '-']\n",
      "['trieved', 'passages', ',', 'Lucy', 'suspects', 'the', 'retrieval', 'com', '-']\n",
      "['ponent', 'is', 'likely', 'underperforming', '.', 'She', 'makes', 'a', 'sim', '-']\n",
      "['ponent', 'is', 'likely', 'underperforming', '.', 'She', 'makes', 'a', 'sim', '-']\n",
      "['ple', 'edit', 'from', 'doc', '.', 'sentences', 'to', 'doc', '.', 'paragraphs']\n",
      "['ple', 'edit', 'from', 'doc', '.', 'sentences', 'to', 'doc', '.', 'paragraphs']\n",
      "['ple', 'edit', 'from', 'doc', '.', 'sentences', 'to', 'doc', '.', 'paragraphs']\n",
      "['and', 'evaluates', 'system', 'performance', 'under', 'different']\n",
      "['input', 'granularity', '.', 'She', 'also', 'realizes', 'the', 'system', 'of', '-']\n",
      "['input', 'granularity', '.', 'She', 'also', 'realizes', 'the', 'system', 'of', '-']\n",
      "['ten', 'retrieves', 'content', 'outside', 'the', 'main', 'body', 'text', '.']\n",
      "['She', 'restricts', 'her', 'traversal', 'to', 'filter', 'out', 'paragraphs']\n",
      "['that', 'overlap', 'with', 'footnotes—', '[', 'p', '.', 'text', 'for', 'p', 'in']\n",
      "['doc', '.', 'paragraphs', 'if', 'len', '(', 'p', '.', 'footnotes', ')', '=', '=', '0', ']', '—']\n",
      "['making', 'clever', 'use', 'of', 'the', 'cross', '-', 'referencing', 'function', '-']\n",
      "['ality', 'to', 'detect', 'when', 'a', 'paragraph', 'is', 'actually', 'coming']\n",
      "['from', 'a', 'footnote', '.', 'This', 'example', 'demonstrates', 'the']\n",
      "['from', 'a', 'footnote', '.', 'This', 'example', 'demonstrates', 'the']\n",
      "['versatility', 'of', 'the', 'affordances', 'provided', 'by', 'magelib', '.']\n",
      "['5']\n",
      "['Conclusion']\n",
      "['In', 'this', 'work', ',', 'we’ve', 'introduced', 'papermage', ',', 'an']\n",
      "['open', '-', 'source', 'Python', 'toolkit', 'for', 'processing', 'scientific']\n",
      "['documents', '.', 'papermage', 'was', 'developed', 'to', 'supply']\n",
      "['documents', '.', 'papermage', 'was', 'developed', 'to', 'supply']\n",
      "['high', '-', 'quality', 'data', 'and', 'reduce', 'friction', 'for', 'research']\n",
      "['prototype', 'development', 'at', 'Semantic', 'Scholar', '.', 'To', '-']\n",
      "['prototype', 'development', 'at', 'Semantic', 'Scholar', '.', 'To', '-']\n",
      "['day', ',', 'it', 'is', 'being', 'used', 'in', 'the', 'production', 'PDF', 'process', '-']\n",
      "['ing', 'pipeline', 'to', 'provide', 'data', 'for', 'both', 'the', 'literature']\n",
      "['graph', '(', 'Ammar', 'et', 'al', '.', ',', '2018', ';', 'Kinney', 'et', 'al', '.', ',', '2023', ')']\n",
      "['and', 'the', 'paper', '-', 'reading', 'interface', '(', 'Lo', 'et', 'al', '.', ',', '2023', ')', '.', 'It']\n",
      "['and', 'the', 'paper', '-', 'reading', 'interface', '(', 'Lo', 'et', 'al', '.', ',', '2023', ')', '.', 'It']\n",
      "['has', 'also', 'been', 'used', 'in', 'working', 'research', 'prototypes']\n",
      "['which', 'have', 'since', 'contributed', 'to', 'research', 'publica', '-']\n",
      "['tions', '(', 'Fok', 'et', 'al', '.', ',', '2023b', ';', 'Kim', 'et', 'al', '.', ',', '2023', ')', '.', '6', 'We']\n",
      "['tions', '(', 'Fok', 'et', 'al', '.', ',', '2023b', ';', 'Kim', 'et', 'al', '.', ',', '2023', ')', '.', '6', 'We']\n",
      "['open', '-', 'source', 'papermage', 'in', 'hopes', 'it', 'will', 'simplify']\n",
      "['research', 'workflows', 'that', 'depend', 'on', 'scientific', 'doc', '-']\n",
      "['uments', 'and', 'promote', 'extensions', 'to', 'other', 'visually', '-']\n",
      "['rich', 'documents', 'like', 'textbooks', '(', 'Lincker', 'et', 'al', '.', ',', '2023', ')']\n",
      "['and', 'digitized', 'print', 'media', '(', 'Lee', 'et', 'al', '.', ',', '2020', ')', '.']\n",
      "['6', 'See', 'a', 'demo', 'of', 'such', 'a', 'prototype', 'papeo', '.', 'app', '/', 'demo', '.']\n",
      "['6', 'See', 'a', 'demo', 'of', 'such', 'a', 'prototype', 'papeo', '.', 'app', '/', 'demo', '.']\n",
      "['Ethical', 'Considerations']\n",
      "['As', 'a', 'toolkit', 'primarily', 'designed', 'to', 'process', 'scientific']\n",
      "['documents', ',', 'there', 'are', 'two', 'areas', 'where', 'papermage']\n",
      "['could', 'cause', 'harms', 'or', 'have', 'unintended', 'effects', '.']\n",
      "['Extraction']\n",
      "['of']\n",
      "['bibliographic']\n",
      "['information']\n",
      "['papermage', 'could', 'be', 'used', 'to', 'parse', 'author', 'names', ',']\n",
      "['affiliation', ',', 'emails', 'from', 'scientific', 'documents', '.', 'Like']\n",
      "['affiliation', ',', 'emails', 'from', 'scientific', 'documents', '.', 'Like']\n",
      "['any', 'software', ',', 'this', 'extraction', 'can', 'be', 'noisy', ',', 'leading']\n",
      "['to', 'incorrect', 'parsing', 'and', 'thus', 'mis', '-', 'attribution', 'of']\n",
      "['manuscripts', '.']\n",
      "['Further', ',', 'since', 'papermage', 'relies']\n",
      "['on', 'static', 'PDF', 'documents', ',', 'rather', 'than', 'metadata']\n",
      "['dynamically', 'retrieved', 'from', 'publishers', ',', 'users', 'of']\n",
      "['papermage', 'need', 'consider', 'how', 'and', 'when', 'extracted']\n",
      "['names', 'should', 'no', 'longer', 'be', 'associated', 'with', 'authors', ',']\n",
      "['a', 'harmful', 'practice', 'called', 'deadnaming', '(', 'Queer', 'in', 'AI']\n",
      "['et', 'al', '.', ',', '2023', ')', '.', 'We', 'recommend', 'papermage', 'users', 'to']\n",
      "['et', 'al', '.', ',', '2023', ')', '.', 'We', 'recommend', 'papermage', 'users', 'to']\n",
      "['exercise', 'caution', 'when', 'using', 'our', 'toolkit', 'to', 'extract']\n",
      "['metadata', ',', 'to', 'cross', '-', 'reference', 'extracted', 'content', 'with']\n",
      "['other', 'sources', 'when', 'possible', ',', 'and', 'to', 'design', 'systems']\n",
      "['such', 'that', 'authors', 'have', 'the', 'ability', 'to', 'manually', 'edit']\n",
      "['any', 'data', 'about', 'themselves', '.']\n",
      "['Misrepresentation', 'or', 'fabrication', 'of', 'informa', '-']\n",
      "['tion', 'in', 'documents']\n",
      "['In', '§', '3', ',', 'we', 'discussed', 'how']\n",
      "['papermage', 'can', 'be', 'easily', 'extended', 'to', 'support', 'high', '-']\n",
      "['level', 'applications', '.', 'Such', 'applications', 'might', 'include']\n",
      "['level', 'applications', '.', 'Such', 'applications', 'might', 'include']\n",
      "['question', 'answering', 'chatbots', ',', 'or', 'AI', 'summarizers']\n",
      "['that', 'perform', 'information', 'synthesis', 'over', 'one', 'or']\n",
      "['more', 'papermage', 'documents', '.', 'Such', 'applications']\n",
      "['more', 'papermage', 'documents', '.', 'Such', 'applications']\n",
      "['typically', 'rely', 'on', 'generative', 'models', 'to', 'produce', 'their']\n",
      "['output', ',', 'which', 'might', 'fabricate', 'incorrect', 'informa', '-']\n",
      "['tion', 'or', 'misstate', 'claims', '.', 'Developers', 'should', 'be', 'vig', '-']\n",
      "['tion', 'or', 'misstate', 'claims', '.', 'Developers', 'should', 'be', 'vig', '-']\n",
      "['ilant', 'when', 'integrating', 'papermage', 'output', 'into', 'any']\n",
      "['downstream', 'application', ',', 'especially', 'in', 'systems', 'that']\n",
      "['purport', 'to', 'represent', 'information', 'gathered', 'from', 'sci', '-']\n",
      "['entific', 'publications', '.']\n",
      "['Acknowledgements']\n",
      "['We', 'thank', 'our', 'teammates', 'at', 'Semantic', 'Scholar', 'for']\n",
      "['their', 'help', 'on', 'this', 'project', '.', 'In', 'particular', ':', 'Rodney']\n",
      "['their', 'help', 'on', 'this', 'project', '.', 'In', 'particular', ':', 'Rodney']\n",
      "['Kinney', 'provided', 'insight', 'during', 'discussions', 'about']\n",
      "['how', 'best', 'to', 'represent', 'data', 'extracted', 'from', 'docu', '-']\n",
      "['ments', ';', 'Paul', 'Sayre', 'provided', 'feedback', 'on', 'initial']\n",
      "['designs', 'of', 'the', 'library', ';', 'Chloe', 'Anastasiades', ',', 'Dany']\n",
      "['Haddad', 'and', 'Egor', 'Klevak', 'tested', 'earlier', 'versions', 'of']\n",
      "['the', 'library', ';', 'Tal', 'August', ',', 'Raymond', 'Fok', ',', 'and', 'Andrew']\n",
      "['Head', 'motivated', 'the', 'need', 'for', 'such', 'a', 'toolkit', 'dur', '-']\n",
      "['ing', 'their', 'internships', 'building', 'augmented', 'reading']\n",
      "['interfaces', ';', 'Jaron', 'Lochner', 'and', 'Kelsey', 'MacMillan']\n",
      "['helped', 'us', 'get', 'additional', 'engineering', 'support', ';', 'and']\n",
      "['Oren', 'Etzioni', 'provided', 'enthusiasm', 'and', 'support', 'for']\n",
      "['continued', 'investment', 'in', 'this', 'toolkit', '.']\n",
      "['This', 'project', 'was', 'supported', 'in', 'part', 'by', 'NSF', 'Grant']\n",
      "['OIA', '-', '2033558', 'and', 'NSF', 'Grant', 'CNS', '-', '2213656', '.']\n",
      "['Author', 'Contributions']\n",
      "['All', 'authors', 'contributed', 'to', 'the', 'implementation', 'of']\n",
      "['papermage', 'and', '/', 'or', 'the', 'writing', 'of', 'this', 'paper', '.']\n",
      "['Core', 'contributors', '.']\n",
      "['Kyle', 'Lo', 'and', 'Zejiang', 'Shen']\n",
      "['initiated', 'the', 'project', 'and', 'co', '-', 'wrote', 'initial', 'implemen', '-']\n",
      "['tations', 'of', 'magelib', 'and', 'some', 'Predictors', '.', 'Later', ',']\n",
      "['tations', 'of', 'magelib', 'and', 'some', 'Predictors', '.', 'Later', ',']\n",
      "['Kyle', 'Lo', 'and', 'Luca', 'Soldaini', 'refactored', 'a', 'majority', 'of']\n",
      "['magelib', ',', 'Predictors', ',', 'and', 'added', 'Recipes', '.', 'Ben', '-']\n",
      "['magelib', ',', 'Predictors', ',', 'and', 'added', 'Recipes', '.', 'Ben', '-']\n",
      "['jamin', 'Newman', 'added', 'new', 'Predictors', 'to', 'support']\n",
      "['use', '-', 'cases', 'like', 'those', 'in', 'the', 'Vignette', '(', '§', '4', ')', '.', 'Joseph']\n",
      "['use', '-', 'cases', 'like', 'those', 'in', 'the', 'Vignette', '(', '§', '4', ')', '.', 'Joseph']\n",
      "['Chee', 'Chang', 'implemented', 'an', 'end', '-', 'to', '-', 'end', 'web', '-', 'based']\n",
      "['visual', 'interface', 'for', 'papermage', 'and', 'helped', 'iterate']\n",
      "['on', 'papermage', '’s', 'designs', '.', 'All', 'core', 'contributors']\n",
      "['on', 'papermage', '’s', 'designs', '.', 'All', 'core', 'contributors']\n",
      "['helped', 'with', 'writing', '.', 'Finally', ',', 'Kyle', 'Lo', 'led', 'all', 'aspects']\n",
      "['helped', 'with', 'writing', '.', 'Finally', ',', 'Kyle', 'Lo', 'led', 'all', 'aspects']\n",
      "['of', 'the', 'project', ',', 'including', 'design', 'and', 'implementa', '-']\n",
      "['tion', ',', 'as', 'well', 'as', 'mentorship', 'of', 'other', 'contributors', 'to']\n",
      "['the', 'toolkit', '(', 'see', 'below', ')', '.']\n",
      "['Other', 'contributors', '.']\n",
      "['Russell', 'Authur', ',', 'Stefan', 'Can', '-']\n",
      "['dra', ',', 'Yoganand', 'Chandrasekhar', ',', 'Regan', 'Huff', ',', 'Aman', '-']\n",
      "['preet', 'Singh', 'and', 'Angele', 'Zamarron', 'each', 'worked']\n",
      "['closely', 'with', 'Kyle', 'Lo', 'to', 'contribute', 'a', 'Predictor']\n",
      "['to', 'papermage', '.', 'Erin', 'Bransom', 'and', 'Bailey', 'Kuehl']\n",
      "['to', 'papermage', '.', 'Erin', 'Bransom', 'and', 'Bailey', 'Kuehl']\n",
      "['helped', 'with', 'data', 'annotation', 'for', 'training', 'and', 'evalu', '-']\n",
      "['ating', 'those', 'Predictors', '.', 'Chris', 'Wilhelm', 'provided']\n",
      "['ating', 'those', 'Predictors', '.', 'Chris', 'Wilhelm', 'provided']\n",
      "['feedback', 'on', 'papermage', '’s', 'design', 'and', 'implemented']\n",
      "['faster', 'indexing', 'of', 'Entities', 'when', 'building', 'Layers', '.']\n",
      "['Finally', ',', 'Marti', 'Hearst', ',', 'Daniel', 'Weld', ',', 'and', 'Doug']\n",
      "['Downey', 'helped', 'with', 'writing', 'and', 'overall', 'advising']\n",
      "['on', 'the', 'project', '.']\n",
      "['References']\n",
      "['Waleed', 'Ammar', ',', 'Dirk', 'Groeneveld', ',', 'Chandra', 'Bhagavat', '-']\n",
      "['ula', ',', 'Iz', 'Beltagy', ',', 'Miles', 'Crawford', ',', 'Doug', 'Downey', ',', 'Ja', '-']\n",
      "['son', 'Dunkelberger', ',', 'Ahmed', 'Elgohary', ',', 'Sergey', 'Feld', '-']\n",
      "['man', ',', 'Vu', 'Ha', ',', 'Rodney', 'Kinney', ',', 'Sebastian', 'Kohlmeier', ',']\n",
      "['Kyle', 'Lo', ',', 'Tyler', 'Murray', ',', 'Hsu', '-', 'Han', 'Ooi', ',', 'Matthew', 'Pe', '-']\n",
      "['ters', ',', 'Joanna', 'Power', ',', 'Sam', 'Skjonsberg', ',', 'Lucy', 'Lu', 'Wang', ',']\n",
      "['Chris', 'Wilhelm', ',', 'Zheng', 'Yuan', ',', 'Madeleine', 'van', 'Zuylen', ',']\n",
      "['and', 'Oren', 'Etzioni', '.', '2018', '.', 'Construction', 'of', 'the', 'litera', '-']\n",
      "['and', 'Oren', 'Etzioni', '.', '2018', '.', 'Construction', 'of', 'the', 'litera', '-']\n",
      "['and', 'Oren', 'Etzioni', '.', '2018', '.', 'Construction', 'of', 'the', 'litera', '-']\n",
      "['ture', 'graph', 'in', 'semantic', 'scholar', '.', 'In', 'Proceedings', 'of']\n",
      "['ture', 'graph', 'in', 'semantic', 'scholar', '.', 'In', 'Proceedings', 'of']\n",
      "['the', '2018', 'Conference', 'of', 'the', 'North', 'American', 'Chap', '-']\n",
      "['ter', 'of', 'the', 'Association', 'for', 'Computational', 'Linguistics', ':']\n",
      "['Human', 'Language', 'Technologies', ',', 'Volume', '3', '(', 'Indus', '-']\n",
      "['try', 'Papers', ')', ',', 'pages', '84', '–', '91', ',', 'New', 'Orleans', '-', 'Louisiana', '.']\n",
      "['Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Tal', 'August', ',', 'Lucy', 'Lu', 'Wang', ',', 'Jonathan', 'Bragg', ',', 'Marti', 'A', '.']\n",
      "['Hearst', ',', 'Andrew', 'Head', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'Paper']\n",
      "['Hearst', ',', 'Andrew', 'Head', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'Paper']\n",
      "['Hearst', ',', 'Andrew', 'Head', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'Paper']\n",
      "['plain', ':', 'Making', 'medical', 'research', 'papers', 'approachable']\n",
      "['to', 'healthcare', 'consumers', 'with', 'natural', 'language', 'pro', '-']\n",
      "['cessing', '.', 'ACM', 'Trans', '.', 'Comput', '.', '-', 'Hum', '.', 'Interact', '.', ',', '30', '(', '5', ')', '.']\n",
      "['cessing', '.', 'ACM', 'Trans', '.', 'Comput', '.', '-', 'Hum', '.', 'Interact', '.', ',', '30', '(', '5', ')', '.']\n",
      "['cessing', '.', 'ACM', 'Trans', '.', 'Comput', '.', '-', 'Hum', '.', 'Interact', '.', ',', '30', '(', '5', ')', '.']\n",
      "['cessing', '.', 'ACM', 'Trans', '.', 'Comput', '.', '-', 'Hum', '.', 'Interact', '.', ',', '30', '(', '5', ')', '.']\n",
      "['cessing', '.', 'ACM', 'Trans', '.', 'Comput', '.', '-', 'Hum', '.', 'Interact', '.', ',', '30', '(', '5', ')', '.']\n",
      "['cessing', '.', 'ACM', 'Trans', '.', 'Comput', '.', '-', 'Hum', '.', 'Interact', '.', ',', '30', '(', '5', ')', '.']\n",
      "['Iz', 'Beltagy', ',', 'Kyle', 'Lo', ',', 'and', 'Arman', 'Cohan', '.', '2019', '.', 'SciB', '-']\n",
      "['Iz', 'Beltagy', ',', 'Kyle', 'Lo', ',', 'and', 'Arman', 'Cohan', '.', '2019', '.', 'SciB', '-']\n",
      "['Iz', 'Beltagy', ',', 'Kyle', 'Lo', ',', 'and', 'Arman', 'Cohan', '.', '2019', '.', 'SciB', '-']\n",
      "['ERT', ':', 'A', 'pretrained', 'language', 'model', 'for', 'scientific', 'text', '.']\n",
      "['In', 'Proceedings', 'of', 'the', '2019', 'Conference', 'on', 'Empirical']\n",
      "['Methods', 'in', 'Natural', 'Language', 'Processing', 'and', 'the']\n",
      "['9th', 'International', 'Joint', 'Conference', 'on', 'Natural', 'Lan', '-']\n",
      "['guage', 'Processing', '(', 'EMNLP', '-', 'IJCNLP', ')', ',', 'pages', '3615', '–']\n",
      "['3620', ',', 'Hong', 'Kong', ',', 'China', '.', 'Association', 'for', 'Computa', '-']\n",
      "['3620', ',', 'Hong', 'Kong', ',', 'China', '.', 'Association', 'for', 'Computa', '-']\n",
      "['tional', 'Linguistics', '.']\n",
      "['Tom', 'Brown', ',', 'Benjamin', 'Mann', ',', 'Nick', 'Ryder', ',', 'Melanie']\n",
      "['Subbiah', ',', 'Jared', 'D', 'Kaplan', ',', 'Prafulla', 'Dhariwal', ',', 'Arvind']\n",
      "['Neelakantan', ',', 'Pranav', 'Shyam', ',', 'Girish', 'Sastry', ',', 'Amanda']\n",
      "['Askell', ',', 'Sandhini', 'Agarwal', ',', 'Ariel', 'Herbert', '-', 'Voss', ',']\n",
      "['Gretchen', 'Krueger', ',', 'Tom', 'Henighan', ',', 'Rewon', 'Child', ',']\n",
      "['Aditya', 'Ramesh', ',', 'Daniel', 'Ziegler', ',', 'Jeffrey', 'Wu', ',', 'Clemens']\n",
      "['Winter', ',', 'Chris', 'Hesse', ',', 'Mark', 'Chen', ',', 'Eric', 'Sigler', ',', 'Ma', '-']\n",
      "['teusz', 'Litwin', ',', 'Scott', 'Gray', ',', 'Benjamin', 'Chess', ',', 'Jack']\n",
      "['Clark', ',', 'Christopher', 'Berner', ',', 'Sam', 'McCandlish', ',', 'Alec']\n",
      "['Radford', ',', 'Ilya', 'Sutskever', ',', 'and', 'Dario', 'Amodei', '.', '2020', '.']\n",
      "['Radford', ',', 'Ilya', 'Sutskever', ',', 'and', 'Dario', 'Amodei', '.', '2020', '.']\n",
      "['Language', 'models', 'are', 'few', '-', 'shot', 'learners', '.']\n",
      "['In', 'Ad', '-']\n",
      "['vances', 'in', 'Neural', 'Information', 'Processing', 'Systems', ',']\n",
      "['volume', '33', ',', 'pages', '1877', '–', '1901', '.', 'Curran', 'Associates', ',']\n",
      "['volume', '33', ',', 'pages', '1877', '–', '1901', '.', 'Curran', 'Associates', ',']\n",
      "['Inc', '.']\n",
      "['Joseph', 'Chee', 'Chang', ',', 'Saleema', 'Amershi', ',', 'and', 'Ece', 'Kamar', '.']\n",
      "['2017', '.', 'Revolt', ':', 'Collaborative', 'crowdsourcing', 'for', 'label', '-']\n",
      "['2017', '.', 'Revolt', ':', 'Collaborative', 'crowdsourcing', 'for', 'label', '-']\n",
      "['ing', 'machine', 'learning', 'datasets', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['ing', 'machine', 'learning', 'datasets', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['2017', 'CHI', 'Conference', 'on', 'Human', 'Factors', 'in', 'Comput', '-']\n",
      "['ing', 'Systems', ',', 'CHI', '’17', ',', 'page', '2334', '–', '2346', ',', 'New', 'York', ',']\n",
      "['NY', ',', 'USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['NY', ',', 'USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['Joseph', 'Chee', 'Chang', ',', 'Amy', 'X', '.', 'Zhang', ',', 'Jonathan', 'Bragg', ',']\n",
      "['Joseph', 'Chee', 'Chang', ',', 'Amy', 'X', '.', 'Zhang', ',', 'Jonathan', 'Bragg', ',']\n",
      "['Andrew', 'Head', ',', 'Kyle', 'Lo', ',', 'Doug', 'Downey', ',', 'and', 'Daniel', 'S', '.']\n",
      "['Weld', '.', '2023', '.', 'Citesee', ':', 'Augmenting', 'citations', 'in', 'scien', '-']\n",
      "['Weld', '.', '2023', '.', 'Citesee', ':', 'Augmenting', 'citations', 'in', 'scien', '-']\n",
      "['Weld', '.', '2023', '.', 'Citesee', ':', 'Augmenting', 'citations', 'in', 'scien', '-']\n",
      "['tific', 'papers', 'with', 'persistent', 'and', 'personalized', 'historical']\n",
      "['context', '.', 'In', 'Proceedings', 'of', 'the', '2023', 'CHI', 'Conference']\n",
      "['context', '.', 'In', 'Proceedings', 'of', 'the', '2023', 'CHI', 'Conference']\n",
      "['on', 'Human', 'Factors', 'in', 'Computing', 'Systems', ',', 'CHI', '’23', ',']\n",
      "['New', 'York', ',', 'NY', ',', 'USA', '.', 'Association', 'for', 'Computing']\n",
      "['New', 'York', ',', 'NY', ',', 'USA', '.', 'Association', 'for', 'Computing']\n",
      "['Machinery', '.']\n",
      "['Catherine', 'Chen', ',', 'Zejiang', 'Shen', ',', 'Dan', 'Klein', ',', 'Gabriel']\n",
      "['Stanovsky', ',', 'Doug', 'Downey', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'Are']\n",
      "['Stanovsky', ',', 'Doug', 'Downey', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'Are']\n",
      "['Stanovsky', ',', 'Doug', 'Downey', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'Are']\n",
      "['layout', '-', 'infused', 'language', 'models', 'robust', 'to', 'layout', 'dis', '-']\n",
      "['tribution', 'shifts', '?', 'a', 'case', 'study', 'with', 'scientific', 'docu', '-']\n",
      "['tribution', 'shifts', '?', 'a', 'case', 'study', 'with', 'scientific', 'docu', '-']\n",
      "['ments', '.', 'In', 'Findings', 'of', 'the', 'Association', 'for', 'Computa', '-']\n",
      "['ments', '.', 'In', 'Findings', 'of', 'the', 'Association', 'for', 'Computa', '-']\n",
      "['tional', 'Linguistics', ':', 'ACL', '2023', ',', 'pages', '13345', '–', '13360', ',']\n",
      "['Toronto', ',', 'Canada', '.', 'Association', 'for', 'Computational', 'Lin', '-']\n",
      "['Toronto', ',', 'Canada', '.', 'Association', 'for', 'Computational', 'Lin', '-']\n",
      "['guistics', '.']\n",
      "['Isaac', 'Councill', ',', 'C', '.', 'Lee', 'Giles', ',', 'and', 'Min', '-', 'Yen', 'Kan', '.', '2008', '.']\n",
      "['Isaac', 'Councill', ',', 'C', '.', 'Lee', 'Giles', ',', 'and', 'Min', '-', 'Yen', 'Kan', '.', '2008', '.']\n",
      "['Isaac', 'Councill', ',', 'C', '.', 'Lee', 'Giles', ',', 'and', 'Min', '-', 'Yen', 'Kan', '.', '2008', '.']\n",
      "['ParsCit', ':', 'an', 'open', '-', 'source', 'CRF', 'reference', 'string', 'pars', '-']\n",
      "['ing', 'package', '.', 'In', 'Proceedings', 'of', 'the', 'Sixth', 'Interna', '-']\n",
      "['ing', 'package', '.', 'In', 'Proceedings', 'of', 'the', 'Sixth', 'Interna', '-']\n",
      "['tional', 'Conference', 'on', 'Language', 'Resources', 'and', 'Eval', '-']\n",
      "['uation', '(', 'LREC’08', ')', ',', 'Marrakech', ',', 'Morocco', '.', 'European']\n",
      "['uation', '(', 'LREC’08', ')', ',', 'Marrakech', ',', 'Morocco', '.', 'European']\n",
      "['Language', 'Resources', 'Association', '(', 'ELRA', ')', '.']\n",
      "['Pradeep', 'Dasigi', ',', 'Kyle', 'Lo', ',', 'Iz', 'Beltagy', ',', 'Arman', 'Cohan', ',']\n",
      "['Noah', 'A', '.', 'Smith', ',', 'and', 'Matt', 'Gardner', '.', '2021', '.', 'A', 'dataset']\n",
      "['Noah', 'A', '.', 'Smith', ',', 'and', 'Matt', 'Gardner', '.', '2021', '.', 'A', 'dataset']\n",
      "['Noah', 'A', '.', 'Smith', ',', 'and', 'Matt', 'Gardner', '.', '2021', '.', 'A', 'dataset']\n",
      "['Noah', 'A', '.', 'Smith', ',', 'and', 'Matt', 'Gardner', '.', '2021', '.', 'A', 'dataset']\n",
      "['of', 'information', '-', 'seeking', 'questions', 'and', 'answers', 'an', '-']\n",
      "['chored', 'in', 'research', 'papers', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['chored', 'in', 'research', 'papers', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['2021', 'Conference', 'of', 'the', 'North', 'American', 'Chapter', 'of']\n",
      "['the', 'Association', 'for', 'Computational', 'Linguistics', ':', 'Hu', '-']\n",
      "['man', 'Language', 'Technologies', ',', 'pages', '4599', '–', '4610', ',', 'On', '-']\n",
      "['line', '.', 'Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['line', '.', 'Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Raymond', 'Fok', ',', 'Joseph', 'Chee', 'Chang', ',', 'Tal', 'August', ',', 'Amy', 'X', '.']\n",
      "['Zhang', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023a', '.', 'Qlarify', ':', 'Bridg', '-']\n",
      "['Zhang', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023a', '.', 'Qlarify', ':', 'Bridg', '-']\n",
      "['Zhang', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023a', '.', 'Qlarify', ':', 'Bridg', '-']\n",
      "['Zhang', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023a', '.', 'Qlarify', ':', 'Bridg', '-']\n",
      "['ing', 'scholarly', 'abstracts', 'and', 'papers', 'with', 'recursively']\n",
      "['expandable', 'summaries', '.', 'arXiv', ',', 'abs', '/', '2310', '.', '07581', '.']\n",
      "['expandable', 'summaries', '.', 'arXiv', ',', 'abs', '/', '2310', '.', '07581', '.']\n",
      "['expandable', 'summaries', '.', 'arXiv', ',', 'abs', '/', '2310', '.', '07581', '.']\n",
      "['Raymond', 'Fok', ',', 'Hita', 'Kambhamettu', ',', 'Luca', 'Soldaini', ',']\n",
      "['Jonathan', 'Bragg', ',', 'Kyle', 'Lo', ',', 'Marti', 'Hearst', ',', 'Andrew']\n",
      "['Head', ',', 'and', 'Daniel', 'S', 'Weld', '.', '2023b', '.', 'Scim', ':', 'Intelligent']\n",
      "['Head', ',', 'and', 'Daniel', 'S', 'Weld', '.', '2023b', '.', 'Scim', ':', 'Intelligent']\n",
      "['Head', ',', 'and', 'Daniel', 'S', 'Weld', '.', '2023b', '.', 'Scim', ':', 'Intelligent']\n",
      "['skimming', 'support', 'for', 'scientific', 'papers', '.', 'In', 'Proceed', '-']\n",
      "['skimming', 'support', 'for', 'scientific', 'papers', '.', 'In', 'Proceed', '-']\n",
      "['ings', 'of', 'the', '28th', 'International', 'Conference', 'on', 'Intelli', '-']\n",
      "['gent', 'User', 'Interfaces', ',', 'IUI', '’23', ',', 'page', '476', '–', '490', ',', 'New']\n",
      "['York', ',', 'NY', ',', 'USA', '.', 'Association', 'for', 'Computing', 'Machin', '-']\n",
      "['York', ',', 'NY', ',', 'USA', '.', 'Association', 'for', 'Computing', 'Machin', '-']\n",
      "['ery', '.']\n",
      "['Grobid', '.', '2008', '–', '2023', '.', 'Grobid', '.', 'https', ':', '/', '/', 'github', '.', 'com', '/']\n",
      "['Grobid', '.', '2008', '–', '2023', '.', 'Grobid', '.', 'https', ':', '/', '/', 'github', '.', 'com', '/']\n",
      "['Grobid', '.', '2008', '–', '2023', '.', 'Grobid', '.', 'https', ':', '/', '/', 'github', '.', 'com', '/']\n",
      "['Grobid', '.', '2008', '–', '2023', '.', 'Grobid', '.', 'https', ':', '/', '/', 'github', '.', 'com', '/']\n",
      "['Grobid', '.', '2008', '–', '2023', '.', 'Grobid', '.', 'https', ':', '/', '/', 'github', '.', 'com', '/']\n",
      "['kermitt2', '/', 'grobid', '.']\n",
      "['Yu', 'Gu', ',', 'Robert', 'Tinn', ',', 'Hao', 'Cheng', ',', 'Michael', 'R', '.', 'Lucas', ',']\n",
      "['Yu', 'Gu', ',', 'Robert', 'Tinn', ',', 'Hao', 'Cheng', ',', 'Michael', 'R', '.', 'Lucas', ',']\n",
      "['Naoto', 'Usuyama', ',', 'Xiaodong', 'Liu', ',', 'Tristan', 'Naumann', ',']\n",
      "['Jianfeng', 'Gao', ',', 'and', 'Hoifung', 'Poon', '.', '2020', '.', 'Domain', '-']\n",
      "['Jianfeng', 'Gao', ',', 'and', 'Hoifung', 'Poon', '.', '2020', '.', 'Domain', '-']\n",
      "['Jianfeng', 'Gao', ',', 'and', 'Hoifung', 'Poon', '.', '2020', '.', 'Domain', '-']\n",
      "['specific', 'language', 'model', 'pretraining', 'for', 'biomedical']\n",
      "['natural', 'language', 'processing', '.', 'ACM', 'Transactions', 'on']\n",
      "['natural', 'language', 'processing', '.', 'ACM', 'Transactions', 'on']\n",
      "['Computing', 'for', 'Healthcare', '(', 'HEALTH', ')', ',', '3', ':', '1', '–', '23', '.']\n",
      "['Andrew', 'Head', ',', 'Kyle', 'Lo', ',', 'Dongyeop', 'Kang', ',', 'Raymond']\n",
      "['Fok', ',', 'Sam', 'Skjonsberg', ',', 'Daniel', 'S', '.', 'Weld', ',', 'and', 'Marti', 'A', '.']\n",
      "['Fok', ',', 'Sam', 'Skjonsberg', ',', 'Daniel', 'S', '.', 'Weld', ',', 'and', 'Marti', 'A', '.']\n",
      "['Hearst', '.', '2021', '.', 'Augmenting', 'scientific', 'papers', 'with', 'just', '-']\n",
      "['Hearst', '.', '2021', '.', 'Augmenting', 'scientific', 'papers', 'with', 'just', '-']\n",
      "['Hearst', '.', '2021', '.', 'Augmenting', 'scientific', 'papers', 'with', 'just', '-']\n",
      "['in', '-', 'time', ',', 'position', '-', 'sensitive', 'definitions', 'of', 'terms', 'and']\n",
      "['symbols', '.', 'In', 'Proceedings', 'of', 'the', '2021', 'CHI', 'Conference']\n",
      "['symbols', '.', 'In', 'Proceedings', 'of', 'the', '2021', 'CHI', 'Conference']\n",
      "['on', 'Human', 'Factors', 'in', 'Computing', 'Systems', ',', 'CHI', '’21', ',']\n",
      "['New', 'York', ',', 'NY', ',', 'USA', '.', 'Association', 'for', 'Computing']\n",
      "['New', 'York', ',', 'NY', ',', 'USA', '.', 'Association', 'for', 'Computing']\n",
      "['Machinery', '.']\n",
      "['Zhi', 'Hong', ',', 'Aswathy', 'Ajith', ',', 'James', 'Pauloski', ',', 'Eamon']\n",
      "['Duede', ',', 'Kyle', 'Chard', ',', 'and', 'Ian', 'Foster', '.', '2023', '.', 'The', 'dimin', '-']\n",
      "['Duede', ',', 'Kyle', 'Chard', ',', 'and', 'Ian', 'Foster', '.', '2023', '.', 'The', 'dimin', '-']\n",
      "['Duede', ',', 'Kyle', 'Chard', ',', 'and', 'Ian', 'Foster', '.', '2023', '.', 'The', 'dimin', '-']\n",
      "['ishing', 'returns', 'of', 'masked', 'language', 'models', 'to', 'science', '.']\n",
      "['In', 'Findings', 'of', 'the', 'Association', 'for', 'Computational']\n",
      "['Linguistics', ':', 'ACL', '2023', ',', 'pages', '1270', '–', '1283', ',', 'Toronto', ',']\n",
      "['Canada', '.', 'Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Canada', '.', 'Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Po', '-', 'Wei', 'Huang', ',', 'Abhinav', 'Ramesh', 'Kashyap', ',', 'Yanxia', 'Qin', ',']\n",
      "['Yajing', 'Yang', ',', 'and', 'Min', '-', 'Yen', 'Kan', '.', '2022a', '.', 'Lightweight']\n",
      "['Yajing', 'Yang', ',', 'and', 'Min', '-', 'Yen', 'Kan', '.', '2022a', '.', 'Lightweight']\n",
      "['Yajing', 'Yang', ',', 'and', 'Min', '-', 'Yen', 'Kan', '.', '2022a', '.', 'Lightweight']\n",
      "['contextual', 'logical', 'structure', 'recovery', '.', 'In', 'Proceedings']\n",
      "['contextual', 'logical', 'structure', 'recovery', '.', 'In', 'Proceedings']\n",
      "['of', 'the', 'Third', 'Workshop', 'on', 'Scholarly', 'Document', 'Pro', '-']\n",
      "['cessing', ',', 'pages', '37', '–', '48', ',', 'Gyeongju', ',', 'Republic', 'of', 'Korea', '.']\n",
      "['Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Yupan', 'Huang', ',', 'Tengchao', 'Lv', ',', 'Lei', 'Cui', ',', 'Yutong', 'Lu', ',', 'and']\n",
      "['Furu', 'Wei', '.', '2022b', '.', 'Layoutlmv3', ':', 'Pre', '-', 'training', 'for', 'doc', '-']\n",
      "['Furu', 'Wei', '.', '2022b', '.', 'Layoutlmv3', ':', 'Pre', '-', 'training', 'for', 'doc', '-']\n",
      "['Furu', 'Wei', '.', '2022b', '.', 'Layoutlmv3', ':', 'Pre', '-', 'training', 'for', 'doc', '-']\n",
      "['ument', 'ai', 'with', 'unified', 'text', 'and', 'image', 'masking', '.', 'Pro', '-']\n",
      "['ument', 'ai', 'with', 'unified', 'text', 'and', 'image', 'masking', '.', 'Pro', '-']\n",
      "['ceedings', 'of', 'the', '30th', 'ACM', 'International', 'Conference']\n",
      "['on', 'Multimedia', '.']\n",
      "['Gautier', 'Izacard', ',', 'Mathilde', 'Caron', ',', 'Lucas', 'Hosseini', ',', 'Sebas', '-']\n",
      "['tian', 'Riedel', ',', 'Piotr', 'Bojanowski', ',', 'Armand', 'Joulin', ',', 'and']\n",
      "['Edouard', 'Grave', '.', '2022', '.', 'Unsupervised', 'dense', 'informa', '-']\n",
      "['Edouard', 'Grave', '.', '2022', '.', 'Unsupervised', 'dense', 'informa', '-']\n",
      "['Edouard', 'Grave', '.', '2022', '.', 'Unsupervised', 'dense', 'informa', '-']\n",
      "['tion', 'retrieval', 'with', 'contrastive', 'learning', '.', 'Transactions']\n",
      "['tion', 'retrieval', 'with', 'contrastive', 'learning', '.', 'Transactions']\n",
      "['on', 'Machine', 'Learning', 'Research', '.']\n",
      "['Sarthak', 'Jain', ',', 'Madeleine', 'van', 'Zuylen', ',', 'Hannaneh', 'Ha', '-']\n",
      "['jishirzi', ',', 'and', 'Iz', 'Beltagy', '.', '2020', '.', 'SciREX', ':', 'A', 'challenge']\n",
      "['jishirzi', ',', 'and', 'Iz', 'Beltagy', '.', '2020', '.', 'SciREX', ':', 'A', 'challenge']\n",
      "['jishirzi', ',', 'and', 'Iz', 'Beltagy', '.', '2020', '.', 'SciREX', ':', 'A', 'challenge']\n",
      "['dataset', 'for', 'document', '-', 'level', 'information', 'extraction', '.', 'In']\n",
      "['dataset', 'for', 'document', '-', 'level', 'information', 'extraction', '.', 'In']\n",
      "['Proceedings', 'of', 'the', '58th', 'Annual', 'Meeting', 'of', 'the', 'Asso', '-']\n",
      "['ciation', 'for', 'Computational', 'Linguistics', ',', 'pages', '7506', '–']\n",
      "['7516', ',', 'Online', '.', 'Association', 'for', 'Computational', 'Lin', '-']\n",
      "['7516', ',', 'Online', '.', 'Association', 'for', 'Computational', 'Lin', '-']\n",
      "['guistics', '.']\n",
      "['Hyeonsu', 'B', '.', 'Kang', ',', 'Joseph', 'Chee', 'Chang', ',', 'Yongsung', 'Kim', ',']\n",
      "['Hyeonsu', 'B', '.', 'Kang', ',', 'Joseph', 'Chee', 'Chang', ',', 'Yongsung', 'Kim', ',']\n",
      "['and', 'Aniket', 'Kittur', '.', '2022', '.', 'Threddy', ':', 'An', 'interactive']\n",
      "['and', 'Aniket', 'Kittur', '.', '2022', '.', 'Threddy', ':', 'An', 'interactive']\n",
      "['and', 'Aniket', 'Kittur', '.', '2022', '.', 'Threddy', ':', 'An', 'interactive']\n",
      "['system', 'for', 'personalized', 'thread', '-', 'based', 'exploration', 'and']\n",
      "['organization', 'of', 'scientific', 'literature', '.', 'In', 'Proceedings', 'of']\n",
      "['organization', 'of', 'scientific', 'literature', '.', 'In', 'Proceedings', 'of']\n",
      "['the', '35th', 'Annual', 'ACM', 'Symposium', 'on', 'User', 'Interface']\n",
      "['Software', 'and', 'Technology', ',', 'UIST', '’22', ',', 'New', 'York', ',', 'NY', ',']\n",
      "['USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['Hyeonsu', 'B', '.', 'Kang', ',', 'Sherry', 'Tongshuang', 'Wu', ',', 'Joseph', 'Chee']\n",
      "['Hyeonsu', 'B', '.', 'Kang', ',', 'Sherry', 'Tongshuang', 'Wu', ',', 'Joseph', 'Chee']\n",
      "['Chang', ',', 'and', 'Aniket', 'Kittur', '.', '2023', '.', 'Synergi', ':', 'A', 'mixed', '-']\n",
      "['Chang', ',', 'and', 'Aniket', 'Kittur', '.', '2023', '.', 'Synergi', ':', 'A', 'mixed', '-']\n",
      "['Chang', ',', 'and', 'Aniket', 'Kittur', '.', '2023', '.', 'Synergi', ':', 'A', 'mixed', '-']\n",
      "['initiative', 'system', 'for', 'scholarly', 'synthesis', 'and', 'sense', '-']\n",
      "['making', '.', 'In', 'Proceedings', 'of', 'the', '36th', 'Annual', 'ACM']\n",
      "['making', '.', 'In', 'Proceedings', 'of', 'the', '36th', 'Annual', 'ACM']\n",
      "['Symposium', 'on', 'User', 'Interface', 'Software', 'and', 'Technol', '-']\n",
      "['ogy', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['ogy', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['Tae', 'Soo', 'Kim', ',', 'Matt', 'Latzke', ',', 'Jonathan', 'Bragg', ',', 'Amy', 'X', '.']\n",
      "['Zhang', ',', 'and', 'Joseph', 'Chee', 'Chang', '.', '2023', '.', 'Papeos', ':', 'Aug', '-']\n",
      "['Zhang', ',', 'and', 'Joseph', 'Chee', 'Chang', '.', '2023', '.', 'Papeos', ':', 'Aug', '-']\n",
      "['Zhang', ',', 'and', 'Joseph', 'Chee', 'Chang', '.', '2023', '.', 'Papeos', ':', 'Aug', '-']\n",
      "['menting', 'research', 'papers', 'with', 'talk', 'videos', '.', 'In', 'Proceed', '-']\n",
      "['menting', 'research', 'papers', 'with', 'talk', 'videos', '.', 'In', 'Proceed', '-']\n",
      "['ings', 'of', 'the', '36th', 'Annual', 'ACM', 'Symposium', 'on', 'User']\n",
      "['Interface', 'Software', 'and', 'Technology', '.']\n",
      "['Rodney', 'Kinney', ',', 'Chloe', 'Anastasiades', ',', 'Russell', 'Authur', ',']\n",
      "['Iz', 'Beltagy', ',', 'Jonathan', 'Bragg', ',', 'Alexandra', 'Buraczyn', '-']\n",
      "['ski', ',', 'Isabel', 'Cachola', ',', 'Stefan', 'Candra', ',', 'Yoganand', 'Chan', '-']\n",
      "['drasekhar', ',', 'Arman', 'Cohan', ',', 'Miles', 'Crawford', ',', 'Doug']\n",
      "['Downey', ',', 'Jason', 'Dunkelberger', ',', 'Oren', 'Etzioni', ',', 'Rob']\n",
      "['Evans', ',', 'Sergey', 'Feldman', ',', 'Joseph', 'Gorney', ',', 'David', 'Gra', '-']\n",
      "['ham', ',', 'Fangzhou', 'Hu', ',', 'Regan', 'Huff', ',', 'Daniel', 'King', ',', 'Se', '-']\n",
      "['bastian', 'Kohlmeier', ',', 'Bailey', 'Kuehl', ',', 'Michael', 'Langan', ',']\n",
      "['Daniel', 'Lin', ',', 'Haokun', 'Liu', ',', 'Kyle', 'Lo', ',', 'Jaron', 'Lochner', ',']\n",
      "['Kelsey', 'MacMillan', ',', 'Tyler', 'Murray', ',', 'Chris', 'Newell', ',']\n",
      "['Smita', 'Rao', ',', 'Shaurya', 'Rohatgi', ',', 'Paul', 'Sayre', ',', 'Zejiang']\n",
      "['Shen', ',', 'Amanpreet', 'Singh', ',', 'Luca', 'Soldaini', ',', 'Shivashankar']\n",
      "['Subramanian', ',', 'Amber', 'Tanaka', ',', 'Alex', 'D', '.', 'Wade', ',', 'Linda']\n",
      "['Subramanian', ',', 'Amber', 'Tanaka', ',', 'Alex', 'D', '.', 'Wade', ',', 'Linda']\n",
      "['Wagner', ',', 'Lucy', 'Lu', 'Wang', ',', 'Chris', 'Wilhelm', ',', 'Caroline', 'Wu', ',']\n",
      "['Jiangjiang', 'Yang', ',', 'Angele', 'Zamarron', ',', 'Madeleine', 'Van']\n",
      "['Zuylen', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Zuylen', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Zuylen', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Zuylen', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Scholar', 'Open', 'Data', 'Platform', '.', 'ArXiv', ',', 'abs', '/', '2301', '.', '10140', '.']\n",
      "['Scholar', 'Open', 'Data', 'Platform', '.', 'ArXiv', ',', 'abs', '/', '2301', '.', '10140', '.']\n",
      "['Scholar', 'Open', 'Data', 'Platform', '.', 'ArXiv', ',', 'abs', '/', '2301', '.', '10140', '.']\n",
      "['Wei', '-', 'Jen', 'Ko', ',', 'Te', '-', 'yuan', 'Chen', ',', 'Yiyan', 'Huang', ',', 'Greg', 'Durrett', ',']\n",
      "['and', 'Junyi', 'Jessy', 'Li', '.', '2020', '.', 'Inquisitive', 'question', 'gener', '-']\n",
      "['and', 'Junyi', 'Jessy', 'Li', '.', '2020', '.', 'Inquisitive', 'question', 'gener', '-']\n",
      "['and', 'Junyi', 'Jessy', 'Li', '.', '2020', '.', 'Inquisitive', 'question', 'gener', '-']\n",
      "['ation', 'for', 'high', 'level', 'text', 'comprehension', '.', 'In', 'Proceed', '-']\n",
      "['ation', 'for', 'high', 'level', 'text', 'comprehension', '.', 'In', 'Proceed', '-']\n",
      "['ings', 'of', 'the', '2020', 'Conference', 'on', 'Empirical', 'Methods']\n",
      "['in', 'Natural', 'Language', 'Processing', '(', 'EMNLP', ')', ',', 'pages']\n",
      "['6544', '–', '6555', ',', 'Online', '.', 'Association', 'for', 'Computational']\n",
      "['6544', '–', '6555', ',', 'Online', '.', 'Association', 'for', 'Computational']\n",
      "['Linguistics', '.']\n",
      "['Benjamin', 'Charles', 'Germain', 'Lee', ',', 'Jaime', 'Mears', ',', 'Eileen']\n",
      "['Jakeway', ',', 'Meghan', 'Ferriter', ',', 'Chris', 'Adams', ',', 'Nathan']\n",
      "['Yarasavage', ',', 'Deborah', 'Thomas', ',', 'Kate', 'Zwaard', ',', 'and']\n",
      "['Daniel', 'S', '.', 'Weld', '.', '2020', '.', 'The', 'newspaper', 'navigator']\n",
      "['Daniel', 'S', '.', 'Weld', '.', '2020', '.', 'The', 'newspaper', 'navigator']\n",
      "['Daniel', 'S', '.', 'Weld', '.', '2020', '.', 'The', 'newspaper', 'navigator']\n",
      "['Daniel', 'S', '.', 'Weld', '.', '2020', '.', 'The', 'newspaper', 'navigator']\n",
      "['dataset', ':', 'Extracting', 'headlines', 'and', 'visual', 'content', 'from']\n",
      "['16', 'million', 'historic', 'newspaper', 'pages', 'in', 'chronicling']\n",
      "['america', '.', 'In', 'Proceedings', 'of', 'the', '29th', 'ACM', 'Interna', '-']\n",
      "['america', '.', 'In', 'Proceedings', 'of', 'the', '29th', 'ACM', 'Interna', '-']\n",
      "['tional', 'Conference', 'on', 'Information', '&', 'Knowledge', 'Man', '-']\n",
      "['agement', ',', 'CIKM', '’20', ',', 'page', '3055', '–', '3062', ',', 'New', 'York', ',']\n",
      "['NY', ',', 'USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['NY', ',', 'USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['Jinhyuk', 'Lee', ',', 'Wonjin', 'Yoon', ',', 'Sungdong', 'Kim', ',', 'Donghyeon']\n",
      "['Kim', ',', 'Sunkyu', 'Kim', ',', 'Chan', 'Ho', 'So', ',', 'and', 'Jaewoo', 'Kang', '.']\n",
      "['2019', '.', 'BioBERT', ':', 'a', 'pre', '-', 'trained', 'biomedical', 'language']\n",
      "['2019', '.', 'BioBERT', ':', 'a', 'pre', '-', 'trained', 'biomedical', 'language']\n",
      "['representation', 'model', 'for', 'biomedical', 'text', 'mining', '.']\n",
      "['Bioinformatics', ',', '36', '(', '4', ')', ':', '1234', '–', '1240', '.']\n",
      "['Yoonjoo', 'Lee', ',', 'Kyungjae', 'Lee', ',', 'Sunghyun', 'Park', ',', 'Dasol']\n",
      "['Hwang', ',', 'Jaehyeon', 'Kim', ',', 'Hong', '-', 'In', 'Lee', ',', 'and', 'Moontae']\n",
      "['Lee', '.', '2023', '.', 'QASA', ':', 'Advanced', 'question', 'answering', 'on']\n",
      "['Lee', '.', '2023', '.', 'QASA', ':', 'Advanced', 'question', 'answering', 'on']\n",
      "['Lee', '.', '2023', '.', 'QASA', ':', 'Advanced', 'question', 'answering', 'on']\n",
      "['scientific', 'articles', '.', 'In', 'Proceedings', 'of', 'the', '40th', 'Inter', '-']\n",
      "['scientific', 'articles', '.', 'In', 'Proceedings', 'of', 'the', '40th', 'Inter', '-']\n",
      "['national', 'Conference', 'on', 'Machine', 'Learning', ',', 'volume']\n",
      "['202', 'of', 'Proceedings', 'of', 'Machine', 'Learning', 'Research', ',']\n",
      "['pages', '19036', '–', '19052', '.', 'PMLR', '.']\n",
      "['pages', '19036', '–', '19052', '.', 'PMLR', '.']\n",
      "['Élise', 'Lincker', ',', 'Olivier', 'Pons', ',', 'Camille', 'Guinaudeau', ',', 'Is', '-']\n",
      "['abelle', 'Barbet', ',', 'Jérôme', 'Dupire', ',', 'Céline', 'Hudelot', ',', 'Vin', '-']\n",
      "['cent', 'Mousseau', ',', 'and', 'Caroline', 'Huron', '.', '2023', '.', 'Layout']\n",
      "['cent', 'Mousseau', ',', 'and', 'Caroline', 'Huron', '.', '2023', '.', 'Layout']\n",
      "['cent', 'Mousseau', ',', 'and', 'Caroline', 'Huron', '.', '2023', '.', 'Layout']\n",
      "['and', 'activity', '-', 'based', 'textbook', 'modeling', 'for', 'automatic']\n",
      "['pdf', 'textbook', 'extraction', '.', 'In', 'iTextbooks', '@', 'AIED', '.']\n",
      "['pdf', 'textbook', 'extraction', '.', 'In', 'iTextbooks', '@', 'AIED', '.']\n",
      "['Yinhan', 'Liu', ',', 'Myle', 'Ott', ',', 'Naman', 'Goyal', ',', 'Jingfei', 'Du', ',', 'Man', '-']\n",
      "['dar', 'Joshi', ',', 'Danqi', 'Chen', ',', 'Omer', 'Levy', ',', 'Mike', 'Lewis', ',']\n",
      "['Luke', 'Zettlemoyer', ',', 'and', 'Veselin', 'Stoyanov', '.', '2019', '.']\n",
      "['Luke', 'Zettlemoyer', ',', 'and', 'Veselin', 'Stoyanov', '.', '2019', '.']\n",
      "['RoBERTa', ':', 'A', 'Robustly', 'Optimized', 'BERT', 'Pretrain', '-']\n",
      "['ing', 'Approach', '.', 'ArXiv', ',', 'abs', '/', '1907', '.', '11692', '.']\n",
      "['ing', 'Approach', '.', 'ArXiv', ',', 'abs', '/', '1907', '.', '11692', '.']\n",
      "['ing', 'Approach', '.', 'ArXiv', ',', 'abs', '/', '1907', '.', '11692', '.']\n",
      "['Kyle', 'Lo', ',', 'Joseph', 'Chee', 'Chang', ',', 'Andrew', 'Head', ',', 'Jonathan']\n",
      "['Bragg', ',', 'Amy', 'X', '.', 'Zhang', ',', 'Cassidy', 'Trier', ',', 'Chloe', 'Anas', '-']\n",
      "['Bragg', ',', 'Amy', 'X', '.', 'Zhang', ',', 'Cassidy', 'Trier', ',', 'Chloe', 'Anas', '-']\n",
      "['tasiades', ',', 'Tal', 'August', ',', 'Russell', 'Authur', ',', 'Danielle', 'Bragg', ',']\n",
      "['Erin', 'Bransom', ',', 'Isabel', 'Cachola', ',', 'Stefan', 'Candra', ',', 'Yo', '-']\n",
      "['ganand', 'Chandrasekhar', ',', 'Yen', '-', 'Sung', 'Chen', ',', 'Evie', 'Yu', '-']\n",
      "['Yen', 'Cheng', ',', 'Yvonne', 'Chou', ',', 'Doug', 'Downey', ',', 'Rob']\n",
      "['Evans', ',', 'Raymond', 'Fok', ',', 'Fangzhou', 'Hu', ',', 'Regan', 'Huff', ',']\n",
      "['Dongyeop', 'Kang', ',', 'Tae', 'Soo', 'Kim', ',', 'Rodney', 'Kinney', ',']\n",
      "['Aniket', 'Kittur', ',', 'Hyeonsu', 'Kang', ',', 'Egor', 'Klevak', ',', 'Bai', '-']\n",
      "['ley', 'Kuehl', ',', 'Michael', 'Langan', ',', 'Matt', 'Latzke', ',', 'Jaron']\n",
      "['Lochner', ',', 'Kelsey', 'MacMillan', ',', 'Eric', 'Marsh', ',', 'Tyler', 'Mur', '-']\n",
      "['ray', ',', 'Aakanksha', 'Naik', ',', 'Ngoc', '-', 'Uyen', 'Nguyen', ',', 'Srishti']\n",
      "['Palani', ',', 'Soya', 'Park', ',', 'Caroline', 'Paulic', ',', 'Napol', 'Rachata', '-']\n",
      "['sumrit', ',', 'Smita', 'Rao', ',', 'Paul', 'Sayre', ',', 'Zejiang', 'Shen', ',', 'Pao']\n",
      "['Siangliulue', ',', 'Luca', 'Soldaini', ',', 'Huy', 'Tran', ',', 'Madeleine', 'van']\n",
      "['Zuylen', ',', 'Lucy', 'Lu', 'Wang', ',', 'Christopher', 'Wilhelm', ',', 'Caro', '-']\n",
      "['line', 'Wu', ',', 'Jiangjiang', 'Yang', ',', 'Angele', 'Zamarron', ',', 'Marti', 'A', '.']\n",
      "['Hearst', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Hearst', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Hearst', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Hearst', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2023', '.', 'The', 'Semantic']\n",
      "['Reader', 'Project', ':', 'Augmenting', 'Scholarly', 'Documents']\n",
      "['through', 'AI', '-', 'Powered', 'Interactive', 'Reading', 'Interfaces', '.']\n",
      "['ArXiv', ',', 'abs', '/', '2303', '.', '14334', '.']\n",
      "['ArXiv', ',', 'abs', '/', '2303', '.', '14334', '.']\n",
      "['Kyle', 'Lo', ',', 'Lucy', 'Lu', 'Wang', ',', 'Mark', 'Neumann', ',', 'Rodney', 'Kin', '-']\n",
      "['ney', ',', 'and', 'Daniel', 'Weld', '.', '2020', '.', 'S2ORC', ':', 'The', 'semantic']\n",
      "['ney', ',', 'and', 'Daniel', 'Weld', '.', '2020', '.', 'S2ORC', ':', 'The', 'semantic']\n",
      "['ney', ',', 'and', 'Daniel', 'Weld', '.', '2020', '.', 'S2ORC', ':', 'The', 'semantic']\n",
      "['scholar', 'open', 'research', 'corpus', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['scholar', 'open', 'research', 'corpus', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['58th', 'Annual', 'Meeting', 'of', 'the', 'Association', 'for', 'Compu', '-']\n",
      "['tational', 'Linguistics', ',', 'pages', '4969', '–', '4983', ',', 'Online', '.', 'Asso', '-']\n",
      "['tational', 'Linguistics', ',', 'pages', '4969', '–', '4983', ',', 'Online', '.', 'Asso', '-']\n",
      "['ciation', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Renqian', 'Luo', ',', 'Liai', 'Sun', ',', 'Yingce', 'Xia', ',', 'Tao', 'Qin', ',', 'Sheng']\n",
      "['Zhang', ',', 'Hoifung', 'Poon', ',', 'and', 'Tie', '-', 'Yan', 'Liu', '.', '2022', '.']\n",
      "['Zhang', ',', 'Hoifung', 'Poon', ',', 'and', 'Tie', '-', 'Yan', 'Liu', '.', '2022', '.']\n",
      "['Biogpt', ':']\n",
      "['Generative', 'pre', '-', 'trained', 'transformer', 'for']\n",
      "['biomedical', 'text', 'generation', 'and', 'mining', '.', 'Briefings']\n",
      "['biomedical', 'text', 'generation', 'and', 'mining', '.', 'Briefings']\n",
      "['in', 'bioinformatics', '.']\n",
      "['Mark', 'Neumann', ',', 'Daniel', 'King', ',', 'Iz', 'Beltagy', ',', 'and', 'Waleed']\n",
      "['Ammar', '.', '2019', '.', 'ScispaCy', ':', 'Fast', 'and', 'robust', 'models']\n",
      "['Ammar', '.', '2019', '.', 'ScispaCy', ':', 'Fast', 'and', 'robust', 'models']\n",
      "['Ammar', '.', '2019', '.', 'ScispaCy', ':', 'Fast', 'and', 'robust', 'models']\n",
      "['for', 'biomedical', 'natural', 'language', 'processing', '.', 'In', 'Pro', '-']\n",
      "['for', 'biomedical', 'natural', 'language', 'processing', '.', 'In', 'Pro', '-']\n",
      "['ceedings', 'of', 'the', '18th', 'BioNLP', 'Workshop', 'and', 'Shared']\n",
      "['Task', ',', 'pages', '319', '–', '327', ',', 'Florence', ',', 'Italy', '.', 'Association', 'for']\n",
      "['Task', ',', 'pages', '319', '–', '327', ',', 'Florence', ',', 'Italy', '.', 'Association', 'for']\n",
      "['Computational', 'Linguistics', '.']\n",
      "['Benjamin', 'Newman', ',', 'Luca', 'Soldaini', ',', 'Raymond', 'Fok', ',', 'Ar', '-']\n",
      "['man', 'Cohan', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'A', 'question', 'answer', '-']\n",
      "['man', 'Cohan', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'A', 'question', 'answer', '-']\n",
      "['man', 'Cohan', ',', 'and', 'Kyle', 'Lo', '.', '2023', '.', 'A', 'question', 'answer', '-']\n",
      "['ing', 'framework', 'for', 'decontextualizing', 'user', '-', 'facing', 'snip', '-']\n",
      "['pets', 'from', 'scientific', 'documents', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['pets', 'from', 'scientific', 'documents', '.', 'In', 'Proceedings', 'of', 'the']\n",
      "['2023', 'Conference', 'on', 'Empirical', 'Methods', 'in', 'Natural']\n",
      "['Language', 'Processing', '(', 'EMNLP', ')', '.']\n",
      "['Organizers', 'of', 'Queer', 'in', 'AI', ',', 'Anaelia', 'Ovalle', ',', 'Arjun', 'Sub', '-']\n",
      "['ramonian', ',', 'Ashwin', 'Singh', ',', 'Claas', 'Voelcker', ',', 'Danica', 'J', '.']\n",
      "['Sutherland', ',', 'Davide', 'Locatelli', ',', 'Eva', 'Breznik', ',', 'Filip', 'Klu', '-']\n",
      "['bicka', ',', 'Hang', 'Yuan', ',', 'Hetvi', 'J', ',', 'Huan', 'Zhang', ',', 'Jaidev']\n",
      "['Shriram', ',', 'Kruno', 'Lehman', ',', 'Luca', 'Soldaini', ',', 'Maarten']\n",
      "['Sap', ',', 'Marc', 'Peter', 'Deisenroth', ',', 'Maria', 'Leonor', 'Pacheco', ',']\n",
      "['Maria', 'Ryskina', ',', 'Martin', 'Mundt', ',', 'Milind', 'Agarwal', ',', 'Nyx']\n",
      "['Mclean', ',', 'Pan', 'Xu', ',', 'A', 'Pranav', ',', 'Raj', 'Korpan', ',', 'Ruchira']\n",
      "['Ray', ',', 'Sarah', 'Mathew', ',', 'Sarthak', 'Arora', ',', 'St', 'John', ',', 'Tanvi']\n",
      "['Anand', ',', 'Vishakha', 'Agrawal', ',', 'William', 'Agnew', ',', 'Yanan']\n",
      "['Long', ',', 'Zijie', 'J', '.', 'Wang', ',', 'Zeerak', 'Talat', ',', 'Avijit', 'Ghosh', ',']\n",
      "['Long', ',', 'Zijie', 'J', '.', 'Wang', ',', 'Zeerak', 'Talat', ',', 'Avijit', 'Ghosh', ',']\n",
      "['Nathaniel', 'Dennler', ',', 'Michael', 'Noseworthy', ',', 'Sharvani']\n",
      "['Jha', ',', 'Emi', 'Baylor', ',', 'Aditya', 'Joshi', ',', 'Natalia', 'Y', '.', 'Bilenko', ',']\n",
      "['Jha', ',', 'Emi', 'Baylor', ',', 'Aditya', 'Joshi', ',', 'Natalia', 'Y', '.', 'Bilenko', ',']\n",
      "['Andrew', 'Mcnamara', ',', 'Raphael', 'Gontijo', '-', 'Lopes', ',', 'Alex']\n",
      "['Markham', ',', 'Evyn', 'Dong', ',', 'Jackie', 'Kay', ',', 'Manu', 'Saraswat', ',']\n",
      "['Nikhil', 'Vytla', ',', 'and', 'Luke', 'Stark', '.', '2023', '.', 'Queer', 'In', 'AI', ':', 'A']\n",
      "['Nikhil', 'Vytla', ',', 'and', 'Luke', 'Stark', '.', '2023', '.', 'Queer', 'In', 'AI', ':', 'A']\n",
      "['Nikhil', 'Vytla', ',', 'and', 'Luke', 'Stark', '.', '2023', '.', 'Queer', 'In', 'AI', ':', 'A']\n",
      "['Case', 'Study', 'in', 'Community', '-', 'Led', 'Participatory', 'AI', '.', 'In']\n",
      "['Case', 'Study', 'in', 'Community', '-', 'Led', 'Participatory', 'AI', '.', 'In']\n",
      "['Proceedings', 'of', 'the', '2023', 'ACM', 'Conference', 'on', 'Fair', '-']\n",
      "['ness', ',', 'Accountability', ',', 'and', 'Transparency', ',', 'FAccT', '’23', ',']\n",
      "['page', '1882', '–', '1895', ',', 'New', 'York', ',', 'NY', ',', 'USA', '.', 'Association']\n",
      "['page', '1882', '–', '1895', ',', 'New', 'York', ',', 'NY', ',', 'USA', '.', 'Association']\n",
      "['for', 'Computing', 'Machinery', '.']\n",
      "['Napol', 'Rachatasumrit', ',', 'Jonathan', 'Bragg', ',', 'Amy', 'X', '.', 'Zhang', ',']\n",
      "['Napol', 'Rachatasumrit', ',', 'Jonathan', 'Bragg', ',', 'Amy', 'X', '.', 'Zhang', ',']\n",
      "['and', 'Daniel', 'S', 'Weld', '.', '2022', '.', 'Citeread', ':', 'Integrating', 'lo', '-']\n",
      "['and', 'Daniel', 'S', 'Weld', '.', '2022', '.', 'Citeread', ':', 'Integrating', 'lo', '-']\n",
      "['and', 'Daniel', 'S', 'Weld', '.', '2022', '.', 'Citeread', ':', 'Integrating', 'lo', '-']\n",
      "['calized', 'citation', 'contexts', 'into', 'scientific', 'paper', 'reading', '.']\n",
      "['In', '27th', 'International', 'Conference', 'on', 'Intelligent', 'User']\n",
      "['Interfaces', ',', 'IUI', '’22', ',', 'page', '707', '–', '719', ',', 'New', 'York', ',', 'NY', ',']\n",
      "['USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['USA', '.', 'Association', 'for', 'Computing', 'Machinery', '.']\n",
      "['Shaoqing', 'Ren', ',', 'Kaiming', 'He', ',', 'Ross', 'B', '.', 'Girshick', ',', 'and', 'Jian']\n",
      "['Shaoqing', 'Ren', ',', 'Kaiming', 'He', ',', 'Ross', 'B', '.', 'Girshick', ',', 'and', 'Jian']\n",
      "['Sun', '.', '2015', '.', 'Faster', 'r', '-', 'cnn', ':', 'Towards', 'real', '-', 'time', 'object', 'de', '-']\n",
      "['Sun', '.', '2015', '.', 'Faster', 'r', '-', 'cnn', ':', 'Towards', 'real', '-', 'time', 'object', 'de', '-']\n",
      "['Sun', '.', '2015', '.', 'Faster', 'r', '-', 'cnn', ':', 'Towards', 'real', '-', 'time', 'object', 'de', '-']\n",
      "['tection', 'with', 'region', 'proposal', 'networks', '.', 'IEEE', 'Trans', '-']\n",
      "['tection', 'with', 'region', 'proposal', 'networks', '.', 'IEEE', 'Trans', '-']\n",
      "['actions', 'on', 'Pattern', 'Analysis', 'and', 'Machine', 'Intelligence', ',']\n",
      "['39', ':', '1137', '–', '1149', '.']\n",
      "['Nipun', 'Sadvilkar', 'and', 'Mark', 'Neumann', '.', '2020', '.', 'PySBD', ':']\n",
      "['Nipun', 'Sadvilkar', 'and', 'Mark', 'Neumann', '.', '2020', '.', 'PySBD', ':']\n",
      "['Nipun', 'Sadvilkar', 'and', 'Mark', 'Neumann', '.', '2020', '.', 'PySBD', ':']\n",
      "['Pragmatic', 'sentence', 'boundary', 'disambiguation', '.', 'In']\n",
      "['Pragmatic', 'sentence', 'boundary', 'disambiguation', '.', 'In']\n",
      "['Proceedings', 'of', 'Second', 'Workshop', 'for', 'NLP', 'Open']\n",
      "['Source', 'Software', '(', 'NLP', '-', 'OSS', ')', ',', 'pages', '110', '–', '114', ',', 'Online', '.']\n",
      "['Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Zejiang', 'Shen', ',', 'Kyle', 'Lo', ',', 'Lucy', 'Lu', 'Wang', ',', 'Bailey', 'Kuehl', ',']\n",
      "['Daniel', 'S', '.', 'Weld', ',', 'and', 'Doug', 'Downey', '.', '2022', '.', 'VILA', ':', 'Im', '-']\n",
      "['Daniel', 'S', '.', 'Weld', ',', 'and', 'Doug', 'Downey', '.', '2022', '.', 'VILA', ':', 'Im', '-']\n",
      "['Daniel', 'S', '.', 'Weld', ',', 'and', 'Doug', 'Downey', '.', '2022', '.', 'VILA', ':', 'Im', '-']\n",
      "['Daniel', 'S', '.', 'Weld', ',', 'and', 'Doug', 'Downey', '.', '2022', '.', 'VILA', ':', 'Im', '-']\n",
      "['proving', 'structured', 'content', 'extraction', 'from', 'scientific']\n",
      "['PDFs', 'using', 'visual', 'layout', 'groups', '.', 'Transactions', 'of', 'the']\n",
      "['PDFs', 'using', 'visual', 'layout', 'groups', '.', 'Transactions', 'of', 'the']\n",
      "['Association', 'for', 'Computational', 'Linguistics', ',', '10', ':', '376', '–']\n",
      "['392', '.']\n",
      "['Zejiang', 'Shen', ',', 'Ruochen', 'Zhang', ',', 'Melissa', 'Dell', ',', 'B', '.', 'Lee', ',']\n",
      "['Zejiang', 'Shen', ',', 'Ruochen', 'Zhang', ',', 'Melissa', 'Dell', ',', 'B', '.', 'Lee', ',']\n",
      "['Jacob', 'Carlson', ',', 'and', 'Weining', 'Li', '.', '2021', '.', 'Layoutparser', ':']\n",
      "['Jacob', 'Carlson', ',', 'and', 'Weining', 'Li', '.', '2021', '.', 'Layoutparser', ':']\n",
      "['Jacob', 'Carlson', ',', 'and', 'Weining', 'Li', '.', '2021', '.', 'Layoutparser', ':']\n",
      "['A', 'unified', 'toolkit', 'for', 'deep', 'learning', 'based', 'document']\n",
      "['image', 'analysis', '.', 'In', 'IEEE', 'International', 'Conference']\n",
      "['image', 'analysis', '.', 'In', 'IEEE', 'International', 'Conference']\n",
      "['on', 'Document', 'Analysis', 'and', 'Recognition', '.']\n",
      "['Sanjay', 'Subramanian', ',', 'Lucy', 'Lu', 'Wang', ',', 'Ben', 'Bogin', ',']\n",
      "['Sachin', 'Mehta', ',', 'Madeleine', 'van', 'Zuylen', ',', 'Sravanthi']\n",
      "['Parasa', ',', 'Sameer', 'Singh', ',', 'Matt', 'Gardner', ',', 'and', 'Hannaneh']\n",
      "['Hajishirzi', '.', '2020', '.', 'MedICaT', ':', 'A', 'dataset', 'of', 'medical']\n",
      "['Hajishirzi', '.', '2020', '.', 'MedICaT', ':', 'A', 'dataset', 'of', 'medical']\n",
      "['Hajishirzi', '.', '2020', '.', 'MedICaT', ':', 'A', 'dataset', 'of', 'medical']\n",
      "['images', ',', 'captions', ',', 'and', 'textual', 'references', '.', 'In', 'Find', '-']\n",
      "['images', ',', 'captions', ',', 'and', 'textual', 'references', '.', 'In', 'Find', '-']\n",
      "['ings', 'of', 'the', 'Association', 'for', 'Computational', 'Linguistics', ':']\n",
      "['EMNLP', '2020', ',', 'pages', '2112', '–', '2120', ',', 'Online', '.', 'Association']\n",
      "['EMNLP', '2020', ',', 'pages', '2112', '–', '2120', ',', 'Online', '.', 'Association']\n",
      "['for', 'Computational', 'Linguistics', '.']\n",
      "['M', '.', 'Tan', ',', 'R', '.', 'Pang', ',', 'and', 'Q', '.', 'V', '.', 'Le', '.', '2020', '.', 'Efficientdet', ':']\n",
      "['M', '.', 'Tan', ',', 'R', '.', 'Pang', ',', 'and', 'Q', '.', 'V', '.', 'Le', '.', '2020', '.', 'Efficientdet', ':']\n",
      "['M', '.', 'Tan', ',', 'R', '.', 'Pang', ',', 'and', 'Q', '.', 'V', '.', 'Le', '.', '2020', '.', 'Efficientdet', ':']\n",
      "['M', '.', 'Tan', ',', 'R', '.', 'Pang', ',', 'and', 'Q', '.', 'V', '.', 'Le', '.', '2020', '.', 'Efficientdet', ':']\n",
      "['M', '.', 'Tan', ',', 'R', '.', 'Pang', ',', 'and', 'Q', '.', 'V', '.', 'Le', '.', '2020', '.', 'Efficientdet', ':']\n",
      "['M', '.', 'Tan', ',', 'R', '.', 'Pang', ',', 'and', 'Q', '.', 'V', '.', 'Le', '.', '2020', '.', 'Efficientdet', ':']\n",
      "['M', '.', 'Tan', ',', 'R', '.', 'Pang', ',', 'and', 'Q', '.', 'V', '.', 'Le', '.', '2020', '.', 'Efficientdet', ':']\n",
      "['Scalable', 'and', 'efficient', 'object', 'detection', '.', 'In', '2020']\n",
      "['Scalable', 'and', 'efficient', 'object', 'detection', '.', 'In', '2020']\n",
      "['IEEE', '/', 'CVF', 'Conference', 'on', 'Computer', 'Vision', 'and', 'Pat', '-']\n",
      "['tern', 'Recognition', '(', 'CVPR', ')', ',', 'pages', '10778', '–', '10787', ',', 'Los']\n",
      "['Alamitos', ',', 'CA', ',', 'USA', '.', 'IEEE', 'Computer', 'Society', '.']\n",
      "['Alamitos', ',', 'CA', ',', 'USA', '.', 'IEEE', 'Computer', 'Society', '.']\n",
      "['Ross', 'Taylor', ',', 'Marcin', 'Kardas', ',', 'Guillem', 'Cucurull', ',', 'Thomas']\n",
      "['Scialom', ',', 'Anthony', 'S', '.', 'Hartshorn', ',', 'Elvis', 'Saravia', ',', 'An', '-']\n",
      "['Scialom', ',', 'Anthony', 'S', '.', 'Hartshorn', ',', 'Elvis', 'Saravia', ',', 'An', '-']\n",
      "['drew', 'Poulton', ',', 'Viktor', 'Kerkez', ',', 'and', 'Robert', 'Stojnic', '.']\n",
      "['2022', '.', 'Galactica', ':', 'A', 'large', 'language', 'model', 'for', 'science', '.']\n",
      "['2022', '.', 'Galactica', ':', 'A', 'large', 'language', 'model', 'for', 'science', '.']\n",
      "['ArXiv', ',', 'abs', '/', '2211', '.', '09085', '.']\n",
      "['ArXiv', ',', 'abs', '/', '2211', '.', '09085', '.']\n",
      "['pdf2image', '.', '2023', '.']\n",
      "['pdf2image', '.', '2023', '.']\n",
      "['pdf2image', '.']\n",
      "['https', ':', '/', '/', 'github', '.']\n",
      "['com', '/', 'Belval', '/', 'pdf2image', '.']\n",
      "['pdfplumber', '.', '2023', '.', 'pdfplumber', '.', 'https', ':', '/', '/', 'github', '.']\n",
      "['pdfplumber', '.', '2023', '.', 'pdfplumber', '.', 'https', ':', '/', '/', 'github', '.']\n",
      "['pdfplumber', '.', '2023', '.', 'pdfplumber', '.', 'https', ':', '/', '/', 'github', '.']\n",
      "['pdfplumber', '.', '2023', '.', 'pdfplumber', '.', 'https', ':', '/', '/', 'github', '.']\n",
      "['com', '/', 'jsvine', '/', 'pdfplumber', '.']\n",
      "['Dominika', 'Tkaczyk', ',', 'Paweł', 'Szostek', ',', 'Mateusz', 'Fedo', '-']\n",
      "['ryszak', ',', 'Piotr', 'Jan', 'Dendek', ',', 'and', 'Lukasz', 'Bolikowski', '.']\n",
      "['2015', '.', 'Cermine', ':', 'Automatic', 'extraction', 'of', 'structured']\n",
      "['2015', '.', 'Cermine', ':', 'Automatic', 'extraction', 'of', 'structured']\n",
      "['metadata', 'from', 'scientific', 'literature', '.', 'Int', '.', 'J', '.', 'Doc', '.', 'Anal', '.']\n",
      "['metadata', 'from', 'scientific', 'literature', '.', 'Int', '.', 'J', '.', 'Doc', '.', 'Anal', '.']\n",
      "['metadata', 'from', 'scientific', 'literature', '.', 'Int', '.', 'J', '.', 'Doc', '.', 'Anal', '.']\n",
      "['metadata', 'from', 'scientific', 'literature', '.', 'Int', '.', 'J', '.', 'Doc', '.', 'Anal', '.']\n",
      "['metadata', 'from', 'scientific', 'literature', '.', 'Int', '.', 'J', '.', 'Doc', '.', 'Anal', '.']\n",
      "['Recognit', '.', ',', '18', '(', '4', ')', ':', '317', '–', '335', '.']\n",
      "['Recognit', '.', ',', '18', '(', '4', ')', ':', '317', '–', '335', '.']\n",
      "['Amalie', 'Trewartha', ',', 'Nicholas', 'Walker', ',', 'Haoyan', 'Huo', ',']\n",
      "['Sanghoon', 'Lee', ',', 'Kevin', 'Cruse', ',', 'John', 'Dagdelen', ',', 'Alex']\n",
      "['Dunn', ',', 'Kristin', 'Aslaug', 'Persson', ',', 'Gerbrand', 'Ceder', ',', 'and']\n",
      "['Anubhav', 'Jain', '.', '2022', '.', 'Quantifying', 'the', 'advantage', 'of']\n",
      "['Anubhav', 'Jain', '.', '2022', '.', 'Quantifying', 'the', 'advantage', 'of']\n",
      "['Anubhav', 'Jain', '.', '2022', '.', 'Quantifying', 'the', 'advantage', 'of']\n",
      "['domain', '-', 'specific', 'pre', '-', 'training', 'on', 'named', 'entity', 'recog', '-']\n",
      "['nition', 'tasks', 'in', 'materials', 'science', '.', 'Patterns', ',', '3', '.']\n",
      "['nition', 'tasks', 'in', 'materials', 'science', '.', 'Patterns', ',', '3', '.']\n",
      "['Lucy', 'Lu', 'Wang', ',', 'Isabel', 'Cachola', ',', 'Jonathan', 'Bragg', ',', 'Evie']\n",
      "['(', 'Yu', '-', 'Yen', ')', 'Cheng', ',', 'Chelsea', 'Hess', 'Haupt', ',', 'Matt', 'Latzke', ',']\n",
      "['Bailey', 'Kuehl', ',', 'Madeleine', 'van', 'Zuylen', ',', 'Linda', 'M', '.', 'Wag', '-']\n",
      "['Bailey', 'Kuehl', ',', 'Madeleine', 'van', 'Zuylen', ',', 'Linda', 'M', '.', 'Wag', '-']\n",
      "['ner', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2021', '.', 'Improving', 'the', 'acces', '-']\n",
      "['ner', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2021', '.', 'Improving', 'the', 'acces', '-']\n",
      "['ner', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2021', '.', 'Improving', 'the', 'acces', '-']\n",
      "['ner', ',', 'and', 'Daniel', 'S', '.', 'Weld', '.', '2021', '.', 'Improving', 'the', 'acces', '-']\n",
      "['sibility', 'of', 'scientific', 'documents', ':', 'Current', 'state', ',', 'user']\n",
      "['needs', ',', 'and', 'a', 'system', 'solution', 'to', 'enhance', 'scientific', 'pdf']\n",
      "['accessibility', 'for', 'blind', 'and', 'low', 'vision', 'users', '.', 'ArXiv', ',']\n",
      "['accessibility', 'for', 'blind', 'and', 'low', 'vision', 'users', '.', 'ArXiv', ',']\n",
      "['abs', '/', '2105', '.', '00076', '.']\n",
      "['abs', '/', '2105', '.', '00076', '.']\n",
      "['Lucy', 'Lu', 'Wang', ',', 'Kyle', 'Lo', ',', 'Yoganand', 'Chandrasekhar', ',']\n",
      "['Russell', 'Reas', ',', 'Jiangjiang', 'Yang', ',', 'Doug', 'Burdick', ',', 'Darrin']\n",
      "['Eide', ',', 'Kathryn', 'Funk', ',', 'Yannis', 'Katsis', ',', 'Rodney', 'Michael']\n",
      "['Kinney', ',', 'Yunyao', 'Li', ',', 'Ziyang', 'Liu', ',', 'William', 'Merrill', ',']\n",
      "['Paul', 'Mooney', ',', 'Dewey', 'A', '.', 'Murdick', ',', 'Devvret', 'Rishi', ',']\n",
      "['Paul', 'Mooney', ',', 'Dewey', 'A', '.', 'Murdick', ',', 'Devvret', 'Rishi', ',']\n",
      "['Jerry', 'Sheehan', ',', 'Zhihong', 'Shen', ',', 'Brandon', 'Stilson', ',']\n",
      "['Alex', 'D', '.', 'Wade', ',', 'Kuansan', 'Wang', ',', 'Nancy', 'Xin', 'Ru', 'Wang', ',']\n",
      "['Alex', 'D', '.', 'Wade', ',', 'Kuansan', 'Wang', ',', 'Nancy', 'Xin', 'Ru', 'Wang', ',']\n",
      "['Christopher', 'Wilhelm', ',', 'Boya', 'Xie', ',', 'Douglas', 'M', '.', 'Ray', '-']\n",
      "['Christopher', 'Wilhelm', ',', 'Boya', 'Xie', ',', 'Douglas', 'M', '.', 'Ray', '-']\n",
      "['mond', ',', 'Daniel', 'S', '.', 'Weld', ',', 'Oren', 'Etzioni', ',', 'and', 'Sebastian']\n",
      "['mond', ',', 'Daniel', 'S', '.', 'Weld', ',', 'Oren', 'Etzioni', ',', 'and', 'Sebastian']\n",
      "['Kohlmeier', '.', '2020', '.', 'CORD', '-', '19', ':', 'The', 'COVID', '-', '19', 'open']\n",
      "['Kohlmeier', '.', '2020', '.', 'CORD', '-', '19', ':', 'The', 'COVID', '-', '19', 'open']\n",
      "['Kohlmeier', '.', '2020', '.', 'CORD', '-', '19', ':', 'The', 'COVID', '-', '19', 'open']\n",
      "['research', 'dataset', '.', 'In', 'Proceedings', 'of', 'the', '1st', 'Work', '-']\n",
      "['research', 'dataset', '.', 'In', 'Proceedings', 'of', 'the', '1st', 'Work', '-']\n",
      "['shop', 'on', 'NLP', 'for', 'COVID', '-', '19', 'at', 'ACL', '2020', ',', 'Online', '.']\n",
      "['Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Rosalee', 'Wolfe', ',', 'John', 'McDonald', ',', 'Ronan', 'Johnson', ',', 'Ben']\n",
      "['Sturr', ',', 'Syd', 'Klinghoffer', ',', 'Anthony', 'Bonzani', ',', 'Andrew']\n",
      "['Alexander', ',', 'and', 'Nicole', 'Barnekow', '.', '2022', '.', 'Supporting']\n",
      "['Alexander', ',', 'and', 'Nicole', 'Barnekow', '.', '2022', '.', 'Supporting']\n",
      "['Alexander', ',', 'and', 'Nicole', 'Barnekow', '.', '2022', '.', 'Supporting']\n",
      "['mouthing', 'in', 'signed', 'languages', ':', 'New', 'innovations', 'and']\n",
      "['a', 'proposal', 'for', 'future', 'corpus', 'building', '.', 'In', 'Proceedings']\n",
      "['a', 'proposal', 'for', 'future', 'corpus', 'building', '.', 'In', 'Proceedings']\n",
      "['of', 'the', '7th', 'International', 'Workshop', 'on', 'Sign', 'Language']\n",
      "['Translation', 'and', 'Avatar', 'Technology', ':', 'The', 'Junction', 'of']\n",
      "['the', 'Visual', 'and', 'the', 'Textual', ':', 'Challenges', 'and', 'Perspec', '-']\n",
      "['tives', ',', 'pages', '125', '–', '130', ',', 'Marseille', ',', 'France', '.', 'European']\n",
      "['tives', ',', 'pages', '125', '–', '130', ',', 'Marseille', ',', 'France', '.', 'European']\n",
      "['Language', 'Resources', 'Association', '.']\n",
      "['Yang', 'Xu', ',', 'Yiheng', 'Xu', ',', 'Tengchao', 'Lv', ',', 'Lei', 'Cui', ',', 'Furu']\n",
      "['Wei', ',', 'Guoxin', 'Wang', ',', 'Yijuan', 'Lu', ',', 'Dinei', 'Florencio', ',', 'Cha']\n",
      "['Zhang', ',', 'Wanxiang', 'Che', ',', 'Min', 'Zhang', ',', 'and', 'Lidong', 'Zhou', '.']\n",
      "['2021', '.', 'LayoutLMv2', ':', 'Multi', '-', 'modal', 'pre', '-', 'training', 'for']\n",
      "['2021', '.', 'LayoutLMv2', ':', 'Multi', '-', 'modal', 'pre', '-', 'training', 'for']\n",
      "['visually', '-', 'rich', 'document', 'understanding', '.', 'In', 'Proceed', '-']\n",
      "['visually', '-', 'rich', 'document', 'understanding', '.', 'In', 'Proceed', '-']\n",
      "['ings', 'of', 'the', '59th', 'Annual', 'Meeting', 'of', 'the', 'Association', 'for']\n",
      "['Computational', 'Linguistics', 'and', 'the', '11th', 'International']\n",
      "['Joint', 'Conference', 'on', 'Natural', 'Language', 'Processing']\n",
      "['(', 'Volume', '1', ':', 'Long', 'Papers', ')', ',', 'pages', '2579', '–', '2591', ',', 'Online', '.']\n",
      "['Association', 'for', 'Computational', 'Linguistics', '.']\n",
      "['Yiheng', 'Xu', ',', 'Minghao', 'Li', ',', 'Lei', 'Cui', ',', 'Shaohan', 'Huang', ',', 'Furu']\n",
      "['Wei', ',', 'and', 'Ming', 'Zhou', '.', '2019', '.', 'Layoutlm', ':', 'Pre', '-', 'training']\n",
      "['Wei', ',', 'and', 'Ming', 'Zhou', '.', '2019', '.', 'Layoutlm', ':', 'Pre', '-', 'training']\n",
      "['Wei', ',', 'and', 'Ming', 'Zhou', '.', '2019', '.', 'Layoutlm', ':', 'Pre', '-', 'training']\n",
      "['of', 'text', 'and', 'layout', 'for', 'document', 'image', 'understanding', '.']\n",
      "['Proceedings', 'of', 'the', '26th', 'ACM', 'SIGKDD', 'International']\n",
      "['Conference', 'on', 'Knowledge', 'Discovery', '&', 'Data', 'Mining', '.']\n",
      "['Xu', 'Zhong', ',', 'Jianbin', 'Tang', ',', 'and', 'Antonio', 'Jimeno', 'Yepes', '.']\n",
      "['2019', '.', 'Publaynet', ':', 'largest', 'dataset', 'ever', 'for', 'document']\n",
      "['2019', '.', 'Publaynet', ':', 'largest', 'dataset', 'ever', 'for', 'document']\n",
      "['layout', 'analysis', '.', 'In', '2019', 'International', 'Conference', 'on']\n",
      "['layout', 'analysis', '.', 'In', '2019', 'International', 'Conference', 'on']\n",
      "['Document', 'Analysis', 'and', 'Recognition', '(', 'ICDAR', ')', ',', 'pages']\n",
      "['1015', '–', '1022', '.', 'IEEE', '.']\n",
      "['1015', '–', '1022', '.', 'IEEE', '.']\n",
      "['A']\n",
      "['Appendix']\n",
      "['A', '.', '1']\n",
      "['A', '.', '1']\n",
      "['Comparison', 'and', 'Compatibility', 'with']\n",
      "['XML']\n",
      "['One', 'can', 'view', 'Layers', 'as', 'capturing', 'content', 'hier', '-']\n",
      "['archy', '(', 'e', '.', 'g', '.', ',', 'tokens', 'vs', 'sentences', ')', 'similar', 'to', 'that', 'of']\n",
      "['other', 'structured', 'document', 'representations', ',', 'like', 'TEI']\n",
      "['XML', 'trees', '.', 'We', 'note', 'that', 'Layers', 'are', 'stored', 'as', 'un', '-']\n",
      "['XML', 'trees', '.', 'We', 'note', 'that', 'Layers', 'are', 'stored', 'as', 'un', '-']\n",
      "['ordered', 'attributes', 'and', 'don’t', 'require', 'nesting', '.', 'This']\n",
      "['ordered', 'attributes', 'and', 'don’t', 'require', 'nesting', '.', 'This']\n",
      "['allows', 'for', 'specific', 'cross', '-', 'layer', 'referencing', 'opera', '-']\n",
      "['tions', 'that', 'don’t', 'adhere', 'to', 'strict', 'nesting', 'relationships', '.']\n",
      "['For', 'example', ':']\n",
      "['1', 'for', 'sentence', 'in', 'doc', '.', 'sentences', ':']\n",
      "['1', 'for', 'sentence', 'in', 'doc', '.', 'sentences', ':']\n",
      "['2']\n",
      "['for', 'line', 'in', 'sentence', '.', 'lines', ':']\n",
      "['for', 'line', 'in', 'sentence', '.', 'lines', ':']\n",
      "['3']\n",
      "['.', '.', '.']\n",
      "['Recall', 'that', 'a', 'sentence', 'can', 'begin', 'or', 'end', 'midway']\n",
      "['through', 'a', 'line', 'and', 'cross', 'multiple', 'lines', '(', '§', '3', '.', '1', ')', '.', 'Sim', '-']\n",
      "['through', 'a', 'line', 'and', 'cross', 'multiple', 'lines', '(', '§', '3', '.', '1', ')', '.', 'Sim', '-']\n",
      "['ilarly', ',', 'not', 'all', 'lines', 'are', 'exactly', 'contained', 'within']\n",
      "['the', 'boundaries', 'of', 'a', 'sentence', '.', 'As', 'such', ',', 'sentences']\n",
      "['the', 'boundaries', 'of', 'a', 'sentence', '.', 'As', 'such', ',', 'sentences']\n",
      "['and', 'lines', 'are', 'not', 'strictly', 'nested', 'within', 'each', 'other', '.']\n",
      "['This', 'relationship', 'would', 'be', 'difficult', 'to', 'encode', 'in', 'an']\n",
      "['XML', 'format', 'adhering', 'to', 'document', 'tree', 'structure', '.']\n",
      "['Regardless', ',', 'the', 'way', 'we', 'represent', 'structure']\n",
      "['in', 'documents', 'is', 'highly', 'versatile', '.']\n",
      "['We', 'demon', '-']\n",
      "['strate', 'this', 'by', 'also', 'implementing', 'GrobidParser']\n",
      "['as', 'an', 'alternative', 'to', 'the', 'PDF2TextParser', 'in', '§', '3', '.', '1', '.']\n",
      "['as', 'an', 'alternative', 'to', 'the', 'PDF2TextParser', 'in', '§', '3', '.', '1', '.']\n",
      "['GrobidParser', 'invokes', 'Grobid', 'to', 'process', 'PDFs', ',']\n",
      "['and', 'reads', 'the', 'resulting', 'TEI', 'XML', 'file', 'generated', 'by']\n",
      "['Grobid', 'by', 'converting', 'each', 'XML', 'tag', 'of', 'a', 'common']\n",
      "['level', 'into', 'an', 'Entity', 'of', 'its', 'own', 'Layer', '.', 'We', 'use', 'this']\n",
      "['level', 'into', 'an', 'Entity', 'of', 'its', 'own', 'Layer', '.', 'We', 'use', 'this']\n",
      "['to', 'perform', 'the', 'evaluation', 'in', 'Table', '2', '.']\n",
      "['A', '.', '2']\n",
      "['A', '.', '2']\n",
      "['Additional', 'magelib', 'Protocols', 'and']\n",
      "['Utilities']\n",
      "['Serialization', '.']\n",
      "['Any', 'Document', 'and', 'all', 'of', 'its']\n",
      "['Layers', 'can', 'be', 'exported', 'to', 'a', 'JSON', 'format', ',', 'and']\n",
      "['perfectly', 'reconstructed', ':']\n",
      "['1', 'import', 'json']\n",
      "['2', 'with', 'open', '(', '\"', '.', '.', '.', '.', 'json', '\"', ',', '\"', 'w', '\"', ')', 'as', 'f', '_', 'out', ':']\n",
      "['3']\n",
      "['json', '.', 'dump', '(', 'doc', '.', 'to', '_', 'json', '(', ')', ',', 'f', '_', 'out', ')']\n",
      "['json', '.', 'dump', '(', 'doc', '.', 'to', '_', 'json', '(', ')', ',', 'f', '_', 'out', ')']\n",
      "['json', '.', 'dump', '(', 'doc', '.', 'to', '_', 'json', '(', ')', ',', 'f', '_', 'out', ')']\n",
      "['4', '5', 'with', 'open', '(', '\"', '.', '.', '.', 'json', '\"', ',', '\"', 'r', '\"', ')', 'as', 'f', '_', 'in', ':']\n",
      "['6']\n",
      "['doc', '=', 'json', '.', 'load', '(', 'f', '_', 'in', ')']\n",
      "['doc', '=', 'json', '.', 'load', '(', 'f', '_', 'in', ')']\n",
      "['A', '.', '3']\n",
      "['A', '.', '3']\n",
      "['Evaluating', 'papermage', '’s', 'CoreRecipe']\n",
      "['against', 'Grobid']\n",
      "['Here', ',', 'we', 'detail', 'how', 'we', 'performed', 'the', 'evaluation']\n",
      "['reported', 'in', '§', '3', '.', '3', '(', 'Table', '2', ')', '.', 'We', 'also', 'provide', 'a', 'full']\n",
      "['reported', 'in', '§', '3', '.', '3', '(', 'Table', '2', ')', '.', 'We', 'also', 'provide', 'a', 'full']\n",
      "['reported', 'in', '§', '3', '.', '3', '(', 'Table', '2', ')', '.', 'We', 'also', 'provide', 'a', 'full']\n",
      "['breakdown', 'by', 'category', 'in', 'Table', '3', '.']\n",
      "['As', 'described', 'earlier', 'in', 'the', 'paper', ',', 'Grobid', 'is', 'quite']\n",
      "['difficult', 'to', 'evaluate', 'as', 'it', 'is', 'developed', 'with', 'tight']\n",
      "['coupling', 'between', 'the', 'PDF', 'parser', '(', 'pdfalto', ')', 'and']\n",
      "['the', 'models', 'it', 'employs', 'to', 'perform', 'logical', 'struc', '-']\n",
      "['ture', 'recovery', 'over', 'the', 'resulting', 'token', 'stream', '.', 'As']\n",
      "['ture', 'recovery', 'over', 'the', 'resulting', 'token', 'stream', '.', 'As']\n",
      "['such', ',', 'there', 'is', 'no', 'straightforward', 'way', 'to', 'run', 'just']\n",
      "['the', 'model', 'components', 'of', 'Grobid', 'on', 'an', 'alternative']\n",
      "['token', 'stream', 'like', 'that', 'provided', 'in', 'the', 'S2', '-', 'VL', '(', 'Shen']\n",
      "['et', 'al', '.', ',', '2022', ')', 'dataset', '.']\n",
      "['To', 'perform', 'this', 'baseline', 'evaluation', ',', 'we', 'ran']\n",
      "['the', 'original', 'PDFs', 'that', 'were', 'annotated', 'for', 'S2', '-', 'VL']\n",
      "['through', 'our', 'GrobidParser', 'using', 'v0', '.', '7', '.', '3', '.', 'Grobid']\n",
      "['through', 'our', 'GrobidParser', 'using', 'v0', '.', '7', '.', '3', '.', 'Grobid']\n",
      "['through', 'our', 'GrobidParser', 'using', 'v0', '.', '7', '.', '3', '.', 'Grobid']\n",
      "['through', 'our', 'GrobidParser', 'using', 'v0', '.', '7', '.', '3', '.', 'Grobid']\n",
      "['also', 'returns', 'bounding', 'boxes', 'of', 'some', 'predicted', 'cat', '-']\n",
      "['egories', '(', 'e', '.', 'g', '.', ',', 'authors', ',', 'abstract', ',', 'paragraphs', ')', '.', 'We']\n",
      "['egories', '(', 'e', '.', 'g', '.', ',', 'authors', ',', 'abstract', ',', 'paragraphs', ')', '.', 'We']\n",
      "['use', 'these', 'bounding', 'boxes', 'to', 'create', 'Entities', 'that']\n",
      "['we', 'annotate', 'on', 'a', 'Document', 'constructed', 'manually']\n",
      "['from', 'from', 'S2', '-', 'VL', 'data', '.', 'Using', 'magelib', 'cross', '-', 'layer']\n",
      "['from', 'from', 'S2', '-', 'VL', 'data', '.', 'Using', 'magelib', 'cross', '-', 'layer']\n",
      "['referencing', ',', 'we', 'were', 'able', 'to', 'match', 'Grobid', 'predic', '-']\n",
      "['tions', 'to', 'S2', '-', 'VL', 'data', 'to', 'perform', 'this', 'evaluation', '.']\n",
      "['Though', 'we', 'found', 'there', 'are', 'certain', 'categories']\n",
      "['for', 'which', 'bounding', 'box', 'information', 'was', 'either', 'not']\n",
      "['available', '(', 'e', '.', 'g', '.', ',', 'Titles', ')', 'or', 'Grobid', 'simply', 'did', 'not', 're', '-']\n",
      "['turn', 'that', 'output', '(', 'e', '.', 'g', '.', ',', 'Figure', 'text', 'extraction', ')', '.', 'These']\n",
      "['turn', 'that', 'output', '(', 'e', '.', 'g', '.', ',', 'Figure', 'text', 'extraction', ')', '.', 'These']\n",
      "['are', 'represented', 'by', 'zeros', 'in', 'Table', '3', ',', 'which', 'con', '-']\n",
      "['tributes', 'to', 'the', 'lower', 'scores', 'in', 'Table', '2', 'after', 'macro']\n",
      "['averaging', '.', 'For', 'a', 'more', 'apples', '-', 'to', '-', 'apples', 'compari', '-']\n",
      "['averaging', '.', 'For', 'a', 'more', 'apples', '-', 'to', '-', 'apples', 'compari', '-']\n",
      "['son', ',', 'we', 'also', 'included', 'a', '“Grobid', 'Subset”', 'evaluation']\n",
      "['which', 'restricted', 'to', 'just', 'categories', 'in', 'S2', '-', 'VL', 'for']\n",
      "['which', 'Grobid', 'produced', 'bounding', 'box', 'information', '.']\n",
      "['In', 'addition', 'to', 'Grobid', ',', 'we', 'evaluate', 'two', 'of', 'our', 'pro', '-']\n",
      "['vided', 'Transformer', '-', 'based', 'models', '.', 'The', 'RoBERTa', '-']\n",
      "['vided', 'Transformer', '-', 'based', 'models', '.', 'The', 'RoBERTa', '-']\n",
      "['large', '(', 'Liu', 'et', 'al', '.', ',', '2019', ')', 'model', 'is', 'a', 'Transformers']\n",
      "['token', 'classification', 'model', 'that', 'we', 'finetuned', 'on', 'the']\n",
      "['S2', '-', 'VL', 'training', 'set', '.', 'The', 'I', '-', 'VILA', 'model', 'is', 'a', 'layout', '-']\n",
      "['S2', '-', 'VL', 'training', 'set', '.', 'The', 'I', '-', 'VILA', 'model', 'is', 'a', 'layout', '-']\n",
      "['infused', 'Transformer', 'model', 'pretrained', 'by', 'Shen', 'et', 'al', '.']\n",
      "['(', '2022', ')', 'on', 'the', 'S2', '-', 'VL', 'training', 'set', '.', 'Like', 'we', 'did', 'with']\n",
      "['(', '2022', ')', 'on', 'the', 'S2', '-', 'VL', 'training', 'set', '.', 'Like', 'we', 'did', 'with']\n",
      "['Grobid', ',', 'we', 'ran', 'our', 'CoreRecipe', 'using', 'these', 'two']\n",
      "['models', 'on', 'the', 'original', 'PDFs', 'in', 'S2', '-', 'VL', ',', 'and', 'per', '-']\n",
      "['formed', 'a', 'similar', 'token', 'mapping', 'operation', 'since', 'our']\n",
      "['PDF2TextParser', 'also', 'produces', 'a', 'different', 'token']\n",
      "['stream', 'than', 'that', 'provided', 'in', 'S2', '-', 'VL', '.']\n",
      "['At', 'the', 'end', 'of', 'the', 'day', ',', 'the', 'Transformer', '-', 'based']\n",
      "['models', 'performed', 'better', 'at', 'this', 'task', 'than', 'Grobid', '.']\n",
      "['This', 'is', 'unsurprising', 'given', 'expected', 'improvements']\n",
      "['using', 'a', 'Transformer', 'model', 'over', 'a', 'CRF', 'or', 'BiL', '-']\n",
      "['STM', '.', 'The', 'Transformer', 'models', 'were', 'also', 'trained']\n",
      "['STM', '.', 'The', 'Transformer', 'models', 'were', 'also', 'trained']\n",
      "['on', 'S2', '-', 'VL', 'data', ',', 'which', 'gave', 'them', 'an', 'advantage', 'over']\n",
      "['Grobid', '.', 'Overall', ',', 'this', 'evaluation', 'intended', 'to', 'show']\n",
      "['Grobid', '.', 'Overall', ',', 'this', 'evaluation', 'intended', 'to', 'show']\n",
      "['how', 'papermage', 'enables', 'cross', '-', 'system', 'comparisons', ',']\n",
      "['even', 'eschewing', 'token', 'stream', 'incompatibility', ',', 'and']\n",
      "['to', 'illustrate', 'an', 'upper', 'bound', 'of', 'the', 'performance', 'left']\n",
      "['on', 'the', 'table', 'by', 'existing', 'software', 'systems', 'that', 'don’t']\n",
      "['use', 'of', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', '.']\n",
      "['StructureCategory']\n",
      "['GROBID', 'CRF']\n",
      "['GROBID', 'NN']\n",
      "['RoBERTa']\n",
      "['I', '-', 'VILA']\n",
      "['P']\n",
      "['R']\n",
      "['F1']\n",
      "['P']\n",
      "['R']\n",
      "['F1']\n",
      "['P']\n",
      "['R']\n",
      "['F1']\n",
      "['P']\n",
      "['R']\n",
      "['F1']\n",
      "['Abstract']\n",
      "['81', '.', '9']\n",
      "['81', '.', '9']\n",
      "['89', '.', '1']\n",
      "['89', '.', '1']\n",
      "['85', '.', '3']\n",
      "['85', '.', '3']\n",
      "['85', '.', '3']\n",
      "['85', '.', '3']\n",
      "['89', '.', '8']\n",
      "['89', '.', '8']\n",
      "['87', '.', '5']\n",
      "['87', '.', '5']\n",
      "['89', '.', '2']\n",
      "['89', '.', '2']\n",
      "['93', '.', '7']\n",
      "['93', '.', '7']\n",
      "['91', '.', '4']\n",
      "['91', '.', '4']\n",
      "['97', '.', '4']\n",
      "['97', '.', '4']\n",
      "['98', '.', '3']\n",
      "['98', '.', '3']\n",
      "['97', '.', '8']\n",
      "['97', '.', '8']\n",
      "['Author']\n",
      "['55', '.', '2']\n",
      "['55', '.', '2']\n",
      "['42', '.', '6']\n",
      "['42', '.', '6']\n",
      "['48', '.', '1']\n",
      "['48', '.', '1']\n",
      "['75', '.', '1']\n",
      "['75', '.', '1']\n",
      "['14', '.', '0']\n",
      "['14', '.', '0']\n",
      "['23', '.', '6']\n",
      "['23', '.', '6']\n",
      "['87', '.', '5']\n",
      "['87', '.', '5']\n",
      "['73', '.', '5']\n",
      "['73', '.', '5']\n",
      "['79', '.', '9']\n",
      "['79', '.', '9']\n",
      "['65', '.', '5']\n",
      "['65', '.', '5']\n",
      "['96', '.', '9']\n",
      "['96', '.', '9']\n",
      "['78', '.', '2']\n",
      "['78', '.', '2']\n",
      "['Bibliography']\n",
      "['96', '.', '5']\n",
      "['96', '.', '5']\n",
      "['98', '.', '6']\n",
      "['98', '.', '6']\n",
      "['97', '.', '5']\n",
      "['97', '.', '5']\n",
      "['95', '.', '5']\n",
      "['95', '.', '5']\n",
      "['97', '.', '6']\n",
      "['97', '.', '6']\n",
      "['96', '.', '5']\n",
      "['96', '.', '5']\n",
      "['93', '.', '6']\n",
      "['93', '.', '6']\n",
      "['93', '.', '3']\n",
      "['93', '.', '3']\n",
      "['93', '.', '5']\n",
      "['93', '.', '5']\n",
      "['99', '.', '7']\n",
      "['99', '.', '7']\n",
      "['98', '.', '2']\n",
      "['98', '.', '2']\n",
      "['99', '.', '0']\n",
      "['99', '.', '0']\n",
      "['Caption']\n",
      "['70', '.', '3']\n",
      "['70', '.', '3']\n",
      "['70', '.', '0']\n",
      "['70', '.', '0']\n",
      "['70', '.', '2']\n",
      "['70', '.', '2']\n",
      "['70', '.', '2']\n",
      "['70', '.', '2']\n",
      "['69', '.', '7']\n",
      "['69', '.', '7']\n",
      "['70', '.', '0']\n",
      "['70', '.', '0']\n",
      "['80', '.', '0']\n",
      "['80', '.', '0']\n",
      "['77', '.', '3']\n",
      "['77', '.', '3']\n",
      "['78', '.', '6']\n",
      "['78', '.', '6']\n",
      "['93', '.', '1']\n",
      "['93', '.', '1']\n",
      "['89', '.', '6']\n",
      "['89', '.', '6']\n",
      "['91', '.', '3']\n",
      "['91', '.', '3']\n",
      "['Equation']\n",
      "['71', '.', '1']\n",
      "['71', '.', '1']\n",
      "['85', '.', '3']\n",
      "['85', '.', '3']\n",
      "['77', '.', '6']\n",
      "['77', '.', '6']\n",
      "['71', '.', '1']\n",
      "['71', '.', '1']\n",
      "['85', '.', '3']\n",
      "['85', '.', '3']\n",
      "['77', '.', '6']\n",
      "['77', '.', '6']\n",
      "['55', '.', '0']\n",
      "['55', '.', '0']\n",
      "['85', '.', '7']\n",
      "['85', '.', '7']\n",
      "['67', '.', '0']\n",
      "['67', '.', '0']\n",
      "['90', '.', '7']\n",
      "['90', '.', '7']\n",
      "['94', '.', '2']\n",
      "['94', '.', '2']\n",
      "['92', '.', '4']\n",
      "['92', '.', '4']\n",
      "['Figure']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['88', '.', '9']\n",
      "['88', '.', '9']\n",
      "['82', '.', '3']\n",
      "['82', '.', '3']\n",
      "['85', '.', '4']\n",
      "['85', '.', '4']\n",
      "['99', '.', '8']\n",
      "['99', '.', '8']\n",
      "['96', '.', '8']\n",
      "['96', '.', '8']\n",
      "['98', '.', '3']\n",
      "['98', '.', '3']\n",
      "['Footer']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['56', '.', '1']\n",
      "['56', '.', '1']\n",
      "['59', '.', '9']\n",
      "['59', '.', '9']\n",
      "['57', '.', '9']\n",
      "['57', '.', '9']\n",
      "['96', '.', '8']\n",
      "['96', '.', '8']\n",
      "['78', '.', '1']\n",
      "['78', '.', '1']\n",
      "['86', '.', '5']\n",
      "['86', '.', '5']\n",
      "['Footnote']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['59', '.', '8']\n",
      "['59', '.', '8']\n",
      "['44', '.', '3']\n",
      "['44', '.', '3']\n",
      "['50', '.', '9']\n",
      "['50', '.', '9']\n",
      "['80', '.', '2']\n",
      "['80', '.', '2']\n",
      "['93', '.', '5']\n",
      "['93', '.', '5']\n",
      "['86', '.', '3']\n",
      "['86', '.', '3']\n",
      "['Header']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['40', '.', '5']\n",
      "['40', '.', '5']\n",
      "['84', '.', '3']\n",
      "['84', '.', '3']\n",
      "['54', '.', '7']\n",
      "['54', '.', '7']\n",
      "['92', '.', '9']\n",
      "['92', '.', '9']\n",
      "['99', '.', '1']\n",
      "['99', '.', '1']\n",
      "['95', '.', '9']\n",
      "['95', '.', '9']\n",
      "['Keywords']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['93', '.', '8']\n",
      "['93', '.', '8']\n",
      "['97', '.', '1']\n",
      "['97', '.', '1']\n",
      "['95', '.', '4']\n",
      "['95', '.', '4']\n",
      "['96', '.', '9']\n",
      "['96', '.', '9']\n",
      "['99', '.', '4']\n",
      "['99', '.', '4']\n",
      "['98', '.', '1']\n",
      "['98', '.', '1']\n",
      "['List']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['61', '.', '9']\n",
      "['61', '.', '9']\n",
      "['63', '.', '8']\n",
      "['63', '.', '8']\n",
      "['62', '.', '9']\n",
      "['62', '.', '9']\n",
      "['76', '.', '7']\n",
      "['76', '.', '7']\n",
      "['82', '.', '4']\n",
      "['82', '.', '4']\n",
      "['79', '.', '4']\n",
      "['79', '.', '4']\n",
      "['Paragraph']\n",
      "['94', '.', '5']\n",
      "['94', '.', '5']\n",
      "['89', '.', '8']\n",
      "['89', '.', '8']\n",
      "['92', '.', '1']\n",
      "['92', '.', '1']\n",
      "['94', '.', '4']\n",
      "['94', '.', '4']\n",
      "['89', '.', '9']\n",
      "['89', '.', '9']\n",
      "['92', '.', '1']\n",
      "['92', '.', '1']\n",
      "['93', '.', '5']\n",
      "['93', '.', '5']\n",
      "['93', '.', '0']\n",
      "['93', '.', '0']\n",
      "['93', '.', '3']\n",
      "['93', '.', '3']\n",
      "['98', '.', '7']\n",
      "['98', '.', '7']\n",
      "['97', '.', '9']\n",
      "['97', '.', '9']\n",
      "['98', '.', '3']\n",
      "['98', '.', '3']\n",
      "['Section']\n",
      "['83', '.', '0']\n",
      "['83', '.', '0']\n",
      "['79', '.', '4']\n",
      "['79', '.', '4']\n",
      "['81', '.', '1']\n",
      "['81', '.', '1']\n",
      "['83', '.', '0']\n",
      "['83', '.', '0']\n",
      "['79', '.', '4']\n",
      "['79', '.', '4']\n",
      "['81', '.', '1']\n",
      "['81', '.', '1']\n",
      "['67', '.', '7']\n",
      "['67', '.', '7']\n",
      "['82', '.', '7']\n",
      "['82', '.', '7']\n",
      "['74', '.', '4']\n",
      "['74', '.', '4']\n",
      "['96', '.', '2']\n",
      "['96', '.', '2']\n",
      "['91', '.', '6']\n",
      "['91', '.', '6']\n",
      "['93', '.', '9']\n",
      "['93', '.', '9']\n",
      "['Table']\n",
      "['97', '.', '3']\n",
      "['97', '.', '3']\n",
      "['58', '.', '6']\n",
      "['58', '.', '6']\n",
      "['73', '.', '2']\n",
      "['73', '.', '2']\n",
      "['97', '.', '9']\n",
      "['97', '.', '9']\n",
      "['58', '.', '6']\n",
      "['58', '.', '6']\n",
      "['73', '.', '3']\n",
      "['73', '.', '3']\n",
      "['94', '.', '7']\n",
      "['94', '.', '7']\n",
      "['71', '.', '8']\n",
      "['71', '.', '8']\n",
      "['81', '.', '7']\n",
      "['81', '.', '7']\n",
      "['96', '.', '1']\n",
      "['96', '.', '1']\n",
      "['94', '.', '9']\n",
      "['94', '.', '9']\n",
      "['95', '.', '5']\n",
      "['95', '.', '5']\n",
      "['Title']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['0', '.', '0']\n",
      "['76', '.', '3']\n",
      "['76', '.', '3']\n",
      "['96', '.', '7']\n",
      "['96', '.', '7']\n",
      "['85', '.', '3']\n",
      "['85', '.', '3']\n",
      "['98', '.', '7']\n",
      "['98', '.', '7']\n",
      "['99', '.', '9']\n",
      "['99', '.', '9']\n",
      "['99', '.', '3']\n",
      "['99', '.', '3']\n",
      "['Macro', 'Avg']\n",
      "['(', 'Full', 'S2', '-', 'VL', ')']\n",
      "['40', '.', '6']\n",
      "['40', '.', '6']\n",
      "['38', '.', '3']\n",
      "['38', '.', '3']\n",
      "['39', '.', '1']\n",
      "['39', '.', '1']\n",
      "['42', '.', '0']\n",
      "['42', '.', '0']\n",
      "['36', '.', '5']\n",
      "['36', '.', '5']\n",
      "['37', '.', '6']\n",
      "['37', '.', '6']\n",
      "['75', '.', '9']\n",
      "['75', '.', '9']\n",
      "['80', '.', '0']\n",
      "['80', '.', '0']\n",
      "['76', '.', '8']\n",
      "['76', '.', '8']\n",
      "['92', '.', '0']\n",
      "['92', '.', '0']\n",
      "['94', '.', '1']\n",
      "['94', '.', '1']\n",
      "['92', '.', '7']\n",
      "['92', '.', '7']\n",
      "['Macro', 'Avg']\n",
      "['(', 'Grobid', 'Subset', ')']\n",
      "['81', '.', '2']\n",
      "['81', '.', '2']\n",
      "['76', '.', '7']\n",
      "['76', '.', '7']\n",
      "['78', '.', '9']\n",
      "['78', '.', '9']\n",
      "['84', '.', '1']\n",
      "['84', '.', '1']\n",
      "['73', '.', '0']\n",
      "['73', '.', '0']\n",
      "['78', '.', '2']\n",
      "['78', '.', '2']\n",
      "['82', '.', '6']\n",
      "['82', '.', '6']\n",
      "['83', '.', '9']\n",
      "['83', '.', '9']\n",
      "['83', '.', '2']\n",
      "['83', '.', '2']\n",
      "['92', '.', '2']\n",
      "['92', '.', '2']\n",
      "['95', '.', '2']\n",
      "['95', '.', '2']\n",
      "['93', '.', '7']\n",
      "['93', '.', '7']\n",
      "['Table', '3', ':', 'Evaluating', 'CoreRecipe', 'for', 'logical', 'structure', 'recovery', 'on', 'S2', '-', 'VL', '(', 'Shen', 'et', 'al', '.', ',', '2022', ')', '.', 'These', 'are', 'per', '-', 'category']\n",
      "['Table', '3', ':', 'Evaluating', 'CoreRecipe', 'for', 'logical', 'structure', 'recovery', 'on', 'S2', '-', 'VL', '(', 'Shen', 'et', 'al', '.', ',', '2022', ')', '.', 'These', 'are', 'per', '-', 'category']\n",
      "['metrics', 'for', 'Table', '2', '.', 'Metrics', 'are', 'computed', 'for', 'token', '-', 'level', 'classification', ',', 'macro', '-', 'averaged', 'over', 'categories', '.', 'The']\n",
      "['metrics', 'for', 'Table', '2', '.', 'Metrics', 'are', 'computed', 'for', 'token', '-', 'level', 'classification', ',', 'macro', '-', 'averaged', 'over', 'categories', '.', 'The']\n",
      "['metrics', 'for', 'Table', '2', '.', 'Metrics', 'are', 'computed', 'for', 'token', '-', 'level', 'classification', ',', 'macro', '-', 'averaged', 'over', 'categories', '.', 'The']\n",
      "['“Grobid', 'Subset”', 'limits', 'evaluation', 'to', 'only', 'categories', 'for', 'which', 'Grobid', 'returns', 'bounding', 'box', 'information', ',', 'which', 'was']\n",
      "['necessary', 'for', 'evaluation', 'on', 'S2', '-', 'VL', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sentences:\n",
    "    for row in sent.rows:\n",
    "        print([token.text for token in row.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('filename.json', 'w') as f_out:\n",
    "    json.dump(doc.to_json(with_images=True), f_out, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papermage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
