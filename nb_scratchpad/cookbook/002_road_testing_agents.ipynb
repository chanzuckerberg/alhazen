{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook for testing agent + tool functionality  \n",
    "\n",
    "> Walkthroughs for testing and investigating agent functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from alhazen.core import OllamaRunner\n",
    "from alhazen.schema_sqla import *\n",
    "from alhazen.tools.basic import *\n",
    "from alhazen.tools.paperqa_emulation_tool import PaperQAEmulationTool\n",
    "from alhazen.tools.metadata_extraction_tool import MetadataExtractionTool, MetadataExtractionWithRAGTool \n",
    "from alhazen.toolkit import AlhazenToolkit\n",
    "from alhazen.utils.jats_text_extractor import NxmlDoc\n",
    "from alhazen.utils.ceifns_db import Ceifns_LiteratureDb, create_ceifns_database, drop_ceifns_database\n",
    "from alhazen.apps.chat import  AlhazenAgentChatBot\n",
    "\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from importlib_resources import files\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import create_engine, exists, func\n",
    "from sqlalchemy.orm import sessionmaker, aliased\n",
    "\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError, HTTPError\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skc = aliased(ScientificKnowledgeCollection)\n",
    "skc_p = aliased(ScientificKnowledgeCollectionProvenance)\n",
    "skc_hm = aliased(ScientificKnowledgeCollectionHasMembers)\n",
    "ske_mo = aliased(ScientificKnowledgeExpressionMemberOf)\n",
    "ske = aliased(ScientificKnowledgeExpression)\n",
    "ske_hr = aliased(ScientificKnowledgeExpressionHasRepresentation)\n",
    "ski = aliased(ScientificKnowledgeItem)\n",
    "ski_hp = aliased(ScientificKnowledgeItemHasPart)\n",
    "skf = aliased(ScientificKnowledgeFragment)\n",
    "\n",
    "n = aliased(Note)\n",
    "skc_hn = aliased(ScientificKnowledgeCollectionHasNotes)\n",
    "ske_hn = aliased(ScientificKnowledgeExpressionHasNotes)\n",
    "ski_hn = aliased(ScientificKnowledgeItemHasNotes)\n",
    "skf_hn = aliased(ScientificKnowledgeFragmentHasNotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to set environmental variables for this code:\n",
    "\n",
    "* `ALHAZEN_DB_NAME` - the name of the PostGresQL database you are storing information into\n",
    "* `LOCAL_FILE_PATH` - the location on disk where you save temporary files, downloaded models or other data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ALHAZEN_DB_NAME'] = 'em_tech'\n",
    "os.environ['LOCAL_FILE_PATH'] = '/users/gully.burns/alhazen/'\n",
    "\n",
    "if os.path.exists(os.environ['LOCAL_FILE_PATH']) is False:\n",
    "    os.makedirs(os.environ['LOCAL_FILE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('ALHAZEN_DB_NAME') is None: \n",
    "    raise Exception('Which database do you want to use for this application?')\n",
    "db_name = os.environ['ALHAZEN_DB_NAME']\n",
    "\n",
    "if os.environ.get('LOCAL_FILE_PATH') is None: \n",
    "    raise Exception('Where are you storing your local literature database?')\n",
    "loc = os.environ['LOCAL_FILE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE TOOLS\n",
      "\tAddCollectionFromEPMCTool\n",
      "\tDescribeCollectionCompositionTool\n",
      "\tDeleteCollectionTool\n",
      "\tRetrieveFullTextTool\n",
      "\tMetadataExtractionTool\n",
      "\tSimpleExtractionWithRAGTool\n",
      "\tPaperQAEmulationTool\n",
      "\tCheckExpressionTool\n",
      "\tIntrospectionTool\n"
     ]
    }
   ],
   "source": [
    "ldb = Ceifns_LiteratureDb(loc=loc, name=db_name)\n",
    "llm = ChatOllama(model='mixtral:instruct')\n",
    "\n",
    "tk = AlhazenToolkit(db=ldb, llm=llm)\n",
    "print('AVAILABLE TOOLS')\n",
    "for t in tk.get_tools():\n",
    "    print('\\t'+type(t).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of all papers in database: 2648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus ID</th>\n",
       "      <th>Corpus Name</th>\n",
       "      <th>Paper Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CryoET Portal (10000-10010)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hierarchical phase-contrast tomography</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Cryo-Electron Tomography</td>\n",
       "      <td>2516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Corpus ID                             Corpus Name  Paper Count\n",
       "0         0             CryoET Portal (10000-10010)           10\n",
       "1         1  Hierarchical phase-contrast tomography          131\n",
       "2         2                Cryo-Electron Tomography         2516"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = ldb.session.query(skc.id, skc.name, func.count(skc_hm.has_members_id)) \\\n",
    "    .filter(skc.id==skc_hm.ScientificKnowledgeCollection_id) \\\n",
    "    .group_by(skc.id, skc.name) \\\n",
    "    .order_by(skc.id)\n",
    "corpora_df = pd.DataFrame(q.all(), columns=['Corpus ID', 'Corpus Name', 'Paper Count'])\n",
    "\n",
    "paper_count = ldb.session.query(func.count(ske.id)).first()\n",
    "print('Count of all papers in database: %d'%(paper_count[0]))\n",
    "\n",
    "corpora_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb.session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29411/29411 [06:11<00:00, 79.22it/s]\n"
     ]
    }
   ],
   "source": [
    "q = ldb.session.query(ske) \\\n",
    "    .outerjoin(ske_mo, ske.id==ske_mo.ScientificKnowledgeExpression_id ) \\\n",
    "    .filter(ske_mo.member_of_id==None)\n",
    "print(len(q.all()))\n",
    "for e in tqdm(q.all()):\n",
    "    ldb.delete_expression(e.id, commit_this=False)\n",
    "ldb.session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an agent in Jupyter \n",
    "cb = AlhazenAgentChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"remove the collection with the id \\\"cryoet_neurons\\\"\"\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"remove the collection with the id \\\"cryoet_neurons\\\"\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"remove the collection with the id \\\"cryoet_neurons\\\"\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Your name is \\\"Alhazen\\\".  \\nRespond to the human as helpfully and accurately as possible. Either follow the user's instructions or answer the users' questions using the tools provided.  But if the user is providing instructions that are not related to scientific research, respond 'Can I help you with your research?' and stop. \\nIf the user is asking 'How might you' perform a task, ALWAYS use the `introspect` tool.\\nYou have access to the following tools:\\nadd_collection_from_epmc_query: This tool executes a search for scientific papers in the EPMC database based on a query and then builds a collection out of the papers returned. Input to this tool has three parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\n- 'query' which is string that defines a query using Boolean logic for search terms.\\n- 'name' which is a string that defines a descriptive name for the collection.\\nThe tool will execute an query over the remote database, create the collection and add papers to the collection to our local database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'A code that serves as the id of the collection we will add papers to.', 'type': 'string'}, 'name': {'title': 'Name', 'description': 'A human-readable name of the collection we will add papers to.', 'type': 'string'}, 'query': {'title': 'Query', 'description': 'A search query (using AND/OR/NOT statements) to be executed on the European PMC online literature repository for scientific publications.', 'type': 'string'}}\\ndescribe_collection_in_local_database: This tool describes the contents of a collection in the database. Input to this tool has one parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\nIf successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'should be the id of the collection we wish to describe', 'type': 'string'}}\\ndelete_collection: This tool deletes a collection from the database., args: {'collection_id': {'title': 'Collection Id', 'description': 'should be the collection_id of the collection we will add papers to', 'type': 'string'}}\\nretrieve_full_text_for_paper_id: This tool invokes a web search for single full text paper from the web given a doi identifier. Input to this tool has one parameter: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'\\nThe tool will search online for the paper, return it and add it's text to the database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'paper_id': {'title': 'Paper Id', 'description': 'the digitial objecty identifier (doi) of the paper being retrieved from external sources', 'type': 'string'}}\\nmetadata_extraction: This tool extracts experimental metadata for an experiment from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using thisInput to this tool has two parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'extraction_type' which denotes the type of experiment being analysed. This should be set to the string 'cryoet'.\\nThe tool will execute an LLM over the paper to extract metadata from available text  and then insert the metadata into the database. The output is a string that returns a completion message (either positive or an error report)., args: {'paper_id': {'title': 'Paper Id'}, 'extraction_type': {'title': 'Extraction Type'}, 'item_type': {'title': 'Item Type', 'default': 'JATSFullText'}}\\nsimple_extraction: This tool extracts information from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using this.Input to this tool has three parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'variable_name' which denotes the name of the variable that the question is attempting to describe (e.g. 'species_name', 'sample_preparation_method').\\n- 'question' which is the question that must be answered to provide a value for the specified variable (e.g., 'Rat', 'Plunge Vitrificiation').\\nThe tool will execute an LLM over the paper to extract answers to the specified question. Output from the tool will be a JSON-formatted string with two fields: 'report' and 'data'.Text in the report field specifies how the tool performed this task., args: {'paper_id': {'title': 'Paper Id'}, 'variable_name': {'title': 'Variable Name'}, 'question': {'title': 'Question'}}\\nsimple_qa_over_papers: This tool answers scientific questions by searching for papers from Alhazen's database, reading a small subset of them and then synthesizing a response in the form of an essay. Only use this tool to respond to scientific questions that you already have built a database for.Input to this tool is a question, the ID value for the collection that the question will be asked over (collection_id), the number of documents to be sampled (n_sample_size), the number of documents to be synthesized in the final analysis.The tool will execute an Map-Reduce RAG pipeline to query the vector store, and then summarize the information returned to write a response to the question., args: {'question': {'title': 'Question'}, 'n_sample_size': {'title': 'N Sample Size', 'default': 15}, 'n_summary_size': {'title': 'N Summary Size', 'default': 5}, 'collection_id': {'title': 'Collection Id', 'default': -1}}\\ndescribe_expression_in_local_database: This tool checks if the database contains a paper. Input to this tool has one parameter: \\n- 'query' which is a search string for the paper (based on either the id or title).\\nThe tool will query the database and report its findings., args: {'query': {'title': 'Query', 'description': 'a search query for the paper in the database', 'type': 'string'}}\\nintrospect: This tool answers questions about the tools I have access to and what they do.Always call this tool when a user asks 'How might you ...?'Call the tool with the description of the task being asked about., args: {'input': {'title': 'Input', 'description': 'A hypothetical input to an agent.', 'type': 'string'}}\\nUse a json blob to specify a tool by providing an action key (tool name)  and an action_input key (tool input).\\nValid \\\"action\\\" values: \\\"Final Answer\\\" or add_collection_from_epmc_query, describe_collection_in_local_database, delete_collection, retrieve_full_text_for_paper_id, metadata_extraction, simple_extraction, simple_qa_over_papers, describe_expression_in_local_database, introspect\\nProvide only ONE action per $JSON_BLOB, as shown:\\n``` {\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n} ```\\nFollow this format:\\nQuestion: input question to answer Thought: consider previous and subsequent steps Action: ``` $JSON_BLOB ``` Observation: action result ... (repeat Thought/Action/Observation N times) Thought: I know what to respond Action: ``` {\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Final response to human\\\"\\n}\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. \\nUse tools if necessary.  Respond directly if appropriate. \\nFormat is Action:```$JSON_BLOB```then Observation\\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"remove the collection with the id \\\"cryoet_neurons\\\"\\n\\n(reminder to respond in a JSON blob no matter what)\\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Your name is \\\"Alhazen\\\".  \\nRespond to the human as helpfully and accurately as possible. Either follow the user's instructions or answer the users' questions using the tools provided.  But if the user is providing instructions that are not related to scientific research, respond 'Can I help you with your research?' and stop. \\nIf the user is asking 'How might you' perform a task, ALWAYS use the `introspect` tool.\\nYou have access to the following tools:\\nadd_collection_from_epmc_query: This tool executes a search for scientific papers in the EPMC database based on a query and then builds a collection out of the papers returned. Input to this tool has three parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\n- 'query' which is string that defines a query using Boolean logic for search terms.\\n- 'name' which is a string that defines a descriptive name for the collection.\\nThe tool will execute an query over the remote database, create the collection and add papers to the collection to our local database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'A code that serves as the id of the collection we will add papers to.', 'type': 'string'}, 'name': {'title': 'Name', 'description': 'A human-readable name of the collection we will add papers to.', 'type': 'string'}, 'query': {'title': 'Query', 'description': 'A search query (using AND/OR/NOT statements) to be executed on the European PMC online literature repository for scientific publications.', 'type': 'string'}}\\ndescribe_collection_in_local_database: This tool describes the contents of a collection in the database. Input to this tool has one parameters: \\n- 'id' which is string that denotes the identifier of the collection in the database.\\nIf successful, it will return 'Final Answer'. If not, it will return an error report., args: {'id': {'title': 'Id', 'description': 'should be the id of the collection we wish to describe', 'type': 'string'}}\\ndelete_collection: This tool deletes a collection from the database., args: {'collection_id': {'title': 'Collection Id', 'description': 'should be the collection_id of the collection we will add papers to', 'type': 'string'}}\\nretrieve_full_text_for_paper_id: This tool invokes a web search for single full text paper from the web given a doi identifier. Input to this tool has one parameter: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'\\nThe tool will search online for the paper, return it and add it's text to the database.If successful, it will return 'Final Answer'. If not, it will return an error report., args: {'paper_id': {'title': 'Paper Id', 'description': 'the digitial objecty identifier (doi) of the paper being retrieved from external sources', 'type': 'string'}}\\nmetadata_extraction: This tool extracts experimental metadata for an experiment from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using thisInput to this tool has two parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'extraction_type' which denotes the type of experiment being analysed. This should be set to the string 'cryoet'.\\nThe tool will execute an LLM over the paper to extract metadata from available text  and then insert the metadata into the database. The output is a string that returns a completion message (either positive or an error report)., args: {'paper_id': {'title': 'Paper Id'}, 'extraction_type': {'title': 'Extraction Type'}, 'item_type': {'title': 'Item Type', 'default': 'JATSFullText'}}\\nsimple_extraction: This tool extracts information from a single full text paper.In order to use this tool, you need to know if there is a full text version of the paper in the databaseIf there is not, you need to retrieve a copy of the full text before using this.Input to this tool has three parameters: \\n- 'paper_id' which is a string that denotes the doi identifier of the paper in question. This must start with the string 'doi:'.\\n- 'variable_name' which denotes the name of the variable that the question is attempting to describe (e.g. 'species_name', 'sample_preparation_method').\\n- 'question' which is the question that must be answered to provide a value for the specified variable (e.g., 'Rat', 'Plunge Vitrificiation').\\nThe tool will execute an LLM over the paper to extract answers to the specified question. Output from the tool will be a JSON-formatted string with two fields: 'report' and 'data'.Text in the report field specifies how the tool performed this task., args: {'paper_id': {'title': 'Paper Id'}, 'variable_name': {'title': 'Variable Name'}, 'question': {'title': 'Question'}}\\nsimple_qa_over_papers: This tool answers scientific questions by searching for papers from Alhazen's database, reading a small subset of them and then synthesizing a response in the form of an essay. Only use this tool to respond to scientific questions that you already have built a database for.Input to this tool is a question, the ID value for the collection that the question will be asked over (collection_id), the number of documents to be sampled (n_sample_size), the number of documents to be synthesized in the final analysis.The tool will execute an Map-Reduce RAG pipeline to query the vector store, and then summarize the information returned to write a response to the question., args: {'question': {'title': 'Question'}, 'n_sample_size': {'title': 'N Sample Size', 'default': 15}, 'n_summary_size': {'title': 'N Summary Size', 'default': 5}, 'collection_id': {'title': 'Collection Id', 'default': -1}}\\ndescribe_expression_in_local_database: This tool checks if the database contains a paper. Input to this tool has one parameter: \\n- 'query' which is a search string for the paper (based on either the id or title).\\nThe tool will query the database and report its findings., args: {'query': {'title': 'Query', 'description': 'a search query for the paper in the database', 'type': 'string'}}\\nintrospect: This tool answers questions about the tools I have access to and what they do.Always call this tool when a user asks 'How might you ...?'Call the tool with the description of the task being asked about., args: {'input': {'title': 'Input', 'description': 'A hypothetical input to an agent.', 'type': 'string'}}\\nUse a json blob to specify a tool by providing an action key (tool name)  and an action_input key (tool input).\\nValid \\\"action\\\" values: \\\"Final Answer\\\" or add_collection_from_epmc_query, describe_collection_in_local_database, delete_collection, retrieve_full_text_for_paper_id, metadata_extraction, simple_extraction, simple_qa_over_papers, describe_expression_in_local_database, introspect\\nProvide only ONE action per $JSON_BLOB, as shown:\\n``` {\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n} ```\\nFollow this format:\\nQuestion: input question to answer Thought: consider previous and subsequent steps Action: ``` $JSON_BLOB ``` Observation: action result ... (repeat Thought/Action/Observation N times) Thought: I know what to respond Action: ``` {\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Final response to human\\\"\\n}\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. \\nUse tools if necessary.  Respond directly if appropriate. \\nFormat is Action:```$JSON_BLOB```then Observation\\n\\nHuman: remove the collection with the id \\\"cryoet_neurons\\\"\\n\\n(reminder to respond in a JSON blob no matter what)\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOllama] [1.99s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" {\\n\\\"action\\\": \\\"delete_collection\\\",\\n\\\"action\\\\_input\\\": {\\n\\\"collection\\\\_id\\\": \\\"cryoet\\\\_neurons\\\"\\n}\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mixtral:instruct\",\n",
      "          \"created_at\": \"2024-02-06T22:42:46.143629Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1988729541,\n",
      "          \"load_duration\": 702708,\n",
      "          \"prompt_eval_duration\": 798100000,\n",
      "          \"eval_count\": 37,\n",
      "          \"eval_duration\": 1189375000\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \" {\\n\\\"action\\\": \\\"delete_collection\\\",\\n\\\"action\\\\_input\\\": {\\n\\\"collection\\\\_id\\\": \\\"cryoet\\\\_neurons\\\"\\n}\\n}\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:JSONAgentOutputParser_withFixes] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:JSONAgentOutputParser_withFixes] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"schema\",\n",
      "    \"agent\",\n",
      "    \"AgentAction\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"tool\": \"delete_collection\",\n",
      "    \"tool_input\": {\n",
      "      \"collection_id\": \"cryoet_neurons\"\n",
      "    },\n",
      "    \"log\": \" {\\n\\\"action\\\": \\\"delete_collection\\\",\\n\\\"action_input\\\": {\\n\\\"collection_id\\\": \\\"cryoet_neurons\\\"\\n}\\n}\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] [2.00s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m {\n",
      "\"action\": \"delete_collection\",\n",
      "\"action_input\": {\n",
      "\"collection_id\": \"cryoet_neurons\"\n",
      "}\n",
      "}\u001b[0m\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:delete_collection] Entering Tool run with input:\n",
      "\u001b[0m\"{'collection_id': 'cryoet_neurons'}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:delete_collection] [324ms] Exiting Tool run with output:\n",
      "\u001b[0m\"{'report': 'Successfully deleted a collection with collection_id:`cryoet_neurons`.'}\"\n",
      "\u001b[38;5;200m\u001b[1;3m{'report': 'Successfully deleted a collection with collection_id:`cryoet_neurons`.'}\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [2.33s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "out = cb.agent_executor.invoke({'input':'remove the collection with the id \"cryoet_neurons\"'},config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool='add_collection_from_epmc_query', tool_input={'query': 'evaluation of embeddings for single-cell transcriptomics datasets'}, log=' Thought: To answer this question, I would need to find scientific papers related to the evaluation of embeddings for single-cell transcriptomics datasets. I can use the `add_collection_from_epmc_query` tool to search for relevant papers and create a collection for further analysis.\\n\\nAction:\\n```json\\n{\\n  \"action\": \"add_collection_from_epmc_query\",\\n  \"action_input\": {\\n    \"query\": \"evaluation of embeddings for single-cell transcriptomics datasets\"\\n  }\\n}\\n```\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = cb.get_introspection_tool().agent\n",
    "t.invoke({'intermediate_steps':[], 'input':'How can I evaluate the quality of an embedding of a single cell transcriptomics dataset?'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
